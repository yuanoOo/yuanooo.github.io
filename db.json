{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"source/doc/Flink+Hudi构建实时仓湖一体化.pdf","path":"doc/Flink+Hudi构建实时仓湖一体化.pdf","modified":0,"renderable":0},{"_id":"source/img/72712952-12404300-3ba7-11ea-86f6-9a6610c616dd.png:Zone.Identifier","path":"img/72712952-12404300-3ba7-11ea-86f6-9a6610c616dd.png:Zone.Identifier","modified":0,"renderable":0},{"_id":"source/img/animal1.jpg","path":"img/animal1.jpg","modified":0,"renderable":0},{"_id":"source/img/cat.png","path":"img/cat.png","modified":0,"renderable":0},{"_id":"source/img/dag1.png","path":"img/dag1.png","modified":0,"renderable":0},{"_id":"source/script/1.jpg","path":"script/1.jpg","modified":0,"renderable":0},{"_id":"source/script/1.png","path":"script/1.png","modified":0,"renderable":0},{"_id":"source/script/img-to-webp.sh","path":"script/img-to-webp.sh","modified":0,"renderable":0},{"_id":"source/script/img-webp.sh","path":"script/img-webp.sh","modified":0,"renderable":0},{"_id":"source/sitemap_template/sitemap_template.txt","path":"sitemap_template/sitemap_template.txt","modified":0,"renderable":0},{"_id":"source/sitemap_template/sitemap_template.xml","path":"sitemap_template/sitemap_template.xml","modified":0,"renderable":0},{"_id":"source/img/code/code.jpg","path":"img/code/code.jpg","modified":0,"renderable":0},{"_id":"source/img/code/linux.png","path":"img/code/linux.png","modified":0,"renderable":0},{"_id":"source/img/code/talk_code_to_me.webp","path":"img/code/talk_code_to_me.webp","modified":0,"renderable":0},{"_id":"source/img/bg/banner.gif","path":"img/bg/banner.gif","modified":0,"renderable":0},{"_id":"source/img/bg/avatar.webp","path":"img/bg/avatar.webp","modified":0,"renderable":0},{"_id":"source/img/bg/banner.webp","path":"img/bg/banner.webp","modified":0,"renderable":0},{"_id":"source/img/bg/banner1.webp","path":"img/bg/banner1.webp","modified":0,"renderable":0},{"_id":"source/img/bg/banner2.webp","path":"img/bg/banner2.webp","modified":0,"renderable":0},{"_id":"source/img/bg/clash.jpg","path":"img/bg/clash.jpg","modified":0,"renderable":0},{"_id":"source/img/bg/default.png","path":"img/bg/default.png","modified":0,"renderable":0},{"_id":"source/img/bg/default_top_img.png","path":"img/bg/default_top_img.png","modified":0,"renderable":0},{"_id":"source/img/bg/index_img.png","path":"img/bg/index_img.png","modified":0,"renderable":0},{"_id":"source/img/bg/poxiao.jpg","path":"img/bg/poxiao.jpg","modified":0,"renderable":0},{"_id":"source/img/bg/web-bg.webp","path":"img/bg/web-bg.webp","modified":0,"renderable":0},{"_id":"source/img/img-link/img-link.txt","path":"img/img-link/img-link.txt","modified":0,"renderable":0},{"_id":"source/img/cut/cdn-test-2.webp","path":"img/cut/cdn-test-2.webp","modified":0,"renderable":0},{"_id":"source/img/cut/cdn-test-1.webp","path":"img/cut/cdn-test-1.webp","modified":0,"renderable":0}],"Cache":[{"_id":"source/script/1.jpg","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1717554924690},{"_id":"source/script/1.png","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1717554924690},{"_id":"source/CNAME","hash":"a44a2add0449f197dfcbdf8314c0005eb7e088d4","modified":1717554924653},{"_id":"source/robots.txt","hash":"c443b65f80db1005b56899172a5e9451af639175","modified":1717554924690},{"_id":"source/_data/link.yml","hash":"4817d09193d8f06ce05f891b1540184227e9f609","modified":1717554924653},{"_id":"source/_drafts/Flink+Hudi构建实时仓湖一体化.md","hash":"e2e90b013d68f479eb8ff2e73c55b5b30180a326","modified":1717554924653},{"_id":"source/_drafts/zhihu-emo-yulu.md","hash":"45b8d824aa5fb8bedcccb51a5763484ea1bcf37d","modified":1717554924655},{"_id":"source/_posts/DNS命令指南.md","hash":"930b7319c2ab6ed605eea20189180a2782788f1a","modified":1717554924655},{"_id":"source/_drafts/hadoop-native-lib和snappy.md","hash":"f20d43c6a316678cef404516e1e15234a384ac5f","modified":1717554924654},{"_id":"source/_posts/Hexo-github-pages-CloudFlare免费CDN最佳实践.md","hash":"c50eac43d4817d234c1fe80ba720e30f0c55bd41","modified":1717554924655},{"_id":"source/_posts/SEO.md","hash":"90771ed49b05262425378f50d6b4505014efd6a9","modified":1717554924655},{"_id":"source/_posts/WSL中的骚操作.md","hash":"e867be15107b91ff2fe04e2fc8c0a2a985951f42","modified":1717554924655},{"_id":"source/_posts/hexo+freenom+cloudflare遇到的一些坑.md","hash":"324008f145db09fabde3321b72fb7181fbf9ea80","modified":1717554924660},{"_id":"source/_posts/hexo的front-matter中的分类问题.md","hash":"1dd153dc85a20af933bc3bcf9cb5657dfabea713","modified":1717554924660},{"_id":"source/_posts/开源项目BugFix合集.md","hash":"3086ddd47a59b71709bf1b9576d099f2fadc9208","modified":1717554924662},{"_id":"source/about/index.md","hash":"a71523c7fdaf64ce310aa6b55d7984b682bb06c9","modified":1717554924662},{"_id":"source/categories/index.md","hash":"b8fa542806eefa2a0c53ae2f258071268a29a728","modified":1717554924663},{"_id":"source/css/background.css","hash":"2b0f6b7fdd3c8b341f9cff58fd66825b35f6defe","modified":1717554924663},{"_id":"source/css/custom.css","hash":"bf0872e7cd7c882a202342448e223b383ec2c2cf","modified":1717554924663},{"_id":"source/img/72712952-12404300-3ba7-11ea-86f6-9a6610c616dd.png:Zone.Identifier","hash":"398656b3a875e9970340d70e52c9511ab4da70ad","modified":1717554924682},{"_id":"source/link/index.md","hash":"389721a6b49d481514c8927da3a434c98d0cc659","modified":1717554924690},{"_id":"source/img/dag1.png","hash":"3f2f19bc2d16289a8bc1c4f98bde466cff4fb387","modified":1717554924690},{"_id":"source/img/animal1.jpg","hash":"e48cb3aeeb030ff094e35e61c52bbd73a662e0a5","modified":1717554924682},{"_id":"source/script/img-to-webp.sh","hash":"ade97b82a996d82be7b20693a4f95e9cba02592d","modified":1717554924690},{"_id":"source/script/img-webp.sh","hash":"b263a739e2c8f5acdb3c625d73e34d1e370079f2","modified":1717554924690},{"_id":"source/tags/index.md","hash":"ff8e7c4329113a8a5eeaec0aefeca67b139b0766","modified":1717554924690},{"_id":"source/sitemap_template/sitemap_template.txt","hash":"cbb0b7198d37b808e6e9bf4bd389743c07f8cf17","modified":1717554924690},{"_id":"source/sitemap_template/sitemap_template.xml","hash":"94026f74ab00f4398ee57381b6c3eaff9d475d35","modified":1717554924690},{"_id":"source/_posts/data-structure/DAG实现与任务调度.md","hash":"c92afd25706e4e2fa30817b60454369a77eb2736","modified":1717554924660},{"_id":"source/_posts/hexo/从github恢复备份hexo博客hexo-git-backup.md","hash":"cf7b11107bb154edf76c7483ef21cbf5f09bdbe0","modified":1717554924660},{"_id":"source/_posts/data-structure/从SS-Table到LSM-Tree.md","hash":"bb990b35a24f6dd2012b98fc0df1e7f6c3e67853","modified":1717554924660},{"_id":"source/_posts/hexo/hexo不显示语雀图床CDN图片的解决办法.md","hash":"d867649182157507cd494828dd06fccd949b0766","modified":1717554924660},{"_id":"source/_posts/bigdata/Doris-Join最佳实践.md","hash":"0f173775ffe5b92b52e10e55b719d5c241f189a9","modified":1717554924655},{"_id":"source/_posts/bigdata/Doris大查询实践与优化.md","hash":"9fd7bdd703521367b73572011a2ebe03dc8da24f","modified":1717554924656},{"_id":"source/_posts/bigdata/Doris中的索引.md","hash":"481ca504559ee41ea389a12bd7485a1351460d3a","modified":1717554924655},{"_id":"source/_posts/bigdata/HBase如何实现MVCC.md","hash":"a4354b5ffdba523355fd0eb66ed05101c25f954e","modified":1717554924656},{"_id":"source/_posts/bigdata/Doris Compaction从入门到跑路.md","hash":"46fa50629b7e06f89b6d67a20382a53acd12c1fc","modified":1717554924655},{"_id":"source/_posts/bigdata/When：何时需要进行Doris Compaction调优.md","hash":"fdfa4d2bbba9d5fe807b9bbb4f0717ccdad5d952","modified":1717554924656},{"_id":"source/_posts/bigdata/doris性能优化（一）.md","hash":"a5e33eb544d5bfb83d7722e17e4575084d5b368c","modified":1717554924656},{"_id":"source/_posts/bigdata/Kyuubi-从入门到跑路.md","hash":"4e9c4c8087b28b49204aac96d8c466a3adc9d3d9","modified":1717554924656},{"_id":"source/_posts/bigdata/yarn公平和容量调度器的异同.md","hash":"8e41dca311b7bebe4ad46ea4b0c93222f646bb40","modified":1717554924659},{"_id":"source/_posts/bigdata/初识Doris.md","hash":"9ab2b7c95344f5cd64e4863da50c7094a1051ed8","modified":1717554924660},{"_id":"source/_posts/bigdata/一文搞懂Kudu的整体架构.md","hash":"0b24a2a9879fb0a861acbd2b1f9f5bc802b2f97c","modified":1717554924659},{"_id":"source/_posts/bigdata/用户画像介绍.md","hash":"30a60a4b482fc7dcb490a9759a7c034e9b1d0772","modified":1717554924660},{"_id":"source/_posts/os/虚拟内存.md","hash":"f4c66fa47c9e97dd4234c00fa96e2360dba25e9d","modified":1717554924662},{"_id":"source/_posts/os/文件与IO.md","hash":"32d713481dfddbbe5c27b486ccd4788816ad24d4","modified":1717554924662},{"_id":"source/_posts/rpc/DolphinScheduler RPC框架源码分析.md","hash":"f6b689e8be65fdf1e8d99aac33290cda61e3587a","modified":1717554924662},{"_id":"source/_posts/rpc/基于Netty的简单RPC实现.md","hash":"8586609fcb5f3db00a66df6ef47e52bfcec4359c","modified":1717554924662},{"_id":"source/_posts/netty/Netty小记.md","hash":"98042e18c84372b0cbe2772d80505bf0ea6e065a","modified":1717554924661},{"_id":"source/_posts/rpc/thrift-从入门到放弃.md","hash":"8cdfa3872e6bd681f4a014da471b626eb1ad189e","modified":1717554924662},{"_id":"source/_posts/bigdata/配置hadoop-snappy那些事.md","hash":"2bff0726303fc0239f721a0c3759bc8aa5468841","modified":1717554924660},{"_id":"source/_posts/netty/Netty粘包、半包问题.md","hash":"c854eddf861d8dcf2441189089cf2cca5df1baaf","modified":1717554924661},{"_id":"source/img/code/code.jpg","hash":"89b5ed059b850c72988b186751153e63bc3e9e6b","modified":1717554924684},{"_id":"source/img/code/linux.png","hash":"cad82d89247115c29cb88e60f03d229bb1e4e8cd","modified":1717554924684},{"_id":"source/_posts/troubleshooting/CLOSE_WAIT问题排查.md","hash":"b501c0638c2598eb71e1ed8fe44ded87803cfe7d","modified":1717554924662},{"_id":"source/_posts/troubleshooting/dolphinscheduler中Hive SQL Task连接Kyuubi read timeout.md","hash":"930839e98a2c2ffe23b446ef64bda2362a0f36dd","modified":1717554924662},{"_id":"source/_posts/troubleshooting/记一次Kylin-Server的JVM-OOM事故排查复盘.md","hash":"356a0e12fced87ca699d47c68b7a03153b2ae6e5","modified":1717554924662},{"_id":"source/img/bg/avatar.webp","hash":"253098016d490151a15192a99d73606607e2c15d","modified":1717554924682},{"_id":"source/img/bg/banner1.webp","hash":"24cfb05fcddfc300d9554b1e9d83e93282052300","modified":1717554924682},{"_id":"source/img/bg/clash.jpg","hash":"a0388a2acdb319902a1b9a45fc48a836d8f5d07d","modified":1717554924683},{"_id":"source/img/bg/index_img.png","hash":"8517f53cbd9416e8b8f16160936499b0b9c8b6dc","modified":1717554924683},{"_id":"source/img/bg/poxiao.jpg","hash":"dcdcaa64fa5817e52cef86ed69824e044105e3a7","modified":1717554924683},{"_id":"source/img/img-link/img-link.txt","hash":"e5812babc02f71df9122496069016ed7b16e8168","modified":1717554924690},{"_id":"source/_posts/java/LockSupport线程同步.md","hash":"78e632393dbf93c38a851c87fab8ec7036211d6e","modified":1717554924660},{"_id":"source/_posts/bigdata/TroubleShooting/Flink-hudi日志超频繁打印.md","hash":"b68879337075192b4676086337c933832d24228a","modified":1717554924656},{"_id":"source/_posts/bigdata/TroubleShooting/修复Kylin4.0.x不正常的push-down query.md","hash":"160bb0a524ef45700ceb4ee093e4a1eacb4cd55e","modified":1717554924656},{"_id":"source/_posts/bigdata/lsm/LSM基本概念.md","hash":"0da73f5c9c2705fa6398a7ddf79a9bd3d157aeb8","modified":1717554924657},{"_id":"source/_posts/bigdata/TroubleShooting/修复hudi metadata table与HDFS3.x不兼容的问题.md","hash":"245927db312bbbc665b410a76f2c7e1bd9eb3bef","modified":1717554924656},{"_id":"source/_posts/bigdata/paimon/Paimon-Flink-Sink源码分析.md","hash":"c8d68b42957719ea282c68ce6d9ea272495d2e65","modified":1717554924658},{"_id":"source/_posts/bigdata/paimon/Paimon动态Bucket设计与实现.md","hash":"a3117548a092f4e08406e7f0053a8770f1cedc48","modified":1717554924658},{"_id":"source/_posts/bigdata/paimon/Spark Batch Read on Paimon.md","hash":"0deb091bc40c50790808243cbbff7e321b9bc0a3","modified":1717554924658},{"_id":"source/_posts/bigdata/paimon/初识paimon && Spark Catalog.md","hash":"d3997d027c11e1e5b5045f3c405deb85bebfea7a","modified":1717554924658},{"_id":"source/_posts/bigdata/druid/初识Apache Druid.md","hash":"8efca3c693ff3eb31acb148086611452d0c81522","modified":1717554924656},{"_id":"source/_posts/bigdata/scala/scala语法demo.md","hash":"36ef05a769f54e3ba5b397015a9c0bc6573f4bac","modified":1717554924658},{"_id":"source/_posts/bigdata/spark/Spark SQL合并小文件.md","hash":"f42cdc41f26e65a694634476e838a137c0065a64","modified":1717554924658},{"_id":"source/_posts/bigdata/spark/insight into Spark ESS.md","hash":"f406df08a6932e1bad5915d576180bd18124b35f","modified":1717554924658},{"_id":"source/_posts/bigdata/spark/spark事件总线.md","hash":"dbe3a6221f8c15886ca05ba951a16bc9c0cd3ca9","modified":1717554924659},{"_id":"source/_posts/bigdata/spark/利用RuntimeReplaceable实现Spark Native function.md","hash":"d81aac961534348d850b327431e13a9cfb9963bf","modified":1717554924659},{"_id":"source/_posts/bigdata/flink/Flink-SQL-Client与Hive集成问题指南.md","hash":"6b64f38a52d0dda9154c68c75a11ad3d2aa7bca6","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/Flink调优.md","hash":"7df15d1460d236b2361a265e443934124243dd1c","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/初入Flink-Table@SQL.md","hash":"2aa053e20b5a6aa7777235b6f2e93437a6b86274","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/增强Flink ogg-json format.md","hash":"72091a074d86824f512ddba3220df85edcf83aee","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/搭建Flink集群.md","hash":"cac2b9518cc61f6846684537636beffc6345280c","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/测试Flink-Doris-Connector.md","hash":"1381c47e57b76cab008786faed1c1f0c29a3242d","modified":1717554924657},{"_id":"source/_posts/bigdata/flink/通过Flink-SQL，将Kafka中的Oracle-CDC-Log同步到Doris.md","hash":"4648422d2441376e7b0441cca33580077ee74318","modified":1717554924657},{"_id":"source/_posts/bigdata/superset/Apache Superset添加exclude函数.md","hash":"842a5aca91cacb881bbcf2691ca34f5072c3f655","modified":1717554924659},{"_id":"source/_posts/bigdata/superset/superset配置LDAP踩坑指南.md","hash":"9131e7e7dafd709ac07f6292dae88ec021eed7e6","modified":1717554924659},{"_id":"source/img/cat.png","hash":"764303a8a1ae8f7fa1fb6e9c21b852a5988882bb","modified":1717554924683},{"_id":"source/img/bg/banner2.webp","hash":"88377d64f128a09855066054fb74a290b0b26613","modified":1717554924683},{"_id":"source/img/bg/default_top_img.png","hash":"980912504113f64d6aaf5b99e9793b77d1511197","modified":1717554924683},{"_id":"source/img/bg/web-bg.webp","hash":"7902ecf0465380a7576910c92881f7d493cfe0dd","modified":1717554924683},{"_id":"source/img/cut/cdn-test-2.webp","hash":"a0121aa1ff44ecaba93ed3449b9a95ee0d9daf83","modified":1717554924689},{"_id":"source/img/cut/cdn-test-1.webp","hash":"e8a063916ee46e49941081c8c4d0e241eaa0bd9f","modified":1717554924689},{"_id":"source/img/code/talk_code_to_me.webp","hash":"a263e3b98ba7a56896369f97690882ce2f583691","modified":1717554924684},{"_id":"source/img/bg/banner.gif","hash":"bb3261150b534f76eda7ece32c05fab1e53e265f","modified":1717554924682},{"_id":"source/img/bg/banner.webp","hash":"365b38933dcfe641a370df16017aea6dddd3cee1","modified":1717554924682},{"_id":"node_modules/hexo-theme-butterfly/README.md","hash":"66b4889591d0f36696c4d363412c753b6fe25519","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/package.json","hash":"bf1e7d13b179a17f1c851033eddf5944ea5993d4","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/languages/en.yml","hash":"d1bb560698eb8b0079495b7b18b44facb610f9fd","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/languages/default.yml","hash":"1e37a3695d50e3e61d7c36e58a6dac872a4a56cd","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-CN.yml","hash":"28b6f0c39155651d747eb595e0a283bc97be2e09","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-TW.yml","hash":"947f794e862bb2813e36887f777bdb760f70a322","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/archive.pug","hash":"bd62286afb64a51c97e800c5945620d51605d5fa","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/.github/stale.yml","hash":"5e8ea535424e8112439135d21afc5262c0bc0b39","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"67e4f5a66d4b8cabadbaad0410628364ee75e0ae","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/additional-js.pug","hash":"594a977ebe8d97e60fa3d7cb40fc260ded4d8a58","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/README_CN.md","hash":"08afd014fd27019909f341a2ad6162665958c6d6","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head.pug","hash":"b9d85155923314aa7e39b37395d875d7cddfabfe","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/layout.pug","hash":"da27c20f0e672103b984e135eb2fe7770ca7fcce","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/rightside.pug","hash":"699d0d2cff233628752956c4434125c8203f7d63","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/sidebar.pug","hash":"8d39473ed112d113674a0f689f63fae06c72abd2","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/init.js","hash":"b4940a5c73d3a5cd8bb5883e3041ecdd905a74e0","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/config.js","hash":"a12b9f11d7d3f52de5b2090d2805d7303e0187a5","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/pagination.pug","hash":"0b80f04950bd0fe5e6c4e7b7559adf4d0ce28436","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/stylus.js","hash":"9819f0996234fbd80d6c50a9e526c56ebf22588d","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/_config.yml","hash":"d5929d2e7fa55a74a089dd329bdae52cb6c12acc","modified":1717554923880},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/welcome.js","hash":"3cfc46c749e2fd7ae9c2a17206238ed0e0e17e7d","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/page.js","hash":"c6611d97087c51845cb1ab4821696a62fa33daeb","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/inject_head_js.js","hash":"b4cd617c619d1a0df93603721a6fa1317526174b","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/related_post.js","hash":"d368a8830e506c8b5eb6512b709ec8db354d5ea1","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/button.js","hash":"91d954f6e9fe6e571eb8ec9f8996294b2dc3688e","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/post_lazyload.js","hash":"932df912976261929f809b7dbd4eb473e7787345","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/random_cover.js","hash":"21379ed2dccb69c43b893895c9d56238c11e5f43","modified":1717554923887},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7dfe7189ffeaebb6db13842237f8e124649bea3d","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":1717554923879},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/hide.js","hash":"396c3ab1bcf1c7693ad7e506eadd13016c6769b6","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/gallery.js","hash":"f79c99f6c5b626c272dc2bed2b0250d6b91bb28a","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/mermaid.js","hash":"531808a290b8bdd66bac2faab211ada8e9646a37","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/note.js","hash":"c16c6eb058af2b36bcd583b2591076c7ebdd51ad","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/timeline.js","hash":"300eb779588bf35a1b687d9f829d866074b707e3","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","hash":"4890a40366d6443f8b8942a4e9a6dce9fe3494f5","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","hash":"04efcbd28b37875cfec88eb87cab7256a9ebb327","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config.pug","hash":"8f41fa9732ea654a10f6e666d9c782c7e27e5ea6","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/analytics.pug","hash":"15530d9ac59c576d79af75dd687efe71e8d261b0","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/Open_Graph.pug","hash":"6c41f49a3e682067533dd9384e6e4511fc3a1349","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/preconnect.pug","hash":"65a23b5170204e55b813ce13a79d799b66b7382c","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/index.pug","hash":"65fa23680af0daf64930a399c2f2ca37809a8149","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/nav.pug","hash":"78a3abd90bb3c18cd773d3d5abac3541e7f415e5","modified":1717554923881},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/post-info.pug","hash":"19a05dccfbffdf31cfa48c3208542b924637303d","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/loading-js.pug","hash":"4cfcf0100e37ce91864703cd44f1cb99cb5493ea","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/social.pug","hash":"0d953e51d04a9294a64153c89c20f491a9ec42d4","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/loading.pug","hash":"5276937fbcceb9d62879dc47be880cd469a27349","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/article-sort.pug","hash":"2fb74d0b0e4b98749427c5a1a1b0acb6c85fadc4","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/post-ui.pug","hash":"b9ebb02af8ccf43e3f73be43db19254fa913c57b","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/flink.pug","hash":"fed069baa9b383f57db32bb631115071d29bdc60","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/tags.pug","hash":"6311eda08e4515281c51bd49f43902a51832383c","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/post-copyright.pug","hash":"ebecba46a5f4efe1c98a386df06c56e26fbd07b9","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/reward.pug","hash":"864869c43fe5b5bb6f4ac6b13dd4bfb16ea47550","modified":1717554923882},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/effect.pug","hash":"6528e86656906117a1af6b90e0349c2c4651d5e1","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pjax.pug","hash":"460c37caeed6e6e72c1e62292e6c5e9699dd5937","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/subtitle.pug","hash":"bae2f32ac96cebef600c1e37eaa8467c9a7e5d92","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_author.pug","hash":"e37468e63db2a0ac09b65d21b7de3e62425bb455","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_newest_comment.pug","hash":"6d93564a8bd13cb9b52ee5e178db3bcbf18b1bc6","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_post_toc.pug","hash":"3057a2f6f051355e35d3b205121af8735100eacf","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_tags.pug","hash":"438aea3e713ed16b7559b9a80a9c5ec0221263df","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_webinfo.pug","hash":"0612aaee878f33ea8d3da0293c7dc3b6cd871466","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_recent_post.pug","hash":"9c1229af6ab48961021886882c473514101fba21","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/index.pug","hash":"7fb096656c8a6c21a4b6a5100885b1081d6021ed","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/aside.styl","hash":"ca58af8903eb1d1d05edae54fc2e23aeac6da6c5","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/chat.styl","hash":"29f48f9370f245e6e575b5836bccf47eb5688d8b","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/comments.styl","hash":"c61dccca690d486c3d9c29cf028d87b777385141","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/footer.styl","hash":"26be2afa9d4e7016cf3c42a6cd166f01e8e4ad5c","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/head.styl","hash":"d97c1722ce0fcc319f1f90ec2d51f9d746748e2b","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/loading.styl","hash":"ef21990de28bd75dcd0f88b8d616e1a7a137502f","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/post.styl","hash":"15056fba0bd5a45ea8dc97eb557f6929ff16797a","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/rightside.styl","hash":"bd88ee30ebf8ca2e7b4d3a034c317fd61733921f","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/reward.styl","hash":"c5cfed620708807a48076b5ee59b0ba84e29aa80","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/sidebar.styl","hash":"631ca35a38bc4ac052e9caf47508ff1f99842fc7","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/function.styl","hash":"644d520fe80cc82058467708ab82ccad313b0c27","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/readmode.styl","hash":"69f8e9414526dfda3af9a71c8e528fdd0ecbbfe5","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/third-party.styl","hash":"8314e9749eb1ae40c4bae9735b7a6638b2d6876a","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/darkmode.styl","hash":"f67177310f5594954b25a591d186d28d5d450b18","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/index.styl","hash":"714f19e7d66df84938bd1b82b33d5667abe1f147","modified":1717554923888},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight.styl","hash":"2f95e99b8351fbecd9037a1bbdc3fee9d6ea8a77","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/archives.styl","hash":"6f4b4ede52305bce9b22c8c897dcbde8af6e2ce4","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/common.styl","hash":"a58d35d698885f1034dedbe99f7dbc1a801412c6","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/homepage.styl","hash":"826dae759062d8f84eb2bf5ab8fdb80e0f79d58b","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1717554923890},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/algolia.styl","hash":"51e45625929d57c9df3ba9090af99b9b7bb9a15b","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/index.styl","hash":"39d61cbe0c1e937f83ba3b147afaa29b4de2f87d","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/local-search.styl","hash":"25e58a7a8bda4b73d0a0e551643ca01b09ccd7e5","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/gallery.styl","hash":"a310e48f826a4cacc55d8e68f43806e5085554f6","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/note.styl","hash":"08493b66b9f31f2bd3e9a3115017a0ce16142b20","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/tabs.styl","hash":"bf9568444dd54e39dc59b461323dcd38942f27d9","modified":1717554923891},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","hash":"ce8131b712dca80f289015aef75f86e727f62981","modified":1717554923892},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"7848ec58c6ec03243abf80a3b22b4dc10f3edf53","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"e3bf847553515174f6085df982f0623e9783db7a","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"58406a7a3bf45815769f652bf3ef81e57dcd07eb","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"e4b7bf91a29bd03181593b63e1f3ee1103af2e48","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"5f648086a33a32d169a2f8d8c549c08aa02f67db","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/katex.pug","hash":"31b007dc0f3de52176f278012ecf17a4bcecde2c","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mathjax.pug","hash":"f4dc7d02c8192979404ae9e134c5048d3d0a76e2","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mermaid.pug","hash":"8e33aca36a4d3ae9e041ba05ced8eff56ae38f77","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/gitter.pug","hash":"d1d2474420bf4edc2e43ccdff6f92b8b082143df","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/index.pug","hash":"3f05f8311ae559d768ee3d0925e84ed767c314d3","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1717554923883},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqus.pug","hash":"a111407fdcafcf1099e26ffa69786f8822c5d9fb","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"693d999777dd16e0566d29ac3203d4c167b2f9a7","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"2d8fc3fb8f9aec61400acf3c94070bd8539058f8","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/giscus.pug","hash":"591ef23c583690bd74af0cafb09af64ba5bd8151","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"22e2ef30fe5eb1db7566e89943c74ece029b2a8e","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/js.pug","hash":"9302837f1e35f153323bb4f166514c7e96e8ecdd","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/index.pug","hash":"e4850f2c9ba5f6b2248808f7257662679e0fab0a","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"81c6070e06ecc2244040c7007566d7972f46ec4e","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/utterances.pug","hash":"a737046e730eb7264606ba0536218964044492f9","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/waline.pug","hash":"15462d1ed04651ad3b430c682842ac400f6f9b47","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/valine.pug","hash":"e55b9c0f8ced231f47eb88bd7f4ec99f29c5c29d","modified":1717554923884},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"04b2a5882e789a988e41d45abe606f0617b08e38","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"e846ddfe4a63b15d1416f6055f5756af5e3da7c6","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"233907dd7f5b5f33412701d2ccffbc0bbae8707b","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"f6506ccfd1ce994b9e53aa95588d0b6dbad11411","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"d19e1c2c0a50f0e4547d71a17b9be88e8152f17c","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/algolia.pug","hash":"e8245d0b4933129bb1c485d8de11a9e52e676348","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"dd0bc119029b62dce5dc965d5de7377e438fa29a","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/index.pug","hash":"da3b9437d061ee68dbc383057db5c73034c49605","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/local-search.pug","hash":"178c9cdcc4ce5a006885b24ce4a3d624e4734899","modified":1717554923885},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/share-js.pug","hash":"f61d63724ea5c5f352568b3a16bde023affefbe5","modified":1717554923886},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1717554923889},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1717554923889},{"_id":"source/img/bg/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1717554924683},{"_id":"source/doc/Flink+Hudi构建实时仓湖一体化.pdf","hash":"b3c11357ed7ef084450565db003b61676909c823","modified":1717554924678},{"_id":"public/css/background.css","hash":"2b0f6b7fdd3c8b341f9cff58fd66825b35f6defe","modified":1717554958719},{"_id":"public/css/custom.css","hash":"bf0872e7cd7c882a202342448e223b383ec2c2cf","modified":1717554958719},{"_id":"public/baidusitemap.xml","hash":"a7d71b62c11a1abd7d13c9d78d0c9ff7e65b8e49","modified":1717554958719},{"_id":"public/sitemap.xml","hash":"db7aa40742cd27c0d931553905380dad0b83365c","modified":1717554958719},{"_id":"public/sitemap.txt","hash":"9a2c9e50943956617cfe099efbbac8b9ef74179b","modified":1717554958719},{"_id":"public/404.html","hash":"309ecd68459b819c67825821a21b49a5b5337c06","modified":1717555064857},{"_id":"public/about/index.html","hash":"102f5222452dc1575fa529b50c5205922b68ec26","modified":1717555064857},{"_id":"public/categories/index.html","hash":"9815ac72c5161813dd2ac8119b75a7478d75a918","modified":1717555064857},{"_id":"public/tags/index.html","hash":"3de6497ec015f2495bf1330761f4371ca8c9e2b4","modified":1717555064857},{"_id":"public/link/index.html","hash":"509bc49fb37f6dcca86eaab2552277987b59fc97","modified":1717555064857},{"_id":"public/posts/7631.html","hash":"f5641d5d1d49b2ed9f073835b33af218a8db32a7","modified":1717554958719},{"_id":"public/posts/55672.html","hash":"c3a9fd9e5cb7e7d1e06d0dfd4bcd8518e387722d","modified":1717554958719},{"_id":"public/posts/5296.html","hash":"ce1f43f237c270eb08d464b64e6005f373584e34","modified":1717554958719},{"_id":"public/posts/24799.html","hash":"6b3d8eacba545e0fa48fc6484746951ab6cb1fd3","modified":1717554958719},{"_id":"public/posts/6282.html","hash":"b63d04865e3920801559a98ae086207ce8ceee19","modified":1717554958719},{"_id":"public/posts/26343.html","hash":"4855627a3b88dd21a6141b12c2aa06c710da6629","modified":1717554958719},{"_id":"public/posts/3025.html","hash":"e0cd6c8d4e6c227468c3f09d9daedf8ca12ce7bd","modified":1717554958719},{"_id":"public/posts/10009.html","hash":"7ef394abe8544ae2d1cb2f526594efe3adfd9032","modified":1717554958719},{"_id":"public/posts/17536.html","hash":"056ade869d44bcea72b593ea513860df697e86fc","modified":1717554958719},{"_id":"public/posts/27532.html","hash":"b1d7cbf48508a730b15cd5be60673fdc8682ade1","modified":1717554958719},{"_id":"public/posts/32468.html","hash":"ddda90b220e3471c9e0d419bcad567929a72ac52","modified":1717554958719},{"_id":"public/posts/61336.html","hash":"c64949b71b7ddca592361fcc59c935fdf673f084","modified":1717554958719},{"_id":"public/posts/9390.html","hash":"23337ef048702bc9c05cedc3fdcb58ce973b2244","modified":1717554958719},{"_id":"public/posts/39051.html","hash":"ce8c7ec6e881fcaf66c1f5497356dd57710b2a9e","modified":1717554958719},{"_id":"public/posts/32133.html","hash":"f7549ecae26b2d81348bd247b3e8aba233b0f5ce","modified":1717554958719},{"_id":"public/posts/5351.html","hash":"d3426ed65482a81557e7a98d66efb1164f38d473","modified":1717554958719},{"_id":"public/posts/54682.html","hash":"7c43d451fe03238ef10da005924c9fb896cab70e","modified":1717554958719},{"_id":"public/posts/3347.html","hash":"43f9aed669c2841d69f55a65180f612a53dde65d","modified":1717554958719},{"_id":"public/posts/50109.html","hash":"c0b4c15067966efcfd3ede667d5a4c6450da9659","modified":1717554958719},{"_id":"public/posts/55687.html","hash":"92d6e6d1252dc1780ec7b320c8f848910d41bb02","modified":1717554958719},{"_id":"public/posts/4987.html","hash":"208cf6c41e659cca1d81ad60d21427a34d7e38a4","modified":1717554958719},{"_id":"public/posts/14619.html","hash":"a573aae3911b2d28bb81c1b03e6ddaa482cdaa9f","modified":1717554958719},{"_id":"public/posts/22006.html","hash":"170ee50fdd24df77bdef8d800a55bdffc5a7c067","modified":1717554958719},{"_id":"public/posts/38776.html","hash":"41b5350f6903b66d6aa840c9cb7410c4b57feee6","modified":1717554958719},{"_id":"public/posts/41988.html","hash":"e3843a9451cde4a68423ccc93aad86fec08edccd","modified":1717554958719},{"_id":"public/posts/27401.html","hash":"7be3bdcdb98815e38b32fc9387f510b35934bf94","modified":1717554958719},{"_id":"public/posts/42607.html","hash":"d0aaa562969f2f0014dd702e8a01ac65a2463b33","modified":1717554958719},{"_id":"public/posts/19576.html","hash":"dc45e51154add8bad47eeb0edf1493b3e03505b0","modified":1717554958719},{"_id":"public/posts/59616.html","hash":"a843fe0c3c82760d7f4e7fba56b23b0258444515","modified":1717554958719},{"_id":"public/posts/60503.html","hash":"25044776f23a1a9ffeeefc7978c98dae4570c727","modified":1717554958719},{"_id":"public/posts/45838.html","hash":"a54bf944c12f357f4d1c2f92f0b667bb1074d219","modified":1717554958719},{"_id":"public/posts/41835.html","hash":"309ae9937ecedf6a677d0d965ed025f4f6685784","modified":1717554958719},{"_id":"public/posts/5143.html","hash":"2ac13dd0c43dc476631fca301260542cebdc3d2f","modified":1717554958719},{"_id":"public/posts/15533.html","hash":"7a9000ab2d55babda1b0fe3014be7df957a0e36a","modified":1717554958719},{"_id":"public/posts/62654.html","hash":"0ec9e1a84ca142cbf79f4439b3dd7e816bfedadc","modified":1717554958719},{"_id":"public/posts/65527.html","hash":"0a8f8f40e04001dd9da81676620f77ae2c8b03a4","modified":1717554958719},{"_id":"public/posts/46224.html","hash":"78f6187d2ba48bdef8ac017321cb3713ed522c06","modified":1717554958719},{"_id":"public/posts/11082.html","hash":"4eb24b6c5e302d4840f93917add4d62917f4e16a","modified":1717554958719},{"_id":"public/posts/29811.html","hash":"878e63abfa26e4c7c77fd02153b68aee7b4f0a34","modified":1717554958719},{"_id":"public/posts/38552.html","hash":"4cf7a0820b74d72e2260b45b6debd258007ccf7a","modified":1717554958719},{"_id":"public/posts/46784.html","hash":"8c449c08b05c70c97f98692bdc1a8000013ddbd4","modified":1717554958719},{"_id":"public/posts/54105.html","hash":"5cf21ddfb27f9166c969150fd126d62555b03422","modified":1717554958719},{"_id":"public/posts/44798.html","hash":"02cd53e28ba5ac69a1be02d07612b84803d23a13","modified":1717554958719},{"_id":"public/posts/24352.html","hash":"dd31520c80fb032acfa6cb7a884e9deef4cbc5f0","modified":1717554958719},{"_id":"public/posts/13792.html","hash":"7eca9b423c91f2ffac8d3d46f73b1097ae4b3112","modified":1717554958719},{"_id":"public/posts/27737.html","hash":"85a6c5be0d06afedf5ce207eea4f25b475a720f0","modified":1717554958719},{"_id":"public/posts/2371.html","hash":"569f753e040b44923046c8d973680980933ec245","modified":1717554958719},{"_id":"public/posts/8819.html","hash":"38dba85bec9ba5e50e43c05bb4d06028ddda1976","modified":1717554958719},{"_id":"public/posts/6299.html","hash":"08d5fccd4fdf497f35b216ef2f5fcc2786b53194","modified":1717554958719},{"_id":"public/posts/28143.html","hash":"b936befd2f5af2100646b253d1d58f3971c773c0","modified":1717554958719},{"_id":"public/posts/21645.html","hash":"532d06ee4caf539dac141d72516170651487bef7","modified":1717554958719},{"_id":"public/posts/2646.html","hash":"dcfbd6569bf915a3f9386a3d573e1aec2bd539d7","modified":1717554958719},{"_id":"public/posts/41454.html","hash":"07efd0b7ae34518f5ee7bd9296941be65ab63be1","modified":1717554958719},{"_id":"public/posts/54794.html","hash":"75325c278a8e365f6e153229e1b42600ec6369e5","modified":1717554958719},{"_id":"public/posts/6111.html","hash":"924e9b9ed08eca5b572cbea0349292084b0e5744","modified":1717554958719},{"_id":"public/posts/51293.html","hash":"0ada8ade7d8ba68527072835c4d412c6dda36430","modified":1717554958719},{"_id":"public/posts/24289.html","hash":"4dba73e738b2a74b1f9ce948671c7ea2faa719bb","modified":1717554958719},{"_id":"public/posts/6631.html","hash":"8a663dfa8a99924cffa28d9d5736f7734a39b826","modified":1717554958719},{"_id":"public/archives/index.html","hash":"1bb40168b242ec7c75ccdc8777bba9ea422dd858","modified":1717555064857},{"_id":"public/archives/page/2/index.html","hash":"8922b40ff54ff338635d41bfb47d14a64072b209","modified":1717555064857},{"_id":"public/archives/page/3/index.html","hash":"06204352d4d6144334a4080081170d5952eaf76e","modified":1717555064857},{"_id":"public/archives/page/4/index.html","hash":"c88a93ac786803b17ef5d7538bd1dd7400fc6e8c","modified":1717555064857},{"_id":"public/archives/page/5/index.html","hash":"0dca90d806d0e98eb988ef8646f770d97215828d","modified":1717555064857},{"_id":"public/archives/2022/index.html","hash":"4ee64667ad2c3d9e515e738c11f4282194e5e488","modified":1717555064857},{"_id":"public/archives/page/6/index.html","hash":"3089cd34df663af2b6d73e208595e6612348b180","modified":1717555064857},{"_id":"public/archives/2022/page/2/index.html","hash":"fc268e60389b6cc72e6ae63d0cecf3a039698d61","modified":1717555064857},{"_id":"public/archives/2022/page/3/index.html","hash":"4e56ed4aca087705501a99f3295b2dd1e6227b63","modified":1717555064857},{"_id":"public/archives/2022/page/4/index.html","hash":"3292e70968b391daf8b5e113191072910803830d","modified":1717555064857},{"_id":"public/archives/2022/06/index.html","hash":"bfb39b304215c22f20741e362aa9c085251a391d","modified":1717555064857},{"_id":"public/archives/2022/07/index.html","hash":"b2bcec5257748068fa748e7022d48d898125287d","modified":1717555064857},{"_id":"public/archives/2022/08/index.html","hash":"f7446bde7ace4d7b287b82625e508d85b11084fa","modified":1717555064857},{"_id":"public/archives/2022/09/index.html","hash":"6778b4d8c775722fb301fd463b7f9cdfe02a80de","modified":1717555064857},{"_id":"public/archives/2022/10/index.html","hash":"5aa4ac02cd8d15368bee6253ca6dd2e9622f3e9c","modified":1717555064857},{"_id":"public/archives/2022/11/index.html","hash":"c515867d9ad39186b672c462b18caa84ef40f12c","modified":1717555064857},{"_id":"public/archives/2022/12/index.html","hash":"273ff304b7ed6569c124939c02cca33167bcd135","modified":1717555064857},{"_id":"public/archives/2023/index.html","hash":"f915553e44988ba1d3319a9e520f54ee0a9d4cc1","modified":1717555064857},{"_id":"public/archives/2023/page/2/index.html","hash":"b9734ae1a9eccfa38372fede6a3b6f04c8365b21","modified":1717555064857},{"_id":"public/archives/2023/01/index.html","hash":"b56cea7e53db4747a592072425cce1b6c26063bb","modified":1717555064857},{"_id":"public/archives/2023/02/index.html","hash":"4d36be21285c0070fecbbb8e0b00521d9d998d8d","modified":1717555064857},{"_id":"public/archives/2023/04/index.html","hash":"d9da158bc54db075a4eed9f40682dbee0dd98eeb","modified":1717555064857},{"_id":"public/archives/2023/03/index.html","hash":"18f4e809e84073a6bc38b121f6c4609e59beac0b","modified":1717555064857},{"_id":"public/archives/2023/05/index.html","hash":"0e920053d24f4696daecad8e6f5ada51dbc0bc5f","modified":1717555064857},{"_id":"public/archives/2023/06/index.html","hash":"239e73797301ab35cf6082392d7efd1c4698b8ad","modified":1717555064857},{"_id":"public/archives/2023/07/index.html","hash":"2c649e3bfc0ba1cc6a7131ced66ed3dfad8a82e3","modified":1717555064857},{"_id":"public/archives/2023/08/index.html","hash":"d3c8986807210949f4b383361f5549a02c9d176f","modified":1717555064857},{"_id":"public/archives/2023/09/index.html","hash":"4b1e7d63be7f7fe76ef918749c8fa8e8b089be8e","modified":1717555064857},{"_id":"public/categories/bigdata/index.html","hash":"707e931cb6b659d5f268016b33091cb4d44f10e6","modified":1717555064857},{"_id":"public/categories/bigdata/page/2/index.html","hash":"8388a6562d894102c180c8b3b75ff684f2313203","modified":1717555064857},{"_id":"public/categories/hexo/index.html","hash":"32e097da66f1b830f18cb6daa7b169259d8d3a54","modified":1717555064857},{"_id":"public/categories/bigdata/page/3/index.html","hash":"4d6d97e694cebe7ee6227bc97806eb7fa9137fc1","modified":1717555064857},{"_id":"public/categories/SEO/index.html","hash":"c3d6bbdb6e3175305b77a11e2775ab95fdf88789","modified":1717555064857},{"_id":"public/categories/Linux/index.html","hash":"1c736969aa67068ea04d349043f00c2cbed7cc4a","modified":1717555064857},{"_id":"public/categories/dns/index.html","hash":"b170f922cfc42d9a349f34c2fd26bef5afaef923","modified":1717555064857},{"_id":"public/categories/bigdata/BugFix/index.html","hash":"602c1743cfd833186c452742d38eead7c6ccda08","modified":1717555064857},{"_id":"public/categories/data-structure/index.html","hash":"5983ef9c59cee356506b7c1d20fe1f059bfdf2c2","modified":1717555064857},{"_id":"public/categories/bigdata/Doris/index.html","hash":"d4ef5df1f53df876c2b65cdfa2890d5fd61bf520","modified":1717555064857},{"_id":"public/categories/Doris/index.html","hash":"45c8f540e3c621f86d46346352dcb39a9ae35459","modified":1717555064857},{"_id":"public/categories/bigdata/HBase/index.html","hash":"102c856c9a5e9efdcfc3b17926619a536bf4cbe3","modified":1717555064857},{"_id":"public/categories/linux/index.html","hash":"221b26f140c0e361c0646bab251804badfce7354","modified":1717555064857},{"_id":"public/categories/bigdata/Kyuubi/index.html","hash":"bf5e2aca1ab1b332ce860e7eec69c5aa073f0a07","modified":1717555064857},{"_id":"public/categories/bigdata/yarn/index.html","hash":"5f49d54d0625178a99f8cd72ff3d847a4c1f97bb","modified":1717555064857},{"_id":"public/categories/bigdata/User-Profile/index.html","hash":"996080ac725cfc3707b3243ce97e00066a473aab","modified":1717555064857},{"_id":"public/categories/os/index.html","hash":"47164c9eb071d6cf6af3879ca5bc7a2d747c26b9","modified":1717555064857},{"_id":"public/categories/RPC/index.html","hash":"4d6e9c6d6a2386cef31d51fe1e6f9ef5b2a545ff","modified":1717555064857},{"_id":"public/categories/bigdata/kudu/index.html","hash":"d853b6d8377ad4a4ef45f28b716f1ce202439ba7","modified":1717555064857},{"_id":"public/categories/Netty/index.html","hash":"30885857975148efe95f53a3fce212c7c969e26c","modified":1717555064857},{"_id":"public/categories/Troubleshooting/index.html","hash":"ba55000a8fa1df1a07765819ee876d9366b6b560","modified":1717555064857},{"_id":"public/categories/bigdata/Kylin/index.html","hash":"1515bc0341a7ff31fef1a84dc3e6692cb509b894","modified":1717555064857},{"_id":"public/categories/Java/index.html","hash":"542a793256d25617b83ff1a4f1ededfd460fe996","modified":1717555064857},{"_id":"public/categories/LSM/index.html","hash":"9b32a0bcb97bd2293533b824bb7a4e13f495ce49","modified":1717555064857},{"_id":"public/categories/bigdata/paimon/index.html","hash":"d3a7a6d99c316d6032b9669fdf86c75a2a1eaf7b","modified":1717555064857},{"_id":"public/categories/Druid/index.html","hash":"e6b77c43d4530ef67a0353f6d1e5df2a1c2d0adb","modified":1717555064857},{"_id":"public/categories/bigdata/spark/index.html","hash":"e585d956a7b23c950a183a3e8280646ad77e7e53","modified":1717555064857},{"_id":"public/categories/bigdata/Flink/index.html","hash":"1bf291aab0eaf1408927dc00173125b5996b9f3e","modified":1717555064857},{"_id":"public/categories/bigdata/scala/index.html","hash":"00711e09facfb2ae183af5eaa7319fe84086e404","modified":1717555064857},{"_id":"public/categories/Troubleshooting/Kylin/index.html","hash":"eca8534d449e4bc0b68e317b346321c401b436d6","modified":1717555064857},{"_id":"public/categories/RPC/thrift/index.html","hash":"8f79570c69b19cae03e792064fae080996f3e3b6","modified":1717555064857},{"_id":"public/categories/Superset/index.html","hash":"e1eeceb73305926c91af11c308b60886edd284f9","modified":1717555064857},{"_id":"public/index.html","hash":"83e9574e888bfb30fde0776795a84351524a9614","modified":1717555064857},{"_id":"public/page/2/index.html","hash":"e1b6e5a395917ad36e91f052ea7755937744219a","modified":1717555064857},{"_id":"public/page/3/index.html","hash":"8cac3f0587ad7f1aa98b89ccd5d5220819c787f4","modified":1717555064857},{"_id":"public/page/4/index.html","hash":"6428b43714664cb4e3fd2e019103f913d5784505","modified":1717555064857},{"_id":"public/page/5/index.html","hash":"b41d2eebf9759efc013e9273fa383a55daf65b40","modified":1717555064857},{"_id":"public/tags/CDN/index.html","hash":"c0334712a6119465deaaef446cd272b9459fdf9e","modified":1717555064857},{"_id":"public/page/6/index.html","hash":"a8023a68b67b5faa64989c892d7c787ddce5fe96","modified":1717555064857},{"_id":"public/tags/cloudflare/index.html","hash":"c5f8c6cd7ff7bb6a18d529230490d1f0d59d71e1","modified":1717555064857},{"_id":"public/tags/hexo/index.html","hash":"c76d595e08b723c0d6cc9e55c336975f6aa56958","modified":1717555064857},{"_id":"public/tags/WSL/index.html","hash":"78c4304ed295e90c9efccb123c6bcc5125c6e562","modified":1717555064857},{"_id":"public/tags/SEO/index.html","hash":"daf0be5fb1e569d646789d722515253a0d44f0d4","modified":1717555064857},{"_id":"public/tags/Linxu/index.html","hash":"ae451e444ca60cad5d026a2bbe49a47a6b3556fc","modified":1717555064857},{"_id":"public/tags/github-pages/index.html","hash":"40258c54e2584fe5b5722bea8c307e59f62c5f2a","modified":1717555064857},{"_id":"public/tags/CFW/index.html","hash":"48190c631be757fe0481ba48294f14b0a14f4c93","modified":1717555064857},{"_id":"public/tags/dns/index.html","hash":"2a402bc801eaf63380580abc5645b33324c4017d","modified":1717555064857},{"_id":"public/tags/BugFix/index.html","hash":"2c1a98c6d5b7b8f90e4117aa0d3b6ebafa123790","modified":1717555064857},{"_id":"public/tags/PR/index.html","hash":"88d691f61ca31f27fa7240f1969003f25d775b72","modified":1717555064857},{"_id":"public/tags/DAG/index.html","hash":"ee3c8a6d56de2979b37dd869490d5132f10b01d0","modified":1717555064857},{"_id":"public/tags/Doris/index.html","hash":"65945dd336ba8a9548ce596b876e515e58983398","modified":1717555064857},{"_id":"public/tags/LSM-Tree/index.html","hash":"1b06e0fac79944236fc7979bf1700f84fc1cc4b8","modified":1717555064857},{"_id":"public/tags/数据中台/index.html","hash":"db752c7bc67848ee2db2eb43636524f080064ce9","modified":1717555064857},{"_id":"public/tags/HBase/index.html","hash":"468c0b450347df65cc3cc94b295318c9a8e69462","modified":1717555064857},{"_id":"public/tags/MVCC/index.html","hash":"b4a861bbd194814e6311b13b6fde1b61e828554c","modified":1717555064857},{"_id":"public/tags/Kyuubi/index.html","hash":"0019ea4d81a225e132ec48134a999bee968b5c8c","modified":1717555064857},{"_id":"public/tags/yarn/index.html","hash":"3fd0f7b526fd3365e4ce1eb0f7c08390b12de74c","modified":1717555064857},{"_id":"public/tags/hadoop/index.html","hash":"c5c25e2c130d82a304c555329d7b032a9e14b8cc","modified":1717555064857},{"_id":"public/tags/用户画像/index.html","hash":"b8846ec469393753974e732816f129828fa01192","modified":1717555064857},{"_id":"public/tags/os/index.html","hash":"3454116611de44d81abf7d5bdf186ad1c3825fe8","modified":1717555064857},{"_id":"public/tags/dolphinscheduler/index.html","hash":"c59660ef20f25468cc3d1037a8a0789c450e63e2","modified":1717555064857},{"_id":"public/tags/kudu/index.html","hash":"4f686d9185522c6feb36d8483fe37c38f6603c03","modified":1717555064857},{"_id":"public/tags/RPC/index.html","hash":"9a4263cfc0fe584b74b6bfc351f25f5ab77707f9","modified":1717555064857},{"_id":"public/tags/Netty/index.html","hash":"913d756f5ebbf9e5d0b0c71e0dbbafe73cb49ee0","modified":1717555064857},{"_id":"public/tags/Troubleshooting/index.html","hash":"e0447be428449f4c1c246ef16105f2cb604ebf19","modified":1717555064857},{"_id":"public/tags/Kylin/index.html","hash":"bb7e5dc37fd36fd5fe59295fea6da3a04a47e132","modified":1717555064857},{"_id":"public/tags/paimon/index.html","hash":"9814a5049f02f2104cc063c3488d9840f5d69fbc","modified":1717555064857},{"_id":"public/tags/JVM/index.html","hash":"7efe021e66b2b0f7b6cb0fd09790cbeb4f4d0a8e","modified":1717555064857},{"_id":"public/tags/MAT/index.html","hash":"77944d463b530b6f7f2504b751ec7a89d2f9a70a","modified":1717555064857},{"_id":"public/tags/Arthas/index.html","hash":"3793643405578315f52503a25dbe2c3725452710","modified":1717555064857},{"_id":"public/tags/Threads/index.html","hash":"b7e197f888486cc2fe01fb904f943131e2cb762b","modified":1717555064857},{"_id":"public/tags/Flink/index.html","hash":"06de7d75b5ea505f210ef47770c917bae99525c9","modified":1717555064857},{"_id":"public/tags/Hudi/index.html","hash":"745a9dbacbbc01b02ce17c74cd07b36ba51e7b9f","modified":1717555064857},{"_id":"public/tags/thrift/index.html","hash":"fea8c3151dff6061a19aaa627a5cf99cd38684e3","modified":1717555064857},{"_id":"public/tags/LSM/index.html","hash":"a09887efef1fe13a6b1e140ac84807b5d865d0c8","modified":1717555064857},{"_id":"public/tags/Druid/index.html","hash":"d4e174d4d984414e468f16571f1b856c87e8a2a5","modified":1717555064857},{"_id":"public/tags/spark/index.html","hash":"477ce4fbe23f7d1576c7e81215873081b87479ec","modified":1717555064857},{"_id":"public/tags/scala/index.html","hash":"50ad6d2055f921d4a201a29b8c0f851548ab1af4","modified":1717555064857},{"_id":"public/script/1.jpg","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1717554958719},{"_id":"public/script/1.png","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1717554958719},{"_id":"public/tags/Apache-Superset/index.html","hash":"7f56467bbd9d614ac0d0b967f8533f849b7b668b","modified":1717555064857},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1717554958719},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1717554958719},{"_id":"public/robots.txt","hash":"c443b65f80db1005b56899172a5e9451af639175","modified":1717554958719},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1717554958719},{"_id":"public/CNAME","hash":"a44a2add0449f197dfcbdf8314c0005eb7e088d4","modified":1717554958719},{"_id":"public/img/72712952-12404300-3ba7-11ea-86f6-9a6610c616dd.png:Zone.Identifier","hash":"398656b3a875e9970340d70e52c9511ab4da70ad","modified":1717554958719},{"_id":"public/img/animal1.jpg","hash":"e48cb3aeeb030ff094e35e61c52bbd73a662e0a5","modified":1717554958719},{"_id":"public/script/img-webp.sh","hash":"b263a739e2c8f5acdb3c625d73e34d1e370079f2","modified":1717554958719},{"_id":"public/script/img-to-webp.sh","hash":"ade97b82a996d82be7b20693a4f95e9cba02592d","modified":1717554958719},{"_id":"public/sitemap_template/sitemap_template.txt","hash":"cbb0b7198d37b808e6e9bf4bd389743c07f8cf17","modified":1717554958719},{"_id":"public/sitemap_template/sitemap_template.xml","hash":"94026f74ab00f4398ee57381b6c3eaff9d475d35","modified":1717554958719},{"_id":"public/img/code/code.jpg","hash":"89b5ed059b850c72988b186751153e63bc3e9e6b","modified":1717554958719},{"_id":"public/img/code/linux.png","hash":"cad82d89247115c29cb88e60f03d229bb1e4e8cd","modified":1717554958719},{"_id":"public/img/bg/avatar.webp","hash":"253098016d490151a15192a99d73606607e2c15d","modified":1717554958719},{"_id":"public/img/dag1.png","hash":"3f2f19bc2d16289a8bc1c4f98bde466cff4fb387","modified":1717554958719},{"_id":"public/img/bg/banner1.webp","hash":"24cfb05fcddfc300d9554b1e9d83e93282052300","modified":1717554958719},{"_id":"public/img/bg/clash.jpg","hash":"a0388a2acdb319902a1b9a45fc48a836d8f5d07d","modified":1717554958719},{"_id":"public/img/bg/poxiao.jpg","hash":"dcdcaa64fa5817e52cef86ed69824e044105e3a7","modified":1717554958719},{"_id":"public/img/bg/index_img.png","hash":"8517f53cbd9416e8b8f16160936499b0b9c8b6dc","modified":1717554958719},{"_id":"public/img/img-link/img-link.txt","hash":"e5812babc02f71df9122496069016ed7b16e8168","modified":1717554958719},{"_id":"public/img/cat.png","hash":"764303a8a1ae8f7fa1fb6e9c21b852a5988882bb","modified":1717554958719},{"_id":"public/img/bg/banner2.webp","hash":"88377d64f128a09855066054fb74a290b0b26613","modified":1717554958719},{"_id":"public/img/bg/default_top_img.png","hash":"980912504113f64d6aaf5b99e9793b77d1511197","modified":1717554958719},{"_id":"public/css/index.css","hash":"6fc9f09ffd41cd14b77c43000c3cdf2375a6b366","modified":1717554958719},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1717554958719},{"_id":"public/js/main.js","hash":"04efcbd28b37875cfec88eb87cab7256a9ebb327","modified":1717554958719},{"_id":"public/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1717554958719},{"_id":"public/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1717554958719},{"_id":"public/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1717554958719},{"_id":"public/js/search/algolia.js","hash":"ce8131b712dca80f289015aef75f86e727f62981","modified":1717554958719},{"_id":"public/img/cut/cdn-test-2.webp","hash":"a0121aa1ff44ecaba93ed3449b9a95ee0d9daf83","modified":1717554958719},{"_id":"public/img/bg/web-bg.webp","hash":"7902ecf0465380a7576910c92881f7d493cfe0dd","modified":1717554958719},{"_id":"public/img/cut/cdn-test-1.webp","hash":"e8a063916ee46e49941081c8c4d0e241eaa0bd9f","modified":1717554958719},{"_id":"public/img/bg/banner.gif","hash":"bb3261150b534f76eda7ece32c05fab1e53e265f","modified":1717554958719},{"_id":"public/img/code/talk_code_to_me.webp","hash":"a263e3b98ba7a56896369f97690882ce2f583691","modified":1717554958719},{"_id":"public/img/bg/banner.webp","hash":"365b38933dcfe641a370df16017aea6dddd3cee1","modified":1717554958719},{"_id":"public/img/bg/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1717554958719},{"_id":"public/doc/Flink+Hudi构建实时仓湖一体化.pdf","hash":"b3c11357ed7ef084450565db003b61676909c823","modified":1717554958719}],"Category":[{"name":"bigdata","_id":"clx17vkst00078j5mah6bhjwp"},{"name":"hexo","_id":"clx17vksu000g8j5m471xd00b"},{"name":"hudi","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vksv000l8j5mdw9x62i3"},{"name":"SEO","_id":"clx17vksw000q8j5mai9zei8n"},{"name":"Linux","_id":"clx17vksw000v8j5m2zcvble4"},{"name":"dns","_id":"clx17vksw00148j5mdoq701hy"},{"name":"BugFix","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vksx001c8j5m4tvofhj0"},{"name":"data-structure","_id":"clx17vksx001k8j5m52obbea3"},{"name":"Doris","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vksz001w8j5m7m9650yh"},{"name":"Doris","_id":"clx17vksz00248j5m6vgq9sd2"},{"name":"linux","_id":"clx17vkt0002g8j5m87ep6ffw"},{"name":"HBase","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt1002w8j5m1ax8g51t"},{"name":"Kyuubi","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt200388j5mffr7gi2d"},{"name":"yarn","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt3003o8j5mhnephtp2"},{"name":"User-Profile","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt3003v8j5m8g8i3vzs"},{"name":"os","_id":"clx17vkt400428j5m6rst8v6l"},{"name":"RPC","_id":"clx17vkt400498j5mhlvffsbv"},{"name":"kudu","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt5004o8j5m7u3u7sun"},{"name":"Netty","_id":"clx17vkt6004z8j5mhuevd9jq"},{"name":"Troubleshooting","_id":"clx17vkt600568j5mbotiahqp"},{"name":"Kylin","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt6005h8j5mcskq9h2a"},{"name":"Java","_id":"clx17vkt6005k8j5m055qaraz"},{"name":"LSM","_id":"clx17vkt700688j5mgk7fa44p"},{"name":"paimon","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt7006b8j5mah5221yo"},{"name":"Druid","_id":"clx17vkt8006r8j5m5uvzem2o"},{"name":"spark","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8006w8j5m0d7gddsi"},{"name":"Flink","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8007e8j5mcvkp2c3m"},{"name":"scala","parent":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt900888j5m64kvbr74"},{"name":"Kylin","parent":"clx17vkt600568j5mbotiahqp","_id":"clx17vkt9008c8j5mb1tqfmgd"},{"name":"thrift","parent":"clx17vkt400498j5mhlvffsbv","_id":"clx17vkt9008j8j5m7k8c7gwy"},{"name":"Superset","_id":"clx17vktb009x8j5m0rt7f06i"}],"Data":[{"_id":"link","data":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}],"Page":[{"title":"关于","date":"2020-02-23T11:20:33.000Z","layout":"about","_content":"\n# 🎉🎉🎉关于博客\n\n本博客站点由**Hexo 、Github、Github Pages、ButterFly、Cloudflare、Freenom**等共同驱动，所用到的组件，不是开源就是免费，完全白嫖(￣▽￣)\"。\n\n因此国内的访问情况不容乐观 ! ! ! \n\n--------------------------------------------\n\n# 🎉🎉🎉关于我\n\n{% flink %}\n- class_name: \n  class_desc: \n  link_list:\n    - name: 破晓\n      link: https://poxiao.tk/\n      avatar: https://poxiao.tk/img/bg/avatar.webp\n      descr: 日拱一卒，功不唐捐\n      \n\n{% endflink %}\n\n-------------------------\n\n# 🎉🎉🎉博客备忘录\n- 添加Twikoo评论系统\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2020-02-23 19:20:33\nlayout: about\n---\n\n# 🎉🎉🎉关于博客\n\n本博客站点由**Hexo 、Github、Github Pages、ButterFly、Cloudflare、Freenom**等共同驱动，所用到的组件，不是开源就是免费，完全白嫖(￣▽￣)\"。\n\n因此国内的访问情况不容乐观 ! ! ! \n\n--------------------------------------------\n\n# 🎉🎉🎉关于我\n\n{% flink %}\n- class_name: \n  class_desc: \n  link_list:\n    - name: 破晓\n      link: https://poxiao.tk/\n      avatar: https://poxiao.tk/img/bg/avatar.webp\n      descr: 日拱一卒，功不唐捐\n      \n\n{% endflink %}\n\n-------------------------\n\n# 🎉🎉🎉博客备忘录\n- 添加Twikoo评论系统\n","updated":"2024-06-05T02:35:24.662Z","path":"about/index.html","comments":1,"_id":"clx17vksp00008j5mh4nq8rmr","content":"<h1 id=\"🎉🎉🎉关于博客\"><a href=\"#🎉🎉🎉关于博客\" class=\"headerlink\" title=\"🎉🎉🎉关于博客\"></a>🎉🎉🎉关于博客</h1><p>本博客站点由<strong>Hexo 、Github、Github Pages、ButterFly、Cloudflare、Freenom</strong>等共同驱动，所用到的组件，不是开源就是免费，完全白嫖(￣▽￣)”。</p>\n<p>因此国内的访问情况不容乐观 ! ! ! </p>\n<hr>\n<h1 id=\"🎉🎉🎉关于我\"><a href=\"#🎉🎉🎉关于我\" class=\"headerlink\" title=\"🎉🎉🎉关于我\"></a>🎉🎉🎉关于我</h1><div class=\"flink\"> <div class=\"flink-list\">\n          <div class=\"flink-list-item\">\n            <a href=\"https://poxiao.tk/\" title=\"破晓\" target=\"_blank\">\n              <div class=\"flink-item-icon\">\n                <img class=\"no-lightbox\" src=\"https://poxiao.tk/img/bg/avatar.webp\" onerror='this.onerror=null;this.src=\"/img/friend_404.gif\"' alt=\"破晓\" />\n              </div>\n              <div class=\"flink-item-name\">破晓</div> \n              <div class=\"flink-item-desc\" title=\"日拱一卒，功不唐捐\">日拱一卒，功不唐捐</div>\n            </a>\n          </div></div></div>\n\n<hr>\n<h1 id=\"🎉🎉🎉博客备忘录\"><a href=\"#🎉🎉🎉博客备忘录\" class=\"headerlink\" title=\"🎉🎉🎉博客备忘录\"></a>🎉🎉🎉博客备忘录</h1><ul>\n<li>添加Twikoo评论系统</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","excerpt":"","more":"<h1 id=\"🎉🎉🎉关于博客\"><a href=\"#🎉🎉🎉关于博客\" class=\"headerlink\" title=\"🎉🎉🎉关于博客\"></a>🎉🎉🎉关于博客</h1><p>本博客站点由<strong>Hexo 、Github、Github Pages、ButterFly、Cloudflare、Freenom</strong>等共同驱动，所用到的组件，不是开源就是免费，完全白嫖(￣▽￣)”。</p>\n<p>因此国内的访问情况不容乐观 ! ! ! </p>\n<hr>\n<h1 id=\"🎉🎉🎉关于我\"><a href=\"#🎉🎉🎉关于我\" class=\"headerlink\" title=\"🎉🎉🎉关于我\"></a>🎉🎉🎉关于我</h1><div class=\"flink\"> <div class=\"flink-list\">\n          <div class=\"flink-list-item\">\n            <a href=\"https://poxiao.tk/\" title=\"破晓\" target=\"_blank\">\n              <div class=\"flink-item-icon\">\n                <img class=\"no-lightbox\" src=\"https://poxiao.tk/img/bg/avatar.webp\" onerror='this.onerror=null;this.src=\"/img/friend_404.gif\"' alt=\"破晓\" />\n              </div>\n              <div class=\"flink-item-name\">破晓</div> \n              <div class=\"flink-item-desc\" title=\"日拱一卒，功不唐捐\">日拱一卒，功不唐捐</div>\n            </a>\n          </div></div></div>\n\n<hr>\n<h1 id=\"🎉🎉🎉博客备忘录\"><a href=\"#🎉🎉🎉博客备忘录\" class=\"headerlink\" title=\"🎉🎉🎉博客备忘录\"></a>🎉🎉🎉博客备忘录</h1><ul>\n<li>添加Twikoo评论系统</li>\n</ul>\n"},{"_content":"body {\n  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n  background-size: 400% 400%;\n  animation: gradient 15s ease infinite;\n  height: 100vh;\n  background-attachment:fixed;\n}\t\n@keyframes gradient {\t\n  0% {\t\n    background-position: 0% 50%;\t\n  }\t\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n","source":"css/background.css","raw":"body {\n  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n  background-size: 400% 400%;\n  animation: gradient 15s ease infinite;\n  height: 100vh;\n  background-attachment:fixed;\n}\t\n@keyframes gradient {\t\n  0% {\t\n    background-position: 0% 50%;\t\n  }\t\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n","date":"2024-06-05T02:35:24.663Z","updated":"2024-06-05T02:35:24.663Z","path":"css/background.css","layout":"false","title":"","comments":1,"_id":"clx17vksr00028j5m18su0owj","content":"body {\n  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n  background-size: 400% 400%;\n  animation: gradient 15s ease infinite;\n  height: 100vh;\n  background-attachment:fixed;\n}\t\n@keyframes gradient {\t\n  0% {\t\n    background-position: 0% 50%;\t\n  }\t\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","excerpt":"","more":"body {\n  background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n  background-size: 400% 400%;\n  animation: gradient 15s ease infinite;\n  height: 100vh;\n  background-attachment:fixed;\n}\t\n@keyframes gradient {\t\n  0% {\t\n    background-position: 0% 50%;\t\n  }\t\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n"},{"title":"分类","date":"2022-06-25T12:12:38.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2022-06-25 20:12:38\ntype: \"categories\"\n---\n","updated":"2024-06-05T02:35:24.663Z","path":"categories/index.html","comments":1,"layout":"page","_id":"clx17vkss00058j5mdl7260px","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","excerpt":"","more":""},{"title":"标签","date":"2022-06-25T12:10:55.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2022-06-25 20:10:55\ntype: \"tags\"\n---\n","updated":"2024-06-05T02:35:24.690Z","path":"tags/index.html","comments":1,"layout":"page","_id":"clx17vkst00088j5ma91q9evj","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","excerpt":"","more":""},{"title":"友情链接","date":"2022-06-25T12:13:44.000Z","type":"link","_content":"\n\n------\n# 欢迎交换友链\n- 我的友链信息\n\n``` yml\n name: 破晓\n link: https://poxiao.tk/\n avatar: https://poxiao.tk/img/bg/avatar.webp\n descr: 日拱一卒，功不唐捐\n\n```\n","source":"link/index.md","raw":"---\ntitle: 友情链接\ndate: 2022-06-25 20:13:44\ntype: \"link\"\n---\n\n\n------\n# 欢迎交换友链\n- 我的友链信息\n\n``` yml\n name: 破晓\n link: https://poxiao.tk/\n avatar: https://poxiao.tk/img/bg/avatar.webp\n descr: 日拱一卒，功不唐捐\n\n```\n","updated":"2024-06-05T02:35:24.690Z","path":"link/index.html","comments":1,"layout":"page","_id":"clx17vkst000a8j5m6h6u959x","content":"<hr>\n<h1 id=\"欢迎交换友链\"><a href=\"#欢迎交换友链\" class=\"headerlink\" title=\"欢迎交换友链\"></a>欢迎交换友链</h1><ul>\n<li>我的友链信息</li>\n</ul>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">name:</span> <span class=\"string\">破晓</span></span><br><span class=\"line\"><span class=\"attr\">link:</span> <span class=\"string\">https://poxiao.tk/</span></span><br><span class=\"line\"><span class=\"attr\">avatar:</span> <span class=\"string\">https://poxiao.tk/img/bg/avatar.webp</span></span><br><span class=\"line\"><span class=\"attr\">descr:</span> <span class=\"string\">日拱一卒，功不唐捐</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","excerpt":"","more":"<hr>\n<h1 id=\"欢迎交换友链\"><a href=\"#欢迎交换友链\" class=\"headerlink\" title=\"欢迎交换友链\"></a>欢迎交换友链</h1><ul>\n<li>我的友链信息</li>\n</ul>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">name:</span> <span class=\"string\">破晓</span></span><br><span class=\"line\"><span class=\"attr\">link:</span> <span class=\"string\">https://poxiao.tk/</span></span><br><span class=\"line\"><span class=\"attr\">avatar:</span> <span class=\"string\">https://poxiao.tk/img/bg/avatar.webp</span></span><br><span class=\"line\"><span class=\"attr\">descr:</span> <span class=\"string\">日拱一卒，功不唐捐</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n"},{"_content":"#recent-posts>.recent-post-item,.layout_page>div:first-child:not(.recent-posts),.layout_post>#page,.layout_post>#post,.read-mode .layout_post>#post {\n    background: var(--light_bg_color)\n}\n\n#aside-content .card-widget {\n    background: var(--light_bg_color)\n}\n\n#web_bg {\n    background: linear-gradient(90deg,rgba(247,149,51,.1),rgba(243,112,85,.1) 15%,rgba(239,78,123,.1) 30%,rgba(161,102,171,.1) 44%,rgba(80,115,184,.1) 58%,rgba(16,152,173,.1) 72%,rgba(7,179,155,.1) 86%,rgba(109,186,130,.1))\n}\n\n#footer {\n    background: rgba(255,255,255,.15);\n    color: #000;\n    border-top-right-radius: 20px;\n    border-top-left-radius: 20px;\n    backdrop-filter: saturate(100%) blur(5px)\n}\n\n#footer::before {\n    background: rgba(255,255,255,.15)\n}\n\n#footer #footer-wrap {\n    color: var(--font-color)\n}\n\n#footer #footer-wrap a {\n    color: var(--font-color)\n}\n","source":"css/custom.css","raw":"#recent-posts>.recent-post-item,.layout_page>div:first-child:not(.recent-posts),.layout_post>#page,.layout_post>#post,.read-mode .layout_post>#post {\n    background: var(--light_bg_color)\n}\n\n#aside-content .card-widget {\n    background: var(--light_bg_color)\n}\n\n#web_bg {\n    background: linear-gradient(90deg,rgba(247,149,51,.1),rgba(243,112,85,.1) 15%,rgba(239,78,123,.1) 30%,rgba(161,102,171,.1) 44%,rgba(80,115,184,.1) 58%,rgba(16,152,173,.1) 72%,rgba(7,179,155,.1) 86%,rgba(109,186,130,.1))\n}\n\n#footer {\n    background: rgba(255,255,255,.15);\n    color: #000;\n    border-top-right-radius: 20px;\n    border-top-left-radius: 20px;\n    backdrop-filter: saturate(100%) blur(5px)\n}\n\n#footer::before {\n    background: rgba(255,255,255,.15)\n}\n\n#footer #footer-wrap {\n    color: var(--font-color)\n}\n\n#footer #footer-wrap a {\n    color: var(--font-color)\n}\n","date":"2024-06-05T02:35:24.663Z","updated":"2024-06-05T02:35:24.663Z","path":"css/custom.css","layout":"false","title":"","comments":1,"_id":"clx17vksu000e8j5m2bxle1ud","content":"#recent-posts>.recent-post-item,.layout_page>div:first-child:not(.recent-posts),.layout_post>#page,.layout_post>#post,.read-mode .layout_post>#post {\n    background: var(--light_bg_color)\n}\n\n#aside-content .card-widget {\n    background: var(--light_bg_color)\n}\n\n#web_bg {\n    background: linear-gradient(90deg,rgba(247,149,51,.1),rgba(243,112,85,.1) 15%,rgba(239,78,123,.1) 30%,rgba(161,102,171,.1) 44%,rgba(80,115,184,.1) 58%,rgba(16,152,173,.1) 72%,rgba(7,179,155,.1) 86%,rgba(109,186,130,.1))\n}\n\n#footer {\n    background: rgba(255,255,255,.15);\n    color: #000;\n    border-top-right-radius: 20px;\n    border-top-left-radius: 20px;\n    backdrop-filter: saturate(100%) blur(5px)\n}\n\n#footer::before {\n    background: rgba(255,255,255,.15)\n}\n\n#footer #footer-wrap {\n    color: var(--font-color)\n}\n\n#footer #footer-wrap a {\n    color: var(--font-color)\n}\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","excerpt":"","more":"#recent-posts>.recent-post-item,.layout_page>div:first-child:not(.recent-posts),.layout_post>#page,.layout_post>#post,.read-mode .layout_post>#post {\n    background: var(--light_bg_color)\n}\n\n#aside-content .card-widget {\n    background: var(--light_bg_color)\n}\n\n#web_bg {\n    background: linear-gradient(90deg,rgba(247,149,51,.1),rgba(243,112,85,.1) 15%,rgba(239,78,123,.1) 30%,rgba(161,102,171,.1) 44%,rgba(80,115,184,.1) 58%,rgba(16,152,173,.1) 72%,rgba(7,179,155,.1) 86%,rgba(109,186,130,.1))\n}\n\n#footer {\n    background: rgba(255,255,255,.15);\n    color: #000;\n    border-top-right-radius: 20px;\n    border-top-left-radius: 20px;\n    backdrop-filter: saturate(100%) blur(5px)\n}\n\n#footer::before {\n    background: rgba(255,255,255,.15)\n}\n\n#footer #footer-wrap {\n    color: var(--font-color)\n}\n\n#footer #footer-wrap a {\n    color: var(--font-color)\n}\n"}],"Post":[{"title":"Flink+Hudi构建实时仓湖一体化","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","date":"2022-08-21T06:22:15.000Z","updated":"2022-08-21T06:22:15.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n{% pdf /doc/Flink+Hudi构建实时仓湖一体化.pdf %}\n","source":"_drafts/Flink+Hudi构建实时仓湖一体化.md","raw":"---\ntitle: Flink+Hudi构建实时仓湖一体化\ntags:\n  - 'hudi'\ncategories:\n  - [bigdata,hudi]\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\ndate: 2022-08-21 14:22:15\nupdated: 2022-08-21 14:22:15\ncover:\ndescription:\nkeywords:\n---\n\n{% pdf /doc/Flink+Hudi构建实时仓湖一体化.pdf %}\n","slug":"Flink+Hudi构建实时仓湖一体化","published":0,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksq00018j5m86cvdt7q","content":"\n\n\t<div class=\"row\">\n    <embed src=\"/doc/Flink+Hudi构建实时仓湖一体化.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"\n\n\t<div class=\"row\">\n    <embed src=\"/doc/Flink+Hudi构建实时仓湖一体化.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n"},{"title":"网易云音乐EMO语录","date":"2022-06-25T09:00:28.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"_content":"\n\n\n![cat](/img/cat.png)\n\n## 网易云EMO语录 - 纯属好玩\n\n\n\n- 小时候不理解老人晒太阳，一坐就是半天。长大了才明白，目之所及皆是回忆，心之所想皆是过往，眼之所看皆是遗憾。                                                   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——网易云热评《红莓花儿开》  \n\n- 未成定局的事就不要弄的人尽皆知 \n\n- 大胆点生活 你没那么多观众\n\n- 一个人在离家2000公里外的山沟里当兵，我坐在山坡上，看着田埂上的小孩放着风筝，大人在屁股后面跟着跑，笑容不知不觉爬上嘴角，太阳很刺眼，我很想家                                                                             ——网易云《Town of Windmill 》\n\n- 陪领导吃饭，把领导全部喝趴了，然后每个人都有人接回家，唯独我没有，一个人走到地铁站。当时走着去路边打车，被风一吹，终于忍不住了，直到吐干净，劲过去了，已经是凌晨四点。\n  摸摸手机，零条短信，零条来电。\n  只有那棵树整整支撑了我五个小时.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ——《身后的故乡》\n- 有一段时间觉得爸爸很古板很烦，后来有一天看到一句话“中年以后的男人，时常会觉得孤独，因为他一睁开眼睛，周围都是要依靠他的人，却没有他可以依靠的人”。 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ——网易云音乐热评 《体温》\n","source":"_drafts/zhihu-emo-yulu.md","raw":"---\ntitle: 网易云音乐EMO语录\ndate: 2022-06-25 17:00:28\n\ncover: \ntop_img: \n\ntags: \n- EMO语录\ncategories:\n- \n\n---\n\n\n\n![cat](/img/cat.png)\n\n## 网易云EMO语录 - 纯属好玩\n\n\n\n- 小时候不理解老人晒太阳，一坐就是半天。长大了才明白，目之所及皆是回忆，心之所想皆是过往，眼之所看皆是遗憾。                                                   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——网易云热评《红莓花儿开》  \n\n- 未成定局的事就不要弄的人尽皆知 \n\n- 大胆点生活 你没那么多观众\n\n- 一个人在离家2000公里外的山沟里当兵，我坐在山坡上，看着田埂上的小孩放着风筝，大人在屁股后面跟着跑，笑容不知不觉爬上嘴角，太阳很刺眼，我很想家                                                                             ——网易云《Town of Windmill 》\n\n- 陪领导吃饭，把领导全部喝趴了，然后每个人都有人接回家，唯独我没有，一个人走到地铁站。当时走着去路边打车，被风一吹，终于忍不住了，直到吐干净，劲过去了，已经是凌晨四点。\n  摸摸手机，零条短信，零条来电。\n  只有那棵树整整支撑了我五个小时.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ——《身后的故乡》\n- 有一段时间觉得爸爸很古板很烦，后来有一天看到一句话“中年以后的男人，时常会觉得孤独，因为他一睁开眼睛，周围都是要依靠他的人，却没有他可以依靠的人”。 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ——网易云音乐热评 《体温》\n","slug":"zhihu-emo-yulu","published":0,"updated":"2024-06-05T02:35:24.655Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkss00038j5m2intd84w","content":"<p><img src=\"/img/cat.png\" alt=\"cat\"></p>\n<h2 id=\"网易云EMO语录-纯属好玩\"><a href=\"#网易云EMO语录-纯属好玩\" class=\"headerlink\" title=\"网易云EMO语录 - 纯属好玩\"></a>网易云EMO语录 - 纯属好玩</h2><ul>\n<li><p>小时候不理解老人晒太阳，一坐就是半天。长大了才明白，目之所及皆是回忆，心之所想皆是过往，眼之所看皆是遗憾。                                                   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——网易云热评《红莓花儿开》  </p>\n</li>\n<li><p>未成定局的事就不要弄的人尽皆知 </p>\n</li>\n<li><p>大胆点生活 你没那么多观众</p>\n</li>\n<li><p>一个人在离家2000公里外的山沟里当兵，我坐在山坡上，看着田埂上的小孩放着风筝，大人在屁股后面跟着跑，笑容不知不觉爬上嘴角，太阳很刺眼，我很想家                                                                             ——网易云《Town of Windmill 》</p>\n</li>\n<li><p>陪领导吃饭，把领导全部喝趴了，然后每个人都有人接回家，唯独我没有，一个人走到地铁站。当时走着去路边打车，被风一吹，终于忍不住了，直到吐干净，劲过去了，已经是凌晨四点。<br>摸摸手机，零条短信，零条来电。<br>只有那棵树整整支撑了我五个小时.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ——《身后的故乡》</p>\n</li>\n<li><p>有一段时间觉得爸爸很古板很烦，后来有一天看到一句话“中年以后的男人，时常会觉得孤独，因为他一睁开眼睛，周围都是要依靠他的人，却没有他可以依靠的人”。 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ——网易云音乐热评 《体温》</p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<p><img src=\"/img/cat.png\" alt=\"cat\"></p>\n<h2 id=\"网易云EMO语录-纯属好玩\"><a href=\"#网易云EMO语录-纯属好玩\" class=\"headerlink\" title=\"网易云EMO语录 - 纯属好玩\"></a>网易云EMO语录 - 纯属好玩</h2><ul>\n<li><p>小时候不理解老人晒太阳，一坐就是半天。长大了才明白，目之所及皆是回忆，心之所想皆是过往，眼之所看皆是遗憾。                                                   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t——网易云热评《红莓花儿开》  </p>\n</li>\n<li><p>未成定局的事就不要弄的人尽皆知 </p>\n</li>\n<li><p>大胆点生活 你没那么多观众</p>\n</li>\n<li><p>一个人在离家2000公里外的山沟里当兵，我坐在山坡上，看着田埂上的小孩放着风筝，大人在屁股后面跟着跑，笑容不知不觉爬上嘴角，太阳很刺眼，我很想家                                                                             ——网易云《Town of Windmill 》</p>\n</li>\n<li><p>陪领导吃饭，把领导全部喝趴了，然后每个人都有人接回家，唯独我没有，一个人走到地铁站。当时走着去路边打车，被风一吹，终于忍不住了，直到吐干净，劲过去了，已经是凌晨四点。<br>摸摸手机，零条短信，零条来电。<br>只有那棵树整整支撑了我五个小时.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ——《身后的故乡》</p>\n</li>\n<li><p>有一段时间觉得爸爸很古板很烦，后来有一天看到一句话“中年以后的男人，时常会觉得孤独，因为他一睁开眼睛，周围都是要依靠他的人，却没有他可以依靠的人”。 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ——网易云音乐热评 《体温》</p>\n</li>\n</ul>\n"},{"title":"配置hadoop native lib和snappy压缩遇到的问题","date":"2022-10-16T10:48:59.000Z","updated":"2022-10-16T10:48:59.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","top_img":null,"description":null,"keywords":null,"_content":"\n\n\n- `Unable to load native-hadoop library``\n\n  `2022-10-16 18:04:07.991 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable`\n\n添加环境变量：`export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native`\n\n- 执行hadoop本地库检查` hadoop checknative -a`报错:`ERROR snappy.SnappyCompressor: failed to load SnappyCompressor`\n\n  安装snappy本地库，执行：`sudo apt install libsnappy-dev`\n","source":"_drafts/hadoop-native-lib和snappy.md","raw":"---\ntitle: 配置hadoop native lib和snappy压缩遇到的问题\ntags:\n  - ''\ncategories:\n  - []\ndate: 2022-10-16 18:48:59\nupdated: 2022-10-16 18:48:59\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n\n\n- `Unable to load native-hadoop library``\n\n  `2022-10-16 18:04:07.991 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable`\n\n添加环境变量：`export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native`\n\n- 执行hadoop本地库检查` hadoop checknative -a`报错:`ERROR snappy.SnappyCompressor: failed to load SnappyCompressor`\n\n  安装snappy本地库，执行：`sudo apt install libsnappy-dev`\n","slug":"hadoop-native-lib和snappy","published":0,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkst00068j5m3v4u8yu5","content":"<ul>\n<li><p>&#96;Unable to load native-hadoop library&#96;&#96;</p>\n<p><code>2022-10-16 18:04:07.991 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code></p>\n</li>\n</ul>\n<p>添加环境变量：<code>export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native</code></p>\n<ul>\n<li><p>执行hadoop本地库检查<code> hadoop checknative -a</code>报错:<code>ERROR snappy.SnappyCompressor: failed to load SnappyCompressor</code></p>\n<p>安装snappy本地库，执行：<code>sudo apt install libsnappy-dev</code></p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<ul>\n<li><p>&#96;Unable to load native-hadoop library&#96;&#96;</p>\n<p><code>2022-10-16 18:04:07.991 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code></p>\n</li>\n</ul>\n<p>添加环境变量：<code>export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native</code></p>\n<ul>\n<li><p>执行hadoop本地库检查<code> hadoop checknative -a</code>报错:<code>ERROR snappy.SnappyCompressor: failed to load SnappyCompressor</code></p>\n<p>安装snappy本地库，执行：<code>sudo apt install libsnappy-dev</code></p>\n</li>\n</ul>\n"},{"title":"github pages-Hexo-CloudFlare免费CDN最佳实践","typora-copy-images-to":"..\\img\\cut","abbrlink":41454,"date":"2022-06-27T13:14:01.000Z","updated":"2022-06-27T13:19:32.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 一、查看网站是否使用了CDN\n\n执行：nslookup  XXX 命令\n\n```\n➜  hexo-blog nslookup clashdingyue.tk\nServer:         172.17.112.1\nAddress:        172.17.112.1#53\n\nNon-authoritative answer:\nName:   clashdingyue.tk\nAddress: 185.199.110.153\nName:   clashdingyue.tk\nAddress: 185.199.109.153\nName:   clashdingyue.tk\nAddress: 185.199.111.153\nName:   clashdingyue.tk\nAddress: 185.199.108.153\n```\n\n```\n➜  hexo-blog nslookup wohensha.tk\nServer:         172.17.112.1\nAddress:        172.17.112.1#53\n\nNon-authoritative answer:\nName:   wohensha.tk\nAddress: 172.67.165.231\nName:   wohensha.tk\nAddress: 104.21.49.190\n```\n\n**两个或两个以上Server IP，则表明使用了CDN，只有一个则表明没有。**\n\n- Github CDN：185.199.110.153、185.199.108.153...\n- CloudFlare CDN：172.67.165.231、104.21.49.190\n\n\n\n## 二、测试两个CDN哪一个在墙内，速度更好\n\n### CloudFlare CDN\n\n略\n\n### Github CDN\n\n![image-cdn](/img/cut/cdn-test-1.webp)\n\n![image-cdn](/img/cut/cdn-test-2.webp)\n\n(￣▽￣)，半斤八两，感觉Github CDN好点\n","source":"_posts/Hexo-github-pages-CloudFlare免费CDN最佳实践.md","raw":"---\ntitle: github pages-Hexo-CloudFlare免费CDN最佳实践\ntags:\n  - CDN\n  - hexo\n  - cloudflare\ncategories:\n  - - hexo\ntypora-copy-images-to: ..\\img\\cut\nabbrlink: 41454\ndate: 2022-06-27 21:14:01\nupdated: 2022-06-27 21:19:32\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 一、查看网站是否使用了CDN\n\n执行：nslookup  XXX 命令\n\n```\n➜  hexo-blog nslookup clashdingyue.tk\nServer:         172.17.112.1\nAddress:        172.17.112.1#53\n\nNon-authoritative answer:\nName:   clashdingyue.tk\nAddress: 185.199.110.153\nName:   clashdingyue.tk\nAddress: 185.199.109.153\nName:   clashdingyue.tk\nAddress: 185.199.111.153\nName:   clashdingyue.tk\nAddress: 185.199.108.153\n```\n\n```\n➜  hexo-blog nslookup wohensha.tk\nServer:         172.17.112.1\nAddress:        172.17.112.1#53\n\nNon-authoritative answer:\nName:   wohensha.tk\nAddress: 172.67.165.231\nName:   wohensha.tk\nAddress: 104.21.49.190\n```\n\n**两个或两个以上Server IP，则表明使用了CDN，只有一个则表明没有。**\n\n- Github CDN：185.199.110.153、185.199.108.153...\n- CloudFlare CDN：172.67.165.231、104.21.49.190\n\n\n\n## 二、测试两个CDN哪一个在墙内，速度更好\n\n### CloudFlare CDN\n\n略\n\n### Github CDN\n\n![image-cdn](/img/cut/cdn-test-1.webp)\n\n![image-cdn](/img/cut/cdn-test-2.webp)\n\n(￣▽￣)，半斤八两，感觉Github CDN好点\n","slug":"Hexo-github-pages-CloudFlare免费CDN最佳实践","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkst00098j5md6o74ysg","content":"<h2 id=\"一、查看网站是否使用了CDN\"><a href=\"#一、查看网站是否使用了CDN\" class=\"headerlink\" title=\"一、查看网站是否使用了CDN\"></a>一、查看网站是否使用了CDN</h2><p>执行：nslookup  XXX 命令</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup clashdingyue.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.110.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.109.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.111.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.108.153</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup wohensha.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 172.67.165.231</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 104.21.49.190</span><br></pre></td></tr></table></figure>\n\n<p><strong>两个或两个以上Server IP，则表明使用了CDN，只有一个则表明没有。</strong></p>\n<ul>\n<li>Github CDN：185.199.110.153、185.199.108.153…</li>\n<li>CloudFlare CDN：172.67.165.231、104.21.49.190</li>\n</ul>\n<h2 id=\"二、测试两个CDN哪一个在墙内，速度更好\"><a href=\"#二、测试两个CDN哪一个在墙内，速度更好\" class=\"headerlink\" title=\"二、测试两个CDN哪一个在墙内，速度更好\"></a>二、测试两个CDN哪一个在墙内，速度更好</h2><h3 id=\"CloudFlare-CDN\"><a href=\"#CloudFlare-CDN\" class=\"headerlink\" title=\"CloudFlare CDN\"></a>CloudFlare CDN</h3><p>略</p>\n<h3 id=\"Github-CDN\"><a href=\"#Github-CDN\" class=\"headerlink\" title=\"Github CDN\"></a>Github CDN</h3><p><img src=\"/img/cut/cdn-test-1.webp\" alt=\"image-cdn\"></p>\n<p><img src=\"/img/cut/cdn-test-2.webp\" alt=\"image-cdn\"></p>\n<p>(￣▽￣)，半斤八两，感觉Github CDN好点</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"一、查看网站是否使用了CDN\"><a href=\"#一、查看网站是否使用了CDN\" class=\"headerlink\" title=\"一、查看网站是否使用了CDN\"></a>一、查看网站是否使用了CDN</h2><p>执行：nslookup  XXX 命令</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup clashdingyue.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.110.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.109.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.111.153</span><br><span class=\"line\">Name:   clashdingyue.tk</span><br><span class=\"line\">Address: 185.199.108.153</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup wohensha.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 172.67.165.231</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 104.21.49.190</span><br></pre></td></tr></table></figure>\n\n<p><strong>两个或两个以上Server IP，则表明使用了CDN，只有一个则表明没有。</strong></p>\n<ul>\n<li>Github CDN：185.199.110.153、185.199.108.153…</li>\n<li>CloudFlare CDN：172.67.165.231、104.21.49.190</li>\n</ul>\n<h2 id=\"二、测试两个CDN哪一个在墙内，速度更好\"><a href=\"#二、测试两个CDN哪一个在墙内，速度更好\" class=\"headerlink\" title=\"二、测试两个CDN哪一个在墙内，速度更好\"></a>二、测试两个CDN哪一个在墙内，速度更好</h2><h3 id=\"CloudFlare-CDN\"><a href=\"#CloudFlare-CDN\" class=\"headerlink\" title=\"CloudFlare CDN\"></a>CloudFlare CDN</h3><p>略</p>\n<h3 id=\"Github-CDN\"><a href=\"#Github-CDN\" class=\"headerlink\" title=\"Github CDN\"></a>Github CDN</h3><p><img src=\"/img/cut/cdn-test-1.webp\" alt=\"image-cdn\"></p>\n<p><img src=\"/img/cut/cdn-test-2.webp\" alt=\"image-cdn\"></p>\n<p>(￣▽￣)，半斤八两，感觉Github CDN好点</p>\n"},{"title":"网站、DNS速度测试和性能分析","description":"网站、DNS速度测试和性能分析","abbrlink":54794,"date":"2022-06-27T11:57:25.000Z","updated":"2022-06-27T11:57:25.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"keywords":null,"_content":"\n### 常用国内外网站测速及性能分析工具\n\n网站多地测速工具/网站，通常是**同时测试并列出众多监测点到网站的主要速度指标**(如解析时间、连接时间、下载速度等)，一般**不用于**检测网站代码及服务器性能优化的情况，而是主要用于选购服务器/VPS、服务器/CDN在各地的网速排查、CDN部署等。\n\n- [**17ce**](https://www.17ce.com/) *(国内网站)*\n  - 60+个国内及香港监测点,适用于**国内**各地访问网站的速度测试\n  - 提供 Get, Ping, 路由追踪, Dns, Cdn等多维度速度测试\n  - 提供监控API(付费)\n- [**卡卡网 Webkaka**](http://www.webkaka.com/) *(国内网站)*\n  - 60+个国内监测点, 30+个海外监测点，适用于**国内**或**全球**各地访问网站的速度测试\n  - 提供 Get, ping, 路由追踪, Dns等多维度速度测试\n  - 提供网站速度诊断功能\n- [**Sucuri Load Time Tester**](https://performance.sucuri.net/) *(国外网站)*\n  - 15+个全球监测点, 适用于**全球**各地访问网站(如外贸网站)的速度测试\n  - 提供网页连接时间、首字节时间、总时间等数据\n- [**Dotcom-Tools Website Speed Test**](https://www.dotcom-tools.com/website-speed-test.aspx) *(国外网站)*\n  - 20+个全球监测点, 适用于**全球**各地访问网站(如外贸网站)的速度测试\n  - 提供两次(首次及重复访问)网页加载速度，且可按节点查看详细信息\n\n\n\n**一、Think with Google**\n\n工具地址：www.thinkwithgoogle.com/feature/testmysite/\n\n这是谷歌推出专门针对移动端访问速测试的工具，该工具模似4G访问网站，而结果也会出现相对应的优化建议，与竞争对手的打开速度比较，这是一款针对TO C移动端用户的专业测速工具。\n\n**二、Webuup**\n\n工具地址：www.webuup.com\n\nWebuup是新出的免费的在线网站质量检测工具，目前知道它的人还不是很多，但它的功能却十分强大，非常好用。它不仅能根据各项关键指标，如页面性能、移动表现、SEO优化和安全隐私来判断网站的质量，还能给出网站优化和提升的建议，Webuup检测工具，可以在页面速度检测栏，清晰的显示全球不同的测速区域与测速服务器状态下，该网站首字节返回时间、页面展示延迟时间、页面完整加载时间。\n\n**三、PageSpeed Insights**\n\n工具地址：pagespeed.web.dev\n\nPageSpeed这款测速工具出自Google，它可以直接总结出测试网站所存在的问题，并提出相应的优化建议，且指示很清晰，而最近谷歌推出的页面体验算法，也是根据这里的三项页面指标（LCP、FID、CLS）优化，此外，PageSpeed的测速包括移动设备和桌面设备两方面，并分别从这两个不同的设备端提出了提升网站加载速度的建议，用起来十分方便。\n\n**四、Gtmetrix**\n\n工具地址：gtmetrix.com\n\nGTmetrix现在由Lighthouse提供技术支持，有网站打开速度和网站性能评测工具，也成为网站用户体验的检测工具，Lighthouse是Google提供的一种开源工具，可以通过多种方式对网页进行评估。它被认为是现代Web性能指标的标准，因此将Lighthouse分析和性能数据集成到GTmetrix中，所以也是这次我们放在推荐网站速度测试工具之一。\n\n**五、Dotcom-tools**\n\n工具地址：www.dotcom-tools.com\n\nDotcom网站全球访问测试工具，主要是分析外贸网站在全球各地的速度测试，输入网址后，会出现每个地区节点的载入时间、重复测试后的载入时间、下载页面大小及遭遇到的错误记录，可以查看页面大小、加载时间和下载速度等前端元素，还可以查看HTML、JavaScript和CSS等后端元素，可以对网站针对性优化。\n\n​\t\t以上五款是不错的免费网站打开速度测试工具，也比较实用，当然还有一些其他的测试工具，但这五款工具已经够用了。正常一个网站，打开速度应该在2~3秒是比较理想的，同时大家在优化好web端的打开速度时，也要注重对于移动端的优化。\n\n","source":"_posts/SEO.md","raw":"---\ntitle: 网站、DNS速度测试和性能分析\ntags:\n  - SEO\ncategories:\n  - - SEO\ndescription: 网站、DNS速度测试和性能分析\nabbrlink: 54794\ndate: 2022-06-27 19:57:25\nupdated: 2022-06-27 19:57:25\ncover:\ntop_img:\nkeywords:\n---\n\n### 常用国内外网站测速及性能分析工具\n\n网站多地测速工具/网站，通常是**同时测试并列出众多监测点到网站的主要速度指标**(如解析时间、连接时间、下载速度等)，一般**不用于**检测网站代码及服务器性能优化的情况，而是主要用于选购服务器/VPS、服务器/CDN在各地的网速排查、CDN部署等。\n\n- [**17ce**](https://www.17ce.com/) *(国内网站)*\n  - 60+个国内及香港监测点,适用于**国内**各地访问网站的速度测试\n  - 提供 Get, Ping, 路由追踪, Dns, Cdn等多维度速度测试\n  - 提供监控API(付费)\n- [**卡卡网 Webkaka**](http://www.webkaka.com/) *(国内网站)*\n  - 60+个国内监测点, 30+个海外监测点，适用于**国内**或**全球**各地访问网站的速度测试\n  - 提供 Get, ping, 路由追踪, Dns等多维度速度测试\n  - 提供网站速度诊断功能\n- [**Sucuri Load Time Tester**](https://performance.sucuri.net/) *(国外网站)*\n  - 15+个全球监测点, 适用于**全球**各地访问网站(如外贸网站)的速度测试\n  - 提供网页连接时间、首字节时间、总时间等数据\n- [**Dotcom-Tools Website Speed Test**](https://www.dotcom-tools.com/website-speed-test.aspx) *(国外网站)*\n  - 20+个全球监测点, 适用于**全球**各地访问网站(如外贸网站)的速度测试\n  - 提供两次(首次及重复访问)网页加载速度，且可按节点查看详细信息\n\n\n\n**一、Think with Google**\n\n工具地址：www.thinkwithgoogle.com/feature/testmysite/\n\n这是谷歌推出专门针对移动端访问速测试的工具，该工具模似4G访问网站，而结果也会出现相对应的优化建议，与竞争对手的打开速度比较，这是一款针对TO C移动端用户的专业测速工具。\n\n**二、Webuup**\n\n工具地址：www.webuup.com\n\nWebuup是新出的免费的在线网站质量检测工具，目前知道它的人还不是很多，但它的功能却十分强大，非常好用。它不仅能根据各项关键指标，如页面性能、移动表现、SEO优化和安全隐私来判断网站的质量，还能给出网站优化和提升的建议，Webuup检测工具，可以在页面速度检测栏，清晰的显示全球不同的测速区域与测速服务器状态下，该网站首字节返回时间、页面展示延迟时间、页面完整加载时间。\n\n**三、PageSpeed Insights**\n\n工具地址：pagespeed.web.dev\n\nPageSpeed这款测速工具出自Google，它可以直接总结出测试网站所存在的问题，并提出相应的优化建议，且指示很清晰，而最近谷歌推出的页面体验算法，也是根据这里的三项页面指标（LCP、FID、CLS）优化，此外，PageSpeed的测速包括移动设备和桌面设备两方面，并分别从这两个不同的设备端提出了提升网站加载速度的建议，用起来十分方便。\n\n**四、Gtmetrix**\n\n工具地址：gtmetrix.com\n\nGTmetrix现在由Lighthouse提供技术支持，有网站打开速度和网站性能评测工具，也成为网站用户体验的检测工具，Lighthouse是Google提供的一种开源工具，可以通过多种方式对网页进行评估。它被认为是现代Web性能指标的标准，因此将Lighthouse分析和性能数据集成到GTmetrix中，所以也是这次我们放在推荐网站速度测试工具之一。\n\n**五、Dotcom-tools**\n\n工具地址：www.dotcom-tools.com\n\nDotcom网站全球访问测试工具，主要是分析外贸网站在全球各地的速度测试，输入网址后，会出现每个地区节点的载入时间、重复测试后的载入时间、下载页面大小及遭遇到的错误记录，可以查看页面大小、加载时间和下载速度等前端元素，还可以查看HTML、JavaScript和CSS等后端元素，可以对网站针对性优化。\n\n​\t\t以上五款是不错的免费网站打开速度测试工具，也比较实用，当然还有一些其他的测试工具，但这五款工具已经够用了。正常一个网站，打开速度应该在2~3秒是比较理想的，同时大家在优化好web端的打开速度时，也要注重对于移动端的优化。\n\n","slug":"SEO","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkst000b8j5mago0hgbs","content":"<h3 id=\"常用国内外网站测速及性能分析工具\"><a href=\"#常用国内外网站测速及性能分析工具\" class=\"headerlink\" title=\"常用国内外网站测速及性能分析工具\"></a>常用国内外网站测速及性能分析工具</h3><p>网站多地测速工具&#x2F;网站，通常是<strong>同时测试并列出众多监测点到网站的主要速度指标</strong>(如解析时间、连接时间、下载速度等)，一般<strong>不用于</strong>检测网站代码及服务器性能优化的情况，而是主要用于选购服务器&#x2F;VPS、服务器&#x2F;CDN在各地的网速排查、CDN部署等。</p>\n<ul>\n<li><a href=\"https://www.17ce.com/\"><strong>17ce</strong></a> <em>(国内网站)</em><ul>\n<li>60+个国内及香港监测点,适用于<strong>国内</strong>各地访问网站的速度测试</li>\n<li>提供 Get, Ping, 路由追踪, Dns, Cdn等多维度速度测试</li>\n<li>提供监控API(付费)</li>\n</ul>\n</li>\n<li><a href=\"http://www.webkaka.com/\"><strong>卡卡网 Webkaka</strong></a> <em>(国内网站)</em><ul>\n<li>60+个国内监测点, 30+个海外监测点，适用于<strong>国内</strong>或<strong>全球</strong>各地访问网站的速度测试</li>\n<li>提供 Get, ping, 路由追踪, Dns等多维度速度测试</li>\n<li>提供网站速度诊断功能</li>\n</ul>\n</li>\n<li><a href=\"https://performance.sucuri.net/\"><strong>Sucuri Load Time Tester</strong></a> <em>(国外网站)</em><ul>\n<li>15+个全球监测点, 适用于<strong>全球</strong>各地访问网站(如外贸网站)的速度测试</li>\n<li>提供网页连接时间、首字节时间、总时间等数据</li>\n</ul>\n</li>\n<li><a href=\"https://www.dotcom-tools.com/website-speed-test.aspx\"><strong>Dotcom-Tools Website Speed Test</strong></a> <em>(国外网站)</em><ul>\n<li>20+个全球监测点, 适用于<strong>全球</strong>各地访问网站(如外贸网站)的速度测试</li>\n<li>提供两次(首次及重复访问)网页加载速度，且可按节点查看详细信息</li>\n</ul>\n</li>\n</ul>\n<p><strong>一、Think with Google</strong></p>\n<p>工具地址：<a href=\"http://www.thinkwithgoogle.com/feature/testmysite/\">www.thinkwithgoogle.com/feature/testmysite/</a></p>\n<p>这是谷歌推出专门针对移动端访问速测试的工具，该工具模似4G访问网站，而结果也会出现相对应的优化建议，与竞争对手的打开速度比较，这是一款针对TO C移动端用户的专业测速工具。</p>\n<p><strong>二、Webuup</strong></p>\n<p>工具地址：<a href=\"http://www.webuup.com/\">www.webuup.com</a></p>\n<p>Webuup是新出的免费的在线网站质量检测工具，目前知道它的人还不是很多，但它的功能却十分强大，非常好用。它不仅能根据各项关键指标，如页面性能、移动表现、SEO优化和安全隐私来判断网站的质量，还能给出网站优化和提升的建议，Webuup检测工具，可以在页面速度检测栏，清晰的显示全球不同的测速区域与测速服务器状态下，该网站首字节返回时间、页面展示延迟时间、页面完整加载时间。</p>\n<p><strong>三、PageSpeed Insights</strong></p>\n<p>工具地址：pagespeed.web.dev</p>\n<p>PageSpeed这款测速工具出自Google，它可以直接总结出测试网站所存在的问题，并提出相应的优化建议，且指示很清晰，而最近谷歌推出的页面体验算法，也是根据这里的三项页面指标（LCP、FID、CLS）优化，此外，PageSpeed的测速包括移动设备和桌面设备两方面，并分别从这两个不同的设备端提出了提升网站加载速度的建议，用起来十分方便。</p>\n<p><strong>四、Gtmetrix</strong></p>\n<p>工具地址：gtmetrix.com</p>\n<p>GTmetrix现在由Lighthouse提供技术支持，有网站打开速度和网站性能评测工具，也成为网站用户体验的检测工具，Lighthouse是Google提供的一种开源工具，可以通过多种方式对网页进行评估。它被认为是现代Web性能指标的标准，因此将Lighthouse分析和性能数据集成到GTmetrix中，所以也是这次我们放在推荐网站速度测试工具之一。</p>\n<p><strong>五、Dotcom-tools</strong></p>\n<p>工具地址：<a href=\"http://www.dotcom-tools.com/\">www.dotcom-tools.com</a></p>\n<p>Dotcom网站全球访问测试工具，主要是分析外贸网站在全球各地的速度测试，输入网址后，会出现每个地区节点的载入时间、重复测试后的载入时间、下载页面大小及遭遇到的错误记录，可以查看页面大小、加载时间和下载速度等前端元素，还可以查看HTML、JavaScript和CSS等后端元素，可以对网站针对性优化。</p>\n<p>​\t\t以上五款是不错的免费网站打开速度测试工具，也比较实用，当然还有一些其他的测试工具，但这五款工具已经够用了。正常一个网站，打开速度应该在2~3秒是比较理想的，同时大家在优化好web端的打开速度时，也要注重对于移动端的优化。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h3 id=\"常用国内外网站测速及性能分析工具\"><a href=\"#常用国内外网站测速及性能分析工具\" class=\"headerlink\" title=\"常用国内外网站测速及性能分析工具\"></a>常用国内外网站测速及性能分析工具</h3><p>网站多地测速工具&#x2F;网站，通常是<strong>同时测试并列出众多监测点到网站的主要速度指标</strong>(如解析时间、连接时间、下载速度等)，一般<strong>不用于</strong>检测网站代码及服务器性能优化的情况，而是主要用于选购服务器&#x2F;VPS、服务器&#x2F;CDN在各地的网速排查、CDN部署等。</p>\n<ul>\n<li><a href=\"https://www.17ce.com/\"><strong>17ce</strong></a> <em>(国内网站)</em><ul>\n<li>60+个国内及香港监测点,适用于<strong>国内</strong>各地访问网站的速度测试</li>\n<li>提供 Get, Ping, 路由追踪, Dns, Cdn等多维度速度测试</li>\n<li>提供监控API(付费)</li>\n</ul>\n</li>\n<li><a href=\"http://www.webkaka.com/\"><strong>卡卡网 Webkaka</strong></a> <em>(国内网站)</em><ul>\n<li>60+个国内监测点, 30+个海外监测点，适用于<strong>国内</strong>或<strong>全球</strong>各地访问网站的速度测试</li>\n<li>提供 Get, ping, 路由追踪, Dns等多维度速度测试</li>\n<li>提供网站速度诊断功能</li>\n</ul>\n</li>\n<li><a href=\"https://performance.sucuri.net/\"><strong>Sucuri Load Time Tester</strong></a> <em>(国外网站)</em><ul>\n<li>15+个全球监测点, 适用于<strong>全球</strong>各地访问网站(如外贸网站)的速度测试</li>\n<li>提供网页连接时间、首字节时间、总时间等数据</li>\n</ul>\n</li>\n<li><a href=\"https://www.dotcom-tools.com/website-speed-test.aspx\"><strong>Dotcom-Tools Website Speed Test</strong></a> <em>(国外网站)</em><ul>\n<li>20+个全球监测点, 适用于<strong>全球</strong>各地访问网站(如外贸网站)的速度测试</li>\n<li>提供两次(首次及重复访问)网页加载速度，且可按节点查看详细信息</li>\n</ul>\n</li>\n</ul>\n<p><strong>一、Think with Google</strong></p>\n<p>工具地址：<a href=\"http://www.thinkwithgoogle.com/feature/testmysite/\">www.thinkwithgoogle.com/feature/testmysite/</a></p>\n<p>这是谷歌推出专门针对移动端访问速测试的工具，该工具模似4G访问网站，而结果也会出现相对应的优化建议，与竞争对手的打开速度比较，这是一款针对TO C移动端用户的专业测速工具。</p>\n<p><strong>二、Webuup</strong></p>\n<p>工具地址：<a href=\"http://www.webuup.com/\">www.webuup.com</a></p>\n<p>Webuup是新出的免费的在线网站质量检测工具，目前知道它的人还不是很多，但它的功能却十分强大，非常好用。它不仅能根据各项关键指标，如页面性能、移动表现、SEO优化和安全隐私来判断网站的质量，还能给出网站优化和提升的建议，Webuup检测工具，可以在页面速度检测栏，清晰的显示全球不同的测速区域与测速服务器状态下，该网站首字节返回时间、页面展示延迟时间、页面完整加载时间。</p>\n<p><strong>三、PageSpeed Insights</strong></p>\n<p>工具地址：pagespeed.web.dev</p>\n<p>PageSpeed这款测速工具出自Google，它可以直接总结出测试网站所存在的问题，并提出相应的优化建议，且指示很清晰，而最近谷歌推出的页面体验算法，也是根据这里的三项页面指标（LCP、FID、CLS）优化，此外，PageSpeed的测速包括移动设备和桌面设备两方面，并分别从这两个不同的设备端提出了提升网站加载速度的建议，用起来十分方便。</p>\n<p><strong>四、Gtmetrix</strong></p>\n<p>工具地址：gtmetrix.com</p>\n<p>GTmetrix现在由Lighthouse提供技术支持，有网站打开速度和网站性能评测工具，也成为网站用户体验的检测工具，Lighthouse是Google提供的一种开源工具，可以通过多种方式对网页进行评估。它被认为是现代Web性能指标的标准，因此将Lighthouse分析和性能数据集成到GTmetrix中，所以也是这次我们放在推荐网站速度测试工具之一。</p>\n<p><strong>五、Dotcom-tools</strong></p>\n<p>工具地址：<a href=\"http://www.dotcom-tools.com/\">www.dotcom-tools.com</a></p>\n<p>Dotcom网站全球访问测试工具，主要是分析外贸网站在全球各地的速度测试，输入网址后，会出现每个地区节点的载入时间、重复测试后的载入时间、下载页面大小及遭遇到的错误记录，可以查看页面大小、加载时间和下载速度等前端元素，还可以查看HTML、JavaScript和CSS等后端元素，可以对网站针对性优化。</p>\n<p>​\t\t以上五款是不错的免费网站打开速度测试工具，也比较实用，当然还有一些其他的测试工具，但这五款工具已经够用了。正常一个网站，打开速度应该在2~3秒是比较理想的，同时大家在优化好web端的打开速度时，也要注重对于移动端的优化。</p>\n"},{"title":"WSL中的骚操作","abbrlink":24289,"date":"2022-06-27T06:27:44.000Z","updated":"2022-06-27T06:27:44.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 配置oh-my-zsh\n\n- 启用zsh，并配上一系列插件，可以极大的提升工作效率。\n\n  ```shell\n  plugins=(z vi-mode zsh-completions web-search git zsh-autosuggestions zsh-syntax-highlighting rand-quote themes cp)\n  ```\n\n  特别是z 、 zsh-completions、zsh-autosuggestions、git都是特别好用的神器。\n\n\n\n\n\n# 关于Hexo\n\n​\t\t由于Linux出色的命令行终端体验，在Linux中部署Hexo静态博客比Windows方便太多了，在加上一些骚操作，体验非常完美！！！\n\n- 配置一些hexo相关的快捷键（zsh）\n\n  ```shell\n  # alias hexo\n  alias hd=\"hexo clean && hexo g && hexo d\"\n  # alias hs=\"hexo clean && hexo g && hexo d && hexo s\"\n  alias hs=\"hexo clean && hexo g && hexo s\"\n  alias hnp=\"hexo new post $1\"\n  ```\n\n  敲上两个字母，就可以完成hexo的远程部署或者本地调试，体验非常良好。\n\n- 借助Windows和Hexo的互操作性，配置typora用linux命令启动\n\n  ```shell\n  # alias windows app\n  alias tp='func() { /mnt/d/Typora/Typora.exe $1 &;}; func'\n  ```\n\n  敲下tp，就可以直接启动typora.exe，在windows环境下书写markdown了。\n","source":"_posts/WSL中的骚操作.md","raw":"---\ntitle: WSL中的骚操作\ntags:\n  - WSL\n  - Linxu\n  - hexo\ncategories:\n  - - Linux\nabbrlink: 24289\ndate: 2022-06-27 14:27:44\nupdated: 2022-06-27 14:27:44\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 配置oh-my-zsh\n\n- 启用zsh，并配上一系列插件，可以极大的提升工作效率。\n\n  ```shell\n  plugins=(z vi-mode zsh-completions web-search git zsh-autosuggestions zsh-syntax-highlighting rand-quote themes cp)\n  ```\n\n  特别是z 、 zsh-completions、zsh-autosuggestions、git都是特别好用的神器。\n\n\n\n\n\n# 关于Hexo\n\n​\t\t由于Linux出色的命令行终端体验，在Linux中部署Hexo静态博客比Windows方便太多了，在加上一些骚操作，体验非常完美！！！\n\n- 配置一些hexo相关的快捷键（zsh）\n\n  ```shell\n  # alias hexo\n  alias hd=\"hexo clean && hexo g && hexo d\"\n  # alias hs=\"hexo clean && hexo g && hexo d && hexo s\"\n  alias hs=\"hexo clean && hexo g && hexo s\"\n  alias hnp=\"hexo new post $1\"\n  ```\n\n  敲上两个字母，就可以完成hexo的远程部署或者本地调试，体验非常良好。\n\n- 借助Windows和Hexo的互操作性，配置typora用linux命令启动\n\n  ```shell\n  # alias windows app\n  alias tp='func() { /mnt/d/Typora/Typora.exe $1 &;}; func'\n  ```\n\n  敲下tp，就可以直接启动typora.exe，在windows环境下书写markdown了。\n","slug":"WSL中的骚操作","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksu000f8j5mgr6xb4x5","content":"<h2 id=\"配置oh-my-zsh\"><a href=\"#配置oh-my-zsh\" class=\"headerlink\" title=\"配置oh-my-zsh\"></a>配置oh-my-zsh</h2><ul>\n<li><p>启用zsh，并配上一系列插件，可以极大的提升工作效率。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(z vi-mode zsh-completions web-search git zsh-autosuggestions zsh-syntax-highlighting rand-quote themes cp)</span><br></pre></td></tr></table></figure>\n\n<p>特别是z 、 zsh-completions、zsh-autosuggestions、git都是特别好用的神器。</p>\n</li>\n</ul>\n<h1 id=\"关于Hexo\"><a href=\"#关于Hexo\" class=\"headerlink\" title=\"关于Hexo\"></a>关于Hexo</h1><p>​\t\t由于Linux出色的命令行终端体验，在Linux中部署Hexo静态博客比Windows方便太多了，在加上一些骚操作，体验非常完美！！！</p>\n<ul>\n<li><p>配置一些hexo相关的快捷键（zsh）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> hexo</span></span><br><span class=\"line\">alias hd=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d&quot;</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> hs=<span class=\"string\">&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d &amp;&amp; hexo s&quot;</span></span></span><br><span class=\"line\">alias hs=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot;</span><br><span class=\"line\">alias hnp=&quot;hexo new post $1&quot;</span><br></pre></td></tr></table></figure>\n\n<p>敲上两个字母，就可以完成hexo的远程部署或者本地调试，体验非常良好。</p>\n</li>\n<li><p>借助Windows和Hexo的互操作性，配置typora用linux命令启动</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> windows app</span></span><br><span class=\"line\">alias tp=&#x27;func() &#123; /mnt/d/Typora/Typora.exe $1 &amp;;&#125;; func&#x27;</span><br></pre></td></tr></table></figure>\n\n<p>敲下tp，就可以直接启动typora.exe，在windows环境下书写markdown了。</p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"配置oh-my-zsh\"><a href=\"#配置oh-my-zsh\" class=\"headerlink\" title=\"配置oh-my-zsh\"></a>配置oh-my-zsh</h2><ul>\n<li><p>启用zsh，并配上一系列插件，可以极大的提升工作效率。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(z vi-mode zsh-completions web-search git zsh-autosuggestions zsh-syntax-highlighting rand-quote themes cp)</span><br></pre></td></tr></table></figure>\n\n<p>特别是z 、 zsh-completions、zsh-autosuggestions、git都是特别好用的神器。</p>\n</li>\n</ul>\n<h1 id=\"关于Hexo\"><a href=\"#关于Hexo\" class=\"headerlink\" title=\"关于Hexo\"></a>关于Hexo</h1><p>​\t\t由于Linux出色的命令行终端体验，在Linux中部署Hexo静态博客比Windows方便太多了，在加上一些骚操作，体验非常完美！！！</p>\n<ul>\n<li><p>配置一些hexo相关的快捷键（zsh）</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> hexo</span></span><br><span class=\"line\">alias hd=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d&quot;</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> hs=<span class=\"string\">&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d &amp;&amp; hexo s&quot;</span></span></span><br><span class=\"line\">alias hs=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot;</span><br><span class=\"line\">alias hnp=&quot;hexo new post $1&quot;</span><br></pre></td></tr></table></figure>\n\n<p>敲上两个字母，就可以完成hexo的远程部署或者本地调试，体验非常良好。</p>\n</li>\n<li><p>借助Windows和Hexo的互操作性，配置typora用linux命令启动</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\"><span class=\"built_in\">alias</span> windows app</span></span><br><span class=\"line\">alias tp=&#x27;func() &#123; /mnt/d/Typora/Typora.exe $1 &amp;;&#125;; func&#x27;</span><br></pre></td></tr></table></figure>\n\n<p>敲下tp，就可以直接启动typora.exe，在windows环境下书写markdown了。</p>\n</li>\n</ul>\n"},{"title":"hexo+freenom+cloudflare遇到的一些坑","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","keywords":"freenom、CloudFlare、clash","abbrlink":6631,"date":"2022-06-27T03:30:32.000Z","updated":"2022-06-27T03:30:32.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"_content":"\n\n\n#### 使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问\n\n> ​\t\t原因就是 CloudFlare到 GitHub Pages这段 回源没有采用 TLS访问，解决的办法也很简单，在 CloudFlare中找到 SSL/TLS中的 概述，把默认的 灵活（加密浏览器与 Cloudflare 之间的流量）改为 完全（端到端加密，使用服务器上的自签名证书）即可。\n\n\n\n#### CFW（Clash For Windows）TUN 模式\n\n> 对于不遵循系统代理的软件，TUN 模式可以接管其流量并交由 CFW 处理，在 Windows 中，TUN 模式性能比 TAP 模式好\n\n> **NOTICE**\n\n> 近期大部分浏览器默认已经开启“**安全 DNS**”功能，此功能会影响 TUN 模式劫持 DNS 请求导致反推域名失败，请在浏览器设置中关闭此功能以保证 TUN 模式正常运行\n\n\n\n","source":"_posts/hexo+freenom+cloudflare遇到的一些坑.md","raw":"---\ntitle: hexo+freenom+cloudflare遇到的一些坑\ntags:\n  - hexo\n  - github pages\n  - CFW\ncategories:\n  - hexo\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nkeywords: freenom、CloudFlare、clash\nabbrlink: 6631\ndate: 2022-06-27 11:30:32\nupdated: 2022-06-27 11:30:32\ncover:\ndescription:\n---\n\n\n\n#### 使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问\n\n> ​\t\t原因就是 CloudFlare到 GitHub Pages这段 回源没有采用 TLS访问，解决的办法也很简单，在 CloudFlare中找到 SSL/TLS中的 概述，把默认的 灵活（加密浏览器与 Cloudflare 之间的流量）改为 完全（端到端加密，使用服务器上的自签名证书）即可。\n\n\n\n#### CFW（Clash For Windows）TUN 模式\n\n> 对于不遵循系统代理的软件，TUN 模式可以接管其流量并交由 CFW 处理，在 Windows 中，TUN 模式性能比 TAP 模式好\n\n> **NOTICE**\n\n> 近期大部分浏览器默认已经开启“**安全 DNS**”功能，此功能会影响 TUN 模式劫持 DNS 请求导致反推域名失败，请在浏览器设置中关闭此功能以保证 TUN 模式正常运行\n\n\n\n","slug":"hexo+freenom+cloudflare遇到的一些坑","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksu000h8j5mcag0ee87","content":"<h4 id=\"使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github-pages无法访问\"><a href=\"#使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github-pages无法访问\" class=\"headerlink\" title=\"使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问\"></a>使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问</h4><blockquote>\n<p>​\t\t原因就是 CloudFlare到 GitHub Pages这段 回源没有采用 TLS访问，解决的办法也很简单，在 CloudFlare中找到 SSL&#x2F;TLS中的 概述，把默认的 灵活（加密浏览器与 Cloudflare 之间的流量）改为 完全（端到端加密，使用服务器上的自签名证书）即可。</p>\n</blockquote>\n<h4 id=\"CFW（Clash-For-Windows）TUN-模式\"><a href=\"#CFW（Clash-For-Windows）TUN-模式\" class=\"headerlink\" title=\"CFW（Clash For Windows）TUN 模式\"></a>CFW（Clash For Windows）TUN 模式</h4><blockquote>\n<p>对于不遵循系统代理的软件，TUN 模式可以接管其流量并交由 CFW 处理，在 Windows 中，TUN 模式性能比 TAP 模式好</p>\n</blockquote>\n<blockquote>\n<p><strong>NOTICE</strong></p>\n</blockquote>\n<blockquote>\n<p>近期大部分浏览器默认已经开启“<strong>安全 DNS</strong>”功能，此功能会影响 TUN 模式劫持 DNS 请求导致反推域名失败，请在浏览器设置中关闭此功能以保证 TUN 模式正常运行</p>\n</blockquote>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h4 id=\"使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github-pages无法访问\"><a href=\"#使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github-pages无法访问\" class=\"headerlink\" title=\"使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问\"></a>使用CloudFlare进行DNS解析，并启用CloudFlare的代理和CDN后，github pages无法访问</h4><blockquote>\n<p>​\t\t原因就是 CloudFlare到 GitHub Pages这段 回源没有采用 TLS访问，解决的办法也很简单，在 CloudFlare中找到 SSL&#x2F;TLS中的 概述，把默认的 灵活（加密浏览器与 Cloudflare 之间的流量）改为 完全（端到端加密，使用服务器上的自签名证书）即可。</p>\n</blockquote>\n<h4 id=\"CFW（Clash-For-Windows）TUN-模式\"><a href=\"#CFW（Clash-For-Windows）TUN-模式\" class=\"headerlink\" title=\"CFW（Clash For Windows）TUN 模式\"></a>CFW（Clash For Windows）TUN 模式</h4><blockquote>\n<p>对于不遵循系统代理的软件，TUN 模式可以接管其流量并交由 CFW 处理，在 Windows 中，TUN 模式性能比 TAP 模式好</p>\n</blockquote>\n<blockquote>\n<p><strong>NOTICE</strong></p>\n</blockquote>\n<blockquote>\n<p>近期大部分浏览器默认已经开启“<strong>安全 DNS</strong>”功能，此功能会影响 TUN 模式劫持 DNS 请求导致反推域名失败，请在浏览器设置中关闭此功能以保证 TUN 模式正常运行</p>\n</blockquote>\n"},{"title":"DNS命令指南。怎么验证是否遭遇DNS污染？查看域名是否解析成功？","keywords":"DNS","abbrlink":6111,"date":"2022-06-27T10:24:43.000Z","updated":"2022-06-27T10:24:43.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"description":null,"_content":"\n# DNS指南\n\n## 查询DNS服务器\n\n- linux：```cat /etc/resolv.conf``` \n\n- windows: ipconfig /all\n\n  ```\n  PS C:\\Users\\sssbb> ipconfig /all\n  \n  Windows IP 配置\n  \n     主机名  . . . . . . . . . . . . . : DESKTOP-KD33OT8\n     主 DNS 后缀 . . . . . . . . . . . :\n     节点类型  . . . . . . . . . . . . : 混合\n     IP 路由已启用 . . . . . . . . . . : 否\n     WINS 代理已启用 . . . . . . . . . : 否\n  \n  未知适配器 Clash:\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Clash Tunnel\n     物理地址. . . . . . . . . . . . . :\n     DHCP 已启用 . . . . . . . . . . . : 否\n     自动配置已启用. . . . . . . . . . : 是\n     IPv4 地址 . . . . . . . . . . . . : 198.18.0.1(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.0.0\n     默认网关. . . . . . . . . . . . . :\n     DNS 服务器  . . . . . . . . . . . : 198.18.0.2\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  \n  无线局域网适配器 本地连接* 1:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter\n     物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-BE-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  无线局域网适配器 本地连接* 10:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter #2\n     物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-B6-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  无线局域网适配器 WLAN:\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Dell Wireless 1830 802.11ac\n     物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n     本地链接 IPv6 地址. . . . . . . . : fe80::5d17:d6:2915:d43c%3(首选)\n     IPv4 地址 . . . . . . . . . . . . : 192.168.31.250(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.255.0\n     获得租约的时间  . . . . . . . . . : 2022年6月27日 13:00:31\n     租约过期的时间  . . . . . . . . . : 2022年6月28日 1:00:34\n     默认网关. . . . . . . . . . . . . : 192.168.31.1\n     DHCP 服务器 . . . . . . . . . . . : 192.168.31.1\n     DHCPv6 IAID . . . . . . . . . . . : 51924786\n     DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99\n     DNS 服务器  . . . . . . . . . . . : 192.168.31.1\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  \n  以太网适配器 蓝牙网络连接:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Bluetooth Device (Personal Area Network)\n     物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-9A\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  以太网适配器 vEthernet (WSL):\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Hyper-V Virtual Ethernet Adapter\n     物理地址. . . . . . . . . . . . . : 00-15-5D-5C-2A-EA\n     DHCP 已启用 . . . . . . . . . . . : 否\n     自动配置已启用. . . . . . . . . . : 是\n     本地链接 IPv6 地址. . . . . . . . : fe80::d82d:5ba6:7b4b:9023%40(首选)\n     IPv4 地址 . . . . . . . . . . . . : 172.17.112.1(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.240.0\n     默认网关. . . . . . . . . . . . . :\n     DHCPv6 IAID . . . . . . . . . . . : 671094109\n     DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99\n     DNS 服务器  . . . . . . . . . . . : fec0:0:0:ffff::1%1\n                                         fec0:0:0:ffff::2%1\n                                         fec0:0:0:ffff::3%1\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  ```\n\n   \n\n## 查看域名是否解析成功：\n\n- 可以直接ping域名，也可以使用nslookup命令（NameServer Lookup）\n\n- 在用 nslookup 查询一个域名时，可能会看到有“非权威应答” 的提示，非权威应答（Non-authoritative answer）意味着answer来自于其他服务器的缓存，而不是权威的服务器（就是该域名配置的DNS解析服务器，如果你的域名解析配置在CF的DNS上，则权威服务器，就是CF的DNS）。缓存会根据 ttl（Time to Live）的值定时的进行更新。\n\n  ```\n  ➜  hexo-blog nslookup wohensha.tk 8.8.8.8\n  Server:         8.8.8.8\n  Address:        8.8.8.8#53\n  \n  Non-authoritative answer:\n  Name:   wohensha.tk\n  Address: 104.21.49.190\n  Name:   wohensha.tk\n  Address: 172.67.165.231\n  Name:   wohensha.tk\n  Address: 2606:4700:3031::ac43:a5e7\n  Name:   wohensha.tk\n  Address: 2606:4700:3031::6815:31be\n  ```\n\n- sds查找权威名字服务器\n\n  ```\n  ➜  hexo-blog nslookup -ty=ns clashdingyue.tk\n  Server:         172.17.112.1\n  Address:        172.17.112.1#53\n  \n  Non-authoritative answer:\n  clashdingyue.tk nameserver = ns04.freenom.com.\n  clashdingyue.tk nameserver = ns02.freenom.com.\n  clashdingyue.tk nameserver = ns01.freenom.com.\n  clashdingyue.tk nameserver = ns03.freenom.com.\n  \n  Authoritative answers can be found from:\n  \n  ➜  hexo-blog nslookup -ty=ns clashdingyue.tk ns04.freenom.com\n  Server:         ns04.freenom.com\n  Address:        104.155.29.241#53\n  \n  clashdingyue.tk nameserver = ns04.freenom.com.\n  clashdingyue.tk nameserver = ns03.freenom.com.\n  clashdingyue.tk nameserver = ns02.freenom.com.\n  clashdingyue.tk nameserver = ns01.freenom.com.\n  ```\n\n\n\n# 一些小技巧\n\n### 怎么验证是否遭遇DNS污染？\n\n​\t\t**DNS污染**即网域服务器缓存污染，又称域名服务器缓存投毒，是指一些刻意制造或无意中制造出来的域名服务器数据包，把域名指往不正确的IP地址。一般来说，在互联网上都有可信赖的网域服务器，但为减低网络上的流量压力，一般的域名服务器都会把从上游的域名服务器获得的解析记录暂存起来，待下次有其他机器要求解析域名时，可以立即提供服务。一旦有关网域的局域域名服务器的缓存受到污染，就会把网域内的计算机导引往错误的服务器。\n\n   \t我们应该怎么去验证自己域名是否遭遇DNS污染呢？输入命令dig +trace clashdingyue.tk（您自己需要检测域名）。**如果域名未被污染我们会得到权威DNS的应答**，如下所示:\n\n```cobol\n➜  hexo-blog dig +trace clashdingyue.tk\n;; Warning: Client COOKIE mismatch\n\n; <<>> DiG 9.16.1-Ubuntu <<>> +trace clashdingyue.tk\n;; global options: +cmd\n.                       200     IN      NS      g.root-servers.net.\n.                       200     IN      NS      j.root-servers.net.\n.                       200     IN      NS      d.root-servers.net.\n.                       200     IN      NS      h.root-servers.net.\n.                       200     IN      NS      m.root-servers.net.\n.                       200     IN      NS      k.root-servers.net.\n.                       200     IN      NS      a.root-servers.net.\n.                       200     IN      NS      i.root-servers.net.\n.                       200     IN      NS      b.root-servers.net.\n.                       200     IN      NS      f.root-servers.net.\n.                       200     IN      NS      c.root-servers.net.\n.                       200     IN      NS      l.root-servers.net.\n.                       200     IN      NS      e.root-servers.net.\n;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms\n\ntk.                     172800  IN      NS      a.ns.tk.\ntk.                     172800  IN      NS      b.ns.tk.\ntk.                     172800  IN      NS      c.ns.tk.\ntk.                     172800  IN      NS      d.ns.tk.\ntk.                     86400   IN      NSEC    tkmaxx. NS RRSIG NSEC\ntk.                     86400   IN      RRSIG   NSEC 8 1 86400 20220710050000 20220627040000 47671 . HwO7QYzt3lI0k1w10qjM7oUf0B71yWgbUu9yCPcUdUng1icIu0lXSebp thdZpvOpLrjTE461RZJSlYaKIPavphtjpQHnUVxlH3Qznw9cBhql9Qnx cEtMo7vlCkCRST9sojkQxRqFW1oQMOoGG1j+SWpejRYwaudILcDCl0bP 4nPu1t5KmGR3Q8DKKO075O69w8MTauU+yfOsxEPvYgmHGzIyU7pBMWyt sUA+5ZpnrQ+0KLcXxnpUPQpBb55RlO1PhRqlJ9bT8qfYfvT+QUL5alwl xJxyZVcLTlGrpggW76yWjN3gq3zzynmd3D5cGeFQSon1+qMR5i6LoQix b4Jycg==\n;; Received 602 bytes from 193.0.14.129#53(k.root-servers.net) in 350 ms\n\nclashdingyue.tk.        300     IN      NS      ns01.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns02.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns03.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns04.freenom.com.\n;; Received 131 bytes from 194.0.41.1#53(d.ns.tk) in 330 ms\n\nclashdingyue.tk.        3600    IN      A       185.199.108.153\nclashdingyue.tk.        3600    IN      A       185.199.110.153\nclashdingyue.tk.        3600    IN      A       185.199.109.153\nclashdingyue.tk.        3600    IN      A       185.199.111.153\nclashdingyue.tk.        300     IN      NS      ns03.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns04.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns02.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns01.freenom.com.\n;; Received 248 bytes from 54.171.131.39#53(ns01.freenom.com) in 470 ms\n```\n\n**如果域名被污染会直接到的一个IP，并不会向权威DNS请求。**如下所示：\n\n```\n➜  hexo-blog dig +trace google.com\n;; Warning: Client COOKIE mismatch\n\n; <<>> DiG 9.16.1-Ubuntu <<>> +trace google.com\n;; global options: +cmd\n.                       1450    IN      NS      f.root-servers.net.\n.                       1450    IN      NS      k.root-servers.net.\n.                       1450    IN      NS      d.root-servers.net.\n.                       1450    IN      NS      j.root-servers.net.\n.                       1450    IN      NS      l.root-servers.net.\n.                       1450    IN      NS      m.root-servers.net.\n.                       1450    IN      NS      h.root-servers.net.\n.                       1450    IN      NS      i.root-servers.net.\n.                       1450    IN      NS      e.root-servers.net.\n.                       1450    IN      NS      c.root-servers.net.\n.                       1450    IN      NS      a.root-servers.net.\n.                       1450    IN      NS      b.root-servers.net.\n.                       1450    IN      NS      g.root-servers.net.\n;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms\n\ngoogle.com.             60      IN      A       8.7.198.46\n;; Received 54 bytes from 192.33.4.12#53(c.root-servers.net) in 20 ms\n```\n\n","source":"_posts/DNS命令指南.md","raw":"---\ntitle: DNS命令指南。怎么验证是否遭遇DNS污染？查看域名是否解析成功？\ntags:\n  - dns\ncategories:\n  - - dns\n  - - linux\nkeywords: DNS\nabbrlink: 6111\ndate: 2022-06-27 18:24:43\nupdated: 2022-06-27 18:24:43\ncover:\ntop_img:\ndescription:\n---\n\n# DNS指南\n\n## 查询DNS服务器\n\n- linux：```cat /etc/resolv.conf``` \n\n- windows: ipconfig /all\n\n  ```\n  PS C:\\Users\\sssbb> ipconfig /all\n  \n  Windows IP 配置\n  \n     主机名  . . . . . . . . . . . . . : DESKTOP-KD33OT8\n     主 DNS 后缀 . . . . . . . . . . . :\n     节点类型  . . . . . . . . . . . . : 混合\n     IP 路由已启用 . . . . . . . . . . : 否\n     WINS 代理已启用 . . . . . . . . . : 否\n  \n  未知适配器 Clash:\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Clash Tunnel\n     物理地址. . . . . . . . . . . . . :\n     DHCP 已启用 . . . . . . . . . . . : 否\n     自动配置已启用. . . . . . . . . . : 是\n     IPv4 地址 . . . . . . . . . . . . : 198.18.0.1(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.0.0\n     默认网关. . . . . . . . . . . . . :\n     DNS 服务器  . . . . . . . . . . . : 198.18.0.2\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  \n  无线局域网适配器 本地连接* 1:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter\n     物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-BE-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  无线局域网适配器 本地连接* 10:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter #2\n     物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-B6-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  无线局域网适配器 WLAN:\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Dell Wireless 1830 802.11ac\n     物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-99\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n     本地链接 IPv6 地址. . . . . . . . : fe80::5d17:d6:2915:d43c%3(首选)\n     IPv4 地址 . . . . . . . . . . . . : 192.168.31.250(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.255.0\n     获得租约的时间  . . . . . . . . . : 2022年6月27日 13:00:31\n     租约过期的时间  . . . . . . . . . : 2022年6月28日 1:00:34\n     默认网关. . . . . . . . . . . . . : 192.168.31.1\n     DHCP 服务器 . . . . . . . . . . . : 192.168.31.1\n     DHCPv6 IAID . . . . . . . . . . . : 51924786\n     DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99\n     DNS 服务器  . . . . . . . . . . . : 192.168.31.1\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  \n  以太网适配器 蓝牙网络连接:\n  \n     媒体状态  . . . . . . . . . . . . : 媒体已断开连接\n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Bluetooth Device (Personal Area Network)\n     物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-9A\n     DHCP 已启用 . . . . . . . . . . . : 是\n     自动配置已启用. . . . . . . . . . : 是\n  \n  以太网适配器 vEthernet (WSL):\n  \n     连接特定的 DNS 后缀 . . . . . . . :\n     描述. . . . . . . . . . . . . . . : Hyper-V Virtual Ethernet Adapter\n     物理地址. . . . . . . . . . . . . : 00-15-5D-5C-2A-EA\n     DHCP 已启用 . . . . . . . . . . . : 否\n     自动配置已启用. . . . . . . . . . : 是\n     本地链接 IPv6 地址. . . . . . . . : fe80::d82d:5ba6:7b4b:9023%40(首选)\n     IPv4 地址 . . . . . . . . . . . . : 172.17.112.1(首选)\n     子网掩码  . . . . . . . . . . . . : 255.255.240.0\n     默认网关. . . . . . . . . . . . . :\n     DHCPv6 IAID . . . . . . . . . . . : 671094109\n     DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99\n     DNS 服务器  . . . . . . . . . . . : fec0:0:0:ffff::1%1\n                                         fec0:0:0:ffff::2%1\n                                         fec0:0:0:ffff::3%1\n     TCPIP 上的 NetBIOS  . . . . . . . : 已启用\n  ```\n\n   \n\n## 查看域名是否解析成功：\n\n- 可以直接ping域名，也可以使用nslookup命令（NameServer Lookup）\n\n- 在用 nslookup 查询一个域名时，可能会看到有“非权威应答” 的提示，非权威应答（Non-authoritative answer）意味着answer来自于其他服务器的缓存，而不是权威的服务器（就是该域名配置的DNS解析服务器，如果你的域名解析配置在CF的DNS上，则权威服务器，就是CF的DNS）。缓存会根据 ttl（Time to Live）的值定时的进行更新。\n\n  ```\n  ➜  hexo-blog nslookup wohensha.tk 8.8.8.8\n  Server:         8.8.8.8\n  Address:        8.8.8.8#53\n  \n  Non-authoritative answer:\n  Name:   wohensha.tk\n  Address: 104.21.49.190\n  Name:   wohensha.tk\n  Address: 172.67.165.231\n  Name:   wohensha.tk\n  Address: 2606:4700:3031::ac43:a5e7\n  Name:   wohensha.tk\n  Address: 2606:4700:3031::6815:31be\n  ```\n\n- sds查找权威名字服务器\n\n  ```\n  ➜  hexo-blog nslookup -ty=ns clashdingyue.tk\n  Server:         172.17.112.1\n  Address:        172.17.112.1#53\n  \n  Non-authoritative answer:\n  clashdingyue.tk nameserver = ns04.freenom.com.\n  clashdingyue.tk nameserver = ns02.freenom.com.\n  clashdingyue.tk nameserver = ns01.freenom.com.\n  clashdingyue.tk nameserver = ns03.freenom.com.\n  \n  Authoritative answers can be found from:\n  \n  ➜  hexo-blog nslookup -ty=ns clashdingyue.tk ns04.freenom.com\n  Server:         ns04.freenom.com\n  Address:        104.155.29.241#53\n  \n  clashdingyue.tk nameserver = ns04.freenom.com.\n  clashdingyue.tk nameserver = ns03.freenom.com.\n  clashdingyue.tk nameserver = ns02.freenom.com.\n  clashdingyue.tk nameserver = ns01.freenom.com.\n  ```\n\n\n\n# 一些小技巧\n\n### 怎么验证是否遭遇DNS污染？\n\n​\t\t**DNS污染**即网域服务器缓存污染，又称域名服务器缓存投毒，是指一些刻意制造或无意中制造出来的域名服务器数据包，把域名指往不正确的IP地址。一般来说，在互联网上都有可信赖的网域服务器，但为减低网络上的流量压力，一般的域名服务器都会把从上游的域名服务器获得的解析记录暂存起来，待下次有其他机器要求解析域名时，可以立即提供服务。一旦有关网域的局域域名服务器的缓存受到污染，就会把网域内的计算机导引往错误的服务器。\n\n   \t我们应该怎么去验证自己域名是否遭遇DNS污染呢？输入命令dig +trace clashdingyue.tk（您自己需要检测域名）。**如果域名未被污染我们会得到权威DNS的应答**，如下所示:\n\n```cobol\n➜  hexo-blog dig +trace clashdingyue.tk\n;; Warning: Client COOKIE mismatch\n\n; <<>> DiG 9.16.1-Ubuntu <<>> +trace clashdingyue.tk\n;; global options: +cmd\n.                       200     IN      NS      g.root-servers.net.\n.                       200     IN      NS      j.root-servers.net.\n.                       200     IN      NS      d.root-servers.net.\n.                       200     IN      NS      h.root-servers.net.\n.                       200     IN      NS      m.root-servers.net.\n.                       200     IN      NS      k.root-servers.net.\n.                       200     IN      NS      a.root-servers.net.\n.                       200     IN      NS      i.root-servers.net.\n.                       200     IN      NS      b.root-servers.net.\n.                       200     IN      NS      f.root-servers.net.\n.                       200     IN      NS      c.root-servers.net.\n.                       200     IN      NS      l.root-servers.net.\n.                       200     IN      NS      e.root-servers.net.\n;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms\n\ntk.                     172800  IN      NS      a.ns.tk.\ntk.                     172800  IN      NS      b.ns.tk.\ntk.                     172800  IN      NS      c.ns.tk.\ntk.                     172800  IN      NS      d.ns.tk.\ntk.                     86400   IN      NSEC    tkmaxx. NS RRSIG NSEC\ntk.                     86400   IN      RRSIG   NSEC 8 1 86400 20220710050000 20220627040000 47671 . HwO7QYzt3lI0k1w10qjM7oUf0B71yWgbUu9yCPcUdUng1icIu0lXSebp thdZpvOpLrjTE461RZJSlYaKIPavphtjpQHnUVxlH3Qznw9cBhql9Qnx cEtMo7vlCkCRST9sojkQxRqFW1oQMOoGG1j+SWpejRYwaudILcDCl0bP 4nPu1t5KmGR3Q8DKKO075O69w8MTauU+yfOsxEPvYgmHGzIyU7pBMWyt sUA+5ZpnrQ+0KLcXxnpUPQpBb55RlO1PhRqlJ9bT8qfYfvT+QUL5alwl xJxyZVcLTlGrpggW76yWjN3gq3zzynmd3D5cGeFQSon1+qMR5i6LoQix b4Jycg==\n;; Received 602 bytes from 193.0.14.129#53(k.root-servers.net) in 350 ms\n\nclashdingyue.tk.        300     IN      NS      ns01.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns02.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns03.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns04.freenom.com.\n;; Received 131 bytes from 194.0.41.1#53(d.ns.tk) in 330 ms\n\nclashdingyue.tk.        3600    IN      A       185.199.108.153\nclashdingyue.tk.        3600    IN      A       185.199.110.153\nclashdingyue.tk.        3600    IN      A       185.199.109.153\nclashdingyue.tk.        3600    IN      A       185.199.111.153\nclashdingyue.tk.        300     IN      NS      ns03.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns04.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns02.freenom.com.\nclashdingyue.tk.        300     IN      NS      ns01.freenom.com.\n;; Received 248 bytes from 54.171.131.39#53(ns01.freenom.com) in 470 ms\n```\n\n**如果域名被污染会直接到的一个IP，并不会向权威DNS请求。**如下所示：\n\n```\n➜  hexo-blog dig +trace google.com\n;; Warning: Client COOKIE mismatch\n\n; <<>> DiG 9.16.1-Ubuntu <<>> +trace google.com\n;; global options: +cmd\n.                       1450    IN      NS      f.root-servers.net.\n.                       1450    IN      NS      k.root-servers.net.\n.                       1450    IN      NS      d.root-servers.net.\n.                       1450    IN      NS      j.root-servers.net.\n.                       1450    IN      NS      l.root-servers.net.\n.                       1450    IN      NS      m.root-servers.net.\n.                       1450    IN      NS      h.root-servers.net.\n.                       1450    IN      NS      i.root-servers.net.\n.                       1450    IN      NS      e.root-servers.net.\n.                       1450    IN      NS      c.root-servers.net.\n.                       1450    IN      NS      a.root-servers.net.\n.                       1450    IN      NS      b.root-servers.net.\n.                       1450    IN      NS      g.root-servers.net.\n;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms\n\ngoogle.com.             60      IN      A       8.7.198.46\n;; Received 54 bytes from 192.33.4.12#53(c.root-servers.net) in 20 ms\n```\n\n","slug":"DNS命令指南","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksv000j8j5m1iol1841","content":"<h1 id=\"DNS指南\"><a href=\"#DNS指南\" class=\"headerlink\" title=\"DNS指南\"></a>DNS指南</h1><h2 id=\"查询DNS服务器\"><a href=\"#查询DNS服务器\" class=\"headerlink\" title=\"查询DNS服务器\"></a>查询DNS服务器</h2><ul>\n<li><p>linux：<code>cat /etc/resolv.conf</code> </p>\n</li>\n<li><p>windows: ipconfig &#x2F;all</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PS C:\\Users\\sssbb&gt; ipconfig /all</span><br><span class=\"line\"></span><br><span class=\"line\">Windows IP 配置</span><br><span class=\"line\"></span><br><span class=\"line\">   主机名  . . . . . . . . . . . . . : DESKTOP-KD33OT8</span><br><span class=\"line\">   主 DNS 后缀 . . . . . . . . . . . :</span><br><span class=\"line\">   节点类型  . . . . . . . . . . . . : 混合</span><br><span class=\"line\">   IP 路由已启用 . . . . . . . . . . : 否</span><br><span class=\"line\">   WINS 代理已启用 . . . . . . . . . : 否</span><br><span class=\"line\"></span><br><span class=\"line\">未知适配器 Clash:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Clash Tunnel</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . :</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 否</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 198.18.0.1(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.0.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . :</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : 198.18.0.2</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 本地连接* 1:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-BE-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 本地连接* 10:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter #2</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-B6-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 WLAN:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Dell Wireless 1830 802.11ac</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::5d17:d6:2915:d43c%3(首选)</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 192.168.31.250(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.255.0</span><br><span class=\"line\">   获得租约的时间  . . . . . . . . . : 2022年6月27日 13:00:31</span><br><span class=\"line\">   租约过期的时间  . . . . . . . . . : 2022年6月28日 1:00:34</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   DHCP 服务器 . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   DHCPv6 IAID . . . . . . . . . . . : 51924786</span><br><span class=\"line\">   DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br><span class=\"line\"></span><br><span class=\"line\">以太网适配器 蓝牙网络连接:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Bluetooth Device (Personal Area Network)</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-9A</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">以太网适配器 vEthernet (WSL):</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Hyper-V Virtual Ethernet Adapter</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 00-15-5D-5C-2A-EA</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 否</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::d82d:5ba6:7b4b:9023%40(首选)</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 172.17.112.1(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.240.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . :</span><br><span class=\"line\">   DHCPv6 IAID . . . . . . . . . . . : 671094109</span><br><span class=\"line\">   DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : fec0:0:0:ffff::1%1</span><br><span class=\"line\">                                       fec0:0:0:ffff::2%1</span><br><span class=\"line\">                                       fec0:0:0:ffff::3%1</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"查看域名是否解析成功：\"><a href=\"#查看域名是否解析成功：\" class=\"headerlink\" title=\"查看域名是否解析成功：\"></a>查看域名是否解析成功：</h2><ul>\n<li><p>可以直接ping域名，也可以使用nslookup命令（NameServer Lookup）</p>\n</li>\n<li><p>在用 nslookup 查询一个域名时，可能会看到有“非权威应答” 的提示，非权威应答（Non-authoritative answer）意味着answer来自于其他服务器的缓存，而不是权威的服务器（就是该域名配置的DNS解析服务器，如果你的域名解析配置在CF的DNS上，则权威服务器，就是CF的DNS）。缓存会根据 ttl（Time to Live）的值定时的进行更新。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup wohensha.tk 8.8.8.8</span><br><span class=\"line\">Server:         8.8.8.8</span><br><span class=\"line\">Address:        8.8.8.8#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 104.21.49.190</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 172.67.165.231</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 2606:4700:3031::ac43:a5e7</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 2606:4700:3031::6815:31be</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sds查找权威名字服务器</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup -ty=ns clashdingyue.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">clashdingyue.tk nameserver = ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns01.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns03.freenom.com.</span><br><span class=\"line\"></span><br><span class=\"line\">Authoritative answers can be found from:</span><br><span class=\"line\"></span><br><span class=\"line\">➜  hexo-blog nslookup -ty=ns clashdingyue.tk ns04.freenom.com</span><br><span class=\"line\">Server:         ns04.freenom.com</span><br><span class=\"line\">Address:        104.155.29.241#53</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk nameserver = ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns01.freenom.com.</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"一些小技巧\"><a href=\"#一些小技巧\" class=\"headerlink\" title=\"一些小技巧\"></a>一些小技巧</h1><h3 id=\"怎么验证是否遭遇DNS污染？\"><a href=\"#怎么验证是否遭遇DNS污染？\" class=\"headerlink\" title=\"怎么验证是否遭遇DNS污染？\"></a>怎么验证是否遭遇DNS污染？</h3><p>​\t\t<strong>DNS污染</strong>即网域服务器缓存污染，又称域名服务器缓存投毒，是指一些刻意制造或无意中制造出来的域名服务器数据包，把域名指往不正确的IP地址。一般来说，在互联网上都有可信赖的网域服务器，但为减低网络上的流量压力，一般的域名服务器都会把从上游的域名服务器获得的解析记录暂存起来，待下次有其他机器要求解析域名时，可以立即提供服务。一旦有关网域的局域域名服务器的缓存受到污染，就会把网域内的计算机导引往错误的服务器。</p>\n<pre><code>   我们应该怎么去验证自己域名是否遭遇DNS污染呢？输入命令dig +trace clashdingyue.tk（您自己需要检测域名）。**如果域名未被污染我们会得到权威DNS的应答**，如下所示:\n</code></pre>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog dig +trace clashdingyue.tk</span><br><span class=\"line\">;; Warning: Client COOKIE mismatch</span><br><span class=\"line\"></span><br><span class=\"line\">; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; +trace clashdingyue.tk</span><br><span class=\"line\">;; global options: +cmd</span><br><span class=\"line\">.                       200     IN      NS      g.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      j.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      d.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      h.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      m.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      k.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      a.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      i.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      b.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      f.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      c.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      l.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      e.root-servers.net.</span><br><span class=\"line\">;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms</span><br><span class=\"line\"></span><br><span class=\"line\">tk.                     172800  IN      NS      a.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      b.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      c.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      d.ns.tk.</span><br><span class=\"line\">tk.                     86400   IN      NSEC    tkmaxx. NS RRSIG NSEC</span><br><span class=\"line\">tk.                     86400   IN      RRSIG   NSEC 8 1 86400 20220710050000 20220627040000 47671 . HwO7QYzt3lI0k1w10qjM7oUf0B71yWgbUu9yCPcUdUng1icIu0lXSebp thdZpvOpLrjTE461RZJSlYaKIPavphtjpQHnUVxlH3Qznw9cBhql9Qnx cEtMo7vlCkCRST9sojkQxRqFW1oQMOoGG1j+SWpejRYwaudILcDCl0bP 4nPu1t5KmGR3Q8DKKO075O69w8MTauU+yfOsxEPvYgmHGzIyU7pBMWyt sUA+5ZpnrQ+0KLcXxnpUPQpBb55RlO1PhRqlJ9bT8qfYfvT+QUL5alwl xJxyZVcLTlGrpggW76yWjN3gq3zzynmd3D5cGeFQSon1+qMR5i6LoQix b4Jycg==</span><br><span class=\"line\">;; Received 602 bytes from 193.0.14.129#53(k.root-servers.net) in 350 ms</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns01.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns04.freenom.com.</span><br><span class=\"line\">;; Received 131 bytes from 194.0.41.1#53(d.ns.tk) in 330 ms</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.108.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.110.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.109.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.111.153</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns01.freenom.com.</span><br><span class=\"line\">;; Received 248 bytes from 54.171.131.39#53(ns01.freenom.com) in 470 ms</span><br></pre></td></tr></table></figure>\n\n<p><strong>如果域名被污染会直接到的一个IP，并不会向权威DNS请求。</strong>如下所示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog dig +trace google.com</span><br><span class=\"line\">;; Warning: Client COOKIE mismatch</span><br><span class=\"line\"></span><br><span class=\"line\">; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; +trace google.com</span><br><span class=\"line\">;; global options: +cmd</span><br><span class=\"line\">.                       1450    IN      NS      f.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      k.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      d.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      j.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      l.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      m.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      h.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      i.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      e.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      c.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      a.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      b.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      g.root-servers.net.</span><br><span class=\"line\">;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms</span><br><span class=\"line\"></span><br><span class=\"line\">google.com.             60      IN      A       8.7.198.46</span><br><span class=\"line\">;; Received 54 bytes from 192.33.4.12#53(c.root-servers.net) in 20 ms</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h1 id=\"DNS指南\"><a href=\"#DNS指南\" class=\"headerlink\" title=\"DNS指南\"></a>DNS指南</h1><h2 id=\"查询DNS服务器\"><a href=\"#查询DNS服务器\" class=\"headerlink\" title=\"查询DNS服务器\"></a>查询DNS服务器</h2><ul>\n<li><p>linux：<code>cat /etc/resolv.conf</code> </p>\n</li>\n<li><p>windows: ipconfig &#x2F;all</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PS C:\\Users\\sssbb&gt; ipconfig /all</span><br><span class=\"line\"></span><br><span class=\"line\">Windows IP 配置</span><br><span class=\"line\"></span><br><span class=\"line\">   主机名  . . . . . . . . . . . . . : DESKTOP-KD33OT8</span><br><span class=\"line\">   主 DNS 后缀 . . . . . . . . . . . :</span><br><span class=\"line\">   节点类型  . . . . . . . . . . . . : 混合</span><br><span class=\"line\">   IP 路由已启用 . . . . . . . . . . : 否</span><br><span class=\"line\">   WINS 代理已启用 . . . . . . . . . : 否</span><br><span class=\"line\"></span><br><span class=\"line\">未知适配器 Clash:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Clash Tunnel</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . :</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 否</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 198.18.0.1(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.0.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . :</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : 198.18.0.2</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 本地连接* 1:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-BE-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 本地连接* 10:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Microsoft Wi-Fi Direct Virtual Adapter #2</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 1A-4F-32-F7-B6-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">无线局域网适配器 WLAN:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Dell Wireless 1830 802.11ac</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-99</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::5d17:d6:2915:d43c%3(首选)</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 192.168.31.250(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.255.0</span><br><span class=\"line\">   获得租约的时间  . . . . . . . . . : 2022年6月27日 13:00:31</span><br><span class=\"line\">   租约过期的时间  . . . . . . . . . : 2022年6月28日 1:00:34</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   DHCP 服务器 . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   DHCPv6 IAID . . . . . . . . . . . : 51924786</span><br><span class=\"line\">   DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : 192.168.31.1</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br><span class=\"line\"></span><br><span class=\"line\">以太网适配器 蓝牙网络连接:</span><br><span class=\"line\"></span><br><span class=\"line\">   媒体状态  . . . . . . . . . . . . : 媒体已断开连接</span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Bluetooth Device (Personal Area Network)</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 18-4F-32-F7-BE-9A</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 是</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\"></span><br><span class=\"line\">以太网适配器 vEthernet (WSL):</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   描述. . . . . . . . . . . . . . . : Hyper-V Virtual Ethernet Adapter</span><br><span class=\"line\">   物理地址. . . . . . . . . . . . . : 00-15-5D-5C-2A-EA</span><br><span class=\"line\">   DHCP 已启用 . . . . . . . . . . . : 否</span><br><span class=\"line\">   自动配置已启用. . . . . . . . . . : 是</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::d82d:5ba6:7b4b:9023%40(首选)</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 172.17.112.1(首选)</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.240.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . :</span><br><span class=\"line\">   DHCPv6 IAID . . . . . . . . . . . : 671094109</span><br><span class=\"line\">   DHCPv6 客户端 DUID  . . . . . . . : 00-01-00-01-2A-3F-FD-A1-18-4F-32-F7-BE-99</span><br><span class=\"line\">   DNS 服务器  . . . . . . . . . . . : fec0:0:0:ffff::1%1</span><br><span class=\"line\">                                       fec0:0:0:ffff::2%1</span><br><span class=\"line\">                                       fec0:0:0:ffff::3%1</span><br><span class=\"line\">   TCPIP 上的 NetBIOS  . . . . . . . : 已启用</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"查看域名是否解析成功：\"><a href=\"#查看域名是否解析成功：\" class=\"headerlink\" title=\"查看域名是否解析成功：\"></a>查看域名是否解析成功：</h2><ul>\n<li><p>可以直接ping域名，也可以使用nslookup命令（NameServer Lookup）</p>\n</li>\n<li><p>在用 nslookup 查询一个域名时，可能会看到有“非权威应答” 的提示，非权威应答（Non-authoritative answer）意味着answer来自于其他服务器的缓存，而不是权威的服务器（就是该域名配置的DNS解析服务器，如果你的域名解析配置在CF的DNS上，则权威服务器，就是CF的DNS）。缓存会根据 ttl（Time to Live）的值定时的进行更新。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup wohensha.tk 8.8.8.8</span><br><span class=\"line\">Server:         8.8.8.8</span><br><span class=\"line\">Address:        8.8.8.8#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 104.21.49.190</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 172.67.165.231</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 2606:4700:3031::ac43:a5e7</span><br><span class=\"line\">Name:   wohensha.tk</span><br><span class=\"line\">Address: 2606:4700:3031::6815:31be</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sds查找权威名字服务器</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog nslookup -ty=ns clashdingyue.tk</span><br><span class=\"line\">Server:         172.17.112.1</span><br><span class=\"line\">Address:        172.17.112.1#53</span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">clashdingyue.tk nameserver = ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns01.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns03.freenom.com.</span><br><span class=\"line\"></span><br><span class=\"line\">Authoritative answers can be found from:</span><br><span class=\"line\"></span><br><span class=\"line\">➜  hexo-blog nslookup -ty=ns clashdingyue.tk ns04.freenom.com</span><br><span class=\"line\">Server:         ns04.freenom.com</span><br><span class=\"line\">Address:        104.155.29.241#53</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk nameserver = ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk nameserver = ns01.freenom.com.</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"一些小技巧\"><a href=\"#一些小技巧\" class=\"headerlink\" title=\"一些小技巧\"></a>一些小技巧</h1><h3 id=\"怎么验证是否遭遇DNS污染？\"><a href=\"#怎么验证是否遭遇DNS污染？\" class=\"headerlink\" title=\"怎么验证是否遭遇DNS污染？\"></a>怎么验证是否遭遇DNS污染？</h3><p>​\t\t<strong>DNS污染</strong>即网域服务器缓存污染，又称域名服务器缓存投毒，是指一些刻意制造或无意中制造出来的域名服务器数据包，把域名指往不正确的IP地址。一般来说，在互联网上都有可信赖的网域服务器，但为减低网络上的流量压力，一般的域名服务器都会把从上游的域名服务器获得的解析记录暂存起来，待下次有其他机器要求解析域名时，可以立即提供服务。一旦有关网域的局域域名服务器的缓存受到污染，就会把网域内的计算机导引往错误的服务器。</p>\n<pre><code>   我们应该怎么去验证自己域名是否遭遇DNS污染呢？输入命令dig +trace clashdingyue.tk（您自己需要检测域名）。**如果域名未被污染我们会得到权威DNS的应答**，如下所示:\n</code></pre>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog dig +trace clashdingyue.tk</span><br><span class=\"line\">;; Warning: Client COOKIE mismatch</span><br><span class=\"line\"></span><br><span class=\"line\">; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; +trace clashdingyue.tk</span><br><span class=\"line\">;; global options: +cmd</span><br><span class=\"line\">.                       200     IN      NS      g.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      j.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      d.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      h.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      m.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      k.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      a.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      i.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      b.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      f.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      c.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      l.root-servers.net.</span><br><span class=\"line\">.                       200     IN      NS      e.root-servers.net.</span><br><span class=\"line\">;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms</span><br><span class=\"line\"></span><br><span class=\"line\">tk.                     172800  IN      NS      a.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      b.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      c.ns.tk.</span><br><span class=\"line\">tk.                     172800  IN      NS      d.ns.tk.</span><br><span class=\"line\">tk.                     86400   IN      NSEC    tkmaxx. NS RRSIG NSEC</span><br><span class=\"line\">tk.                     86400   IN      RRSIG   NSEC 8 1 86400 20220710050000 20220627040000 47671 . HwO7QYzt3lI0k1w10qjM7oUf0B71yWgbUu9yCPcUdUng1icIu0lXSebp thdZpvOpLrjTE461RZJSlYaKIPavphtjpQHnUVxlH3Qznw9cBhql9Qnx cEtMo7vlCkCRST9sojkQxRqFW1oQMOoGG1j+SWpejRYwaudILcDCl0bP 4nPu1t5KmGR3Q8DKKO075O69w8MTauU+yfOsxEPvYgmHGzIyU7pBMWyt sUA+5ZpnrQ+0KLcXxnpUPQpBb55RlO1PhRqlJ9bT8qfYfvT+QUL5alwl xJxyZVcLTlGrpggW76yWjN3gq3zzynmd3D5cGeFQSon1+qMR5i6LoQix b4Jycg==</span><br><span class=\"line\">;; Received 602 bytes from 193.0.14.129#53(k.root-servers.net) in 350 ms</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns01.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns04.freenom.com.</span><br><span class=\"line\">;; Received 131 bytes from 194.0.41.1#53(d.ns.tk) in 330 ms</span><br><span class=\"line\"></span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.108.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.110.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.109.153</span><br><span class=\"line\">clashdingyue.tk.        3600    IN      A       185.199.111.153</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns03.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns04.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns02.freenom.com.</span><br><span class=\"line\">clashdingyue.tk.        300     IN      NS      ns01.freenom.com.</span><br><span class=\"line\">;; Received 248 bytes from 54.171.131.39#53(ns01.freenom.com) in 470 ms</span><br></pre></td></tr></table></figure>\n\n<p><strong>如果域名被污染会直接到的一个IP，并不会向权威DNS请求。</strong>如下所示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  hexo-blog dig +trace google.com</span><br><span class=\"line\">;; Warning: Client COOKIE mismatch</span><br><span class=\"line\"></span><br><span class=\"line\">; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; +trace google.com</span><br><span class=\"line\">;; global options: +cmd</span><br><span class=\"line\">.                       1450    IN      NS      f.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      k.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      d.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      j.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      l.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      m.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      h.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      i.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      e.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      c.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      a.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      b.root-servers.net.</span><br><span class=\"line\">.                       1450    IN      NS      g.root-servers.net.</span><br><span class=\"line\">;; Received 443 bytes from 172.17.112.1#53(172.17.112.1) in 840 ms</span><br><span class=\"line\"></span><br><span class=\"line\">google.com.             60      IN      A       8.7.198.46</span><br><span class=\"line\">;; Received 54 bytes from 192.33.4.12#53(c.root-servers.net) in 20 ms</span><br></pre></td></tr></table></figure>\n\n"},{"title":"hexo的front-matter中的tags和categories","abbrlink":51293,"date":"2022-06-27T06:44:40.000Z","updated":"2022-06-27T06:44:40.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","top_img":null,"description":null,"keywords":null,"_content":"\n## 分类和标签\n\n​\t\t只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 `Foo, Bar` 不等于 `Bar, Foo`；而标签没有顺序和层次。\n\n```\ncategories:\n- Diary\ntags:\n- PS3\n- Games\n```\n\n> 分类方法的分歧\n>\n> 如果您有过使用 WordPress 的经验，就很容易误解 Hexo 的分类方式。WordPress 支持对一篇文章设置多个分类，而且这些分类可以是同级的，也可以是父子分类。但是 Hexo 不支持指定多个同级分类。下面的指定方法：\n>\n> ```\n> categories:\n>   - Diary\n>   - Life\n> ```\n>\n> 会使分类`Life`成为`Diary`的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。\n>\n> 如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。\n>\n> ```\n> categories:\n> - [Diary, PlayStation]\n> - [Diary, Games]\n> - [Life]\n> ```\n>\n> 此时这篇文章同时包括三个分类： `PlayStation` 和 `Games` 分别都是父分类 `Diary` 的子分类，同时 `Life` 是一个没有子分类的分类。\n","source":"_posts/hexo的front-matter中的分类问题.md","raw":"---\ntitle: hexo的front-matter中的tags和categories\ntags:\n  - hexo\ncategories:\n  - - hexo\nabbrlink: 51293\ndate: 2022-06-27 14:44:40\nupdated: 2022-06-27 14:44:40\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 分类和标签\n\n​\t\t只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 `Foo, Bar` 不等于 `Bar, Foo`；而标签没有顺序和层次。\n\n```\ncategories:\n- Diary\ntags:\n- PS3\n- Games\n```\n\n> 分类方法的分歧\n>\n> 如果您有过使用 WordPress 的经验，就很容易误解 Hexo 的分类方式。WordPress 支持对一篇文章设置多个分类，而且这些分类可以是同级的，也可以是父子分类。但是 Hexo 不支持指定多个同级分类。下面的指定方法：\n>\n> ```\n> categories:\n>   - Diary\n>   - Life\n> ```\n>\n> 会使分类`Life`成为`Diary`的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。\n>\n> 如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。\n>\n> ```\n> categories:\n> - [Diary, PlayStation]\n> - [Diary, Games]\n> - [Life]\n> ```\n>\n> 此时这篇文章同时包括三个分类： `PlayStation` 和 `Games` 分别都是父分类 `Diary` 的子分类，同时 `Life` 是一个没有子分类的分类。\n","slug":"hexo的front-matter中的分类问题","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksv000m8j5mflxf23ys","content":"<h2 id=\"分类和标签\"><a href=\"#分类和标签\" class=\"headerlink\" title=\"分类和标签\"></a>分类和标签</h2><p>​\t\t只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">- Diary</span><br><span class=\"line\">tags:</span><br><span class=\"line\">- PS3</span><br><span class=\"line\">- Games</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分类方法的分歧</p>\n<p>如果您有过使用 WordPress 的经验，就很容易误解 Hexo 的分类方式。WordPress 支持对一篇文章设置多个分类，而且这些分类可以是同级的，也可以是父子分类。但是 Hexo 不支持指定多个同级分类。下面的指定方法：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">  - Diary</span><br><span class=\"line\">  - Life</span><br></pre></td></tr></table></figure>\n\n<p>会使分类<code>Life</code>成为<code>Diary</code>的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。</p>\n<p>如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">- [Diary, PlayStation]</span><br><span class=\"line\">- [Diary, Games]</span><br><span class=\"line\">- [Life]</span><br></pre></td></tr></table></figure>\n\n<p>此时这篇文章同时包括三个分类： <code>PlayStation</code> 和 <code>Games</code> 分别都是父分类 <code>Diary</code> 的子分类，同时 <code>Life</code> 是一个没有子分类的分类。</p>\n</blockquote>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"分类和标签\"><a href=\"#分类和标签\" class=\"headerlink\" title=\"分类和标签\"></a>分类和标签</h2><p>​\t\t只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">- Diary</span><br><span class=\"line\">tags:</span><br><span class=\"line\">- PS3</span><br><span class=\"line\">- Games</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分类方法的分歧</p>\n<p>如果您有过使用 WordPress 的经验，就很容易误解 Hexo 的分类方式。WordPress 支持对一篇文章设置多个分类，而且这些分类可以是同级的，也可以是父子分类。但是 Hexo 不支持指定多个同级分类。下面的指定方法：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">  - Diary</span><br><span class=\"line\">  - Life</span><br></pre></td></tr></table></figure>\n\n<p>会使分类<code>Life</code>成为<code>Diary</code>的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。</p>\n<p>如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories:</span><br><span class=\"line\">- [Diary, PlayStation]</span><br><span class=\"line\">- [Diary, Games]</span><br><span class=\"line\">- [Life]</span><br></pre></td></tr></table></figure>\n\n<p>此时这篇文章同时包括三个分类： <code>PlayStation</code> 和 <code>Games</code> 分别都是父分类 <code>Diary</code> 的子分类，同时 <code>Life</code> 是一个没有子分类的分类。</p>\n</blockquote>\n"},{"title":"开源项目BugFix合集","abbrlink":38776,"date":"2022-11-29T11:35:27.000Z","updated":"2022-11-29T11:35:27.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n### 修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug \n\n- http-alert告警插件仅仅发送用户预定义好的post body信息，丢失最重要Task运行告警信息。这是一个非常简单的Bug：https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25\n\n- git clone dolphinscheduler项目，然后切到2.0.5-release分支，执行`mvn -U clean package -Prelease -Dmaven.test.skip=true`进行编译打包。打包成功后，将生成的`dolphinscheduler\\dolphinscheduler-alert\\dolphinscheduler-alert-plugins\\dolphinscheduler-alert-http\\target\\dolphinscheduler-alert-http-2.0.6-SNAPSHOT.jar`替换掉原来的jar包。\n\n- 启停 Alert \n\n  ```shell\n  sh ./bin/dolphinscheduler-daemon.sh start alert-server\n  sh ./bin/dolphinscheduler-daemon.sh stop alert-server\n  ```\n\n\n### 修复Hadoop3.2.1中Logger Level错误提升的Bug\n\n- [Flink-Hudi日志超频繁打印问题](https://poxiao.tk/2022/12/bigdata/TroubleShooting/Flink-hudi%E6%97%A5%E5%BF%97%E8%B6%85%E9%A2%91%E7%B9%81%E6%89%93%E5%8D%B0/)\n- 修复流程：1、反编译相关Class文件。2、修改源码，并重新进行编译。3、打包回jar包。4、对jar包进行替换，重启相关服务。\n- https://issues.apache.org/jira/browse/HDFS-14759\n\n### 修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时\n\n- 发现kylin4.0.x中的push-down query对于明细查询`select * from table limit 10`非常慢，往往好耗时几分钟，这非常不正常。通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。\n- https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/\n\n## 改写Superset源码，为Apache Superset添加新功能\n\n- https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/\n\n## 修复dolphinscheduler Hive SQL数据源Read timed out错误\n\n- https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/\n","source":"_posts/开源项目BugFix合集.md","raw":"---\ntitle: 开源项目BugFix合集\ntags:\n  - BugFix\n  - PR\ncategories:\n  - - bigdata\n    - BugFix\nabbrlink: 38776\ndate: 2022-11-29 19:35:27\nupdated: 2022-11-29 19:35:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n### 修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug \n\n- http-alert告警插件仅仅发送用户预定义好的post body信息，丢失最重要Task运行告警信息。这是一个非常简单的Bug：https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25\n\n- git clone dolphinscheduler项目，然后切到2.0.5-release分支，执行`mvn -U clean package -Prelease -Dmaven.test.skip=true`进行编译打包。打包成功后，将生成的`dolphinscheduler\\dolphinscheduler-alert\\dolphinscheduler-alert-plugins\\dolphinscheduler-alert-http\\target\\dolphinscheduler-alert-http-2.0.6-SNAPSHOT.jar`替换掉原来的jar包。\n\n- 启停 Alert \n\n  ```shell\n  sh ./bin/dolphinscheduler-daemon.sh start alert-server\n  sh ./bin/dolphinscheduler-daemon.sh stop alert-server\n  ```\n\n\n### 修复Hadoop3.2.1中Logger Level错误提升的Bug\n\n- [Flink-Hudi日志超频繁打印问题](https://poxiao.tk/2022/12/bigdata/TroubleShooting/Flink-hudi%E6%97%A5%E5%BF%97%E8%B6%85%E9%A2%91%E7%B9%81%E6%89%93%E5%8D%B0/)\n- 修复流程：1、反编译相关Class文件。2、修改源码，并重新进行编译。3、打包回jar包。4、对jar包进行替换，重启相关服务。\n- https://issues.apache.org/jira/browse/HDFS-14759\n\n### 修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时\n\n- 发现kylin4.0.x中的push-down query对于明细查询`select * from table limit 10`非常慢，往往好耗时几分钟，这非常不正常。通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。\n- https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/\n\n## 改写Superset源码，为Apache Superset添加新功能\n\n- https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/\n\n## 修复dolphinscheduler Hive SQL数据源Read timed out错误\n\n- https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/\n","slug":"开源项目BugFix合集","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksv000o8j5m43et1f2f","content":"<h3 id=\"修复dolphinscheduler2-0-5中http-alert-plugin丢失告警信息的Bug\"><a href=\"#修复dolphinscheduler2-0-5中http-alert-plugin丢失告警信息的Bug\" class=\"headerlink\" title=\"修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug\"></a>修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug</h3><ul>\n<li><p>http-alert告警插件仅仅发送用户预定义好的post body信息，丢失最重要Task运行告警信息。这是一个非常简单的Bug：<a href=\"https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25\">https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25</a></p>\n</li>\n<li><p>git clone dolphinscheduler项目，然后切到2.0.5-release分支，执行<code>mvn -U clean package -Prelease -Dmaven.test.skip=true</code>进行编译打包。打包成功后，将生成的<code>dolphinscheduler\\dolphinscheduler-alert\\dolphinscheduler-alert-plugins\\dolphinscheduler-alert-http\\target\\dolphinscheduler-alert-http-2.0.6-SNAPSHOT.jar</code>替换掉原来的jar包。</p>\n</li>\n<li><p>启停 Alert </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class=\"line\">sh ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"修复Hadoop3-2-1中Logger-Level错误提升的Bug\"><a href=\"#修复Hadoop3-2-1中Logger-Level错误提升的Bug\" class=\"headerlink\" title=\"修复Hadoop3.2.1中Logger Level错误提升的Bug\"></a>修复Hadoop3.2.1中Logger Level错误提升的Bug</h3><ul>\n<li><a href=\"https://poxiao.tk/2022/12/bigdata/TroubleShooting/Flink-hudi%E6%97%A5%E5%BF%97%E8%B6%85%E9%A2%91%E7%B9%81%E6%89%93%E5%8D%B0/\">Flink-Hudi日志超频繁打印问题</a></li>\n<li>修复流程：1、反编译相关Class文件。2、修改源码，并重新进行编译。3、打包回jar包。4、对jar包进行替换，重启相关服务。</li>\n<li><a href=\"https://issues.apache.org/jira/browse/HDFS-14759\">https://issues.apache.org/jira/browse/HDFS-14759</a></li>\n</ul>\n<h3 id=\"修复Kylin4-0-x中push-down-query由于查询计划导致的不正常查询延时\"><a href=\"#修复Kylin4-0-x中push-down-query由于查询计划导致的不正常查询延时\" class=\"headerlink\" title=\"修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时\"></a>修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时</h3><ul>\n<li>发现kylin4.0.x中的push-down query对于明细查询<code>select * from table limit 10</code>非常慢，往往好耗时几分钟，这非常不正常。通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。</li>\n<li><a href=\"https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/\">https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/</a></li>\n</ul>\n<h2 id=\"改写Superset源码，为Apache-Superset添加新功能\"><a href=\"#改写Superset源码，为Apache-Superset添加新功能\" class=\"headerlink\" title=\"改写Superset源码，为Apache Superset添加新功能\"></a>改写Superset源码，为Apache Superset添加新功能</h2><ul>\n<li><a href=\"https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/\">https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/</a></li>\n</ul>\n<h2 id=\"修复dolphinscheduler-Hive-SQL数据源Read-timed-out错误\"><a href=\"#修复dolphinscheduler-Hive-SQL数据源Read-timed-out错误\" class=\"headerlink\" title=\"修复dolphinscheduler Hive SQL数据源Read timed out错误\"></a>修复dolphinscheduler Hive SQL数据源Read timed out错误</h2><ul>\n<li><a href=\"https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/\">https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/</a></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h3 id=\"修复dolphinscheduler2-0-5中http-alert-plugin丢失告警信息的Bug\"><a href=\"#修复dolphinscheduler2-0-5中http-alert-plugin丢失告警信息的Bug\" class=\"headerlink\" title=\"修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug\"></a>修复dolphinscheduler2.0.5中http-alert plugin丢失告警信息的Bug</h3><ul>\n<li><p>http-alert告警插件仅仅发送用户预定义好的post body信息，丢失最重要Task运行告警信息。这是一个非常简单的Bug：<a href=\"https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25\">https://github.com/apache/dolphinscheduler/commit/6021c228a1261a45ba8d02606f7132cd0a9b4c25</a></p>\n</li>\n<li><p>git clone dolphinscheduler项目，然后切到2.0.5-release分支，执行<code>mvn -U clean package -Prelease -Dmaven.test.skip=true</code>进行编译打包。打包成功后，将生成的<code>dolphinscheduler\\dolphinscheduler-alert\\dolphinscheduler-alert-plugins\\dolphinscheduler-alert-http\\target\\dolphinscheduler-alert-http-2.0.6-SNAPSHOT.jar</code>替换掉原来的jar包。</p>\n</li>\n<li><p>启停 Alert </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh ./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class=\"line\">sh ./bin/dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"修复Hadoop3-2-1中Logger-Level错误提升的Bug\"><a href=\"#修复Hadoop3-2-1中Logger-Level错误提升的Bug\" class=\"headerlink\" title=\"修复Hadoop3.2.1中Logger Level错误提升的Bug\"></a>修复Hadoop3.2.1中Logger Level错误提升的Bug</h3><ul>\n<li><a href=\"https://poxiao.tk/2022/12/bigdata/TroubleShooting/Flink-hudi%E6%97%A5%E5%BF%97%E8%B6%85%E9%A2%91%E7%B9%81%E6%89%93%E5%8D%B0/\">Flink-Hudi日志超频繁打印问题</a></li>\n<li>修复流程：1、反编译相关Class文件。2、修改源码，并重新进行编译。3、打包回jar包。4、对jar包进行替换，重启相关服务。</li>\n<li><a href=\"https://issues.apache.org/jira/browse/HDFS-14759\">https://issues.apache.org/jira/browse/HDFS-14759</a></li>\n</ul>\n<h3 id=\"修复Kylin4-0-x中push-down-query由于查询计划导致的不正常查询延时\"><a href=\"#修复Kylin4-0-x中push-down-query由于查询计划导致的不正常查询延时\" class=\"headerlink\" title=\"修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时\"></a>修复Kylin4.0.x中push-down query由于查询计划导致的不正常查询延时</h3><ul>\n<li>发现kylin4.0.x中的push-down query对于明细查询<code>select * from table limit 10</code>非常慢，往往好耗时几分钟，这非常不正常。通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。</li>\n<li><a href=\"https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/\">https://poxiao.tk/2023/01/bigdata/TroubleShooting/%E4%BF%AE%E5%A4%8DKylin4.0.x%E4%B8%8D%E6%AD%A3%E5%B8%B8%E7%9A%84push-down%20query/</a></li>\n</ul>\n<h2 id=\"改写Superset源码，为Apache-Superset添加新功能\"><a href=\"#改写Superset源码，为Apache-Superset添加新功能\" class=\"headerlink\" title=\"改写Superset源码，为Apache Superset添加新功能\"></a>改写Superset源码，为Apache Superset添加新功能</h2><ul>\n<li><a href=\"https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/\">https://poxiao.tk/2023/01/bigdata/Apache%20Superset%E6%B7%BB%E5%8A%A0exclude%E5%87%BD%E6%95%B0/</a></li>\n</ul>\n<h2 id=\"修复dolphinscheduler-Hive-SQL数据源Read-timed-out错误\"><a href=\"#修复dolphinscheduler-Hive-SQL数据源Read-timed-out错误\" class=\"headerlink\" title=\"修复dolphinscheduler Hive SQL数据源Read timed out错误\"></a>修复dolphinscheduler Hive SQL数据源Read timed out错误</h2><ul>\n<li><a href=\"https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/\">https://poxiao.tk/2023/06/troubleshooting/dolphinscheduler%E4%B8%ADHive%20SQL%20Task%E8%BF%9E%E6%8E%A5Kyuubi%20read%20timeout/</a></li>\n</ul>\n"},{"title":"DAG实现与任务调度","abbrlink":55672,"date":"2023-08-30T10:45:16.000Z","updated":"2023-08-30T10:45:16.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 前言\n\n在任务调度场景中，常常通过DAG将多个任务编排成一个复杂的Job，进而满足复杂的任务调度应用场景。特别是在大数据领域，这类调度系统是必须的，比如Azkaba、DolphinScheduler、AirFlow...。而这些系统正是通过DAG进行任务编排的，那么下面让我们试着简单的实现一个DAG调度程序。\n\n\n\n## Code\n\n#### 1、抽象出一个任务执行接口\n\n```java\npublic interface Executor {\n    boolean execute();\n}\n```\n\n\n\n#### 2、简单实现一个示例Task，需实现Executor接口。\n\n```java\npublic class Task implements Executor{\n    private Long id;\n    private String name;\n    private int state;\n    public Task(Long id, String name, int state) {\n        this.id = id;\n        this.name = name;\n        this.state = state;\n    }\n    public boolean execute() {\n        System.out.println(\"Task id: [\" + id + \"], \" + \"task name: [\" + name +\"] is running\");\n        state = 1;\n        return true;\n    }\n    public boolean hasExecuted() {\n        return state == 1;\n    }\n\n    public Long getId() {\n        return id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public int getState() {\n        return state;\n    }\n}\n\n```\n\n\n\n#### 3、实现DAG数据结构\n\n这个类使用了邻接表来表示有向无环图。\n\ntasks是顶点集合，也就是任务集合。\n\nmap是任务依赖关系集合。key是一个任务，value是它的前置任务集合。\n\n一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n\n```java\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Set;\n\n\n/**\n * 这个类使用了邻接表来表示有向无环图。\n *\n * tasks是顶点集合，也就是任务集合。\n *\n * map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。\n *\n * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n */\npublic class Digraph {\n    private Set<Task> tasks;\n    private Map<Task, Set<Task>> map;\n    public Digraph() {\n        this.tasks = new HashSet<Task>();\n        this.map = new HashMap<Task, Set<Task>>();\n    }\n    public void addEdge(Task task, Task prev) {\n        if (!tasks.contains(task) || !tasks.contains(prev)) {\n            throw new IllegalArgumentException();\n        }\n        Set<Task> prevs = map.get(task);\n        if (prevs == null) {\n            prevs = new HashSet<Task>();\n            map.put(task, prevs);\n        }\n        if (prevs.contains(prev)) {\n            throw new IllegalArgumentException();\n        }\n        prevs.add(prev);\n    }\n    public void addTask(Task task) {\n        if (tasks.contains(task)) {\n            throw new IllegalArgumentException();\n        }\n        tasks.add(task);\n    }\n    public void remove(Task task) {\n        if (!tasks.contains(task)) {\n            return;\n        }\n        if (map.containsKey(task)) {\n            map.remove(task);\n        }\n        for (Set<Task> set : map.values()) {\n            if (set.contains(task)) {\n                set.remove(task);\n            }\n        }\n    }\n    public Set<Task> getTasks() {\n        return tasks;\n    }\n    public void setTasks(Set<Task> tasks) {\n        this.tasks = tasks;\n    }\n    public Map<Task, Set<Task>> getMap() {\n        return map;\n    }\n    public void setMap(Map<Task, Set<Task>> map) {\n        this.map = map;\n    }\n}\n```\n\n\n\n#### 4、实现调度器：提交DAG进行调度执行\n\n调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行（若考虑性能，可优化为并行执行）。\n\n若List为空，说明所有任务都已执行，则这一次任务调度结束。\n\n```java\npackage cn.jxau.yuan.dag;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\n/**\n * 调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行\n * （若考虑性能，可优化为并行执行）。\n *\n * 若List为空，说明所有任务都已执行，则这一次任务调度结束。\n *\n * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n */\npublic class Scheduler {\n    public void schedule(Digraph digraph) {\n        while (true) {\n            List<Task> todo = new ArrayList<Task>();\n            System.out.println();\n            for (Task task : digraph.getTasks()) {\n                if (!task.hasExecuted()) {\n                    Set<Task> prevs = digraph.getMap().get(task);\n                    if (prevs != null && !prevs.isEmpty()) {\n                        // 或者是它的前置任务集合中的任务都是已执行的状态\n                        boolean toAdd = true;\n                        for (Task task1 : prevs) {\n                            if (!task1.hasExecuted()) {\n                                toAdd = false;\n                                break;\n                            }\n                        }\n                        if (toAdd) {\n                            todo.add(task);\n                            String log = String.format(\"%s需要被执行，因为其前置任务[%s]都已经执行成功！！！\\n\",\n                                    task.getName(), prevs.stream().map(Task::getName).collect(Collectors.toList()));\n                            System.out.printf(log);\n                        }\n                    } else {\n                        // 一个任务执行的前提是它在map中没有以它作为key的entry\n                        todo.add(task);\n                        String log = String.format(\"%s需要被执行，因为他是DAG开始执行的起点\\n\", task.getName());\n                        System.out.printf(log);\n                    }\n                }\n            }\n            if (!todo.isEmpty()) {\n                System.out.println(\"这些任务将被并行执行： \" + todo.stream().map(Task::getName).collect(Collectors.toList()));\n                // 这里可以优化为并行执行\n                for (Task task : todo) {\n                    if (!task.execute()) {\n                        throw new RuntimeException();\n                    }\n                }\n            } else {\n                break;\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        Digraph digraph = new Digraph();\n        Task task1 = new Task(1L, \"task1\", 0);\n        Task task2 = new Task(2L, \"task2\", 0);\n        Task task3 = new Task(3L, \"task3\", 0);\n        Task task4 = new Task(4L, \"task4\", 0);\n        Task task5 = new Task(5L, \"task5\", 0);\n        Task task6 = new Task(6L, \"task6\", 0);\n        digraph.addTask(task1);\n        digraph.addTask(task2);\n        digraph.addTask(task3);\n        digraph.addTask(task4);\n        digraph.addTask(task5);\n        digraph.addTask(task6);\n        digraph.addEdge(task1, task2);\n        digraph.addEdge(task1, task5);\n        digraph.addEdge(task6, task2);\n        digraph.addEdge(task2, task3);\n        digraph.addEdge(task2, task4);\n        Scheduler scheduler = new Scheduler();\n        scheduler.schedule(digraph);\n    }\n}\n```\n\n\n\n## IDEA运行结果\n\nDemo中的任务编排，如下图所示：\n\n![DAG](/img/dag1.png)\n\n```shell\ntask3需要被执行，因为他是DAG开始执行的起点\ntask4需要被执行，因为他是DAG开始执行的起点\ntask5需要被执行，因为他是DAG开始执行的起点\n这些任务将被并行执行： [task3, task4, task5]\nTask id: [3], task name: [task3] is running\nTask id: [4], task name: [task4] is running\nTask id: [5], task name: [task5] is running\n\ntask2需要被执行，因为其前置任务[[task3, task4]]都已经执行成功！！！\n这些任务将被并行执行： [task2]\nTask id: [2], task name: [task2] is running\n\ntask6需要被执行，因为其前置任务[[task2]]都已经执行成功！！！\ntask1需要被执行，因为其前置任务[[task2, task5]]都已经执行成功！！！\n这些任务将被并行执行： [task6, task1]\nTask id: [6], task name: [task6] is running\nTask id: [1], task name: [task1] is running\n\n\nProcess finished with exit code 0\n```\n\n","source":"_posts/data-structure/DAG实现与任务调度.md","raw":"---\ntitle: DAG实现与任务调度\ntags:\n  - DAG\ncategories:\n  - - data-structure\nabbrlink: 55672\ndate: 2023-08-30 18:45:16\nupdated: 2023-08-30 18:45:16\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n在任务调度场景中，常常通过DAG将多个任务编排成一个复杂的Job，进而满足复杂的任务调度应用场景。特别是在大数据领域，这类调度系统是必须的，比如Azkaba、DolphinScheduler、AirFlow...。而这些系统正是通过DAG进行任务编排的，那么下面让我们试着简单的实现一个DAG调度程序。\n\n\n\n## Code\n\n#### 1、抽象出一个任务执行接口\n\n```java\npublic interface Executor {\n    boolean execute();\n}\n```\n\n\n\n#### 2、简单实现一个示例Task，需实现Executor接口。\n\n```java\npublic class Task implements Executor{\n    private Long id;\n    private String name;\n    private int state;\n    public Task(Long id, String name, int state) {\n        this.id = id;\n        this.name = name;\n        this.state = state;\n    }\n    public boolean execute() {\n        System.out.println(\"Task id: [\" + id + \"], \" + \"task name: [\" + name +\"] is running\");\n        state = 1;\n        return true;\n    }\n    public boolean hasExecuted() {\n        return state == 1;\n    }\n\n    public Long getId() {\n        return id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public int getState() {\n        return state;\n    }\n}\n\n```\n\n\n\n#### 3、实现DAG数据结构\n\n这个类使用了邻接表来表示有向无环图。\n\ntasks是顶点集合，也就是任务集合。\n\nmap是任务依赖关系集合。key是一个任务，value是它的前置任务集合。\n\n一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n\n```java\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Set;\n\n\n/**\n * 这个类使用了邻接表来表示有向无环图。\n *\n * tasks是顶点集合，也就是任务集合。\n *\n * map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。\n *\n * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n */\npublic class Digraph {\n    private Set<Task> tasks;\n    private Map<Task, Set<Task>> map;\n    public Digraph() {\n        this.tasks = new HashSet<Task>();\n        this.map = new HashMap<Task, Set<Task>>();\n    }\n    public void addEdge(Task task, Task prev) {\n        if (!tasks.contains(task) || !tasks.contains(prev)) {\n            throw new IllegalArgumentException();\n        }\n        Set<Task> prevs = map.get(task);\n        if (prevs == null) {\n            prevs = new HashSet<Task>();\n            map.put(task, prevs);\n        }\n        if (prevs.contains(prev)) {\n            throw new IllegalArgumentException();\n        }\n        prevs.add(prev);\n    }\n    public void addTask(Task task) {\n        if (tasks.contains(task)) {\n            throw new IllegalArgumentException();\n        }\n        tasks.add(task);\n    }\n    public void remove(Task task) {\n        if (!tasks.contains(task)) {\n            return;\n        }\n        if (map.containsKey(task)) {\n            map.remove(task);\n        }\n        for (Set<Task> set : map.values()) {\n            if (set.contains(task)) {\n                set.remove(task);\n            }\n        }\n    }\n    public Set<Task> getTasks() {\n        return tasks;\n    }\n    public void setTasks(Set<Task> tasks) {\n        this.tasks = tasks;\n    }\n    public Map<Task, Set<Task>> getMap() {\n        return map;\n    }\n    public void setMap(Map<Task, Set<Task>> map) {\n        this.map = map;\n    }\n}\n```\n\n\n\n#### 4、实现调度器：提交DAG进行调度执行\n\n调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行（若考虑性能，可优化为并行执行）。\n\n若List为空，说明所有任务都已执行，则这一次任务调度结束。\n\n```java\npackage cn.jxau.yuan.dag;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\n/**\n * 调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行\n * （若考虑性能，可优化为并行执行）。\n *\n * 若List为空，说明所有任务都已执行，则这一次任务调度结束。\n *\n * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。\n */\npublic class Scheduler {\n    public void schedule(Digraph digraph) {\n        while (true) {\n            List<Task> todo = new ArrayList<Task>();\n            System.out.println();\n            for (Task task : digraph.getTasks()) {\n                if (!task.hasExecuted()) {\n                    Set<Task> prevs = digraph.getMap().get(task);\n                    if (prevs != null && !prevs.isEmpty()) {\n                        // 或者是它的前置任务集合中的任务都是已执行的状态\n                        boolean toAdd = true;\n                        for (Task task1 : prevs) {\n                            if (!task1.hasExecuted()) {\n                                toAdd = false;\n                                break;\n                            }\n                        }\n                        if (toAdd) {\n                            todo.add(task);\n                            String log = String.format(\"%s需要被执行，因为其前置任务[%s]都已经执行成功！！！\\n\",\n                                    task.getName(), prevs.stream().map(Task::getName).collect(Collectors.toList()));\n                            System.out.printf(log);\n                        }\n                    } else {\n                        // 一个任务执行的前提是它在map中没有以它作为key的entry\n                        todo.add(task);\n                        String log = String.format(\"%s需要被执行，因为他是DAG开始执行的起点\\n\", task.getName());\n                        System.out.printf(log);\n                    }\n                }\n            }\n            if (!todo.isEmpty()) {\n                System.out.println(\"这些任务将被并行执行： \" + todo.stream().map(Task::getName).collect(Collectors.toList()));\n                // 这里可以优化为并行执行\n                for (Task task : todo) {\n                    if (!task.execute()) {\n                        throw new RuntimeException();\n                    }\n                }\n            } else {\n                break;\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        Digraph digraph = new Digraph();\n        Task task1 = new Task(1L, \"task1\", 0);\n        Task task2 = new Task(2L, \"task2\", 0);\n        Task task3 = new Task(3L, \"task3\", 0);\n        Task task4 = new Task(4L, \"task4\", 0);\n        Task task5 = new Task(5L, \"task5\", 0);\n        Task task6 = new Task(6L, \"task6\", 0);\n        digraph.addTask(task1);\n        digraph.addTask(task2);\n        digraph.addTask(task3);\n        digraph.addTask(task4);\n        digraph.addTask(task5);\n        digraph.addTask(task6);\n        digraph.addEdge(task1, task2);\n        digraph.addEdge(task1, task5);\n        digraph.addEdge(task6, task2);\n        digraph.addEdge(task2, task3);\n        digraph.addEdge(task2, task4);\n        Scheduler scheduler = new Scheduler();\n        scheduler.schedule(digraph);\n    }\n}\n```\n\n\n\n## IDEA运行结果\n\nDemo中的任务编排，如下图所示：\n\n![DAG](/img/dag1.png)\n\n```shell\ntask3需要被执行，因为他是DAG开始执行的起点\ntask4需要被执行，因为他是DAG开始执行的起点\ntask5需要被执行，因为他是DAG开始执行的起点\n这些任务将被并行执行： [task3, task4, task5]\nTask id: [3], task name: [task3] is running\nTask id: [4], task name: [task4] is running\nTask id: [5], task name: [task5] is running\n\ntask2需要被执行，因为其前置任务[[task3, task4]]都已经执行成功！！！\n这些任务将被并行执行： [task2]\nTask id: [2], task name: [task2] is running\n\ntask6需要被执行，因为其前置任务[[task2]]都已经执行成功！！！\ntask1需要被执行，因为其前置任务[[task2, task5]]都已经执行成功！！！\n这些任务将被并行执行： [task6, task1]\nTask id: [6], task name: [task6] is running\nTask id: [1], task name: [task1] is running\n\n\nProcess finished with exit code 0\n```\n\n","slug":"data-structure/DAG实现与任务调度","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksw000r8j5mhd8od7jd","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在任务调度场景中，常常通过DAG将多个任务编排成一个复杂的Job，进而满足复杂的任务调度应用场景。特别是在大数据领域，这类调度系统是必须的，比如Azkaba、DolphinScheduler、AirFlow…。而这些系统正是通过DAG进行任务编排的，那么下面让我们试着简单的实现一个DAG调度程序。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><h4 id=\"1、抽象出一个任务执行接口\"><a href=\"#1、抽象出一个任务执行接口\" class=\"headerlink\" title=\"1、抽象出一个任务执行接口\"></a>1、抽象出一个任务执行接口</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">Executor</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">boolean</span> <span class=\"title function_\">execute</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"2、简单实现一个示例Task，需实现Executor接口。\"><a href=\"#2、简单实现一个示例Task，需实现Executor接口。\" class=\"headerlink\" title=\"2、简单实现一个示例Task，需实现Executor接口。\"></a>2、简单实现一个示例Task，需实现Executor接口。</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Task</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Executor</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long id;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> state;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Task</span><span class=\"params\">(Long id, String name, <span class=\"type\">int</span> state)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.id = id;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.name = name;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.state = state;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">execute</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Task id: [&quot;</span> + id + <span class=\"string\">&quot;], &quot;</span> + <span class=\"string\">&quot;task name: [&quot;</span> + name +<span class=\"string\">&quot;] is running&quot;</span>);</span><br><span class=\"line\">        state = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">hasExecuted</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state == <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Long <span class=\"title function_\">getId</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getName</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getState</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"3、实现DAG数据结构\"><a href=\"#3、实现DAG数据结构\" class=\"headerlink\" title=\"3、实现DAG数据结构\"></a>3、实现DAG数据结构</h4><p>这个类使用了邻接表来表示有向无环图。</p>\n<p>tasks是顶点集合，也就是任务集合。</p>\n<p>map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。</p>\n<p>一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.HashMap;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.HashSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Map;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Set;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 这个类使用了邻接表来表示有向无环图。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * tasks是顶点集合，也就是任务集合。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Digraph</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Set&lt;Task&gt; tasks;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Map&lt;Task, Set&lt;Task&gt;&gt; map;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Digraph</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.tasks = <span class=\"keyword\">new</span> <span class=\"title class_\">HashSet</span>&lt;Task&gt;();</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.map = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;Task, Set&lt;Task&gt;&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">addEdge</span><span class=\"params\">(Task task, Task prev)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tasks.contains(task) || !tasks.contains(prev)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Set&lt;Task&gt; prevs = map.get(task);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prevs == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">            prevs = <span class=\"keyword\">new</span> <span class=\"title class_\">HashSet</span>&lt;Task&gt;();</span><br><span class=\"line\">            map.put(task, prevs);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prevs.contains(prev)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        prevs.add(prev);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">addTask</span><span class=\"params\">(Task task)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (tasks.contains(task)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        tasks.add(task);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">remove</span><span class=\"params\">(Task task)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tasks.contains(task)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (map.containsKey(task)) &#123;</span><br><span class=\"line\">            map.remove(task);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Set&lt;Task&gt; set : map.values()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (set.contains(task)) &#123;</span><br><span class=\"line\">                set.remove(task);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> Set&lt;Task&gt; <span class=\"title function_\">getTasks</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tasks;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setTasks</span><span class=\"params\">(Set&lt;Task&gt; tasks)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.tasks = tasks;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> Map&lt;Task, Set&lt;Task&gt;&gt; <span class=\"title function_\">getMap</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> map;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setMap</span><span class=\"params\">(Map&lt;Task, Set&lt;Task&gt;&gt; map)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.map = map;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"4、实现调度器：提交DAG进行调度执行\"><a href=\"#4、实现调度器：提交DAG进行调度执行\" class=\"headerlink\" title=\"4、实现调度器：提交DAG进行调度执行\"></a>4、实现调度器：提交DAG进行调度执行</h4><p>调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行（若考虑性能，可优化为并行执行）。</p>\n<p>若List为空，说明所有任务都已执行，则这一次任务调度结束。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau.yuan.dag;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.ArrayList;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Set;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.stream.Collectors;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行</span></span><br><span class=\"line\"><span class=\"comment\"> * （若考虑性能，可优化为并行执行）。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 若List为空，说明所有任务都已执行，则这一次任务调度结束。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Scheduler</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">schedule</span><span class=\"params\">(Digraph digraph)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">            List&lt;Task&gt; todo = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;Task&gt;();</span><br><span class=\"line\">            System.out.println();</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Task task : digraph.getTasks()) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!task.hasExecuted()) &#123;</span><br><span class=\"line\">                    Set&lt;Task&gt; prevs = digraph.getMap().get(task);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (prevs != <span class=\"literal\">null</span> &amp;&amp; !prevs.isEmpty()) &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// 或者是它的前置任务集合中的任务都是已执行的状态</span></span><br><span class=\"line\">                        <span class=\"type\">boolean</span> <span class=\"variable\">toAdd</span> <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">                        <span class=\"keyword\">for</span> (Task task1 : prevs) &#123;</span><br><span class=\"line\">                            <span class=\"keyword\">if</span> (!task1.hasExecuted()) &#123;</span><br><span class=\"line\">                                toAdd = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (toAdd) &#123;</span><br><span class=\"line\">                            todo.add(task);</span><br><span class=\"line\">                            <span class=\"type\">String</span> <span class=\"variable\">log</span> <span class=\"operator\">=</span> String.format(<span class=\"string\">&quot;%s需要被执行，因为其前置任务[%s]都已经执行成功！！！\\n&quot;</span>,</span><br><span class=\"line\">                                    task.getName(), prevs.stream().map(Task::getName).collect(Collectors.toList()));</span><br><span class=\"line\">                            System.out.printf(log);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// 一个任务执行的前提是它在map中没有以它作为key的entry</span></span><br><span class=\"line\">                        todo.add(task);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">log</span> <span class=\"operator\">=</span> String.format(<span class=\"string\">&quot;%s需要被执行，因为他是DAG开始执行的起点\\n&quot;</span>, task.getName());</span><br><span class=\"line\">                        System.out.printf(log);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!todo.isEmpty()) &#123;</span><br><span class=\"line\">                System.out.println(<span class=\"string\">&quot;这些任务将被并行执行： &quot;</span> + todo.stream().map(Task::getName).collect(Collectors.toList()));</span><br><span class=\"line\">                <span class=\"comment\">// 这里可以优化为并行执行</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (Task task : todo) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (!task.execute()) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Digraph</span> <span class=\"variable\">digraph</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Digraph</span>();</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task1</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">1L</span>, <span class=\"string\">&quot;task1&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task2</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">2L</span>, <span class=\"string\">&quot;task2&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task3</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">3L</span>, <span class=\"string\">&quot;task3&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task4</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">4L</span>, <span class=\"string\">&quot;task4&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task5</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">5L</span>, <span class=\"string\">&quot;task5&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task6</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">6L</span>, <span class=\"string\">&quot;task6&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        digraph.addTask(task1);</span><br><span class=\"line\">        digraph.addTask(task2);</span><br><span class=\"line\">        digraph.addTask(task3);</span><br><span class=\"line\">        digraph.addTask(task4);</span><br><span class=\"line\">        digraph.addTask(task5);</span><br><span class=\"line\">        digraph.addTask(task6);</span><br><span class=\"line\">        digraph.addEdge(task1, task2);</span><br><span class=\"line\">        digraph.addEdge(task1, task5);</span><br><span class=\"line\">        digraph.addEdge(task6, task2);</span><br><span class=\"line\">        digraph.addEdge(task2, task3);</span><br><span class=\"line\">        digraph.addEdge(task2, task4);</span><br><span class=\"line\">        <span class=\"type\">Scheduler</span> <span class=\"variable\">scheduler</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Scheduler</span>();</span><br><span class=\"line\">        scheduler.schedule(digraph);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"IDEA运行结果\"><a href=\"#IDEA运行结果\" class=\"headerlink\" title=\"IDEA运行结果\"></a>IDEA运行结果</h2><p>Demo中的任务编排，如下图所示：</p>\n<p><img src=\"/img/dag1.png\" alt=\"DAG\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">task3需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">task4需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">task5需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">这些任务将被并行执行： [task3, task4, task5]</span><br><span class=\"line\">Task id: [3], task name: [task3] is running</span><br><span class=\"line\">Task id: [4], task name: [task4] is running</span><br><span class=\"line\">Task id: [5], task name: [task5] is running</span><br><span class=\"line\"></span><br><span class=\"line\">task2需要被执行，因为其前置任务[[task3, task4]]都已经执行成功！！！</span><br><span class=\"line\">这些任务将被并行执行： [task2]</span><br><span class=\"line\">Task id: [2], task name: [task2] is running</span><br><span class=\"line\"></span><br><span class=\"line\">task6需要被执行，因为其前置任务[[task2]]都已经执行成功！！！</span><br><span class=\"line\">task1需要被执行，因为其前置任务[[task2, task5]]都已经执行成功！！！</span><br><span class=\"line\">这些任务将被并行执行： [task6, task1]</span><br><span class=\"line\">Task id: [6], task name: [task6] is running</span><br><span class=\"line\">Task id: [1], task name: [task1] is running</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code 0</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在任务调度场景中，常常通过DAG将多个任务编排成一个复杂的Job，进而满足复杂的任务调度应用场景。特别是在大数据领域，这类调度系统是必须的，比如Azkaba、DolphinScheduler、AirFlow…。而这些系统正是通过DAG进行任务编排的，那么下面让我们试着简单的实现一个DAG调度程序。</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><h4 id=\"1、抽象出一个任务执行接口\"><a href=\"#1、抽象出一个任务执行接口\" class=\"headerlink\" title=\"1、抽象出一个任务执行接口\"></a>1、抽象出一个任务执行接口</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">Executor</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">boolean</span> <span class=\"title function_\">execute</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"2、简单实现一个示例Task，需实现Executor接口。\"><a href=\"#2、简单实现一个示例Task，需实现Executor接口。\" class=\"headerlink\" title=\"2、简单实现一个示例Task，需实现Executor接口。\"></a>2、简单实现一个示例Task，需实现Executor接口。</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Task</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Executor</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long id;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String name;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> state;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Task</span><span class=\"params\">(Long id, String name, <span class=\"type\">int</span> state)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.id = id;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.name = name;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.state = state;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">execute</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;Task id: [&quot;</span> + id + <span class=\"string\">&quot;], &quot;</span> + <span class=\"string\">&quot;task name: [&quot;</span> + name +<span class=\"string\">&quot;] is running&quot;</span>);</span><br><span class=\"line\">        state = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">boolean</span> <span class=\"title function_\">hasExecuted</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state == <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Long <span class=\"title function_\">getId</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> id;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">getName</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> name;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">getState</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"3、实现DAG数据结构\"><a href=\"#3、实现DAG数据结构\" class=\"headerlink\" title=\"3、实现DAG数据结构\"></a>3、实现DAG数据结构</h4><p>这个类使用了邻接表来表示有向无环图。</p>\n<p>tasks是顶点集合，也就是任务集合。</p>\n<p>map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。</p>\n<p>一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.HashMap;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.HashSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Map;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Set;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 这个类使用了邻接表来表示有向无环图。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * tasks是顶点集合，也就是任务集合。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * map是任务依赖关系集合。key是一个任务，value是它的前置任务集合。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Digraph</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Set&lt;Task&gt; tasks;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Map&lt;Task, Set&lt;Task&gt;&gt; map;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">Digraph</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.tasks = <span class=\"keyword\">new</span> <span class=\"title class_\">HashSet</span>&lt;Task&gt;();</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.map = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;Task, Set&lt;Task&gt;&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">addEdge</span><span class=\"params\">(Task task, Task prev)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tasks.contains(task) || !tasks.contains(prev)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Set&lt;Task&gt; prevs = map.get(task);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prevs == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">            prevs = <span class=\"keyword\">new</span> <span class=\"title class_\">HashSet</span>&lt;Task&gt;();</span><br><span class=\"line\">            map.put(task, prevs);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prevs.contains(prev)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        prevs.add(prev);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">addTask</span><span class=\"params\">(Task task)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (tasks.contains(task)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalArgumentException</span>();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        tasks.add(task);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">remove</span><span class=\"params\">(Task task)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!tasks.contains(task)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (map.containsKey(task)) &#123;</span><br><span class=\"line\">            map.remove(task);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Set&lt;Task&gt; set : map.values()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (set.contains(task)) &#123;</span><br><span class=\"line\">                set.remove(task);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> Set&lt;Task&gt; <span class=\"title function_\">getTasks</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tasks;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setTasks</span><span class=\"params\">(Set&lt;Task&gt; tasks)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.tasks = tasks;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> Map&lt;Task, Set&lt;Task&gt;&gt; <span class=\"title function_\">getMap</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> map;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setMap</span><span class=\"params\">(Map&lt;Task, Set&lt;Task&gt;&gt; map)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.map = map;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"4、实现调度器：提交DAG进行调度执行\"><a href=\"#4、实现调度器：提交DAG进行调度执行\" class=\"headerlink\" title=\"4、实现调度器：提交DAG进行调度执行\"></a>4、实现调度器：提交DAG进行调度执行</h4><p>调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行（若考虑性能，可优化为并行执行）。</p>\n<p>若List为空，说明所有任务都已执行，则这一次任务调度结束。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau.yuan.dag;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.ArrayList;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Set;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.stream.Collectors;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 调度器的实现比较简单，就是遍历任务集合，找出待执行的任务集合，放到一个List中，再串行执行</span></span><br><span class=\"line\"><span class=\"comment\"> * （若考虑性能，可优化为并行执行）。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 若List为空，说明所有任务都已执行，则这一次任务调度结束。</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * 一个任务执行的前提是它在map中没有以它作为key的entry，或者是它的前置任务集合中的任务都是已执行的状态。</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Scheduler</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">schedule</span><span class=\"params\">(Digraph digraph)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">            List&lt;Task&gt; todo = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;Task&gt;();</span><br><span class=\"line\">            System.out.println();</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Task task : digraph.getTasks()) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!task.hasExecuted()) &#123;</span><br><span class=\"line\">                    Set&lt;Task&gt; prevs = digraph.getMap().get(task);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (prevs != <span class=\"literal\">null</span> &amp;&amp; !prevs.isEmpty()) &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// 或者是它的前置任务集合中的任务都是已执行的状态</span></span><br><span class=\"line\">                        <span class=\"type\">boolean</span> <span class=\"variable\">toAdd</span> <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">                        <span class=\"keyword\">for</span> (Task task1 : prevs) &#123;</span><br><span class=\"line\">                            <span class=\"keyword\">if</span> (!task1.hasExecuted()) &#123;</span><br><span class=\"line\">                                toAdd = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (toAdd) &#123;</span><br><span class=\"line\">                            todo.add(task);</span><br><span class=\"line\">                            <span class=\"type\">String</span> <span class=\"variable\">log</span> <span class=\"operator\">=</span> String.format(<span class=\"string\">&quot;%s需要被执行，因为其前置任务[%s]都已经执行成功！！！\\n&quot;</span>,</span><br><span class=\"line\">                                    task.getName(), prevs.stream().map(Task::getName).collect(Collectors.toList()));</span><br><span class=\"line\">                            System.out.printf(log);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// 一个任务执行的前提是它在map中没有以它作为key的entry</span></span><br><span class=\"line\">                        todo.add(task);</span><br><span class=\"line\">                        <span class=\"type\">String</span> <span class=\"variable\">log</span> <span class=\"operator\">=</span> String.format(<span class=\"string\">&quot;%s需要被执行，因为他是DAG开始执行的起点\\n&quot;</span>, task.getName());</span><br><span class=\"line\">                        System.out.printf(log);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!todo.isEmpty()) &#123;</span><br><span class=\"line\">                System.out.println(<span class=\"string\">&quot;这些任务将被并行执行： &quot;</span> + todo.stream().map(Task::getName).collect(Collectors.toList()));</span><br><span class=\"line\">                <span class=\"comment\">// 这里可以优化为并行执行</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (Task task : todo) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (!task.execute()) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Digraph</span> <span class=\"variable\">digraph</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Digraph</span>();</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task1</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">1L</span>, <span class=\"string\">&quot;task1&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task2</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">2L</span>, <span class=\"string\">&quot;task2&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task3</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">3L</span>, <span class=\"string\">&quot;task3&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task4</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">4L</span>, <span class=\"string\">&quot;task4&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task5</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">5L</span>, <span class=\"string\">&quot;task5&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">Task</span> <span class=\"variable\">task6</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Task</span>(<span class=\"number\">6L</span>, <span class=\"string\">&quot;task6&quot;</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">        digraph.addTask(task1);</span><br><span class=\"line\">        digraph.addTask(task2);</span><br><span class=\"line\">        digraph.addTask(task3);</span><br><span class=\"line\">        digraph.addTask(task4);</span><br><span class=\"line\">        digraph.addTask(task5);</span><br><span class=\"line\">        digraph.addTask(task6);</span><br><span class=\"line\">        digraph.addEdge(task1, task2);</span><br><span class=\"line\">        digraph.addEdge(task1, task5);</span><br><span class=\"line\">        digraph.addEdge(task6, task2);</span><br><span class=\"line\">        digraph.addEdge(task2, task3);</span><br><span class=\"line\">        digraph.addEdge(task2, task4);</span><br><span class=\"line\">        <span class=\"type\">Scheduler</span> <span class=\"variable\">scheduler</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Scheduler</span>();</span><br><span class=\"line\">        scheduler.schedule(digraph);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"IDEA运行结果\"><a href=\"#IDEA运行结果\" class=\"headerlink\" title=\"IDEA运行结果\"></a>IDEA运行结果</h2><p>Demo中的任务编排，如下图所示：</p>\n<p><img src=\"/img/dag1.png\" alt=\"DAG\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">task3需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">task4需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">task5需要被执行，因为他是DAG开始执行的起点</span><br><span class=\"line\">这些任务将被并行执行： [task3, task4, task5]</span><br><span class=\"line\">Task id: [3], task name: [task3] is running</span><br><span class=\"line\">Task id: [4], task name: [task4] is running</span><br><span class=\"line\">Task id: [5], task name: [task5] is running</span><br><span class=\"line\"></span><br><span class=\"line\">task2需要被执行，因为其前置任务[[task3, task4]]都已经执行成功！！！</span><br><span class=\"line\">这些任务将被并行执行： [task2]</span><br><span class=\"line\">Task id: [2], task name: [task2] is running</span><br><span class=\"line\"></span><br><span class=\"line\">task6需要被执行，因为其前置任务[[task2]]都已经执行成功！！！</span><br><span class=\"line\">task1需要被执行，因为其前置任务[[task2, task5]]都已经执行成功！！！</span><br><span class=\"line\">这些任务将被并行执行： [task6, task1]</span><br><span class=\"line\">Task id: [6], task name: [task6] is running</span><br><span class=\"line\">Task id: [1], task name: [task1] is running</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code 0</span><br></pre></td></tr></table></figure>\n\n"},{"title":"从SS-Table到LSM-Tree","abbrlink":62654,"date":"2022-09-11T03:45:16.000Z","updated":"2022-09-11T03:45:16.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## SS-Table\n\n- SSTable 最早出自 Google 的 Bigtable 论文\n\n  >An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.\n  >\n  >\n  >\n  >通过以上描述，我们可以把 SSTable 抽象为以下结构，每个 SSTable 包含了很多按照 key 排序的 key-value 对，key 和 value 都是任意的字节数组。SSTable 可以方便的支持基于 key 的查找和范围扫描。SSTable 会把数据分成块进行存储，并在 SSTable 文件尾部保存块索引(Block Index), 块索引记录每个块结束的 key 及对应的offset。块索引一般会在 SSTable 打开的时候载入内存。每次读取 SSTable 的时候，在内存中找到对应的块，再进行一次磁盘访问，读取到块中的数据。当然，把 SSTable 大小限定在可以加载进内存的大小，每次直接加载进内存访问也是一种方法。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868513341-45eab64f-2958-447b-8fc5-328a4942dfa1.png)\n\n> SSTable本身是个简单而有用的数据结构, 而往往由于工业界对于它的overload, 导致大家的误解\n> 它本身就像他的名字一样, 就是a set of sorted key-value pairs\n> 如下图左, 当文件比较大的时候, 也可以建立key:offset的index, 用于快速分段定位, 但这个是可选的.\n>\n> 这个结构和普通的key-value pairs的区别, **可以support range query和random r/w**\n\n## SSTables and Log Structured Merge Trees\n\n仅仅SSTable数据结构本身仍然无法support高效的range query和random r/w的场景\n还需要一整套的机制来完成从memory sort, flush to disk, compaction以及快速读取……这样的一个完成的机制和架构称为,\"[The Log-Structured Merge-Tree](http://nosqlsummer.org/paper/lsm-tree)\" (**LSM Tree**)\n名字很形象, 首先是基于log的, 不断产生SSTable结构的log文件, 并且是需要不断merge以提高效率的\n\n下图很好的描绘了LSM Tree的结构和大部分操作\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868915058-01727c5c-f5e9-402b-8737-41408cc5323d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/webp/2500465/1662868956879-bb12a4e1-38b5-47c0-a25e-4d0d8f8ce788.webp)\n\n> - 写操作：Tablet 把响应的操作写入操作日志（tablet log），然后将具体的内容写入内存中 MemTable\n> - 读操作：读操作需要同时读 Memtable 和 SSTable，将结果合并返回。Memtable 和 SSTable 都是按照 key 有序的，可以快速的进行类似归并排序的合并。\n> - Minor Compaction：随着写请求的不断增多，Memtable 在内存中的空间不断增大，当 Memtable 的大小达到一定阈值时，Memtable 被 dump 到 GFS 中成为不可变的 SSTable。\n> - Merging Compaction：随着 Memtable 不断的变为 SSTable，SSTable 也不断增多，意味着读操作需要读取的 SSTable 也越来越多，为了限制 SSTable 的个数，Tablet Server 会在后台将多个 SSTable 合并为一个\n> - Major Compaction：Major Compaction 是一种特殊的 Merging Compaction，只把所有的 SSTable 合并为一个 SSTable，在 非 Major Compaction 产生的 SSTable 中可能包含已经删除的数据，Major Compaction 的过程会将这些数据真正的剔除掉，避免这些数据浪费存储空间。\n\nLSM Tree 的索引机制和 B+ Tree 的索引机制是明显不同的，B+ Tree 为所有的数据维护了一个索引，LSM Tree 则是为每个 磁盘文件维护了一个 Index。\n\n\n\n## 列行存储\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869279366-ddf2ea3e-ed85-4708-854f-7306682043b4.png)\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869286634-2e3183d7-1ce6-410d-95c2-68a26db82e60.png)\n\n>在Doris中，数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。\n\n## 参考\n\n- https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\n","source":"_posts/data-structure/从SS-Table到LSM-Tree.md","raw":"---\ntitle: 从SS-Table到LSM-Tree\ntags:\n  - LSM-Tree\ncategories:\n  - - data-structure\nabbrlink: 62654\ndate: 2022-09-11 11:45:16\nupdated: 2022-09-11 11:45:16\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## SS-Table\n\n- SSTable 最早出自 Google 的 Bigtable 论文\n\n  >An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.\n  >\n  >\n  >\n  >通过以上描述，我们可以把 SSTable 抽象为以下结构，每个 SSTable 包含了很多按照 key 排序的 key-value 对，key 和 value 都是任意的字节数组。SSTable 可以方便的支持基于 key 的查找和范围扫描。SSTable 会把数据分成块进行存储，并在 SSTable 文件尾部保存块索引(Block Index), 块索引记录每个块结束的 key 及对应的offset。块索引一般会在 SSTable 打开的时候载入内存。每次读取 SSTable 的时候，在内存中找到对应的块，再进行一次磁盘访问，读取到块中的数据。当然，把 SSTable 大小限定在可以加载进内存的大小，每次直接加载进内存访问也是一种方法。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868513341-45eab64f-2958-447b-8fc5-328a4942dfa1.png)\n\n> SSTable本身是个简单而有用的数据结构, 而往往由于工业界对于它的overload, 导致大家的误解\n> 它本身就像他的名字一样, 就是a set of sorted key-value pairs\n> 如下图左, 当文件比较大的时候, 也可以建立key:offset的index, 用于快速分段定位, 但这个是可选的.\n>\n> 这个结构和普通的key-value pairs的区别, **可以support range query和random r/w**\n\n## SSTables and Log Structured Merge Trees\n\n仅仅SSTable数据结构本身仍然无法support高效的range query和random r/w的场景\n还需要一整套的机制来完成从memory sort, flush to disk, compaction以及快速读取……这样的一个完成的机制和架构称为,\"[The Log-Structured Merge-Tree](http://nosqlsummer.org/paper/lsm-tree)\" (**LSM Tree**)\n名字很形象, 首先是基于log的, 不断产生SSTable结构的log文件, 并且是需要不断merge以提高效率的\n\n下图很好的描绘了LSM Tree的结构和大部分操作\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868915058-01727c5c-f5e9-402b-8737-41408cc5323d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/webp/2500465/1662868956879-bb12a4e1-38b5-47c0-a25e-4d0d8f8ce788.webp)\n\n> - 写操作：Tablet 把响应的操作写入操作日志（tablet log），然后将具体的内容写入内存中 MemTable\n> - 读操作：读操作需要同时读 Memtable 和 SSTable，将结果合并返回。Memtable 和 SSTable 都是按照 key 有序的，可以快速的进行类似归并排序的合并。\n> - Minor Compaction：随着写请求的不断增多，Memtable 在内存中的空间不断增大，当 Memtable 的大小达到一定阈值时，Memtable 被 dump 到 GFS 中成为不可变的 SSTable。\n> - Merging Compaction：随着 Memtable 不断的变为 SSTable，SSTable 也不断增多，意味着读操作需要读取的 SSTable 也越来越多，为了限制 SSTable 的个数，Tablet Server 会在后台将多个 SSTable 合并为一个\n> - Major Compaction：Major Compaction 是一种特殊的 Merging Compaction，只把所有的 SSTable 合并为一个 SSTable，在 非 Major Compaction 产生的 SSTable 中可能包含已经删除的数据，Major Compaction 的过程会将这些数据真正的剔除掉，避免这些数据浪费存储空间。\n\nLSM Tree 的索引机制和 B+ Tree 的索引机制是明显不同的，B+ Tree 为所有的数据维护了一个索引，LSM Tree 则是为每个 磁盘文件维护了一个 Index。\n\n\n\n## 列行存储\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869279366-ddf2ea3e-ed85-4708-854f-7306682043b4.png)\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869286634-2e3183d7-1ce6-410d-95c2-68a26db82e60.png)\n\n>在Doris中，数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。\n\n## 参考\n\n- https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\n","slug":"data-structure/从SS-Table到LSM-Tree","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksw000u8j5m9pda04rk","content":"<h2 id=\"SS-Table\"><a href=\"#SS-Table\" class=\"headerlink\" title=\"SS-Table\"></a>SS-Table</h2><ul>\n<li><p>SSTable 最早出自 Google 的 Bigtable 论文</p>\n<blockquote>\n<p>An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key&#x2F;value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.</p>\n<p>通过以上描述，我们可以把 SSTable 抽象为以下结构，每个 SSTable 包含了很多按照 key 排序的 key-value 对，key 和 value 都是任意的字节数组。SSTable 可以方便的支持基于 key 的查找和范围扫描。SSTable 会把数据分成块进行存储，并在 SSTable 文件尾部保存块索引(Block Index), 块索引记录每个块结束的 key 及对应的offset。块索引一般会在 SSTable 打开的时候载入内存。每次读取 SSTable 的时候，在内存中找到对应的块，再进行一次磁盘访问，读取到块中的数据。当然，把 SSTable 大小限定在可以加载进内存的大小，每次直接加载进内存访问也是一种方法。</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868513341-45eab64f-2958-447b-8fc5-328a4942dfa1.png\" alt=\"img\"></p>\n<blockquote>\n<p>SSTable本身是个简单而有用的数据结构, 而往往由于工业界对于它的overload, 导致大家的误解<br>它本身就像他的名字一样, 就是a set of sorted key-value pairs<br>如下图左, 当文件比较大的时候, 也可以建立key:offset的index, 用于快速分段定位, 但这个是可选的.</p>\n<p>这个结构和普通的key-value pairs的区别, <strong>可以support range query和random r&#x2F;w</strong></p>\n</blockquote>\n<h2 id=\"SSTables-and-Log-Structured-Merge-Trees\"><a href=\"#SSTables-and-Log-Structured-Merge-Trees\" class=\"headerlink\" title=\"SSTables and Log Structured Merge Trees\"></a>SSTables and Log Structured Merge Trees</h2><p>仅仅SSTable数据结构本身仍然无法support高效的range query和random r&#x2F;w的场景<br>还需要一整套的机制来完成从memory sort, flush to disk, compaction以及快速读取……这样的一个完成的机制和架构称为,”<a href=\"http://nosqlsummer.org/paper/lsm-tree\">The Log-Structured Merge-Tree</a>“ (<strong>LSM Tree</strong>)<br>名字很形象, 首先是基于log的, 不断产生SSTable结构的log文件, 并且是需要不断merge以提高效率的</p>\n<p>下图很好的描绘了LSM Tree的结构和大部分操作</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868915058-01727c5c-f5e9-402b-8737-41408cc5323d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/webp/2500465/1662868956879-bb12a4e1-38b5-47c0-a25e-4d0d8f8ce788.webp\" alt=\"img\"></p>\n<blockquote>\n<ul>\n<li>写操作：Tablet 把响应的操作写入操作日志（tablet log），然后将具体的内容写入内存中 MemTable</li>\n<li>读操作：读操作需要同时读 Memtable 和 SSTable，将结果合并返回。Memtable 和 SSTable 都是按照 key 有序的，可以快速的进行类似归并排序的合并。</li>\n<li>Minor Compaction：随着写请求的不断增多，Memtable 在内存中的空间不断增大，当 Memtable 的大小达到一定阈值时，Memtable 被 dump 到 GFS 中成为不可变的 SSTable。</li>\n<li>Merging Compaction：随着 Memtable 不断的变为 SSTable，SSTable 也不断增多，意味着读操作需要读取的 SSTable 也越来越多，为了限制 SSTable 的个数，Tablet Server 会在后台将多个 SSTable 合并为一个</li>\n<li>Major Compaction：Major Compaction 是一种特殊的 Merging Compaction，只把所有的 SSTable 合并为一个 SSTable，在 非 Major Compaction 产生的 SSTable 中可能包含已经删除的数据，Major Compaction 的过程会将这些数据真正的剔除掉，避免这些数据浪费存储空间。</li>\n</ul>\n</blockquote>\n<p>LSM Tree 的索引机制和 B+ Tree 的索引机制是明显不同的，B+ Tree 为所有的数据维护了一个索引，LSM Tree 则是为每个 磁盘文件维护了一个 Index。</p>\n<h2 id=\"列行存储\"><a href=\"#列行存储\" class=\"headerlink\" title=\"列行存储\"></a>列行存储</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869279366-ddf2ea3e-ed85-4708-854f-7306682043b4.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869286634-2e3183d7-1ce6-410d-95c2-68a26db82e60.png\" alt=\"image.png\"></p>\n<blockquote>\n<p>在Doris中，数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。</p>\n</blockquote>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\">https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/</a></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"SS-Table\"><a href=\"#SS-Table\" class=\"headerlink\" title=\"SS-Table\"></a>SS-Table</h2><ul>\n<li><p>SSTable 最早出自 Google 的 Bigtable 论文</p>\n<blockquote>\n<p>An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key&#x2F;value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.</p>\n<p>通过以上描述，我们可以把 SSTable 抽象为以下结构，每个 SSTable 包含了很多按照 key 排序的 key-value 对，key 和 value 都是任意的字节数组。SSTable 可以方便的支持基于 key 的查找和范围扫描。SSTable 会把数据分成块进行存储，并在 SSTable 文件尾部保存块索引(Block Index), 块索引记录每个块结束的 key 及对应的offset。块索引一般会在 SSTable 打开的时候载入内存。每次读取 SSTable 的时候，在内存中找到对应的块，再进行一次磁盘访问，读取到块中的数据。当然，把 SSTable 大小限定在可以加载进内存的大小，每次直接加载进内存访问也是一种方法。</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868513341-45eab64f-2958-447b-8fc5-328a4942dfa1.png\" alt=\"img\"></p>\n<blockquote>\n<p>SSTable本身是个简单而有用的数据结构, 而往往由于工业界对于它的overload, 导致大家的误解<br>它本身就像他的名字一样, 就是a set of sorted key-value pairs<br>如下图左, 当文件比较大的时候, 也可以建立key:offset的index, 用于快速分段定位, 但这个是可选的.</p>\n<p>这个结构和普通的key-value pairs的区别, <strong>可以support range query和random r&#x2F;w</strong></p>\n</blockquote>\n<h2 id=\"SSTables-and-Log-Structured-Merge-Trees\"><a href=\"#SSTables-and-Log-Structured-Merge-Trees\" class=\"headerlink\" title=\"SSTables and Log Structured Merge Trees\"></a>SSTables and Log Structured Merge Trees</h2><p>仅仅SSTable数据结构本身仍然无法support高效的range query和random r&#x2F;w的场景<br>还需要一整套的机制来完成从memory sort, flush to disk, compaction以及快速读取……这样的一个完成的机制和架构称为,”<a href=\"http://nosqlsummer.org/paper/lsm-tree\">The Log-Structured Merge-Tree</a>“ (<strong>LSM Tree</strong>)<br>名字很形象, 首先是基于log的, 不断产生SSTable结构的log文件, 并且是需要不断merge以提高效率的</p>\n<p>下图很好的描绘了LSM Tree的结构和大部分操作</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662868915058-01727c5c-f5e9-402b-8737-41408cc5323d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/webp/2500465/1662868956879-bb12a4e1-38b5-47c0-a25e-4d0d8f8ce788.webp\" alt=\"img\"></p>\n<blockquote>\n<ul>\n<li>写操作：Tablet 把响应的操作写入操作日志（tablet log），然后将具体的内容写入内存中 MemTable</li>\n<li>读操作：读操作需要同时读 Memtable 和 SSTable，将结果合并返回。Memtable 和 SSTable 都是按照 key 有序的，可以快速的进行类似归并排序的合并。</li>\n<li>Minor Compaction：随着写请求的不断增多，Memtable 在内存中的空间不断增大，当 Memtable 的大小达到一定阈值时，Memtable 被 dump 到 GFS 中成为不可变的 SSTable。</li>\n<li>Merging Compaction：随着 Memtable 不断的变为 SSTable，SSTable 也不断增多，意味着读操作需要读取的 SSTable 也越来越多，为了限制 SSTable 的个数，Tablet Server 会在后台将多个 SSTable 合并为一个</li>\n<li>Major Compaction：Major Compaction 是一种特殊的 Merging Compaction，只把所有的 SSTable 合并为一个 SSTable，在 非 Major Compaction 产生的 SSTable 中可能包含已经删除的数据，Major Compaction 的过程会将这些数据真正的剔除掉，避免这些数据浪费存储空间。</li>\n</ul>\n</blockquote>\n<p>LSM Tree 的索引机制和 B+ Tree 的索引机制是明显不同的，B+ Tree 为所有的数据维护了一个索引，LSM Tree 则是为每个 磁盘文件维护了一个 Index。</p>\n<h2 id=\"列行存储\"><a href=\"#列行存储\" class=\"headerlink\" title=\"列行存储\"></a>列行存储</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869279366-ddf2ea3e-ed85-4708-854f-7306682043b4.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662869286634-2e3183d7-1ce6-410d-95c2-68a26db82e60.png\" alt=\"image.png\"></p>\n<blockquote>\n<p>在Doris中，数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。</p>\n</blockquote>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\">https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/</a></li>\n</ul>\n"},{"title":"hexo不显示语雀图床CDN图片的解决办法","abbrlink":21645,"date":"2022-07-02T03:22:17.000Z","updated":"2022-07-02T03:22:17.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n# 前言\n\n在语雀中写了一点东西，于是想着一起发到hexo上面，本地Typora显示完全没有问题，但是打开博客一看，图片全挂了！！！\n\n于是复制图片链接到浏览器上，竟然是直接下载，什么情况，直接懵逼。又试了试正常显示的图片，是在浏览器打开的。Google了半天，原来是语雀的防盗链搞得。\n\n\n\n# 解决方法\n\n\n\n### 1、在Hexo的.md文件加上`<meta name=\"referrer\" content=\"no-referrer\" />`\n\n- 可以在post模板中直接加上,就像下面这样，每次`hexo new post`创建都会自动加上，就不用每次都添加了。\n\n``` markdown\n---\ntitle: \ntags:\n  - ''\ncategories:\n  - []\ntop_img: \ndate: \nupdated: \ncover:\ndescription:\nkeywords:\n---\n  \n<meta name=\"referrer\" content=\"no-referrer\" />\n```\n\n\n\n### 2、以`<img src=\"xxxx\" referrerpolicy=\"no-referrer\">`的形式插入图片\n\n- 太麻烦了,每次都要设置`referrerpolicy=\"no-referrer\"`\n\n\n\n### 3、在html模版的头信息中添加`<meta name=\"referrer\" content=\"no-referrer\" />`\n\n#### 1、butterfly主题\n\n在hexo-theme-butterfly/layout/includes目录下的head.pug文件中添加`meta(name=\"referrer\" content=\"no-referrer\")`\n\n```typescript\nmeta(charset='UTF-8')\nmeta(http-equiv=\"X-UA-Compatible\" content=\"IE=edge\")\nmeta(name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\")\ntitle= tabTitle\nif pageKeywords\n  meta(name=\"keywords\" content=pageKeywords)\nmeta(name=\"author\" content=pageAuthor)\nmeta(name=\"copyright\" content=pageCopyright)\nmeta(name =\"format-detection\" content=\"telephone=no\")\nmeta(name=\"theme-color\" content=themeColor)\n\nmeta(name=\"referrer\" content=\"no-referrer\")\n```\n\n\n\n# 参考资料\n\n- https://github.com/x-cold/yuque-hexo/issues/41\n\n","source":"_posts/hexo/hexo不显示语雀图床CDN图片的解决办法.md","raw":"---\ntitle: hexo不显示语雀图床CDN图片的解决办法\ntags:\n  - hexo\ncategories:\n  - - hexo\nabbrlink: 21645\ndate: 2022-07-02 11:22:17\nupdated: 2022-07-02 11:22:17\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n# 前言\n\n在语雀中写了一点东西，于是想着一起发到hexo上面，本地Typora显示完全没有问题，但是打开博客一看，图片全挂了！！！\n\n于是复制图片链接到浏览器上，竟然是直接下载，什么情况，直接懵逼。又试了试正常显示的图片，是在浏览器打开的。Google了半天，原来是语雀的防盗链搞得。\n\n\n\n# 解决方法\n\n\n\n### 1、在Hexo的.md文件加上`<meta name=\"referrer\" content=\"no-referrer\" />`\n\n- 可以在post模板中直接加上,就像下面这样，每次`hexo new post`创建都会自动加上，就不用每次都添加了。\n\n``` markdown\n---\ntitle: \ntags:\n  - ''\ncategories:\n  - []\ntop_img: \ndate: \nupdated: \ncover:\ndescription:\nkeywords:\n---\n  \n<meta name=\"referrer\" content=\"no-referrer\" />\n```\n\n\n\n### 2、以`<img src=\"xxxx\" referrerpolicy=\"no-referrer\">`的形式插入图片\n\n- 太麻烦了,每次都要设置`referrerpolicy=\"no-referrer\"`\n\n\n\n### 3、在html模版的头信息中添加`<meta name=\"referrer\" content=\"no-referrer\" />`\n\n#### 1、butterfly主题\n\n在hexo-theme-butterfly/layout/includes目录下的head.pug文件中添加`meta(name=\"referrer\" content=\"no-referrer\")`\n\n```typescript\nmeta(charset='UTF-8')\nmeta(http-equiv=\"X-UA-Compatible\" content=\"IE=edge\")\nmeta(name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\")\ntitle= tabTitle\nif pageKeywords\n  meta(name=\"keywords\" content=pageKeywords)\nmeta(name=\"author\" content=pageAuthor)\nmeta(name=\"copyright\" content=pageCopyright)\nmeta(name =\"format-detection\" content=\"telephone=no\")\nmeta(name=\"theme-color\" content=themeColor)\n\nmeta(name=\"referrer\" content=\"no-referrer\")\n```\n\n\n\n# 参考资料\n\n- https://github.com/x-cold/yuque-hexo/issues/41\n\n","slug":"hexo/hexo不显示语雀图床CDN图片的解决办法","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksw000w8j5mfxa87l48","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在语雀中写了一点东西，于是想着一起发到hexo上面，本地Typora显示完全没有问题，但是打开博客一看，图片全挂了！！！</p>\n<p>于是复制图片链接到浏览器上，竟然是直接下载，什么情况，直接懵逼。又试了试正常显示的图片，是在浏览器打开的。Google了半天，原来是语雀的防盗链搞得。</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><h3 id=\"1、在Hexo的-md文件加上-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\"><a href=\"#1、在Hexo的-md文件加上-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\" class=\"headerlink\" title=\"1、在Hexo的.md文件加上&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;\"></a>1、在Hexo的.md文件加上<code>&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;</code></h3><ul>\n<li>可以在post模板中直接加上,就像下面这样，每次<code>hexo new post</code>创建都会自动加上，就不用每次都添加了。</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: </span><br><span class=\"line\">tags:</span><br><span class=\"line\"><span class=\"bullet\">  -</span> &#x27;&#x27;</span><br><span class=\"line\">categories:</span><br><span class=\"line\"><span class=\"bullet\">  -</span> []</span><br><span class=\"line\">top<span class=\"emphasis\">_img: </span></span><br><span class=\"line\"><span class=\"emphasis\">date: </span></span><br><span class=\"line\"><span class=\"emphasis\">updated: </span></span><br><span class=\"line\"><span class=\"emphasis\">cover:</span></span><br><span class=\"line\"><span class=\"emphasis\">description:</span></span><br><span class=\"line\"><span class=\"emphasis\">keywords:</span></span><br><span class=\"line\"><span class=\"emphasis\">---</span></span><br><span class=\"line\"><span class=\"emphasis\">  </span></span><br><span class=\"line\"><span class=\"emphasis\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;referrer&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;no-referrer&quot;</span> /&gt;</span></span></span></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"2、以-lt-img-src-quot-xxxx-quot-referrerpolicy-quot-no-referrer-quot-gt-的形式插入图片\"><a href=\"#2、以-lt-img-src-quot-xxxx-quot-referrerpolicy-quot-no-referrer-quot-gt-的形式插入图片\" class=\"headerlink\" title=\"2、以&lt;img src=&quot;xxxx&quot; referrerpolicy=&quot;no-referrer&quot;&gt;的形式插入图片\"></a>2、以<code>&lt;img src=&quot;xxxx&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</code>的形式插入图片</h3><ul>\n<li>太麻烦了,每次都要设置<code>referrerpolicy=&quot;no-referrer&quot;</code></li>\n</ul>\n<h3 id=\"3、在html模版的头信息中添加-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\"><a href=\"#3、在html模版的头信息中添加-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\" class=\"headerlink\" title=\"3、在html模版的头信息中添加&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;\"></a>3、在html模版的头信息中添加<code>&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;</code></h3><h4 id=\"1、butterfly主题\"><a href=\"#1、butterfly主题\" class=\"headerlink\" title=\"1、butterfly主题\"></a>1、butterfly主题</h4><p>在hexo-theme-butterfly&#x2F;layout&#x2F;includes目录下的head.pug文件中添加<code>meta(name=&quot;referrer&quot; content=&quot;no-referrer&quot;)</code></p>\n<figure class=\"highlight typescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">meta</span>(charset=<span class=\"string\">&#x27;UTF-8&#x27;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(http-equiv=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> content=<span class=\"string\">&quot;IE=edge&quot;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;viewport&quot;</span> content=<span class=\"string\">&quot;width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot;</span>)</span><br><span class=\"line\">title= tabTitle</span><br><span class=\"line\"><span class=\"keyword\">if</span> pageKeywords</span><br><span class=\"line\">  <span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;keywords&quot;</span> content=pageKeywords)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;author&quot;</span> content=pageAuthor)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;copyright&quot;</span> content=pageCopyright)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name =<span class=\"string\">&quot;format-detection&quot;</span> content=<span class=\"string\">&quot;telephone=no&quot;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;theme-color&quot;</span> content=themeColor)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;referrer&quot;</span> content=<span class=\"string\">&quot;no-referrer&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"https://github.com/x-cold/yuque-hexo/issues/41\">https://github.com/x-cold/yuque-hexo/issues/41</a></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>在语雀中写了一点东西，于是想着一起发到hexo上面，本地Typora显示完全没有问题，但是打开博客一看，图片全挂了！！！</p>\n<p>于是复制图片链接到浏览器上，竟然是直接下载，什么情况，直接懵逼。又试了试正常显示的图片，是在浏览器打开的。Google了半天，原来是语雀的防盗链搞得。</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><h3 id=\"1、在Hexo的-md文件加上-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\"><a href=\"#1、在Hexo的-md文件加上-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\" class=\"headerlink\" title=\"1、在Hexo的.md文件加上&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;\"></a>1、在Hexo的.md文件加上<code>&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;</code></h3><ul>\n<li>可以在post模板中直接加上,就像下面这样，每次<code>hexo new post</code>创建都会自动加上，就不用每次都添加了。</li>\n</ul>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: </span><br><span class=\"line\">tags:</span><br><span class=\"line\"><span class=\"bullet\">  -</span> &#x27;&#x27;</span><br><span class=\"line\">categories:</span><br><span class=\"line\"><span class=\"bullet\">  -</span> []</span><br><span class=\"line\">top<span class=\"emphasis\">_img: </span></span><br><span class=\"line\"><span class=\"emphasis\">date: </span></span><br><span class=\"line\"><span class=\"emphasis\">updated: </span></span><br><span class=\"line\"><span class=\"emphasis\">cover:</span></span><br><span class=\"line\"><span class=\"emphasis\">description:</span></span><br><span class=\"line\"><span class=\"emphasis\">keywords:</span></span><br><span class=\"line\"><span class=\"emphasis\">---</span></span><br><span class=\"line\"><span class=\"emphasis\">  </span></span><br><span class=\"line\"><span class=\"emphasis\"><span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;referrer&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;no-referrer&quot;</span> /&gt;</span></span></span></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"2、以-lt-img-src-quot-xxxx-quot-referrerpolicy-quot-no-referrer-quot-gt-的形式插入图片\"><a href=\"#2、以-lt-img-src-quot-xxxx-quot-referrerpolicy-quot-no-referrer-quot-gt-的形式插入图片\" class=\"headerlink\" title=\"2、以&lt;img src=&quot;xxxx&quot; referrerpolicy=&quot;no-referrer&quot;&gt;的形式插入图片\"></a>2、以<code>&lt;img src=&quot;xxxx&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</code>的形式插入图片</h3><ul>\n<li>太麻烦了,每次都要设置<code>referrerpolicy=&quot;no-referrer&quot;</code></li>\n</ul>\n<h3 id=\"3、在html模版的头信息中添加-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\"><a href=\"#3、在html模版的头信息中添加-lt-meta-name-quot-referrer-quot-content-quot-no-referrer-quot-gt\" class=\"headerlink\" title=\"3、在html模版的头信息中添加&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;\"></a>3、在html模版的头信息中添加<code>&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;</code></h3><h4 id=\"1、butterfly主题\"><a href=\"#1、butterfly主题\" class=\"headerlink\" title=\"1、butterfly主题\"></a>1、butterfly主题</h4><p>在hexo-theme-butterfly&#x2F;layout&#x2F;includes目录下的head.pug文件中添加<code>meta(name=&quot;referrer&quot; content=&quot;no-referrer&quot;)</code></p>\n<figure class=\"highlight typescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">meta</span>(charset=<span class=\"string\">&#x27;UTF-8&#x27;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(http-equiv=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> content=<span class=\"string\">&quot;IE=edge&quot;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;viewport&quot;</span> content=<span class=\"string\">&quot;width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot;</span>)</span><br><span class=\"line\">title= tabTitle</span><br><span class=\"line\"><span class=\"keyword\">if</span> pageKeywords</span><br><span class=\"line\">  <span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;keywords&quot;</span> content=pageKeywords)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;author&quot;</span> content=pageAuthor)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;copyright&quot;</span> content=pageCopyright)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name =<span class=\"string\">&quot;format-detection&quot;</span> content=<span class=\"string\">&quot;telephone=no&quot;</span>)</span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;theme-color&quot;</span> content=themeColor)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title function_\">meta</span>(name=<span class=\"string\">&quot;referrer&quot;</span> content=<span class=\"string\">&quot;no-referrer&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"https://github.com/x-cold/yuque-hexo/issues/41\">https://github.com/x-cold/yuque-hexo/issues/41</a></li>\n</ul>\n"},{"title":"从github恢复备份hexo博客By hexo-git-backup","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":2646,"date":"2022-06-29T05:54:12.000Z","updated":"2022-06-29T05:54:12.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n\n\n​\t\t利用hexo + github pages构建静态博客网站，hexo发布到github上的内容是渲染过后的文件。而我们自己写的markdown文件并没有推送到github上面，因此如果发生电脑挂掉、磁盘挂掉等意外，我们的.md源文件以及我们的博客配置文件就会丢失。丢失后要想还原回去，就需要费好大力气了。\n\n​\t\t因此我们需要备份源数据，并且最好每次部署博客的时候，就自动进行备份，而不需要再手动去备份。\n\n\n\n## 利用hexo-git-backup插件备份源文件\n\n- 1、安装hexo-git-backup插件\n\n  ```\n  $ npm install hexo-git-backup --save\n  ```\n\n- 2、配置插件，同步源数据到github仓库\n\n  **强烈建议：备份到博客所在的同一个git仓库的不同分支，方便管理，下面是备份到hexo分支**\n\n  > 编辑hexo的配置文件_config.yml，添加需要备份到仓库\n\n  ```yml\n  # 备份插件：hexo-git-backup\n  backup:\n      type: git\n      repository:\n         github: git@github.com:xxxxx.git,hexo\n  ```\n\n- 3、hexo根目录执行hexo b命令，即可完成备份\n\n\n\n##  每次更新博客后，自动进行备份\n\n- 使用windows的同学，强烈建议(￣▽￣)\"开启linux子系统WSL，在WSL中部署Hexo：https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/\n\n### 利用shell alias，部署后自动备份\n\n- 编辑zsh的配置文件~/.zshrc，添加别名alias：\n\n```shell\nalias hd=\"hexo clean && hexo g && hexo d && hexo b\"\n```\n\n- 每次要更新博客进行部署的时候，直接执行`hd`命令，就会自动完成部署和备份的工作。\n\n  \n\n  \n","source":"_posts/hexo/从github恢复备份hexo博客hexo-git-backup.md","raw":"---\ntitle: 从github恢复备份hexo博客By hexo-git-backup\ntags:\n  - hexo\ncategories:\n  - - hexo\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 2646\ndate: 2022-06-29 13:54:12\nupdated: 2022-06-29 13:54:12\ncover:\ndescription:\nkeywords:\n---\n\n\n\n​\t\t利用hexo + github pages构建静态博客网站，hexo发布到github上的内容是渲染过后的文件。而我们自己写的markdown文件并没有推送到github上面，因此如果发生电脑挂掉、磁盘挂掉等意外，我们的.md源文件以及我们的博客配置文件就会丢失。丢失后要想还原回去，就需要费好大力气了。\n\n​\t\t因此我们需要备份源数据，并且最好每次部署博客的时候，就自动进行备份，而不需要再手动去备份。\n\n\n\n## 利用hexo-git-backup插件备份源文件\n\n- 1、安装hexo-git-backup插件\n\n  ```\n  $ npm install hexo-git-backup --save\n  ```\n\n- 2、配置插件，同步源数据到github仓库\n\n  **强烈建议：备份到博客所在的同一个git仓库的不同分支，方便管理，下面是备份到hexo分支**\n\n  > 编辑hexo的配置文件_config.yml，添加需要备份到仓库\n\n  ```yml\n  # 备份插件：hexo-git-backup\n  backup:\n      type: git\n      repository:\n         github: git@github.com:xxxxx.git,hexo\n  ```\n\n- 3、hexo根目录执行hexo b命令，即可完成备份\n\n\n\n##  每次更新博客后，自动进行备份\n\n- 使用windows的同学，强烈建议(￣▽￣)\"开启linux子系统WSL，在WSL中部署Hexo：https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/\n\n### 利用shell alias，部署后自动备份\n\n- 编辑zsh的配置文件~/.zshrc，添加别名alias：\n\n```shell\nalias hd=\"hexo clean && hexo g && hexo d && hexo b\"\n```\n\n- 每次要更新博客进行部署的时候，直接执行`hd`命令，就会自动完成部署和备份的工作。\n\n  \n\n  \n","slug":"hexo/从github恢复备份hexo博客hexo-git-backup","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksw000y8j5m2ys488ia","content":"<p>​\t\t利用hexo + github pages构建静态博客网站，hexo发布到github上的内容是渲染过后的文件。而我们自己写的markdown文件并没有推送到github上面，因此如果发生电脑挂掉、磁盘挂掉等意外，我们的.md源文件以及我们的博客配置文件就会丢失。丢失后要想还原回去，就需要费好大力气了。</p>\n<p>​\t\t因此我们需要备份源数据，并且最好每次部署博客的时候，就自动进行备份，而不需要再手动去备份。</p>\n<h2 id=\"利用hexo-git-backup插件备份源文件\"><a href=\"#利用hexo-git-backup插件备份源文件\" class=\"headerlink\" title=\"利用hexo-git-backup插件备份源文件\"></a>利用hexo-git-backup插件备份源文件</h2><ul>\n<li><p>1、安装hexo-git-backup插件</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-git-backup --save</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、配置插件，同步源数据到github仓库</p>\n<p><strong>强烈建议：备份到博客所在的同一个git仓库的不同分支，方便管理，下面是备份到hexo分支</strong></p>\n<blockquote>\n<p>编辑hexo的配置文件_config.yml，添加需要备份到仓库</p>\n</blockquote>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 备份插件：hexo-git-backup</span></span><br><span class=\"line\"><span class=\"attr\">backup:</span></span><br><span class=\"line\">    <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">    <span class=\"attr\">repository:</span></span><br><span class=\"line\">       <span class=\"attr\">github:</span> <span class=\"string\">git@github.com:xxxxx.git,hexo</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、hexo根目录执行hexo b命令，即可完成备份</p>\n</li>\n</ul>\n<h2 id=\"每次更新博客后，自动进行备份\"><a href=\"#每次更新博客后，自动进行备份\" class=\"headerlink\" title=\"每次更新博客后，自动进行备份\"></a>每次更新博客后，自动进行备份</h2><ul>\n<li>使用windows的同学，强烈建议(￣▽￣)”开启linux子系统WSL，在WSL中部署Hexo：<a href=\"https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/\">https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/</a></li>\n</ul>\n<h3 id=\"利用shell-alias，部署后自动备份\"><a href=\"#利用shell-alias，部署后自动备份\" class=\"headerlink\" title=\"利用shell alias，部署后自动备份\"></a>利用shell alias，部署后自动备份</h3><ul>\n<li>编辑zsh的配置文件~&#x2F;.zshrc，添加别名alias：</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias hd=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d &amp;&amp; hexo b&quot;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>每次要更新博客进行部署的时候，直接执行<code>hd</code>命令，就会自动完成部署和备份的工作。</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<p>​\t\t利用hexo + github pages构建静态博客网站，hexo发布到github上的内容是渲染过后的文件。而我们自己写的markdown文件并没有推送到github上面，因此如果发生电脑挂掉、磁盘挂掉等意外，我们的.md源文件以及我们的博客配置文件就会丢失。丢失后要想还原回去，就需要费好大力气了。</p>\n<p>​\t\t因此我们需要备份源数据，并且最好每次部署博客的时候，就自动进行备份，而不需要再手动去备份。</p>\n<h2 id=\"利用hexo-git-backup插件备份源文件\"><a href=\"#利用hexo-git-backup插件备份源文件\" class=\"headerlink\" title=\"利用hexo-git-backup插件备份源文件\"></a>利用hexo-git-backup插件备份源文件</h2><ul>\n<li><p>1、安装hexo-git-backup插件</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-git-backup --save</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、配置插件，同步源数据到github仓库</p>\n<p><strong>强烈建议：备份到博客所在的同一个git仓库的不同分支，方便管理，下面是备份到hexo分支</strong></p>\n<blockquote>\n<p>编辑hexo的配置文件_config.yml，添加需要备份到仓库</p>\n</blockquote>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 备份插件：hexo-git-backup</span></span><br><span class=\"line\"><span class=\"attr\">backup:</span></span><br><span class=\"line\">    <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">    <span class=\"attr\">repository:</span></span><br><span class=\"line\">       <span class=\"attr\">github:</span> <span class=\"string\">git@github.com:xxxxx.git,hexo</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、hexo根目录执行hexo b命令，即可完成备份</p>\n</li>\n</ul>\n<h2 id=\"每次更新博客后，自动进行备份\"><a href=\"#每次更新博客后，自动进行备份\" class=\"headerlink\" title=\"每次更新博客后，自动进行备份\"></a>每次更新博客后，自动进行备份</h2><ul>\n<li>使用windows的同学，强烈建议(￣▽￣)”开启linux子系统WSL，在WSL中部署Hexo：<a href=\"https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/\">https://poxiao.tk/2022/06/WSL%E4%B8%AD%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/</a></li>\n</ul>\n<h3 id=\"利用shell-alias，部署后自动备份\"><a href=\"#利用shell-alias，部署后自动备份\" class=\"headerlink\" title=\"利用shell alias，部署后自动备份\"></a>利用shell alias，部署后自动备份</h3><ul>\n<li>编辑zsh的配置文件~&#x2F;.zshrc，添加别名alias：</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias hd=&quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo d &amp;&amp; hexo b&quot;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>每次要更新博客进行部署的时候，直接执行<code>hd</code>命令，就会自动完成部署和备份的工作。</li>\n</ul>\n"},{"title":"Doris Join最佳实践","abbrlink":41835,"date":"2022-09-25T06:51:43.000Z","updated":"2022-09-25T06:51:43.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 前言\n\n Doris 支持两种物理算子，一类是 **Hash Join**，另一类是 **Nest Loop Join**。\n\n- Hash Join：在右表上根据等值 Join 列建立哈希表，左表流式的利用哈希表进行 Join 计算，它的限制是只能适用于等值 Join。\n- Nest Loop Join：通过两个 for 循环，很直观。然后它适用的场景就是不等值的 Join，例如：大于小于或者是需要求笛卡尔积的场景。它是一个通用的 Join 算子，但是性能表现差。\n\n作为分布式的 MPP 数据库， 在 Join 的过程中是需要进行数据的 Shuffle。数据需要进行拆分调度，才能保证最终的 Join 结果是正确的。举个简单的例子，假设关系S 和 R 进行Join，N 表示参与 Join 计算的节点的数量；T 则表示关系的 Tuple 数目。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664090756329-d7a09f4c-4837-414c-8910-8f5efe1eb247.png?x-oss-process=image%2Fresize%2Cw_1875%2Climit_0)\n\n## Doris Join 调优方法\n\nDoris Join 调优的方法：\n\n- 利用 Doris 本身提供的 Profile，去定位查询的瓶颈。Profile 会记录 Doris 整个查询当中各种信息，这是进行性能调优的一手资料。\n- 了解 Doris 的 Join 机制。知其然知其所以然、了解它的机制，才能分析它为什么比较慢。\n- 利用 Session 变量去改变 Join 的一些行为，从而实现 Join 的调优。\n- 查看 Query Plan 去分析这个调优是否生效。\n\n上面的 4 步基本上完成了一个标准的 Join 调优流程，接着就是实际去查询验证它，看看效果到底怎么样。\n\n如果前面 4 种方式串联起来之后，还是不奏效。这时候可能就需要去做 Join 语句的改写，或者是数据分布的调整、需要重新去 Recheck 整个数据分布是否合理，包括查询 Join 语句，可能需要做一些手动的调整。当然这种方式是心智成本是比较高的，也就是说要在尝试前面方式不奏效的情况下，才需要去做进一步的分析。\n\n## Doris Join 调优建议\n\n最后我们总结 Doris Join 优化调优的四点建议：\n\n- 第一点：在做 Join 的时候，要尽量选择同类型或者简单类型的列，同类型的话就减少它的数据 Cast，简单类型本身 Join 计算就很快。\n- 第二点：尽量选择 Key 列进行 Join， 原因前面在 Runtime Filter 的时候也介绍了，Key 列在延迟物化上能起到一个比较好的效果。\n- 第三点：大表之间的 Join ，尽量让它 Co-location ，因为大表之间的网络开销是很大的，如果需要去做 Shuffle 的话，代价是很高的。\n- 第四点：合理的使用 Runtime Filter，它在 Join 过滤率高的场景下效果是非常显著的。但是它并不是万灵药，而是有一定副作用的，所以需要根据具体的 SQL 的粒度做开关。\n- 最后：要涉及到多表 Join 的时候，需要去判断 Join 的合理性。尽量保证左表为大表，右表为小表，然后 Hash Join 会优于 Nest Loop Join。必要的时可以通过 SQL Rewrite，利用 Hint 去调整 Join 的顺序。\n\n\n\n## 调优案例实战\n\n### 案例一\n\n一个四张表 Join 的查询，通过 Profile 的时候发现第二个 Join 耗时很高，耗时 14 秒。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799212-d06d6eeb-01fa-41a7-8a74-614d99a626fd.png)\n\n进一步分析 Profile 之后，发现 BuildRows，就是右表的数据量是大概 2500 万。而 ProbeRows （ ProbeRows 是左表的数据量）只有 1 万多。这种场景下右表是远远大于左表，这显然是个不合理的情况。这显然说明 Join 的顺序出现了一些问题。这时候尝试改变 Session 变量，开启 Join Reorder。\n\n```text\nset enable_cost_based_join_reorder = true\n```\n\n\n\n这次耗时从 14 秒降到了 4 秒，性能提升了 3 倍多。\n\n此时再 Check Profile 的时候，左右表的顺序已经调整正确，即右表是大表，左表是小表。基于小表去构建哈希表，开销是很小的，这就是典型的一个利用 Join Reorder 去提升 Join 性能的一个场景\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-2556ed2c-a041-48b3-b32c-d4fac1ab7ace.png)\n\n### 案例二\n\n存在一个慢查询，查看 Profile 之后，整个 Join 节点耗时大概44秒。它的右表有 1000 万，左表有 6000 万，最终返回的结果也只有 6000 万。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-6289e219-dd31-495b-967f-dd6400b945f0.png)\n\n这里可以大致的估算出过滤率是很高的，那为什么 Runtime Filter 没有生效呢？通过 Query Plan 去查看它，发现它只开启了 IN 的 Runtime Filter。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799132-41ae2249-b72d-43f0-b6fc-e2d9ddc07ae8.png)\n\n当右表超过1024行的话， IN 是不生效的，所以根本起不到什么过滤的效果，所以尝试调整 RuntimeFilter 的类型。\n\n这里改为了 BloomFilter，左表的 6000 万条数据过滤了 5900 万条。基本上 99% 的数据都被过滤掉了，这个效果是很显著的。查询也从原来的 44 秒降到了 13 秒，性能提升了大概也是三倍多。\n\n### 案例三\n\n下面是一个比较极端的 Case，通过一些环境变量调优也没有办法解决，因为它涉及到 SQL Rewrite，所以这里列出来了原始的 SQL 。\n\n```sql\nselect 100.00 * sum (case\n        when P_type like 'PROMOS'\n        then 1 extendedprice * (1 - 1 discount)\n        else 0\n        end ) / sum(1 extendedprice * (1 - 1 discount)) as promo revenue\nfrom lineitem, part\nwhere\n    1_partkey = p_partkey\n    and 1_shipdate >= date '1997-06-01'\n    and 1 shipdate < date '1997-06-01' + interval '1' month\n```\n\n\n\n这个 Join 查询是很简单的，单纯的一个左右表的 Join 。当然它上面有一些过滤条件，打开 Profile 的时候，发现整个查询 Hash Join 执行了三分多钟，它是一个 BroadCast 的 Join，它的右表有 2 亿条，左表只有 70 万。在这种情况下选择了 Broadcast Join 是不合理的，这相当于要把 2 亿条做一个 Hash Table，然后用 70 万条遍历两亿条的 Hash Table ，这显然是不合理的。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092798965-5f747810-f38d-468f-8f5b-2846a8dc8928.png)\n\n为什么会产生不合理的 Join 顺序呢？其实这个左表是一个 10 亿条级别的大表，它上面加了两个过滤条件，加完这两个过滤条件之后， 10 亿条的数据就剩 70 万条了。但 Doris 目前没有一个好的统计信息收集的框架，所以它不知道这个过滤条件的过滤率到底怎么样。所以这个 Join 顺序安排的时候，就选择了错误的 Join 的左右表顺序，导致它的性能是极其低下的。\n\n下图是改写完成之后的一个 SQL 语句，在 Join 后面添加了一个Join Hint，在Join 后面加一个方括号，然后把需要的 Join 方式写入。这里选择了 Shuffle Join，可以看到右边它实际查询计划里面看到这个数据确实是做了 Partition ，原先 3 分钟的耗时通过这样的改写完之后只剩下 7 秒，性能提升明显\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092800013-fc558dfd-5923-447e-aceb-4b97111f9fc2.png)\n","source":"_posts/bigdata/Doris-Join最佳实践.md","raw":"---\ntitle: Doris Join最佳实践\ntags:\n  - Doris\ncategories:\n  - - bigdata\n    - Doris\nabbrlink: 41835\ndate: 2022-09-25 14:51:43\nupdated: 2022-09-25 14:51:43\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 前言\n\n Doris 支持两种物理算子，一类是 **Hash Join**，另一类是 **Nest Loop Join**。\n\n- Hash Join：在右表上根据等值 Join 列建立哈希表，左表流式的利用哈希表进行 Join 计算，它的限制是只能适用于等值 Join。\n- Nest Loop Join：通过两个 for 循环，很直观。然后它适用的场景就是不等值的 Join，例如：大于小于或者是需要求笛卡尔积的场景。它是一个通用的 Join 算子，但是性能表现差。\n\n作为分布式的 MPP 数据库， 在 Join 的过程中是需要进行数据的 Shuffle。数据需要进行拆分调度，才能保证最终的 Join 结果是正确的。举个简单的例子，假设关系S 和 R 进行Join，N 表示参与 Join 计算的节点的数量；T 则表示关系的 Tuple 数目。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664090756329-d7a09f4c-4837-414c-8910-8f5efe1eb247.png?x-oss-process=image%2Fresize%2Cw_1875%2Climit_0)\n\n## Doris Join 调优方法\n\nDoris Join 调优的方法：\n\n- 利用 Doris 本身提供的 Profile，去定位查询的瓶颈。Profile 会记录 Doris 整个查询当中各种信息，这是进行性能调优的一手资料。\n- 了解 Doris 的 Join 机制。知其然知其所以然、了解它的机制，才能分析它为什么比较慢。\n- 利用 Session 变量去改变 Join 的一些行为，从而实现 Join 的调优。\n- 查看 Query Plan 去分析这个调优是否生效。\n\n上面的 4 步基本上完成了一个标准的 Join 调优流程，接着就是实际去查询验证它，看看效果到底怎么样。\n\n如果前面 4 种方式串联起来之后，还是不奏效。这时候可能就需要去做 Join 语句的改写，或者是数据分布的调整、需要重新去 Recheck 整个数据分布是否合理，包括查询 Join 语句，可能需要做一些手动的调整。当然这种方式是心智成本是比较高的，也就是说要在尝试前面方式不奏效的情况下，才需要去做进一步的分析。\n\n## Doris Join 调优建议\n\n最后我们总结 Doris Join 优化调优的四点建议：\n\n- 第一点：在做 Join 的时候，要尽量选择同类型或者简单类型的列，同类型的话就减少它的数据 Cast，简单类型本身 Join 计算就很快。\n- 第二点：尽量选择 Key 列进行 Join， 原因前面在 Runtime Filter 的时候也介绍了，Key 列在延迟物化上能起到一个比较好的效果。\n- 第三点：大表之间的 Join ，尽量让它 Co-location ，因为大表之间的网络开销是很大的，如果需要去做 Shuffle 的话，代价是很高的。\n- 第四点：合理的使用 Runtime Filter，它在 Join 过滤率高的场景下效果是非常显著的。但是它并不是万灵药，而是有一定副作用的，所以需要根据具体的 SQL 的粒度做开关。\n- 最后：要涉及到多表 Join 的时候，需要去判断 Join 的合理性。尽量保证左表为大表，右表为小表，然后 Hash Join 会优于 Nest Loop Join。必要的时可以通过 SQL Rewrite，利用 Hint 去调整 Join 的顺序。\n\n\n\n## 调优案例实战\n\n### 案例一\n\n一个四张表 Join 的查询，通过 Profile 的时候发现第二个 Join 耗时很高，耗时 14 秒。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799212-d06d6eeb-01fa-41a7-8a74-614d99a626fd.png)\n\n进一步分析 Profile 之后，发现 BuildRows，就是右表的数据量是大概 2500 万。而 ProbeRows （ ProbeRows 是左表的数据量）只有 1 万多。这种场景下右表是远远大于左表，这显然是个不合理的情况。这显然说明 Join 的顺序出现了一些问题。这时候尝试改变 Session 变量，开启 Join Reorder。\n\n```text\nset enable_cost_based_join_reorder = true\n```\n\n\n\n这次耗时从 14 秒降到了 4 秒，性能提升了 3 倍多。\n\n此时再 Check Profile 的时候，左右表的顺序已经调整正确，即右表是大表，左表是小表。基于小表去构建哈希表，开销是很小的，这就是典型的一个利用 Join Reorder 去提升 Join 性能的一个场景\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-2556ed2c-a041-48b3-b32c-d4fac1ab7ace.png)\n\n### 案例二\n\n存在一个慢查询，查看 Profile 之后，整个 Join 节点耗时大概44秒。它的右表有 1000 万，左表有 6000 万，最终返回的结果也只有 6000 万。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-6289e219-dd31-495b-967f-dd6400b945f0.png)\n\n这里可以大致的估算出过滤率是很高的，那为什么 Runtime Filter 没有生效呢？通过 Query Plan 去查看它，发现它只开启了 IN 的 Runtime Filter。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799132-41ae2249-b72d-43f0-b6fc-e2d9ddc07ae8.png)\n\n当右表超过1024行的话， IN 是不生效的，所以根本起不到什么过滤的效果，所以尝试调整 RuntimeFilter 的类型。\n\n这里改为了 BloomFilter，左表的 6000 万条数据过滤了 5900 万条。基本上 99% 的数据都被过滤掉了，这个效果是很显著的。查询也从原来的 44 秒降到了 13 秒，性能提升了大概也是三倍多。\n\n### 案例三\n\n下面是一个比较极端的 Case，通过一些环境变量调优也没有办法解决，因为它涉及到 SQL Rewrite，所以这里列出来了原始的 SQL 。\n\n```sql\nselect 100.00 * sum (case\n        when P_type like 'PROMOS'\n        then 1 extendedprice * (1 - 1 discount)\n        else 0\n        end ) / sum(1 extendedprice * (1 - 1 discount)) as promo revenue\nfrom lineitem, part\nwhere\n    1_partkey = p_partkey\n    and 1_shipdate >= date '1997-06-01'\n    and 1 shipdate < date '1997-06-01' + interval '1' month\n```\n\n\n\n这个 Join 查询是很简单的，单纯的一个左右表的 Join 。当然它上面有一些过滤条件，打开 Profile 的时候，发现整个查询 Hash Join 执行了三分多钟，它是一个 BroadCast 的 Join，它的右表有 2 亿条，左表只有 70 万。在这种情况下选择了 Broadcast Join 是不合理的，这相当于要把 2 亿条做一个 Hash Table，然后用 70 万条遍历两亿条的 Hash Table ，这显然是不合理的。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092798965-5f747810-f38d-468f-8f5b-2846a8dc8928.png)\n\n为什么会产生不合理的 Join 顺序呢？其实这个左表是一个 10 亿条级别的大表，它上面加了两个过滤条件，加完这两个过滤条件之后， 10 亿条的数据就剩 70 万条了。但 Doris 目前没有一个好的统计信息收集的框架，所以它不知道这个过滤条件的过滤率到底怎么样。所以这个 Join 顺序安排的时候，就选择了错误的 Join 的左右表顺序，导致它的性能是极其低下的。\n\n下图是改写完成之后的一个 SQL 语句，在 Join 后面添加了一个Join Hint，在Join 后面加一个方括号，然后把需要的 Join 方式写入。这里选择了 Shuffle Join，可以看到右边它实际查询计划里面看到这个数据确实是做了 Partition ，原先 3 分钟的耗时通过这样的改写完之后只剩下 7 秒，性能提升明显\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092800013-fc558dfd-5923-447e-aceb-4b97111f9fc2.png)\n","slug":"bigdata/Doris-Join最佳实践","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksw00128j5mfqp5atgp","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p> Doris 支持两种物理算子，一类是 <strong>Hash Join</strong>，另一类是 <strong>Nest Loop Join</strong>。</p>\n<ul>\n<li>Hash Join：在右表上根据等值 Join 列建立哈希表，左表流式的利用哈希表进行 Join 计算，它的限制是只能适用于等值 Join。</li>\n<li>Nest Loop Join：通过两个 for 循环，很直观。然后它适用的场景就是不等值的 Join，例如：大于小于或者是需要求笛卡尔积的场景。它是一个通用的 Join 算子，但是性能表现差。</li>\n</ul>\n<p>作为分布式的 MPP 数据库， 在 Join 的过程中是需要进行数据的 Shuffle。数据需要进行拆分调度，才能保证最终的 Join 结果是正确的。举个简单的例子，假设关系S 和 R 进行Join，N 表示参与 Join 计算的节点的数量；T 则表示关系的 Tuple 数目。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664090756329-d7a09f4c-4837-414c-8910-8f5efe1eb247.png?x-oss-process=image/resize,w_1875,limit_0\" alt=\"image.png\"></p>\n<h2 id=\"Doris-Join-调优方法\"><a href=\"#Doris-Join-调优方法\" class=\"headerlink\" title=\"Doris Join 调优方法\"></a>Doris Join 调优方法</h2><p>Doris Join 调优的方法：</p>\n<ul>\n<li>利用 Doris 本身提供的 Profile，去定位查询的瓶颈。Profile 会记录 Doris 整个查询当中各种信息，这是进行性能调优的一手资料。</li>\n<li>了解 Doris 的 Join 机制。知其然知其所以然、了解它的机制，才能分析它为什么比较慢。</li>\n<li>利用 Session 变量去改变 Join 的一些行为，从而实现 Join 的调优。</li>\n<li>查看 Query Plan 去分析这个调优是否生效。</li>\n</ul>\n<p>上面的 4 步基本上完成了一个标准的 Join 调优流程，接着就是实际去查询验证它，看看效果到底怎么样。</p>\n<p>如果前面 4 种方式串联起来之后，还是不奏效。这时候可能就需要去做 Join 语句的改写，或者是数据分布的调整、需要重新去 Recheck 整个数据分布是否合理，包括查询 Join 语句，可能需要做一些手动的调整。当然这种方式是心智成本是比较高的，也就是说要在尝试前面方式不奏效的情况下，才需要去做进一步的分析。</p>\n<h2 id=\"Doris-Join-调优建议\"><a href=\"#Doris-Join-调优建议\" class=\"headerlink\" title=\"Doris Join 调优建议\"></a>Doris Join 调优建议</h2><p>最后我们总结 Doris Join 优化调优的四点建议：</p>\n<ul>\n<li>第一点：在做 Join 的时候，要尽量选择同类型或者简单类型的列，同类型的话就减少它的数据 Cast，简单类型本身 Join 计算就很快。</li>\n<li>第二点：尽量选择 Key 列进行 Join， 原因前面在 Runtime Filter 的时候也介绍了，Key 列在延迟物化上能起到一个比较好的效果。</li>\n<li>第三点：大表之间的 Join ，尽量让它 Co-location ，因为大表之间的网络开销是很大的，如果需要去做 Shuffle 的话，代价是很高的。</li>\n<li>第四点：合理的使用 Runtime Filter，它在 Join 过滤率高的场景下效果是非常显著的。但是它并不是万灵药，而是有一定副作用的，所以需要根据具体的 SQL 的粒度做开关。</li>\n<li>最后：要涉及到多表 Join 的时候，需要去判断 Join 的合理性。尽量保证左表为大表，右表为小表，然后 Hash Join 会优于 Nest Loop Join。必要的时可以通过 SQL Rewrite，利用 Hint 去调整 Join 的顺序。</li>\n</ul>\n<h2 id=\"调优案例实战\"><a href=\"#调优案例实战\" class=\"headerlink\" title=\"调优案例实战\"></a>调优案例实战</h2><h3 id=\"案例一\"><a href=\"#案例一\" class=\"headerlink\" title=\"案例一\"></a>案例一</h3><p>一个四张表 Join 的查询，通过 Profile 的时候发现第二个 Join 耗时很高，耗时 14 秒。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799212-d06d6eeb-01fa-41a7-8a74-614d99a626fd.png\" alt=\"img\"></p>\n<p>进一步分析 Profile 之后，发现 BuildRows，就是右表的数据量是大概 2500 万。而 ProbeRows （ ProbeRows 是左表的数据量）只有 1 万多。这种场景下右表是远远大于左表，这显然是个不合理的情况。这显然说明 Join 的顺序出现了一些问题。这时候尝试改变 Session 变量，开启 Join Reorder。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set enable_cost_based_join_reorder = true</span><br></pre></td></tr></table></figure>\n\n\n\n<p>这次耗时从 14 秒降到了 4 秒，性能提升了 3 倍多。</p>\n<p>此时再 Check Profile 的时候，左右表的顺序已经调整正确，即右表是大表，左表是小表。基于小表去构建哈希表，开销是很小的，这就是典型的一个利用 Join Reorder 去提升 Join 性能的一个场景</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-2556ed2c-a041-48b3-b32c-d4fac1ab7ace.png\" alt=\"img\"></p>\n<h3 id=\"案例二\"><a href=\"#案例二\" class=\"headerlink\" title=\"案例二\"></a>案例二</h3><p>存在一个慢查询，查看 Profile 之后，整个 Join 节点耗时大概44秒。它的右表有 1000 万，左表有 6000 万，最终返回的结果也只有 6000 万。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-6289e219-dd31-495b-967f-dd6400b945f0.png\" alt=\"image.png\"></p>\n<p>这里可以大致的估算出过滤率是很高的，那为什么 Runtime Filter 没有生效呢？通过 Query Plan 去查看它，发现它只开启了 IN 的 Runtime Filter。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799132-41ae2249-b72d-43f0-b6fc-e2d9ddc07ae8.png\" alt=\"image.png\"></p>\n<p>当右表超过1024行的话， IN 是不生效的，所以根本起不到什么过滤的效果，所以尝试调整 RuntimeFilter 的类型。</p>\n<p>这里改为了 BloomFilter，左表的 6000 万条数据过滤了 5900 万条。基本上 99% 的数据都被过滤掉了，这个效果是很显著的。查询也从原来的 44 秒降到了 13 秒，性能提升了大概也是三倍多。</p>\n<h3 id=\"案例三\"><a href=\"#案例三\" class=\"headerlink\" title=\"案例三\"></a>案例三</h3><p>下面是一个比较极端的 Case，通过一些环境变量调优也没有办法解决，因为它涉及到 SQL Rewrite，所以这里列出来了原始的 SQL 。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"number\">100.00</span> <span class=\"operator\">*</span> <span class=\"built_in\">sum</span> (<span class=\"keyword\">case</span></span><br><span class=\"line\">        <span class=\"keyword\">when</span> P_type <span class=\"keyword\">like</span> <span class=\"string\">&#x27;PROMOS&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">then</span> <span class=\"number\">1</span> extendedprice <span class=\"operator\">*</span> (<span class=\"number\">1</span> <span class=\"operator\">-</span> <span class=\"number\">1</span> discount)</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">end</span> ) <span class=\"operator\">/</span> <span class=\"built_in\">sum</span>(<span class=\"number\">1</span> extendedprice <span class=\"operator\">*</span> (<span class=\"number\">1</span> <span class=\"operator\">-</span> <span class=\"number\">1</span> discount)) <span class=\"keyword\">as</span> promo revenue</span><br><span class=\"line\"><span class=\"keyword\">from</span> lineitem, part</span><br><span class=\"line\"><span class=\"keyword\">where</span></span><br><span class=\"line\">    <span class=\"number\">1</span>_partkey <span class=\"operator\">=</span> p_partkey</span><br><span class=\"line\">    <span class=\"keyword\">and</span> <span class=\"number\">1</span>_shipdate <span class=\"operator\">&gt;=</span> <span class=\"type\">date</span> <span class=\"string\">&#x27;1997-06-01&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> <span class=\"number\">1</span> shipdate <span class=\"operator\">&lt;</span> <span class=\"type\">date</span> <span class=\"string\">&#x27;1997-06-01&#x27;</span> <span class=\"operator\">+</span> <span class=\"type\">interval</span> <span class=\"string\">&#x27;1&#x27;</span> <span class=\"keyword\">month</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p>这个 Join 查询是很简单的，单纯的一个左右表的 Join 。当然它上面有一些过滤条件，打开 Profile 的时候，发现整个查询 Hash Join 执行了三分多钟，它是一个 BroadCast 的 Join，它的右表有 2 亿条，左表只有 70 万。在这种情况下选择了 Broadcast Join 是不合理的，这相当于要把 2 亿条做一个 Hash Table，然后用 70 万条遍历两亿条的 Hash Table ，这显然是不合理的。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092798965-5f747810-f38d-468f-8f5b-2846a8dc8928.png\" alt=\"img\"></p>\n<p>为什么会产生不合理的 Join 顺序呢？其实这个左表是一个 10 亿条级别的大表，它上面加了两个过滤条件，加完这两个过滤条件之后， 10 亿条的数据就剩 70 万条了。但 Doris 目前没有一个好的统计信息收集的框架，所以它不知道这个过滤条件的过滤率到底怎么样。所以这个 Join 顺序安排的时候，就选择了错误的 Join 的左右表顺序，导致它的性能是极其低下的。</p>\n<p>下图是改写完成之后的一个 SQL 语句，在 Join 后面添加了一个Join Hint，在Join 后面加一个方括号，然后把需要的 Join 方式写入。这里选择了 Shuffle Join，可以看到右边它实际查询计划里面看到这个数据确实是做了 Partition ，原先 3 分钟的耗时通过这样的改写完之后只剩下 7 秒，性能提升明显</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092800013-fc558dfd-5923-447e-aceb-4b97111f9fc2.png\" alt=\"image.png\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p> Doris 支持两种物理算子，一类是 <strong>Hash Join</strong>，另一类是 <strong>Nest Loop Join</strong>。</p>\n<ul>\n<li>Hash Join：在右表上根据等值 Join 列建立哈希表，左表流式的利用哈希表进行 Join 计算，它的限制是只能适用于等值 Join。</li>\n<li>Nest Loop Join：通过两个 for 循环，很直观。然后它适用的场景就是不等值的 Join，例如：大于小于或者是需要求笛卡尔积的场景。它是一个通用的 Join 算子，但是性能表现差。</li>\n</ul>\n<p>作为分布式的 MPP 数据库， 在 Join 的过程中是需要进行数据的 Shuffle。数据需要进行拆分调度，才能保证最终的 Join 结果是正确的。举个简单的例子，假设关系S 和 R 进行Join，N 表示参与 Join 计算的节点的数量；T 则表示关系的 Tuple 数目。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664090756329-d7a09f4c-4837-414c-8910-8f5efe1eb247.png?x-oss-process=image/resize,w_1875,limit_0\" alt=\"image.png\"></p>\n<h2 id=\"Doris-Join-调优方法\"><a href=\"#Doris-Join-调优方法\" class=\"headerlink\" title=\"Doris Join 调优方法\"></a>Doris Join 调优方法</h2><p>Doris Join 调优的方法：</p>\n<ul>\n<li>利用 Doris 本身提供的 Profile，去定位查询的瓶颈。Profile 会记录 Doris 整个查询当中各种信息，这是进行性能调优的一手资料。</li>\n<li>了解 Doris 的 Join 机制。知其然知其所以然、了解它的机制，才能分析它为什么比较慢。</li>\n<li>利用 Session 变量去改变 Join 的一些行为，从而实现 Join 的调优。</li>\n<li>查看 Query Plan 去分析这个调优是否生效。</li>\n</ul>\n<p>上面的 4 步基本上完成了一个标准的 Join 调优流程，接着就是实际去查询验证它，看看效果到底怎么样。</p>\n<p>如果前面 4 种方式串联起来之后，还是不奏效。这时候可能就需要去做 Join 语句的改写，或者是数据分布的调整、需要重新去 Recheck 整个数据分布是否合理，包括查询 Join 语句，可能需要做一些手动的调整。当然这种方式是心智成本是比较高的，也就是说要在尝试前面方式不奏效的情况下，才需要去做进一步的分析。</p>\n<h2 id=\"Doris-Join-调优建议\"><a href=\"#Doris-Join-调优建议\" class=\"headerlink\" title=\"Doris Join 调优建议\"></a>Doris Join 调优建议</h2><p>最后我们总结 Doris Join 优化调优的四点建议：</p>\n<ul>\n<li>第一点：在做 Join 的时候，要尽量选择同类型或者简单类型的列，同类型的话就减少它的数据 Cast，简单类型本身 Join 计算就很快。</li>\n<li>第二点：尽量选择 Key 列进行 Join， 原因前面在 Runtime Filter 的时候也介绍了，Key 列在延迟物化上能起到一个比较好的效果。</li>\n<li>第三点：大表之间的 Join ，尽量让它 Co-location ，因为大表之间的网络开销是很大的，如果需要去做 Shuffle 的话，代价是很高的。</li>\n<li>第四点：合理的使用 Runtime Filter，它在 Join 过滤率高的场景下效果是非常显著的。但是它并不是万灵药，而是有一定副作用的，所以需要根据具体的 SQL 的粒度做开关。</li>\n<li>最后：要涉及到多表 Join 的时候，需要去判断 Join 的合理性。尽量保证左表为大表，右表为小表，然后 Hash Join 会优于 Nest Loop Join。必要的时可以通过 SQL Rewrite，利用 Hint 去调整 Join 的顺序。</li>\n</ul>\n<h2 id=\"调优案例实战\"><a href=\"#调优案例实战\" class=\"headerlink\" title=\"调优案例实战\"></a>调优案例实战</h2><h3 id=\"案例一\"><a href=\"#案例一\" class=\"headerlink\" title=\"案例一\"></a>案例一</h3><p>一个四张表 Join 的查询，通过 Profile 的时候发现第二个 Join 耗时很高，耗时 14 秒。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799212-d06d6eeb-01fa-41a7-8a74-614d99a626fd.png\" alt=\"img\"></p>\n<p>进一步分析 Profile 之后，发现 BuildRows，就是右表的数据量是大概 2500 万。而 ProbeRows （ ProbeRows 是左表的数据量）只有 1 万多。这种场景下右表是远远大于左表，这显然是个不合理的情况。这显然说明 Join 的顺序出现了一些问题。这时候尝试改变 Session 变量，开启 Join Reorder。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set enable_cost_based_join_reorder = true</span><br></pre></td></tr></table></figure>\n\n\n\n<p>这次耗时从 14 秒降到了 4 秒，性能提升了 3 倍多。</p>\n<p>此时再 Check Profile 的时候，左右表的顺序已经调整正确，即右表是大表，左表是小表。基于小表去构建哈希表，开销是很小的，这就是典型的一个利用 Join Reorder 去提升 Join 性能的一个场景</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-2556ed2c-a041-48b3-b32c-d4fac1ab7ace.png\" alt=\"img\"></p>\n<h3 id=\"案例二\"><a href=\"#案例二\" class=\"headerlink\" title=\"案例二\"></a>案例二</h3><p>存在一个慢查询，查看 Profile 之后，整个 Join 节点耗时大概44秒。它的右表有 1000 万，左表有 6000 万，最终返回的结果也只有 6000 万。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799211-6289e219-dd31-495b-967f-dd6400b945f0.png\" alt=\"image.png\"></p>\n<p>这里可以大致的估算出过滤率是很高的，那为什么 Runtime Filter 没有生效呢？通过 Query Plan 去查看它，发现它只开启了 IN 的 Runtime Filter。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092799132-41ae2249-b72d-43f0-b6fc-e2d9ddc07ae8.png\" alt=\"image.png\"></p>\n<p>当右表超过1024行的话， IN 是不生效的，所以根本起不到什么过滤的效果，所以尝试调整 RuntimeFilter 的类型。</p>\n<p>这里改为了 BloomFilter，左表的 6000 万条数据过滤了 5900 万条。基本上 99% 的数据都被过滤掉了，这个效果是很显著的。查询也从原来的 44 秒降到了 13 秒，性能提升了大概也是三倍多。</p>\n<h3 id=\"案例三\"><a href=\"#案例三\" class=\"headerlink\" title=\"案例三\"></a>案例三</h3><p>下面是一个比较极端的 Case，通过一些环境变量调优也没有办法解决，因为它涉及到 SQL Rewrite，所以这里列出来了原始的 SQL 。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"number\">100.00</span> <span class=\"operator\">*</span> <span class=\"built_in\">sum</span> (<span class=\"keyword\">case</span></span><br><span class=\"line\">        <span class=\"keyword\">when</span> P_type <span class=\"keyword\">like</span> <span class=\"string\">&#x27;PROMOS&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">then</span> <span class=\"number\">1</span> extendedprice <span class=\"operator\">*</span> (<span class=\"number\">1</span> <span class=\"operator\">-</span> <span class=\"number\">1</span> discount)</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">end</span> ) <span class=\"operator\">/</span> <span class=\"built_in\">sum</span>(<span class=\"number\">1</span> extendedprice <span class=\"operator\">*</span> (<span class=\"number\">1</span> <span class=\"operator\">-</span> <span class=\"number\">1</span> discount)) <span class=\"keyword\">as</span> promo revenue</span><br><span class=\"line\"><span class=\"keyword\">from</span> lineitem, part</span><br><span class=\"line\"><span class=\"keyword\">where</span></span><br><span class=\"line\">    <span class=\"number\">1</span>_partkey <span class=\"operator\">=</span> p_partkey</span><br><span class=\"line\">    <span class=\"keyword\">and</span> <span class=\"number\">1</span>_shipdate <span class=\"operator\">&gt;=</span> <span class=\"type\">date</span> <span class=\"string\">&#x27;1997-06-01&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> <span class=\"number\">1</span> shipdate <span class=\"operator\">&lt;</span> <span class=\"type\">date</span> <span class=\"string\">&#x27;1997-06-01&#x27;</span> <span class=\"operator\">+</span> <span class=\"type\">interval</span> <span class=\"string\">&#x27;1&#x27;</span> <span class=\"keyword\">month</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p>这个 Join 查询是很简单的，单纯的一个左右表的 Join 。当然它上面有一些过滤条件，打开 Profile 的时候，发现整个查询 Hash Join 执行了三分多钟，它是一个 BroadCast 的 Join，它的右表有 2 亿条，左表只有 70 万。在这种情况下选择了 Broadcast Join 是不合理的，这相当于要把 2 亿条做一个 Hash Table，然后用 70 万条遍历两亿条的 Hash Table ，这显然是不合理的。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092798965-5f747810-f38d-468f-8f5b-2846a8dc8928.png\" alt=\"img\"></p>\n<p>为什么会产生不合理的 Join 顺序呢？其实这个左表是一个 10 亿条级别的大表，它上面加了两个过滤条件，加完这两个过滤条件之后， 10 亿条的数据就剩 70 万条了。但 Doris 目前没有一个好的统计信息收集的框架，所以它不知道这个过滤条件的过滤率到底怎么样。所以这个 Join 顺序安排的时候，就选择了错误的 Join 的左右表顺序，导致它的性能是极其低下的。</p>\n<p>下图是改写完成之后的一个 SQL 语句，在 Join 后面添加了一个Join Hint，在Join 后面加一个方括号，然后把需要的 Join 方式写入。这里选择了 Shuffle Join，可以看到右边它实际查询计划里面看到这个数据确实是做了 Partition ，原先 3 分钟的耗时通过这样的改写完之后只剩下 7 秒，性能提升明显</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1664092800013-fc558dfd-5923-447e-aceb-4b97111f9fc2.png\" alt=\"image.png\"></p>\n"},{"title":"Doris Compaction从入门到跑路","abbrlink":15533,"date":"2022-09-03T15:40:51.000Z","updated":"2022-09-03T15:40:51.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n>\n>\n>Doris 中用于控制Compaction的参数非常多。本文尝试以下方面，介绍这些参数的含义以及如果通过调整参数来适配场景。\n>\n>\n>\n>1. 数据版本是如何产生的，哪些因素影响数据版本的产出。\n>2. 为什么需要 Base 和 Cumulative 两种类型的 Compaction。\n>3. Compaction 机制是如何挑选数据分片进行 Compaction 的。\n>4. 对于一个数据分片，Compaction 机制是如何确定哪些数据版本参与 Compaction 的。\n>5. 在高频导入场景下，可以修改哪些参数来优化 Compaction 逻辑。\n>6. Compaction 相关的查看和管理命令。\n\n# Why  need Compaction\n\nDoris 的数据写入模型使用了 LSM-Tree 类似的数据结构。数据都是以追加（Append）的方式写入磁盘的。这种数据结构可以将随机写变为顺序写。这是一种面向写优化的数据结构，他能增强系统的写入吞吐，但是在读逻辑中，需要通过 Merge-on-Read 的方式，在读取时合并多次写入的数据，从而处理写入时的数据变更。\n\nMerge-on-Read 会影响读取的效率，为了降低读取时需要合并的数据量，基于 LSM-Tree 的系统都会引入后台数据合并的逻辑，以一定策略定期的对数据进行合并。Doris 中这种机制被称为 Compaction。\n\nDoris 中每次数据写入会生成一个数据版本。Compaction的过程就是讲多个数据版本合并成一个更大的版本。Compaction 可以带来以下好处：\n\n> - 1.数据更加有序\n>\n>   每个数据版本内的数据是按主键有序的，但是版本之间的数据是无序的。Compaction后形成的大版本将多个小版本的数据变成有序数据。在有序数据中进行数据检索的效率更高。\n>\n> - 2.消除数据变更\n>\n>   数据都是以追加的方式写入的，因此 Delete、Update 等操作都是写入一个标记。Compaction 操作可以处理这些标记，进行真正的数据删除或更新，从而在读取时，不再需要根据这些标记来过滤数据。\n>\n> - 3.增加数据聚合度\n> 在聚合模型下，Compaction 能进一步聚合不同数据版本中相同 key 的数据行，从而增加数据聚合度，减少读取时需要实时进行的聚合计算。\n\n# Compaction 的问题\n\n用户可能需要根据实际的使用场景来调整 Compaction 的策略，否则可能遇到如下问题：\n\n1. Compaction 速度低于数据写入速度\n\n   在高频写入场景下，短时间内会产生大量的数据版本。如果 Compaction 不及时，就会造成大量版本堆积，最终严重影响写入速度。\n\n2. 写放大问题\n\n   Compaction 本质上是将已经写入的数据读取后重写写回的过程，这种数据重复写入被称为写放大。一个好的Compaction策略应该在保证效率的前提下，尽量降低写放大系数。过多的 Compaction 会占用大量的磁盘IO资源，影响系统整体效率。\n\n# Something about Compaction(How)\n\n## 数据版本的产生\n\n首先，用户的数据表会按照分区和分桶规则，切分成若干个数据分片（Tablet）存储在不同 BE 节点上。每个 Tablet 都有多个副本（默认为3副本）。Compaction 是在每个 BE 上独立进行的，Compaction 逻辑处理的就是一个 BE 节点上所有的数据分片。\n\nDoris的数据都是以追加的方式写入系统的。Doris目前的写入依然是以微批的方式进行的，每一批次的数据针对每个 Tablet 都会形成一个 rowset。而一个 Tablet 是由多个Rowset 组成的。每个 Rowset 都有一个对应的起始版本和终止版本。对于新增Rowset，起始版本和终止版本相同，表示为 [6-6]、[7-7] 等。多个Rowset经过 Compaction 形成一个大的 Rowset，起始版本和终止版本为多个版本的并集，如 [6-6]、[7-7]、[8-8] 合并后变成 [6-8]。\n\nRowset 的数量直接影响到 Compaction 是否能够及时完成。那么一批次导入会生成多少个 Rowset 呢？这里我们举一个例子：\n\n假设集群有3个 BE 节点。每个BE节点2块盘。只有一张表，2个分区，每个分区3个分桶，默认3副本。那么总分片数量是（2 * 3 * 3）18 个，如果均匀分布在所有节点上，则每个盘上3个tablet。假设一次导入涉及到其中一个分区，则一次导入总共产生9个Rowset，即平均每块盘产生1-2个 Rowset。（这里仅考虑数据完全均匀分布的情况下，实际情况中，可能多个 Tablet 集中在某一块磁盘上。）\n\n从上面的例子我们可以得出，rowset的数量直接取决于表的分片数量。举个极端的例子，如果一个Doris集群只有3个BE节点，但是一个表有9000个分片。那么一次导入，每个BE节点就会新增3000个rowset，则至少要进行3000次compaction，才能处理完所有的分片。所以：\n\n> **合理的设置表的分区、分桶和副本数量，避免过多的分片，可以降低Compaction的开销。**\n\n## Base & Cumulative Compaction\n\nDoris 中有两种 Compaction 操作，分别称为 Base Compaction(BC) 和 Cumulative Compaction(CC)。BC 是将基线数据版本（以0为起始版本的数据）和增量数据版本合并的过程，而CC是增量数据间的合并过程。BC操作因为涉及到基线数据，而基线数据通常比较大，所以操作耗时会比CC长。\n\n如果只有 Base Compaction，则每次增量数据都要和全量的基线数据合并，写放大问题会非常严重，并且每次 Compaction 都相当耗时。因此我们需要引入 Cumulative Compaction 来先对增量数据进行合并，当增量数据合并后的大小达到一定阈值后，再和基线数据合并。这里我们有一个比较通用的 Compaction 调优策略：\n\n> **在合理范围内，尽量减少 Base Compaction 操作。**\n\nBC 和 CC 之间的分界线成为 Cumulative Point（CP），这是一个动态变化的版本号。比CP小的数据版本会只会触发 BC，而比CP大的数据版本，只会触发CC。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046764-f9891cea-fe8d-4eb2-89a4-911ffb10e7e2.png)\n\n\n\n整个过程有点类似 2048 小游戏：只有合并后大小足够，才能继续和更大的数据版本合并。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046833-aa0766ec-9852-4766-b57d-5b24448cd2b4.png)\n\n## 数据分片选择策略\n\n**Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。**因此，Compaction 策略的重点问题，就是如何选择合适的 tablet，以保证节点上不会出现数据版本过多的数据分片。\n\n### Compaction 分数\n\n一个自然的想法，就是每次都选择数据版本最多的数据分片进行 Compaction。这个策略也是 Doris 的默认策略。这个策略在大部分场景下都能很好的工作。但是考虑到一种情况，就是版本多的分片，可能并不是最频繁访问的分片。而 Compaction 的目的就是优化读性能。那么有可能某一张 “写多读少” 表一直在 Compaction，而另一张 “读多写少” 的表不能及时的 Compaction，导致读性能变差。\n\n因此，Doris 在选择数据分片时还引入了 “读取频率” 的因素。“读取频率” 和 “版本数量” 会根据各自的权重，综合计算出一个 Compaction 分数，分数越高的分片，优先做 Compaction。这两个因素的权重由以下 BE 参数控制（取值越大，权重越高）：\n\n> **compaction_tablet_scan_frequency_factor：“读取频率” 的权重值，默认为 0。**\n>\n> **compaction_tablet_compaction_score_factor：“版本数量” 的权重，默认为 1。**\n\n> “读取频率” 的权重值默认为0，即默认仅考虑 “版本数量” 这个因素。*\n\n### 生产者与消费者\n\nCompaction 是一个 生产者-消费者 模型。由一个生产者线程负责选择需要做 Compaction 的数据分片，而多个消费者负责执行 Compaction 操作。\n\n生产者线程只有一个，会定期扫描所有 tablet 来选择合适的 compaction 对象。因为 Base Compaction 和 Cumulative Compaction 是不同类型的任务，因此目前的策略是每生成 9 个 CC 任务，生成一个 BC 任务。任务生成的频率由以下两个参数控制：\n\n> **cumulative_compaction_rounds_for_each_base_compaction_round：多少个CC任务后生成一个BC任务。**\n>\n> **generate_compaction_tasks_min_interval_ms：任务生成的间隔。**\n\n> *这两个参数通常情况下不需要调整。*\n\n生产者线程产生的任务会被提交到消费者线程池。因为 Compaction 是一个IO密集型的任务，为了保证 Compaction 任务不会过多的占用IO资源，Doris 限制了每个磁盘上能够同时进行的 Compaction 任务数量，以及节点整体的任务数量，这些限制由以下参数控制：\n\n> compaction_task_num_per_disk：每个磁盘上的任务数，默认为2。该参数必须大于等于2，以保证 BC 和 CC 任务各自至少有一个线程。\n>\n> max_compaction_threads：消费者线程，即Compaction线程的总数。默认为 10。\n>\n\n举个例子，假设一个 BE 节点配置了3个数据目录（即3块磁盘），每个磁盘上的任务数配置为2，总线程数为5。则同一时间，最多有5个 Compaction 任务在进行，而每块磁盘上最多有2个任务在进行。并且最多有3个 BC 任务在进行，因为每块盘上会自动预留一个线程给CC任务。\n\n**另一方面，Compaction 任务同时也是一个内存密集型任务，因为其本质是一个多路归并排序的过程，每一路是一个数据版本。**如果一个 Compaction 任务涉及的数据版本很多，则会占用更多的内存，如果仅限制任务数，而不考虑任务的内存开销，则有可能导致系统内存超限。因此，Doris 在上述任务个数限制之外，还增加了一个任务配额限制：\n\n> total_permits_for_compaction_score：Compaction 任务配额，默认 10000。\n\n每个 Compaction 任务都有一个配额，其数值就是任务涉及的数据版本数量。假设一个任务需要合并100个版本，则其配额为100。当正在运行的任务配额总和超过配置后，新的任务将被拒绝。\n\n三个配置共同决定了节点所能承受的 Compaction 任务数量。\n\n### 数据版本选择策略\n\n一个 Compaction 任务对应的是一个数据分片（Tablet）。消费线程拿到一个 Compaction 任务后，会根据 Compaction 的任务类型，选择 tablet 中合适的数据版本（Rowset）进行数据合并。下面分别介绍 Base Compaction 和 Cumulative Compaction 的数据分片选择策略。\n\n#### Base Compaction\n\n前文说过，BC 任务是增量数据和基线数据的合并任务。并且只有比 Cumulative Point（CP） 小的数据版本才会参与 BC 任务。因此，BC 任务的数据版本选取策略比较简单。\n\n首先，会选取所有版本在 0到 CP之间的 rowset。然后根据以下几个配置参数，判断是否启动一个 BC 任务：\n\n> base_compaction_num_cumulative_deltas：一次 BC 任务最小版本数量限制。默认为5。该参数主要为了避免过多 BC 任务。当数据版本数量较少时，BC 是没有必要的。\n>\n> base_compaction_interval_seconds_since_last_operation：第一个参数限制了当版本数量少时，不会进行 BC 任务。但我们需要避免另一种情况，即某些 tablet 可能仅会导入少量批次的数据，因此当 Doris 发现一个 tablet 长时间没有执行过 BC 任务时，也会触发 BC 任务。这个参数就是控制这个时间的，默认是 86400，单位是秒。\n>\n\n*> 以上两个参数通常情况下不需要修改，在某些情况下如何需要想尽快合并基线数据，可以尝试改小 **base_compaction_num_cumulative_deltas 参数。但这个参数只会影响到 “被选中的 tablet”。而 “被选中” 的前提是这个 tablet 的数据版本数量是最多的。***\n\n#### Cumulative Compaction\n\nCC 任务只会选取版本比 CP 大的数据版本。其本身的选取策略也比较简单，即从 CP 版本开始，依次向后选取数据版本。最终的数据版本集合由以下参数控制：\n\n> min_cumulative_compaction_num_singleton_deltas：一次 CC 任务最少的版本数量限制。这个配置是和 cumulative_size_based_compaction_lower_size_mbytes 配置同时判断的。即如果版本数量小于阈值，并且数据量也小于阈值，则不会触发 CC 任务。以避免过多不必要的 CC 任务。默认是5。\n>\n> max_cumulative_compaction_num_singleton_deltas：一次 CC 任务最大的版本数量限制。以防止一次 CC 任务合并的版本数量过多，占用过多资源。默认是1000。\n>\n> cumulative_size_based_compaction_lower_size_mbytes：一次 CC 任务最少的数据量，和min_cumulative_compaction_num_singleton_delta 同时判断。默认是 64，单位是 MB。\n>\n\n简单来说，默认配置下，就是从 CP 版本开始往后选取 rowset。最少选5个，最多选 1000 个，然后判断数据量是否大于阈值即可。\n\nCC 任务还有一个重要步骤，就是在合并任务结束后，设置新的 Cumulative Point。CC 任务合并完成后，会产生一个合并后的新的数据版本，而我们要做的就是判断这个新的数据版是 “晋升” 到 BC 任务区，还是依然保留在 CC 任务区。举个例子：\n\n假设当前 CP 是 10。有一个 CC 任务合并了 [10-13] [14-14] [15-15] 后生成了 [10-15] 这个版本。如果决定将 [10-15] 版本移动到 BC 任务区，则需修改 CP 为 15，否则 CP 保持不变，依然为 10。\n\n**CP 只会增加，不会减少。** 以下参数决定了是否更新 CP：\n\n> cumulative_size_based_promotion_ratio：晋升比率。默认 0.05。\n>\n> cumulative_size_based_promotion_min_size_mbytes：最小晋升大小，默认 64，单位 MB。\n>\n> cumulative_size_based_promotion_size_mbytes：最大晋升大小，默认 1024，单位 MB。\n>\n\n以上参数比较难理解，这里我们先解释下 “晋升” 的原则。一个 CC 任务生成的 rowset 的晋升原则，是其数据大小和基线数据的大小在 “同一量级”。这个类似 2048 小游戏，只有相同的数字才能合并形成更大的数字。而上面三个参数，就是用于判断一个新的rowset是否匹配基线数据的数量级。举例说明：\n\n在默认配置下，假设当前基线数据（即所有 CP 之前的数据版本）的数据量为 10GB，则晋升量级为 （10GB * 0.05）512MB。这个数值大于 64 MB 小于 1024 MB，满足条件。所以如果 CC 任务生成的新的 rowset 的大小大于 512 MB，则可以晋升，即 CP 增加。而假设基线数据为 50GB，则晋升量级为（50GB * 0.05）2.5GB。这个数值大于 64 MB 也大于 1024 MB，因此晋升量级会被调整为 1024 MB。所以如果 CC 任务生成的新的 rowset 的大小大于 1024 MB，则可以晋升，即 CP 增加。\n\n从上面的例子可以看出，cumulative_size_based_promotion_ratio 用于定义 “同一量级”，0.05 即表示数据量大于基线数据的 5% 的 rowset 都有晋升的可能，而 cumulative_size_based_promotion_min_size_mbytes 和 cumulative_size_based_promotion_size_mbytes 用于保证晋升不会过于频繁或过于严格。\n\n> *这三个参数会直接影响 BC 和 CC 任务的频率，尤其在高频导入场景下需要适当调整。我们会在后续文章中举例说明。*\n\n### 其他 Compaction 参数和注意事项\n\n还有一些参数和 Compaction 相关，在某些情况下需要修改：\n\ndisable_auto_compaction：默认为 false，修改为 true 则会禁止 Compaction 操作。该参数仅在一些调试情况，或者 compaction 异常需要临时关闭的情况下才需使用。\n\n#### Delete 灾难\n\n通过 DELETE FROM 语句执行的数据删除操作，在 Doris 中也会生成一个数据版本用于标记删除。这种类型的数据版本比较特殊，我们成为 “删除版本”。删除版本只能通过 Base Compaction 任务处理。因此在在遇到删除版本时，Cumulative Point 会强制增加，将删除版本移动到 BC 任务区。**因此数据导入和删除交替发生的场景通常会导致 Compaction 灾难**。比如以下版本序列：\n\n```\n[0-10]\n[11-11] 删除版本\n[12-12]\n[13-13] 删除版本\n[14-14]\n[15-15] 删除版本\n[16-16]\n[17-17] 删除版本\n...\n```\n\n在这种情况下，CC 任务几乎不会被触发（因为CC任务只能选择一个版本，而无法处理删除版本），所有版本都会交给 Base Compaction 处理，导致 Compaction 进度缓慢。目前Doris还无法很好的处理这种场景，因此需要在业务上尽量避免。\n","source":"_posts/bigdata/Doris Compaction从入门到跑路.md","raw":"---\ntitle: Doris Compaction从入门到跑路\ntags:\n  - Doris\ncategories:\n  - - Doris\nabbrlink: 15533\ndate: 2022-09-03 23:40:51\nupdated: 2022-09-03 23:40:51\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n>\n>\n>Doris 中用于控制Compaction的参数非常多。本文尝试以下方面，介绍这些参数的含义以及如果通过调整参数来适配场景。\n>\n>\n>\n>1. 数据版本是如何产生的，哪些因素影响数据版本的产出。\n>2. 为什么需要 Base 和 Cumulative 两种类型的 Compaction。\n>3. Compaction 机制是如何挑选数据分片进行 Compaction 的。\n>4. 对于一个数据分片，Compaction 机制是如何确定哪些数据版本参与 Compaction 的。\n>5. 在高频导入场景下，可以修改哪些参数来优化 Compaction 逻辑。\n>6. Compaction 相关的查看和管理命令。\n\n# Why  need Compaction\n\nDoris 的数据写入模型使用了 LSM-Tree 类似的数据结构。数据都是以追加（Append）的方式写入磁盘的。这种数据结构可以将随机写变为顺序写。这是一种面向写优化的数据结构，他能增强系统的写入吞吐，但是在读逻辑中，需要通过 Merge-on-Read 的方式，在读取时合并多次写入的数据，从而处理写入时的数据变更。\n\nMerge-on-Read 会影响读取的效率，为了降低读取时需要合并的数据量，基于 LSM-Tree 的系统都会引入后台数据合并的逻辑，以一定策略定期的对数据进行合并。Doris 中这种机制被称为 Compaction。\n\nDoris 中每次数据写入会生成一个数据版本。Compaction的过程就是讲多个数据版本合并成一个更大的版本。Compaction 可以带来以下好处：\n\n> - 1.数据更加有序\n>\n>   每个数据版本内的数据是按主键有序的，但是版本之间的数据是无序的。Compaction后形成的大版本将多个小版本的数据变成有序数据。在有序数据中进行数据检索的效率更高。\n>\n> - 2.消除数据变更\n>\n>   数据都是以追加的方式写入的，因此 Delete、Update 等操作都是写入一个标记。Compaction 操作可以处理这些标记，进行真正的数据删除或更新，从而在读取时，不再需要根据这些标记来过滤数据。\n>\n> - 3.增加数据聚合度\n> 在聚合模型下，Compaction 能进一步聚合不同数据版本中相同 key 的数据行，从而增加数据聚合度，减少读取时需要实时进行的聚合计算。\n\n# Compaction 的问题\n\n用户可能需要根据实际的使用场景来调整 Compaction 的策略，否则可能遇到如下问题：\n\n1. Compaction 速度低于数据写入速度\n\n   在高频写入场景下，短时间内会产生大量的数据版本。如果 Compaction 不及时，就会造成大量版本堆积，最终严重影响写入速度。\n\n2. 写放大问题\n\n   Compaction 本质上是将已经写入的数据读取后重写写回的过程，这种数据重复写入被称为写放大。一个好的Compaction策略应该在保证效率的前提下，尽量降低写放大系数。过多的 Compaction 会占用大量的磁盘IO资源，影响系统整体效率。\n\n# Something about Compaction(How)\n\n## 数据版本的产生\n\n首先，用户的数据表会按照分区和分桶规则，切分成若干个数据分片（Tablet）存储在不同 BE 节点上。每个 Tablet 都有多个副本（默认为3副本）。Compaction 是在每个 BE 上独立进行的，Compaction 逻辑处理的就是一个 BE 节点上所有的数据分片。\n\nDoris的数据都是以追加的方式写入系统的。Doris目前的写入依然是以微批的方式进行的，每一批次的数据针对每个 Tablet 都会形成一个 rowset。而一个 Tablet 是由多个Rowset 组成的。每个 Rowset 都有一个对应的起始版本和终止版本。对于新增Rowset，起始版本和终止版本相同，表示为 [6-6]、[7-7] 等。多个Rowset经过 Compaction 形成一个大的 Rowset，起始版本和终止版本为多个版本的并集，如 [6-6]、[7-7]、[8-8] 合并后变成 [6-8]。\n\nRowset 的数量直接影响到 Compaction 是否能够及时完成。那么一批次导入会生成多少个 Rowset 呢？这里我们举一个例子：\n\n假设集群有3个 BE 节点。每个BE节点2块盘。只有一张表，2个分区，每个分区3个分桶，默认3副本。那么总分片数量是（2 * 3 * 3）18 个，如果均匀分布在所有节点上，则每个盘上3个tablet。假设一次导入涉及到其中一个分区，则一次导入总共产生9个Rowset，即平均每块盘产生1-2个 Rowset。（这里仅考虑数据完全均匀分布的情况下，实际情况中，可能多个 Tablet 集中在某一块磁盘上。）\n\n从上面的例子我们可以得出，rowset的数量直接取决于表的分片数量。举个极端的例子，如果一个Doris集群只有3个BE节点，但是一个表有9000个分片。那么一次导入，每个BE节点就会新增3000个rowset，则至少要进行3000次compaction，才能处理完所有的分片。所以：\n\n> **合理的设置表的分区、分桶和副本数量，避免过多的分片，可以降低Compaction的开销。**\n\n## Base & Cumulative Compaction\n\nDoris 中有两种 Compaction 操作，分别称为 Base Compaction(BC) 和 Cumulative Compaction(CC)。BC 是将基线数据版本（以0为起始版本的数据）和增量数据版本合并的过程，而CC是增量数据间的合并过程。BC操作因为涉及到基线数据，而基线数据通常比较大，所以操作耗时会比CC长。\n\n如果只有 Base Compaction，则每次增量数据都要和全量的基线数据合并，写放大问题会非常严重，并且每次 Compaction 都相当耗时。因此我们需要引入 Cumulative Compaction 来先对增量数据进行合并，当增量数据合并后的大小达到一定阈值后，再和基线数据合并。这里我们有一个比较通用的 Compaction 调优策略：\n\n> **在合理范围内，尽量减少 Base Compaction 操作。**\n\nBC 和 CC 之间的分界线成为 Cumulative Point（CP），这是一个动态变化的版本号。比CP小的数据版本会只会触发 BC，而比CP大的数据版本，只会触发CC。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046764-f9891cea-fe8d-4eb2-89a4-911ffb10e7e2.png)\n\n\n\n整个过程有点类似 2048 小游戏：只有合并后大小足够，才能继续和更大的数据版本合并。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046833-aa0766ec-9852-4766-b57d-5b24448cd2b4.png)\n\n## 数据分片选择策略\n\n**Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。**因此，Compaction 策略的重点问题，就是如何选择合适的 tablet，以保证节点上不会出现数据版本过多的数据分片。\n\n### Compaction 分数\n\n一个自然的想法，就是每次都选择数据版本最多的数据分片进行 Compaction。这个策略也是 Doris 的默认策略。这个策略在大部分场景下都能很好的工作。但是考虑到一种情况，就是版本多的分片，可能并不是最频繁访问的分片。而 Compaction 的目的就是优化读性能。那么有可能某一张 “写多读少” 表一直在 Compaction，而另一张 “读多写少” 的表不能及时的 Compaction，导致读性能变差。\n\n因此，Doris 在选择数据分片时还引入了 “读取频率” 的因素。“读取频率” 和 “版本数量” 会根据各自的权重，综合计算出一个 Compaction 分数，分数越高的分片，优先做 Compaction。这两个因素的权重由以下 BE 参数控制（取值越大，权重越高）：\n\n> **compaction_tablet_scan_frequency_factor：“读取频率” 的权重值，默认为 0。**\n>\n> **compaction_tablet_compaction_score_factor：“版本数量” 的权重，默认为 1。**\n\n> “读取频率” 的权重值默认为0，即默认仅考虑 “版本数量” 这个因素。*\n\n### 生产者与消费者\n\nCompaction 是一个 生产者-消费者 模型。由一个生产者线程负责选择需要做 Compaction 的数据分片，而多个消费者负责执行 Compaction 操作。\n\n生产者线程只有一个，会定期扫描所有 tablet 来选择合适的 compaction 对象。因为 Base Compaction 和 Cumulative Compaction 是不同类型的任务，因此目前的策略是每生成 9 个 CC 任务，生成一个 BC 任务。任务生成的频率由以下两个参数控制：\n\n> **cumulative_compaction_rounds_for_each_base_compaction_round：多少个CC任务后生成一个BC任务。**\n>\n> **generate_compaction_tasks_min_interval_ms：任务生成的间隔。**\n\n> *这两个参数通常情况下不需要调整。*\n\n生产者线程产生的任务会被提交到消费者线程池。因为 Compaction 是一个IO密集型的任务，为了保证 Compaction 任务不会过多的占用IO资源，Doris 限制了每个磁盘上能够同时进行的 Compaction 任务数量，以及节点整体的任务数量，这些限制由以下参数控制：\n\n> compaction_task_num_per_disk：每个磁盘上的任务数，默认为2。该参数必须大于等于2，以保证 BC 和 CC 任务各自至少有一个线程。\n>\n> max_compaction_threads：消费者线程，即Compaction线程的总数。默认为 10。\n>\n\n举个例子，假设一个 BE 节点配置了3个数据目录（即3块磁盘），每个磁盘上的任务数配置为2，总线程数为5。则同一时间，最多有5个 Compaction 任务在进行，而每块磁盘上最多有2个任务在进行。并且最多有3个 BC 任务在进行，因为每块盘上会自动预留一个线程给CC任务。\n\n**另一方面，Compaction 任务同时也是一个内存密集型任务，因为其本质是一个多路归并排序的过程，每一路是一个数据版本。**如果一个 Compaction 任务涉及的数据版本很多，则会占用更多的内存，如果仅限制任务数，而不考虑任务的内存开销，则有可能导致系统内存超限。因此，Doris 在上述任务个数限制之外，还增加了一个任务配额限制：\n\n> total_permits_for_compaction_score：Compaction 任务配额，默认 10000。\n\n每个 Compaction 任务都有一个配额，其数值就是任务涉及的数据版本数量。假设一个任务需要合并100个版本，则其配额为100。当正在运行的任务配额总和超过配置后，新的任务将被拒绝。\n\n三个配置共同决定了节点所能承受的 Compaction 任务数量。\n\n### 数据版本选择策略\n\n一个 Compaction 任务对应的是一个数据分片（Tablet）。消费线程拿到一个 Compaction 任务后，会根据 Compaction 的任务类型，选择 tablet 中合适的数据版本（Rowset）进行数据合并。下面分别介绍 Base Compaction 和 Cumulative Compaction 的数据分片选择策略。\n\n#### Base Compaction\n\n前文说过，BC 任务是增量数据和基线数据的合并任务。并且只有比 Cumulative Point（CP） 小的数据版本才会参与 BC 任务。因此，BC 任务的数据版本选取策略比较简单。\n\n首先，会选取所有版本在 0到 CP之间的 rowset。然后根据以下几个配置参数，判断是否启动一个 BC 任务：\n\n> base_compaction_num_cumulative_deltas：一次 BC 任务最小版本数量限制。默认为5。该参数主要为了避免过多 BC 任务。当数据版本数量较少时，BC 是没有必要的。\n>\n> base_compaction_interval_seconds_since_last_operation：第一个参数限制了当版本数量少时，不会进行 BC 任务。但我们需要避免另一种情况，即某些 tablet 可能仅会导入少量批次的数据，因此当 Doris 发现一个 tablet 长时间没有执行过 BC 任务时，也会触发 BC 任务。这个参数就是控制这个时间的，默认是 86400，单位是秒。\n>\n\n*> 以上两个参数通常情况下不需要修改，在某些情况下如何需要想尽快合并基线数据，可以尝试改小 **base_compaction_num_cumulative_deltas 参数。但这个参数只会影响到 “被选中的 tablet”。而 “被选中” 的前提是这个 tablet 的数据版本数量是最多的。***\n\n#### Cumulative Compaction\n\nCC 任务只会选取版本比 CP 大的数据版本。其本身的选取策略也比较简单，即从 CP 版本开始，依次向后选取数据版本。最终的数据版本集合由以下参数控制：\n\n> min_cumulative_compaction_num_singleton_deltas：一次 CC 任务最少的版本数量限制。这个配置是和 cumulative_size_based_compaction_lower_size_mbytes 配置同时判断的。即如果版本数量小于阈值，并且数据量也小于阈值，则不会触发 CC 任务。以避免过多不必要的 CC 任务。默认是5。\n>\n> max_cumulative_compaction_num_singleton_deltas：一次 CC 任务最大的版本数量限制。以防止一次 CC 任务合并的版本数量过多，占用过多资源。默认是1000。\n>\n> cumulative_size_based_compaction_lower_size_mbytes：一次 CC 任务最少的数据量，和min_cumulative_compaction_num_singleton_delta 同时判断。默认是 64，单位是 MB。\n>\n\n简单来说，默认配置下，就是从 CP 版本开始往后选取 rowset。最少选5个，最多选 1000 个，然后判断数据量是否大于阈值即可。\n\nCC 任务还有一个重要步骤，就是在合并任务结束后，设置新的 Cumulative Point。CC 任务合并完成后，会产生一个合并后的新的数据版本，而我们要做的就是判断这个新的数据版是 “晋升” 到 BC 任务区，还是依然保留在 CC 任务区。举个例子：\n\n假设当前 CP 是 10。有一个 CC 任务合并了 [10-13] [14-14] [15-15] 后生成了 [10-15] 这个版本。如果决定将 [10-15] 版本移动到 BC 任务区，则需修改 CP 为 15，否则 CP 保持不变，依然为 10。\n\n**CP 只会增加，不会减少。** 以下参数决定了是否更新 CP：\n\n> cumulative_size_based_promotion_ratio：晋升比率。默认 0.05。\n>\n> cumulative_size_based_promotion_min_size_mbytes：最小晋升大小，默认 64，单位 MB。\n>\n> cumulative_size_based_promotion_size_mbytes：最大晋升大小，默认 1024，单位 MB。\n>\n\n以上参数比较难理解，这里我们先解释下 “晋升” 的原则。一个 CC 任务生成的 rowset 的晋升原则，是其数据大小和基线数据的大小在 “同一量级”。这个类似 2048 小游戏，只有相同的数字才能合并形成更大的数字。而上面三个参数，就是用于判断一个新的rowset是否匹配基线数据的数量级。举例说明：\n\n在默认配置下，假设当前基线数据（即所有 CP 之前的数据版本）的数据量为 10GB，则晋升量级为 （10GB * 0.05）512MB。这个数值大于 64 MB 小于 1024 MB，满足条件。所以如果 CC 任务生成的新的 rowset 的大小大于 512 MB，则可以晋升，即 CP 增加。而假设基线数据为 50GB，则晋升量级为（50GB * 0.05）2.5GB。这个数值大于 64 MB 也大于 1024 MB，因此晋升量级会被调整为 1024 MB。所以如果 CC 任务生成的新的 rowset 的大小大于 1024 MB，则可以晋升，即 CP 增加。\n\n从上面的例子可以看出，cumulative_size_based_promotion_ratio 用于定义 “同一量级”，0.05 即表示数据量大于基线数据的 5% 的 rowset 都有晋升的可能，而 cumulative_size_based_promotion_min_size_mbytes 和 cumulative_size_based_promotion_size_mbytes 用于保证晋升不会过于频繁或过于严格。\n\n> *这三个参数会直接影响 BC 和 CC 任务的频率，尤其在高频导入场景下需要适当调整。我们会在后续文章中举例说明。*\n\n### 其他 Compaction 参数和注意事项\n\n还有一些参数和 Compaction 相关，在某些情况下需要修改：\n\ndisable_auto_compaction：默认为 false，修改为 true 则会禁止 Compaction 操作。该参数仅在一些调试情况，或者 compaction 异常需要临时关闭的情况下才需使用。\n\n#### Delete 灾难\n\n通过 DELETE FROM 语句执行的数据删除操作，在 Doris 中也会生成一个数据版本用于标记删除。这种类型的数据版本比较特殊，我们成为 “删除版本”。删除版本只能通过 Base Compaction 任务处理。因此在在遇到删除版本时，Cumulative Point 会强制增加，将删除版本移动到 BC 任务区。**因此数据导入和删除交替发生的场景通常会导致 Compaction 灾难**。比如以下版本序列：\n\n```\n[0-10]\n[11-11] 删除版本\n[12-12]\n[13-13] 删除版本\n[14-14]\n[15-15] 删除版本\n[16-16]\n[17-17] 删除版本\n...\n```\n\n在这种情况下，CC 任务几乎不会被触发（因为CC任务只能选择一个版本，而无法处理删除版本），所有版本都会交给 Base Compaction 处理，导致 Compaction 进度缓慢。目前Doris还无法很好的处理这种场景，因此需要在业务上尽量避免。\n","slug":"bigdata/Doris Compaction从入门到跑路","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksx00168j5m2r6n6sjw","content":"<blockquote>\n<p>Doris 中用于控制Compaction的参数非常多。本文尝试以下方面，介绍这些参数的含义以及如果通过调整参数来适配场景。</p>\n<ol>\n<li>数据版本是如何产生的，哪些因素影响数据版本的产出。</li>\n<li>为什么需要 Base 和 Cumulative 两种类型的 Compaction。</li>\n<li>Compaction 机制是如何挑选数据分片进行 Compaction 的。</li>\n<li>对于一个数据分片，Compaction 机制是如何确定哪些数据版本参与 Compaction 的。</li>\n<li>在高频导入场景下，可以修改哪些参数来优化 Compaction 逻辑。</li>\n<li>Compaction 相关的查看和管理命令。</li>\n</ol>\n</blockquote>\n<h1 id=\"Why-need-Compaction\"><a href=\"#Why-need-Compaction\" class=\"headerlink\" title=\"Why  need Compaction\"></a>Why  need Compaction</h1><p>Doris 的数据写入模型使用了 LSM-Tree 类似的数据结构。数据都是以追加（Append）的方式写入磁盘的。这种数据结构可以将随机写变为顺序写。这是一种面向写优化的数据结构，他能增强系统的写入吞吐，但是在读逻辑中，需要通过 Merge-on-Read 的方式，在读取时合并多次写入的数据，从而处理写入时的数据变更。</p>\n<p>Merge-on-Read 会影响读取的效率，为了降低读取时需要合并的数据量，基于 LSM-Tree 的系统都会引入后台数据合并的逻辑，以一定策略定期的对数据进行合并。Doris 中这种机制被称为 Compaction。</p>\n<p>Doris 中每次数据写入会生成一个数据版本。Compaction的过程就是讲多个数据版本合并成一个更大的版本。Compaction 可以带来以下好处：</p>\n<blockquote>\n<ul>\n<li><p>1.数据更加有序</p>\n<p>每个数据版本内的数据是按主键有序的，但是版本之间的数据是无序的。Compaction后形成的大版本将多个小版本的数据变成有序数据。在有序数据中进行数据检索的效率更高。</p>\n</li>\n<li><p>2.消除数据变更</p>\n<p>数据都是以追加的方式写入的，因此 Delete、Update 等操作都是写入一个标记。Compaction 操作可以处理这些标记，进行真正的数据删除或更新，从而在读取时，不再需要根据这些标记来过滤数据。</p>\n</li>\n<li><p>3.增加数据聚合度<br>在聚合模型下，Compaction 能进一步聚合不同数据版本中相同 key 的数据行，从而增加数据聚合度，减少读取时需要实时进行的聚合计算。</p>\n</li>\n</ul>\n</blockquote>\n<h1 id=\"Compaction-的问题\"><a href=\"#Compaction-的问题\" class=\"headerlink\" title=\"Compaction 的问题\"></a>Compaction 的问题</h1><p>用户可能需要根据实际的使用场景来调整 Compaction 的策略，否则可能遇到如下问题：</p>\n<ol>\n<li><p>Compaction 速度低于数据写入速度</p>\n<p>在高频写入场景下，短时间内会产生大量的数据版本。如果 Compaction 不及时，就会造成大量版本堆积，最终严重影响写入速度。</p>\n</li>\n<li><p>写放大问题</p>\n<p>Compaction 本质上是将已经写入的数据读取后重写写回的过程，这种数据重复写入被称为写放大。一个好的Compaction策略应该在保证效率的前提下，尽量降低写放大系数。过多的 Compaction 会占用大量的磁盘IO资源，影响系统整体效率。</p>\n</li>\n</ol>\n<h1 id=\"Something-about-Compaction-How\"><a href=\"#Something-about-Compaction-How\" class=\"headerlink\" title=\"Something about Compaction(How)\"></a>Something about Compaction(How)</h1><h2 id=\"数据版本的产生\"><a href=\"#数据版本的产生\" class=\"headerlink\" title=\"数据版本的产生\"></a>数据版本的产生</h2><p>首先，用户的数据表会按照分区和分桶规则，切分成若干个数据分片（Tablet）存储在不同 BE 节点上。每个 Tablet 都有多个副本（默认为3副本）。Compaction 是在每个 BE 上独立进行的，Compaction 逻辑处理的就是一个 BE 节点上所有的数据分片。</p>\n<p>Doris的数据都是以追加的方式写入系统的。Doris目前的写入依然是以微批的方式进行的，每一批次的数据针对每个 Tablet 都会形成一个 rowset。而一个 Tablet 是由多个Rowset 组成的。每个 Rowset 都有一个对应的起始版本和终止版本。对于新增Rowset，起始版本和终止版本相同，表示为 [6-6]、[7-7] 等。多个Rowset经过 Compaction 形成一个大的 Rowset，起始版本和终止版本为多个版本的并集，如 [6-6]、[7-7]、[8-8] 合并后变成 [6-8]。</p>\n<p>Rowset 的数量直接影响到 Compaction 是否能够及时完成。那么一批次导入会生成多少个 Rowset 呢？这里我们举一个例子：</p>\n<p>假设集群有3个 BE 节点。每个BE节点2块盘。只有一张表，2个分区，每个分区3个分桶，默认3副本。那么总分片数量是（2 * 3 * 3）18 个，如果均匀分布在所有节点上，则每个盘上3个tablet。假设一次导入涉及到其中一个分区，则一次导入总共产生9个Rowset，即平均每块盘产生1-2个 Rowset。（这里仅考虑数据完全均匀分布的情况下，实际情况中，可能多个 Tablet 集中在某一块磁盘上。）</p>\n<p>从上面的例子我们可以得出，rowset的数量直接取决于表的分片数量。举个极端的例子，如果一个Doris集群只有3个BE节点，但是一个表有9000个分片。那么一次导入，每个BE节点就会新增3000个rowset，则至少要进行3000次compaction，才能处理完所有的分片。所以：</p>\n<blockquote>\n<p><strong>合理的设置表的分区、分桶和副本数量，避免过多的分片，可以降低Compaction的开销。</strong></p>\n</blockquote>\n<h2 id=\"Base-amp-Cumulative-Compaction\"><a href=\"#Base-amp-Cumulative-Compaction\" class=\"headerlink\" title=\"Base &amp; Cumulative Compaction\"></a>Base &amp; Cumulative Compaction</h2><p>Doris 中有两种 Compaction 操作，分别称为 Base Compaction(BC) 和 Cumulative Compaction(CC)。BC 是将基线数据版本（以0为起始版本的数据）和增量数据版本合并的过程，而CC是增量数据间的合并过程。BC操作因为涉及到基线数据，而基线数据通常比较大，所以操作耗时会比CC长。</p>\n<p>如果只有 Base Compaction，则每次增量数据都要和全量的基线数据合并，写放大问题会非常严重，并且每次 Compaction 都相当耗时。因此我们需要引入 Cumulative Compaction 来先对增量数据进行合并，当增量数据合并后的大小达到一定阈值后，再和基线数据合并。这里我们有一个比较通用的 Compaction 调优策略：</p>\n<blockquote>\n<p><strong>在合理范围内，尽量减少 Base Compaction 操作。</strong></p>\n</blockquote>\n<p>BC 和 CC 之间的分界线成为 Cumulative Point（CP），这是一个动态变化的版本号。比CP小的数据版本会只会触发 BC，而比CP大的数据版本，只会触发CC。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046764-f9891cea-fe8d-4eb2-89a4-911ffb10e7e2.png\" alt=\"img\"></p>\n<p>整个过程有点类似 2048 小游戏：只有合并后大小足够，才能继续和更大的数据版本合并。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046833-aa0766ec-9852-4766-b57d-5b24448cd2b4.png\" alt=\"img\"></p>\n<h2 id=\"数据分片选择策略\"><a href=\"#数据分片选择策略\" class=\"headerlink\" title=\"数据分片选择策略\"></a>数据分片选择策略</h2><p><strong>Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。</strong>因此，Compaction 策略的重点问题，就是如何选择合适的 tablet，以保证节点上不会出现数据版本过多的数据分片。</p>\n<h3 id=\"Compaction-分数\"><a href=\"#Compaction-分数\" class=\"headerlink\" title=\"Compaction 分数\"></a>Compaction 分数</h3><p>一个自然的想法，就是每次都选择数据版本最多的数据分片进行 Compaction。这个策略也是 Doris 的默认策略。这个策略在大部分场景下都能很好的工作。但是考虑到一种情况，就是版本多的分片，可能并不是最频繁访问的分片。而 Compaction 的目的就是优化读性能。那么有可能某一张 “写多读少” 表一直在 Compaction，而另一张 “读多写少” 的表不能及时的 Compaction，导致读性能变差。</p>\n<p>因此，Doris 在选择数据分片时还引入了 “读取频率” 的因素。“读取频率” 和 “版本数量” 会根据各自的权重，综合计算出一个 Compaction 分数，分数越高的分片，优先做 Compaction。这两个因素的权重由以下 BE 参数控制（取值越大，权重越高）：</p>\n<blockquote>\n<p><strong>compaction_tablet_scan_frequency_factor：“读取频率” 的权重值，默认为 0。</strong></p>\n<p><strong>compaction_tablet_compaction_score_factor：“版本数量” 的权重，默认为 1。</strong></p>\n</blockquote>\n<blockquote>\n<p>“读取频率” 的权重值默认为0，即默认仅考虑 “版本数量” 这个因素。*</p>\n</blockquote>\n<h3 id=\"生产者与消费者\"><a href=\"#生产者与消费者\" class=\"headerlink\" title=\"生产者与消费者\"></a>生产者与消费者</h3><p>Compaction 是一个 生产者-消费者 模型。由一个生产者线程负责选择需要做 Compaction 的数据分片，而多个消费者负责执行 Compaction 操作。</p>\n<p>生产者线程只有一个，会定期扫描所有 tablet 来选择合适的 compaction 对象。因为 Base Compaction 和 Cumulative Compaction 是不同类型的任务，因此目前的策略是每生成 9 个 CC 任务，生成一个 BC 任务。任务生成的频率由以下两个参数控制：</p>\n<blockquote>\n<p><strong>cumulative_compaction_rounds_for_each_base_compaction_round：多少个CC任务后生成一个BC任务。</strong></p>\n<p><strong>generate_compaction_tasks_min_interval_ms：任务生成的间隔。</strong></p>\n</blockquote>\n<blockquote>\n<p><em>这两个参数通常情况下不需要调整。</em></p>\n</blockquote>\n<p>生产者线程产生的任务会被提交到消费者线程池。因为 Compaction 是一个IO密集型的任务，为了保证 Compaction 任务不会过多的占用IO资源，Doris 限制了每个磁盘上能够同时进行的 Compaction 任务数量，以及节点整体的任务数量，这些限制由以下参数控制：</p>\n<blockquote>\n<p>compaction_task_num_per_disk：每个磁盘上的任务数，默认为2。该参数必须大于等于2，以保证 BC 和 CC 任务各自至少有一个线程。</p>\n<p>max_compaction_threads：消费者线程，即Compaction线程的总数。默认为 10。</p>\n</blockquote>\n<p>举个例子，假设一个 BE 节点配置了3个数据目录（即3块磁盘），每个磁盘上的任务数配置为2，总线程数为5。则同一时间，最多有5个 Compaction 任务在进行，而每块磁盘上最多有2个任务在进行。并且最多有3个 BC 任务在进行，因为每块盘上会自动预留一个线程给CC任务。</p>\n<p><strong>另一方面，Compaction 任务同时也是一个内存密集型任务，因为其本质是一个多路归并排序的过程，每一路是一个数据版本。</strong>如果一个 Compaction 任务涉及的数据版本很多，则会占用更多的内存，如果仅限制任务数，而不考虑任务的内存开销，则有可能导致系统内存超限。因此，Doris 在上述任务个数限制之外，还增加了一个任务配额限制：</p>\n<blockquote>\n<p>total_permits_for_compaction_score：Compaction 任务配额，默认 10000。</p>\n</blockquote>\n<p>每个 Compaction 任务都有一个配额，其数值就是任务涉及的数据版本数量。假设一个任务需要合并100个版本，则其配额为100。当正在运行的任务配额总和超过配置后，新的任务将被拒绝。</p>\n<p>三个配置共同决定了节点所能承受的 Compaction 任务数量。</p>\n<h3 id=\"数据版本选择策略\"><a href=\"#数据版本选择策略\" class=\"headerlink\" title=\"数据版本选择策略\"></a>数据版本选择策略</h3><p>一个 Compaction 任务对应的是一个数据分片（Tablet）。消费线程拿到一个 Compaction 任务后，会根据 Compaction 的任务类型，选择 tablet 中合适的数据版本（Rowset）进行数据合并。下面分别介绍 Base Compaction 和 Cumulative Compaction 的数据分片选择策略。</p>\n<h4 id=\"Base-Compaction\"><a href=\"#Base-Compaction\" class=\"headerlink\" title=\"Base Compaction\"></a>Base Compaction</h4><p>前文说过，BC 任务是增量数据和基线数据的合并任务。并且只有比 Cumulative Point（CP） 小的数据版本才会参与 BC 任务。因此，BC 任务的数据版本选取策略比较简单。</p>\n<p>首先，会选取所有版本在 0到 CP之间的 rowset。然后根据以下几个配置参数，判断是否启动一个 BC 任务：</p>\n<blockquote>\n<p>base_compaction_num_cumulative_deltas：一次 BC 任务最小版本数量限制。默认为5。该参数主要为了避免过多 BC 任务。当数据版本数量较少时，BC 是没有必要的。</p>\n<p>base_compaction_interval_seconds_since_last_operation：第一个参数限制了当版本数量少时，不会进行 BC 任务。但我们需要避免另一种情况，即某些 tablet 可能仅会导入少量批次的数据，因此当 Doris 发现一个 tablet 长时间没有执行过 BC 任务时，也会触发 BC 任务。这个参数就是控制这个时间的，默认是 86400，单位是秒。</p>\n</blockquote>\n<p><em>&gt; 以上两个参数通常情况下不需要修改，在某些情况下如何需要想尽快合并基线数据，可以尝试改小 <strong>base_compaction_num_cumulative_deltas 参数。但这个参数只会影响到 “被选中的 tablet”。而 “被选中” 的前提是这个 tablet 的数据版本数量是最多的。</strong></em></p>\n<h4 id=\"Cumulative-Compaction\"><a href=\"#Cumulative-Compaction\" class=\"headerlink\" title=\"Cumulative Compaction\"></a>Cumulative Compaction</h4><p>CC 任务只会选取版本比 CP 大的数据版本。其本身的选取策略也比较简单，即从 CP 版本开始，依次向后选取数据版本。最终的数据版本集合由以下参数控制：</p>\n<blockquote>\n<p>min_cumulative_compaction_num_singleton_deltas：一次 CC 任务最少的版本数量限制。这个配置是和 cumulative_size_based_compaction_lower_size_mbytes 配置同时判断的。即如果版本数量小于阈值，并且数据量也小于阈值，则不会触发 CC 任务。以避免过多不必要的 CC 任务。默认是5。</p>\n<p>max_cumulative_compaction_num_singleton_deltas：一次 CC 任务最大的版本数量限制。以防止一次 CC 任务合并的版本数量过多，占用过多资源。默认是1000。</p>\n<p>cumulative_size_based_compaction_lower_size_mbytes：一次 CC 任务最少的数据量，和min_cumulative_compaction_num_singleton_delta 同时判断。默认是 64，单位是 MB。</p>\n</blockquote>\n<p>简单来说，默认配置下，就是从 CP 版本开始往后选取 rowset。最少选5个，最多选 1000 个，然后判断数据量是否大于阈值即可。</p>\n<p>CC 任务还有一个重要步骤，就是在合并任务结束后，设置新的 Cumulative Point。CC 任务合并完成后，会产生一个合并后的新的数据版本，而我们要做的就是判断这个新的数据版是 “晋升” 到 BC 任务区，还是依然保留在 CC 任务区。举个例子：</p>\n<p>假设当前 CP 是 10。有一个 CC 任务合并了 [10-13] [14-14] [15-15] 后生成了 [10-15] 这个版本。如果决定将 [10-15] 版本移动到 BC 任务区，则需修改 CP 为 15，否则 CP 保持不变，依然为 10。</p>\n<p><strong>CP 只会增加，不会减少。</strong> 以下参数决定了是否更新 CP：</p>\n<blockquote>\n<p>cumulative_size_based_promotion_ratio：晋升比率。默认 0.05。</p>\n<p>cumulative_size_based_promotion_min_size_mbytes：最小晋升大小，默认 64，单位 MB。</p>\n<p>cumulative_size_based_promotion_size_mbytes：最大晋升大小，默认 1024，单位 MB。</p>\n</blockquote>\n<p>以上参数比较难理解，这里我们先解释下 “晋升” 的原则。一个 CC 任务生成的 rowset 的晋升原则，是其数据大小和基线数据的大小在 “同一量级”。这个类似 2048 小游戏，只有相同的数字才能合并形成更大的数字。而上面三个参数，就是用于判断一个新的rowset是否匹配基线数据的数量级。举例说明：</p>\n<p>在默认配置下，假设当前基线数据（即所有 CP 之前的数据版本）的数据量为 10GB，则晋升量级为 （10GB * 0.05）512MB。这个数值大于 64 MB 小于 1024 MB，满足条件。所以如果 CC 任务生成的新的 rowset 的大小大于 512 MB，则可以晋升，即 CP 增加。而假设基线数据为 50GB，则晋升量级为（50GB * 0.05）2.5GB。这个数值大于 64 MB 也大于 1024 MB，因此晋升量级会被调整为 1024 MB。所以如果 CC 任务生成的新的 rowset 的大小大于 1024 MB，则可以晋升，即 CP 增加。</p>\n<p>从上面的例子可以看出，cumulative_size_based_promotion_ratio 用于定义 “同一量级”，0.05 即表示数据量大于基线数据的 5% 的 rowset 都有晋升的可能，而 cumulative_size_based_promotion_min_size_mbytes 和 cumulative_size_based_promotion_size_mbytes 用于保证晋升不会过于频繁或过于严格。</p>\n<blockquote>\n<p><em>这三个参数会直接影响 BC 和 CC 任务的频率，尤其在高频导入场景下需要适当调整。我们会在后续文章中举例说明。</em></p>\n</blockquote>\n<h3 id=\"其他-Compaction-参数和注意事项\"><a href=\"#其他-Compaction-参数和注意事项\" class=\"headerlink\" title=\"其他 Compaction 参数和注意事项\"></a>其他 Compaction 参数和注意事项</h3><p>还有一些参数和 Compaction 相关，在某些情况下需要修改：</p>\n<p>disable_auto_compaction：默认为 false，修改为 true 则会禁止 Compaction 操作。该参数仅在一些调试情况，或者 compaction 异常需要临时关闭的情况下才需使用。</p>\n<h4 id=\"Delete-灾难\"><a href=\"#Delete-灾难\" class=\"headerlink\" title=\"Delete 灾难\"></a>Delete 灾难</h4><p>通过 DELETE FROM 语句执行的数据删除操作，在 Doris 中也会生成一个数据版本用于标记删除。这种类型的数据版本比较特殊，我们成为 “删除版本”。删除版本只能通过 Base Compaction 任务处理。因此在在遇到删除版本时，Cumulative Point 会强制增加，将删除版本移动到 BC 任务区。<strong>因此数据导入和删除交替发生的场景通常会导致 Compaction 灾难</strong>。比如以下版本序列：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[0-10]</span><br><span class=\"line\">[11-11] 删除版本</span><br><span class=\"line\">[12-12]</span><br><span class=\"line\">[13-13] 删除版本</span><br><span class=\"line\">[14-14]</span><br><span class=\"line\">[15-15] 删除版本</span><br><span class=\"line\">[16-16]</span><br><span class=\"line\">[17-17] 删除版本</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>在这种情况下，CC 任务几乎不会被触发（因为CC任务只能选择一个版本，而无法处理删除版本），所有版本都会交给 Base Compaction 处理，导致 Compaction 进度缓慢。目前Doris还无法很好的处理这种场景，因此需要在业务上尽量避免。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>Doris 中用于控制Compaction的参数非常多。本文尝试以下方面，介绍这些参数的含义以及如果通过调整参数来适配场景。</p>\n<ol>\n<li>数据版本是如何产生的，哪些因素影响数据版本的产出。</li>\n<li>为什么需要 Base 和 Cumulative 两种类型的 Compaction。</li>\n<li>Compaction 机制是如何挑选数据分片进行 Compaction 的。</li>\n<li>对于一个数据分片，Compaction 机制是如何确定哪些数据版本参与 Compaction 的。</li>\n<li>在高频导入场景下，可以修改哪些参数来优化 Compaction 逻辑。</li>\n<li>Compaction 相关的查看和管理命令。</li>\n</ol>\n</blockquote>\n<h1 id=\"Why-need-Compaction\"><a href=\"#Why-need-Compaction\" class=\"headerlink\" title=\"Why  need Compaction\"></a>Why  need Compaction</h1><p>Doris 的数据写入模型使用了 LSM-Tree 类似的数据结构。数据都是以追加（Append）的方式写入磁盘的。这种数据结构可以将随机写变为顺序写。这是一种面向写优化的数据结构，他能增强系统的写入吞吐，但是在读逻辑中，需要通过 Merge-on-Read 的方式，在读取时合并多次写入的数据，从而处理写入时的数据变更。</p>\n<p>Merge-on-Read 会影响读取的效率，为了降低读取时需要合并的数据量，基于 LSM-Tree 的系统都会引入后台数据合并的逻辑，以一定策略定期的对数据进行合并。Doris 中这种机制被称为 Compaction。</p>\n<p>Doris 中每次数据写入会生成一个数据版本。Compaction的过程就是讲多个数据版本合并成一个更大的版本。Compaction 可以带来以下好处：</p>\n<blockquote>\n<ul>\n<li><p>1.数据更加有序</p>\n<p>每个数据版本内的数据是按主键有序的，但是版本之间的数据是无序的。Compaction后形成的大版本将多个小版本的数据变成有序数据。在有序数据中进行数据检索的效率更高。</p>\n</li>\n<li><p>2.消除数据变更</p>\n<p>数据都是以追加的方式写入的，因此 Delete、Update 等操作都是写入一个标记。Compaction 操作可以处理这些标记，进行真正的数据删除或更新，从而在读取时，不再需要根据这些标记来过滤数据。</p>\n</li>\n<li><p>3.增加数据聚合度<br>在聚合模型下，Compaction 能进一步聚合不同数据版本中相同 key 的数据行，从而增加数据聚合度，减少读取时需要实时进行的聚合计算。</p>\n</li>\n</ul>\n</blockquote>\n<h1 id=\"Compaction-的问题\"><a href=\"#Compaction-的问题\" class=\"headerlink\" title=\"Compaction 的问题\"></a>Compaction 的问题</h1><p>用户可能需要根据实际的使用场景来调整 Compaction 的策略，否则可能遇到如下问题：</p>\n<ol>\n<li><p>Compaction 速度低于数据写入速度</p>\n<p>在高频写入场景下，短时间内会产生大量的数据版本。如果 Compaction 不及时，就会造成大量版本堆积，最终严重影响写入速度。</p>\n</li>\n<li><p>写放大问题</p>\n<p>Compaction 本质上是将已经写入的数据读取后重写写回的过程，这种数据重复写入被称为写放大。一个好的Compaction策略应该在保证效率的前提下，尽量降低写放大系数。过多的 Compaction 会占用大量的磁盘IO资源，影响系统整体效率。</p>\n</li>\n</ol>\n<h1 id=\"Something-about-Compaction-How\"><a href=\"#Something-about-Compaction-How\" class=\"headerlink\" title=\"Something about Compaction(How)\"></a>Something about Compaction(How)</h1><h2 id=\"数据版本的产生\"><a href=\"#数据版本的产生\" class=\"headerlink\" title=\"数据版本的产生\"></a>数据版本的产生</h2><p>首先，用户的数据表会按照分区和分桶规则，切分成若干个数据分片（Tablet）存储在不同 BE 节点上。每个 Tablet 都有多个副本（默认为3副本）。Compaction 是在每个 BE 上独立进行的，Compaction 逻辑处理的就是一个 BE 节点上所有的数据分片。</p>\n<p>Doris的数据都是以追加的方式写入系统的。Doris目前的写入依然是以微批的方式进行的，每一批次的数据针对每个 Tablet 都会形成一个 rowset。而一个 Tablet 是由多个Rowset 组成的。每个 Rowset 都有一个对应的起始版本和终止版本。对于新增Rowset，起始版本和终止版本相同，表示为 [6-6]、[7-7] 等。多个Rowset经过 Compaction 形成一个大的 Rowset，起始版本和终止版本为多个版本的并集，如 [6-6]、[7-7]、[8-8] 合并后变成 [6-8]。</p>\n<p>Rowset 的数量直接影响到 Compaction 是否能够及时完成。那么一批次导入会生成多少个 Rowset 呢？这里我们举一个例子：</p>\n<p>假设集群有3个 BE 节点。每个BE节点2块盘。只有一张表，2个分区，每个分区3个分桶，默认3副本。那么总分片数量是（2 * 3 * 3）18 个，如果均匀分布在所有节点上，则每个盘上3个tablet。假设一次导入涉及到其中一个分区，则一次导入总共产生9个Rowset，即平均每块盘产生1-2个 Rowset。（这里仅考虑数据完全均匀分布的情况下，实际情况中，可能多个 Tablet 集中在某一块磁盘上。）</p>\n<p>从上面的例子我们可以得出，rowset的数量直接取决于表的分片数量。举个极端的例子，如果一个Doris集群只有3个BE节点，但是一个表有9000个分片。那么一次导入，每个BE节点就会新增3000个rowset，则至少要进行3000次compaction，才能处理完所有的分片。所以：</p>\n<blockquote>\n<p><strong>合理的设置表的分区、分桶和副本数量，避免过多的分片，可以降低Compaction的开销。</strong></p>\n</blockquote>\n<h2 id=\"Base-amp-Cumulative-Compaction\"><a href=\"#Base-amp-Cumulative-Compaction\" class=\"headerlink\" title=\"Base &amp; Cumulative Compaction\"></a>Base &amp; Cumulative Compaction</h2><p>Doris 中有两种 Compaction 操作，分别称为 Base Compaction(BC) 和 Cumulative Compaction(CC)。BC 是将基线数据版本（以0为起始版本的数据）和增量数据版本合并的过程，而CC是增量数据间的合并过程。BC操作因为涉及到基线数据，而基线数据通常比较大，所以操作耗时会比CC长。</p>\n<p>如果只有 Base Compaction，则每次增量数据都要和全量的基线数据合并，写放大问题会非常严重，并且每次 Compaction 都相当耗时。因此我们需要引入 Cumulative Compaction 来先对增量数据进行合并，当增量数据合并后的大小达到一定阈值后，再和基线数据合并。这里我们有一个比较通用的 Compaction 调优策略：</p>\n<blockquote>\n<p><strong>在合理范围内，尽量减少 Base Compaction 操作。</strong></p>\n</blockquote>\n<p>BC 和 CC 之间的分界线成为 Cumulative Point（CP），这是一个动态变化的版本号。比CP小的数据版本会只会触发 BC，而比CP大的数据版本，只会触发CC。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046764-f9891cea-fe8d-4eb2-89a4-911ffb10e7e2.png\" alt=\"img\"></p>\n<p>整个过程有点类似 2048 小游戏：只有合并后大小足够，才能继续和更大的数据版本合并。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662219046833-aa0766ec-9852-4766-b57d-5b24448cd2b4.png\" alt=\"img\"></p>\n<h2 id=\"数据分片选择策略\"><a href=\"#数据分片选择策略\" class=\"headerlink\" title=\"数据分片选择策略\"></a>数据分片选择策略</h2><p><strong>Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。</strong>因此，Compaction 策略的重点问题，就是如何选择合适的 tablet，以保证节点上不会出现数据版本过多的数据分片。</p>\n<h3 id=\"Compaction-分数\"><a href=\"#Compaction-分数\" class=\"headerlink\" title=\"Compaction 分数\"></a>Compaction 分数</h3><p>一个自然的想法，就是每次都选择数据版本最多的数据分片进行 Compaction。这个策略也是 Doris 的默认策略。这个策略在大部分场景下都能很好的工作。但是考虑到一种情况，就是版本多的分片，可能并不是最频繁访问的分片。而 Compaction 的目的就是优化读性能。那么有可能某一张 “写多读少” 表一直在 Compaction，而另一张 “读多写少” 的表不能及时的 Compaction，导致读性能变差。</p>\n<p>因此，Doris 在选择数据分片时还引入了 “读取频率” 的因素。“读取频率” 和 “版本数量” 会根据各自的权重，综合计算出一个 Compaction 分数，分数越高的分片，优先做 Compaction。这两个因素的权重由以下 BE 参数控制（取值越大，权重越高）：</p>\n<blockquote>\n<p><strong>compaction_tablet_scan_frequency_factor：“读取频率” 的权重值，默认为 0。</strong></p>\n<p><strong>compaction_tablet_compaction_score_factor：“版本数量” 的权重，默认为 1。</strong></p>\n</blockquote>\n<blockquote>\n<p>“读取频率” 的权重值默认为0，即默认仅考虑 “版本数量” 这个因素。*</p>\n</blockquote>\n<h3 id=\"生产者与消费者\"><a href=\"#生产者与消费者\" class=\"headerlink\" title=\"生产者与消费者\"></a>生产者与消费者</h3><p>Compaction 是一个 生产者-消费者 模型。由一个生产者线程负责选择需要做 Compaction 的数据分片，而多个消费者负责执行 Compaction 操作。</p>\n<p>生产者线程只有一个，会定期扫描所有 tablet 来选择合适的 compaction 对象。因为 Base Compaction 和 Cumulative Compaction 是不同类型的任务，因此目前的策略是每生成 9 个 CC 任务，生成一个 BC 任务。任务生成的频率由以下两个参数控制：</p>\n<blockquote>\n<p><strong>cumulative_compaction_rounds_for_each_base_compaction_round：多少个CC任务后生成一个BC任务。</strong></p>\n<p><strong>generate_compaction_tasks_min_interval_ms：任务生成的间隔。</strong></p>\n</blockquote>\n<blockquote>\n<p><em>这两个参数通常情况下不需要调整。</em></p>\n</blockquote>\n<p>生产者线程产生的任务会被提交到消费者线程池。因为 Compaction 是一个IO密集型的任务，为了保证 Compaction 任务不会过多的占用IO资源，Doris 限制了每个磁盘上能够同时进行的 Compaction 任务数量，以及节点整体的任务数量，这些限制由以下参数控制：</p>\n<blockquote>\n<p>compaction_task_num_per_disk：每个磁盘上的任务数，默认为2。该参数必须大于等于2，以保证 BC 和 CC 任务各自至少有一个线程。</p>\n<p>max_compaction_threads：消费者线程，即Compaction线程的总数。默认为 10。</p>\n</blockquote>\n<p>举个例子，假设一个 BE 节点配置了3个数据目录（即3块磁盘），每个磁盘上的任务数配置为2，总线程数为5。则同一时间，最多有5个 Compaction 任务在进行，而每块磁盘上最多有2个任务在进行。并且最多有3个 BC 任务在进行，因为每块盘上会自动预留一个线程给CC任务。</p>\n<p><strong>另一方面，Compaction 任务同时也是一个内存密集型任务，因为其本质是一个多路归并排序的过程，每一路是一个数据版本。</strong>如果一个 Compaction 任务涉及的数据版本很多，则会占用更多的内存，如果仅限制任务数，而不考虑任务的内存开销，则有可能导致系统内存超限。因此，Doris 在上述任务个数限制之外，还增加了一个任务配额限制：</p>\n<blockquote>\n<p>total_permits_for_compaction_score：Compaction 任务配额，默认 10000。</p>\n</blockquote>\n<p>每个 Compaction 任务都有一个配额，其数值就是任务涉及的数据版本数量。假设一个任务需要合并100个版本，则其配额为100。当正在运行的任务配额总和超过配置后，新的任务将被拒绝。</p>\n<p>三个配置共同决定了节点所能承受的 Compaction 任务数量。</p>\n<h3 id=\"数据版本选择策略\"><a href=\"#数据版本选择策略\" class=\"headerlink\" title=\"数据版本选择策略\"></a>数据版本选择策略</h3><p>一个 Compaction 任务对应的是一个数据分片（Tablet）。消费线程拿到一个 Compaction 任务后，会根据 Compaction 的任务类型，选择 tablet 中合适的数据版本（Rowset）进行数据合并。下面分别介绍 Base Compaction 和 Cumulative Compaction 的数据分片选择策略。</p>\n<h4 id=\"Base-Compaction\"><a href=\"#Base-Compaction\" class=\"headerlink\" title=\"Base Compaction\"></a>Base Compaction</h4><p>前文说过，BC 任务是增量数据和基线数据的合并任务。并且只有比 Cumulative Point（CP） 小的数据版本才会参与 BC 任务。因此，BC 任务的数据版本选取策略比较简单。</p>\n<p>首先，会选取所有版本在 0到 CP之间的 rowset。然后根据以下几个配置参数，判断是否启动一个 BC 任务：</p>\n<blockquote>\n<p>base_compaction_num_cumulative_deltas：一次 BC 任务最小版本数量限制。默认为5。该参数主要为了避免过多 BC 任务。当数据版本数量较少时，BC 是没有必要的。</p>\n<p>base_compaction_interval_seconds_since_last_operation：第一个参数限制了当版本数量少时，不会进行 BC 任务。但我们需要避免另一种情况，即某些 tablet 可能仅会导入少量批次的数据，因此当 Doris 发现一个 tablet 长时间没有执行过 BC 任务时，也会触发 BC 任务。这个参数就是控制这个时间的，默认是 86400，单位是秒。</p>\n</blockquote>\n<p><em>&gt; 以上两个参数通常情况下不需要修改，在某些情况下如何需要想尽快合并基线数据，可以尝试改小 <strong>base_compaction_num_cumulative_deltas 参数。但这个参数只会影响到 “被选中的 tablet”。而 “被选中” 的前提是这个 tablet 的数据版本数量是最多的。</strong></em></p>\n<h4 id=\"Cumulative-Compaction\"><a href=\"#Cumulative-Compaction\" class=\"headerlink\" title=\"Cumulative Compaction\"></a>Cumulative Compaction</h4><p>CC 任务只会选取版本比 CP 大的数据版本。其本身的选取策略也比较简单，即从 CP 版本开始，依次向后选取数据版本。最终的数据版本集合由以下参数控制：</p>\n<blockquote>\n<p>min_cumulative_compaction_num_singleton_deltas：一次 CC 任务最少的版本数量限制。这个配置是和 cumulative_size_based_compaction_lower_size_mbytes 配置同时判断的。即如果版本数量小于阈值，并且数据量也小于阈值，则不会触发 CC 任务。以避免过多不必要的 CC 任务。默认是5。</p>\n<p>max_cumulative_compaction_num_singleton_deltas：一次 CC 任务最大的版本数量限制。以防止一次 CC 任务合并的版本数量过多，占用过多资源。默认是1000。</p>\n<p>cumulative_size_based_compaction_lower_size_mbytes：一次 CC 任务最少的数据量，和min_cumulative_compaction_num_singleton_delta 同时判断。默认是 64，单位是 MB。</p>\n</blockquote>\n<p>简单来说，默认配置下，就是从 CP 版本开始往后选取 rowset。最少选5个，最多选 1000 个，然后判断数据量是否大于阈值即可。</p>\n<p>CC 任务还有一个重要步骤，就是在合并任务结束后，设置新的 Cumulative Point。CC 任务合并完成后，会产生一个合并后的新的数据版本，而我们要做的就是判断这个新的数据版是 “晋升” 到 BC 任务区，还是依然保留在 CC 任务区。举个例子：</p>\n<p>假设当前 CP 是 10。有一个 CC 任务合并了 [10-13] [14-14] [15-15] 后生成了 [10-15] 这个版本。如果决定将 [10-15] 版本移动到 BC 任务区，则需修改 CP 为 15，否则 CP 保持不变，依然为 10。</p>\n<p><strong>CP 只会增加，不会减少。</strong> 以下参数决定了是否更新 CP：</p>\n<blockquote>\n<p>cumulative_size_based_promotion_ratio：晋升比率。默认 0.05。</p>\n<p>cumulative_size_based_promotion_min_size_mbytes：最小晋升大小，默认 64，单位 MB。</p>\n<p>cumulative_size_based_promotion_size_mbytes：最大晋升大小，默认 1024，单位 MB。</p>\n</blockquote>\n<p>以上参数比较难理解，这里我们先解释下 “晋升” 的原则。一个 CC 任务生成的 rowset 的晋升原则，是其数据大小和基线数据的大小在 “同一量级”。这个类似 2048 小游戏，只有相同的数字才能合并形成更大的数字。而上面三个参数，就是用于判断一个新的rowset是否匹配基线数据的数量级。举例说明：</p>\n<p>在默认配置下，假设当前基线数据（即所有 CP 之前的数据版本）的数据量为 10GB，则晋升量级为 （10GB * 0.05）512MB。这个数值大于 64 MB 小于 1024 MB，满足条件。所以如果 CC 任务生成的新的 rowset 的大小大于 512 MB，则可以晋升，即 CP 增加。而假设基线数据为 50GB，则晋升量级为（50GB * 0.05）2.5GB。这个数值大于 64 MB 也大于 1024 MB，因此晋升量级会被调整为 1024 MB。所以如果 CC 任务生成的新的 rowset 的大小大于 1024 MB，则可以晋升，即 CP 增加。</p>\n<p>从上面的例子可以看出，cumulative_size_based_promotion_ratio 用于定义 “同一量级”，0.05 即表示数据量大于基线数据的 5% 的 rowset 都有晋升的可能，而 cumulative_size_based_promotion_min_size_mbytes 和 cumulative_size_based_promotion_size_mbytes 用于保证晋升不会过于频繁或过于严格。</p>\n<blockquote>\n<p><em>这三个参数会直接影响 BC 和 CC 任务的频率，尤其在高频导入场景下需要适当调整。我们会在后续文章中举例说明。</em></p>\n</blockquote>\n<h3 id=\"其他-Compaction-参数和注意事项\"><a href=\"#其他-Compaction-参数和注意事项\" class=\"headerlink\" title=\"其他 Compaction 参数和注意事项\"></a>其他 Compaction 参数和注意事项</h3><p>还有一些参数和 Compaction 相关，在某些情况下需要修改：</p>\n<p>disable_auto_compaction：默认为 false，修改为 true 则会禁止 Compaction 操作。该参数仅在一些调试情况，或者 compaction 异常需要临时关闭的情况下才需使用。</p>\n<h4 id=\"Delete-灾难\"><a href=\"#Delete-灾难\" class=\"headerlink\" title=\"Delete 灾难\"></a>Delete 灾难</h4><p>通过 DELETE FROM 语句执行的数据删除操作，在 Doris 中也会生成一个数据版本用于标记删除。这种类型的数据版本比较特殊，我们成为 “删除版本”。删除版本只能通过 Base Compaction 任务处理。因此在在遇到删除版本时，Cumulative Point 会强制增加，将删除版本移动到 BC 任务区。<strong>因此数据导入和删除交替发生的场景通常会导致 Compaction 灾难</strong>。比如以下版本序列：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[0-10]</span><br><span class=\"line\">[11-11] 删除版本</span><br><span class=\"line\">[12-12]</span><br><span class=\"line\">[13-13] 删除版本</span><br><span class=\"line\">[14-14]</span><br><span class=\"line\">[15-15] 删除版本</span><br><span class=\"line\">[16-16]</span><br><span class=\"line\">[17-17] 删除版本</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>在这种情况下，CC 任务几乎不会被触发（因为CC任务只能选择一个版本，而无法处理删除版本），所有版本都会交给 Base Compaction 处理，导致 Compaction 进度缓慢。目前Doris还无法很好的处理这种场景，因此需要在业务上尽量避免。</p>\n"},{"title":"基于Doris的数据中台的实践与优化","abbrlink":60503,"date":"2022-09-30T07:42:16.000Z","updated":"2022-09-30T07:42:16.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 前言\n\n> 随着数据量不断膨胀，基于Oracle的ETL和BI报表可视化业务越来越慢，难以保证数据服务的SLA，为了减少整个大数据平台的复杂度，决定开始调研-'以 Apache Doris 为核心建设一站式数据中台'。\n>\n> 展望：\n>\n> 1、所有业务数据通过Flink实时导入到Doris\n>\n> 2、ETL全部在Doris中完成 \n>\n> 3、ETL后，基于Doris的ad-hoc能力，可以直接作为ADS层对外提供服务。\n\n## 遇到的问题\n\n- 由于业务非常复杂，ETL过程也非常繁琐，往往涉及数十张表的Join，这非常考验Doris的查询优化器。同时ETL SQL中常常需要开窗排序，非常容易造成内存溢出，导致ETL SQL无法完成。以应对大查询，做了以下参数方面的优化。\n\n  >SET enable_profile = true; \n  >\n  >SET query_timeout = 30000;\n  >\n  >SET enable_spilling = true;\n  >\n  >SET exec_mem_limit = 10 * 1024 * 1024 * 1024;\n  >\n  >SET parallel_fragment_exec_instance_num = 8;\n  >\n  >SET enable_cost_based_join_reorder = true;\n\n\n\n## 调优\n\n- 理解`SHOW TABLETS FROM example_db.table_name PARTITIONS(p1, p2);`所有参数的意义，重点是Tablet数据大小，是否有数据倾斜。\n\n  > 在当前设计下，衡量分桶是否规范的唯一标准就是数据量，将每个tablet的数据量控制在100M-1G左右（2.4及以后版本，每个分桶的数据量建议控制在压缩前10G左右），是比较推荐的。这里说的数据量是指压缩后的数据量，StarRocks是列存，使用LZ4压缩，压缩率根据数据类型和数据情况的不同可能有2-8倍不等。\n\n- tablet和segment的关系。tablet writer write failed, tablet_id=3121890, txn_id=241689, err=-238错误。通常出现在同一批导入数据量过大的情况，从而导致某一个 tablet 的 Segment 文件过多。\n\n- RuntimeFilter是否生效。\n\n- 会分析Query Profile，能找到瓶颈在哪里。https://forum.mirrorship.cn/t/topic/2367\n\n- 参考：https://juejin.cn/post/7127943272634253343#heading-17，优化Doris。例如：max_segment_num_per_rowset\n","source":"_posts/bigdata/Doris大查询实践与优化.md","raw":"---\ntitle: 基于Doris的数据中台的实践与优化\ntags:\n  - Doris\n  - 数据中台\ncategories:\n  - - bigdata\n    - Doris\nabbrlink: 60503\ndate: 2022-09-30 15:42:16\nupdated: 2022-09-30 15:42:16\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 前言\n\n> 随着数据量不断膨胀，基于Oracle的ETL和BI报表可视化业务越来越慢，难以保证数据服务的SLA，为了减少整个大数据平台的复杂度，决定开始调研-'以 Apache Doris 为核心建设一站式数据中台'。\n>\n> 展望：\n>\n> 1、所有业务数据通过Flink实时导入到Doris\n>\n> 2、ETL全部在Doris中完成 \n>\n> 3、ETL后，基于Doris的ad-hoc能力，可以直接作为ADS层对外提供服务。\n\n## 遇到的问题\n\n- 由于业务非常复杂，ETL过程也非常繁琐，往往涉及数十张表的Join，这非常考验Doris的查询优化器。同时ETL SQL中常常需要开窗排序，非常容易造成内存溢出，导致ETL SQL无法完成。以应对大查询，做了以下参数方面的优化。\n\n  >SET enable_profile = true; \n  >\n  >SET query_timeout = 30000;\n  >\n  >SET enable_spilling = true;\n  >\n  >SET exec_mem_limit = 10 * 1024 * 1024 * 1024;\n  >\n  >SET parallel_fragment_exec_instance_num = 8;\n  >\n  >SET enable_cost_based_join_reorder = true;\n\n\n\n## 调优\n\n- 理解`SHOW TABLETS FROM example_db.table_name PARTITIONS(p1, p2);`所有参数的意义，重点是Tablet数据大小，是否有数据倾斜。\n\n  > 在当前设计下，衡量分桶是否规范的唯一标准就是数据量，将每个tablet的数据量控制在100M-1G左右（2.4及以后版本，每个分桶的数据量建议控制在压缩前10G左右），是比较推荐的。这里说的数据量是指压缩后的数据量，StarRocks是列存，使用LZ4压缩，压缩率根据数据类型和数据情况的不同可能有2-8倍不等。\n\n- tablet和segment的关系。tablet writer write failed, tablet_id=3121890, txn_id=241689, err=-238错误。通常出现在同一批导入数据量过大的情况，从而导致某一个 tablet 的 Segment 文件过多。\n\n- RuntimeFilter是否生效。\n\n- 会分析Query Profile，能找到瓶颈在哪里。https://forum.mirrorship.cn/t/topic/2367\n\n- 参考：https://juejin.cn/post/7127943272634253343#heading-17，优化Doris。例如：max_segment_num_per_rowset\n","slug":"bigdata/Doris大查询实践与优化","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksx001a8j5m4q7mbyoa","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>随着数据量不断膨胀，基于Oracle的ETL和BI报表可视化业务越来越慢，难以保证数据服务的SLA，为了减少整个大数据平台的复杂度，决定开始调研-‘以 Apache Doris 为核心建设一站式数据中台’。</p>\n<p>展望：</p>\n<p>1、所有业务数据通过Flink实时导入到Doris</p>\n<p>2、ETL全部在Doris中完成 </p>\n<p>3、ETL后，基于Doris的ad-hoc能力，可以直接作为ADS层对外提供服务。</p>\n</blockquote>\n<h2 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h2><ul>\n<li><p>由于业务非常复杂，ETL过程也非常繁琐，往往涉及数十张表的Join，这非常考验Doris的查询优化器。同时ETL SQL中常常需要开窗排序，非常容易造成内存溢出，导致ETL SQL无法完成。以应对大查询，做了以下参数方面的优化。</p>\n<blockquote>\n<p>SET enable_profile &#x3D; true; </p>\n<p>SET query_timeout &#x3D; 30000;</p>\n<p>SET enable_spilling &#x3D; true;</p>\n<p>SET exec_mem_limit &#x3D; 10 * 1024 * 1024 * 1024;</p>\n<p>SET parallel_fragment_exec_instance_num &#x3D; 8;</p>\n<p>SET enable_cost_based_join_reorder &#x3D; true;</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"调优\"><a href=\"#调优\" class=\"headerlink\" title=\"调优\"></a>调优</h2><ul>\n<li><p>理解<code>SHOW TABLETS FROM example_db.table_name PARTITIONS(p1, p2);</code>所有参数的意义，重点是Tablet数据大小，是否有数据倾斜。</p>\n<blockquote>\n<p>在当前设计下，衡量分桶是否规范的唯一标准就是数据量，将每个tablet的数据量控制在100M-1G左右（2.4及以后版本，每个分桶的数据量建议控制在压缩前10G左右），是比较推荐的。这里说的数据量是指压缩后的数据量，StarRocks是列存，使用LZ4压缩，压缩率根据数据类型和数据情况的不同可能有2-8倍不等。</p>\n</blockquote>\n</li>\n<li><p>tablet和segment的关系。tablet writer write failed, tablet_id&#x3D;3121890, txn_id&#x3D;241689, err&#x3D;-238错误。通常出现在同一批导入数据量过大的情况，从而导致某一个 tablet 的 Segment 文件过多。</p>\n</li>\n<li><p>RuntimeFilter是否生效。</p>\n</li>\n<li><p>会分析Query Profile，能找到瓶颈在哪里。<a href=\"https://forum.mirrorship.cn/t/topic/2367\">https://forum.mirrorship.cn/t/topic/2367</a></p>\n</li>\n<li><p>参考：<a href=\"https://juejin.cn/post/7127943272634253343#heading-17%EF%BC%8C%E4%BC%98%E5%8C%96Doris%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%9Amax_segment_num_per_rowset\">https://juejin.cn/post/7127943272634253343#heading-17，优化Doris。例如：max_segment_num_per_rowset</a></p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>随着数据量不断膨胀，基于Oracle的ETL和BI报表可视化业务越来越慢，难以保证数据服务的SLA，为了减少整个大数据平台的复杂度，决定开始调研-‘以 Apache Doris 为核心建设一站式数据中台’。</p>\n<p>展望：</p>\n<p>1、所有业务数据通过Flink实时导入到Doris</p>\n<p>2、ETL全部在Doris中完成 </p>\n<p>3、ETL后，基于Doris的ad-hoc能力，可以直接作为ADS层对外提供服务。</p>\n</blockquote>\n<h2 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h2><ul>\n<li><p>由于业务非常复杂，ETL过程也非常繁琐，往往涉及数十张表的Join，这非常考验Doris的查询优化器。同时ETL SQL中常常需要开窗排序，非常容易造成内存溢出，导致ETL SQL无法完成。以应对大查询，做了以下参数方面的优化。</p>\n<blockquote>\n<p>SET enable_profile &#x3D; true; </p>\n<p>SET query_timeout &#x3D; 30000;</p>\n<p>SET enable_spilling &#x3D; true;</p>\n<p>SET exec_mem_limit &#x3D; 10 * 1024 * 1024 * 1024;</p>\n<p>SET parallel_fragment_exec_instance_num &#x3D; 8;</p>\n<p>SET enable_cost_based_join_reorder &#x3D; true;</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"调优\"><a href=\"#调优\" class=\"headerlink\" title=\"调优\"></a>调优</h2><ul>\n<li><p>理解<code>SHOW TABLETS FROM example_db.table_name PARTITIONS(p1, p2);</code>所有参数的意义，重点是Tablet数据大小，是否有数据倾斜。</p>\n<blockquote>\n<p>在当前设计下，衡量分桶是否规范的唯一标准就是数据量，将每个tablet的数据量控制在100M-1G左右（2.4及以后版本，每个分桶的数据量建议控制在压缩前10G左右），是比较推荐的。这里说的数据量是指压缩后的数据量，StarRocks是列存，使用LZ4压缩，压缩率根据数据类型和数据情况的不同可能有2-8倍不等。</p>\n</blockquote>\n</li>\n<li><p>tablet和segment的关系。tablet writer write failed, tablet_id&#x3D;3121890, txn_id&#x3D;241689, err&#x3D;-238错误。通常出现在同一批导入数据量过大的情况，从而导致某一个 tablet 的 Segment 文件过多。</p>\n</li>\n<li><p>RuntimeFilter是否生效。</p>\n</li>\n<li><p>会分析Query Profile，能找到瓶颈在哪里。<a href=\"https://forum.mirrorship.cn/t/topic/2367\">https://forum.mirrorship.cn/t/topic/2367</a></p>\n</li>\n<li><p>参考：<a href=\"https://juejin.cn/post/7127943272634253343#heading-17%EF%BC%8C%E4%BC%98%E5%8C%96Doris%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%9Amax_segment_num_per_rowset\">https://juejin.cn/post/7127943272634253343#heading-17，优化Doris。例如：max_segment_num_per_rowset</a></p>\n</li>\n</ul>\n"},{"title":"Doris中的索引","abbrlink":24352,"date":"2022-08-07T02:51:08.000Z","updated":"2022-08-07T02:51:08.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n> **不同于传统的数据库设计，Doris 不支持在任意列上创建索引。**Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。\n>\n> Doris 支持比较丰富的索引结构，来减少数据的扫描：\n>\n> - Sorted Compound Key Index，可以最多指定三个列组成复合排序键，通过该索引，能够有效进行数据裁剪，从而能够更好支持高并发的报表场景\n> - Z-order Index ：使用 Z-order 索引，可以高效对数据模型中的任意字段组合进行范围查询\n> - Min/Max ：有效过滤数值类型的等值和范围查询\n> - Bloom Filter ：对高基数列的等值过滤裁剪非常有效\n> - Invert Index ：能够对任意字段实现快速检索\n\n\n\n# 前缀索引\n\n## 基本概念\n\n不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。\n\n本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。\n\n在 Aggregate、Unique 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表语句中，AGGREGATE KEY、UNIQUE KEY 和 DUPLICATE KEY 中指定的列进行排序存储的。\n\n而前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方式。\n\n## 示例\n\n我们将一行数据的前 **36 个字节** 作为这行数据的前缀索引。当遇到 VARCHAR 类型时，前缀索引会直接截断。我们举例说明：\n\n1. 以下表结构的前缀索引为 user_id(8 Bytes) + age(4 Bytes) + message(prefix 20 Bytes)。\n\n   | ColumnName     | Type         |\n   | -------------- | ------------ |\n   | user_id        | BIGINT       |\n   | age            | INT          |\n   | message        | VARCHAR(100) |\n   | max_dwell_time | DATETIME     |\n   | min_dwell_time | DATETIME     |\n\n2. 以下表结构的前缀索引为 user_name(20 Bytes)。即使没有达到 36 个字节，因为遇到 VARCHAR，所以直接截断，不再往后继续。\n\n   | ColumnName     | Type         |\n   | -------------- | ------------ |\n   | user_name      | VARCHAR(20)  |\n   | age            | INT          |\n   | message        | VARCHAR(100) |\n   | max_dwell_time | DATETIME     |\n   | min_dwell_time | DATETIME     |\n\n当我们的查询条件，是**前缀索引的前缀**时，可以极大的加快查询速度。比如在第一个例子中，我们执行如下查询：\n\n```sql\nSELECT * FROM table WHERE user_id=1829239 and age=20；\n```\n\n该查询的效率会**远高于**如下查询：\n\n```sql\nSELECT * FROM table WHERE age=20；\n```\n\n所以在建表时，**正确的选择列顺序，能够极大地提高查询效率**。\n\n## 通过ROLLUP来调整前缀索引\n\n因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过创建 ROLLUP 来人为的调整列顺序。详情可参考[ROLLUP](https://doris.apache.org/zh-CN/docs/data-table/hit-the-rollup)。\n\n\n\n# BloomFilter索引\n\n## Doris BloomFilter索引及使用使用场景\n\n举个例子：如果要查找一个占用100字节存储空间大小的短行，一个64KB的HFile数据块应该包含(64 * 1024)/100 = 655.53 = ~700行，如果仅能在整个数据块的起始行键上建立索引，那么它是无法给你提供细粒度的索引信息的。因为要查找的行数据可能会落在该数据块的行区间上，也可能行数据没在该数据块上，也可能是表中根本就不存在该行数据，也或者是行数据在另一个HFile里，甚至在MemStore里。以上这几种情况，都会导致从磁盘读取数据块时带来额外的IO开销，也会滥用数据块的缓存，当面对一个巨大的数据集且处于高并发读时，会严重影响性能。\n\n因此，HBase提供了布隆过滤器，它允许你对存储在每个数据块的数据做一个反向测试。当某行被请求时，通过布隆过滤器先检查该行是否不在这个数据块，布隆过滤器要么确定回答该行不在，要么回答它不知道。这就是为什么我们称它是反向测试。布隆过滤器同样也可以应用到行里的单元上，当访问某列标识符时可以先使用同样的反向测试。\n\n但布隆过滤器也不是没有代价。存储这个额外的索引层次会占用额外的空间。布隆过滤器随着它们的索引对象数据增长而增长，所以行级布隆过滤器比列标识符级布隆过滤器占用空间要少。当空间不是问题时，它们可以帮助你榨干系统的性能潜力。 Doris的BloomFilter索引可以通过建表的时候指定，或者通过表的ALTER操作来完成。Bloom Filter本质上是一种位图结构，用于快速的判断一个给定的值是否在一个集合中。这种判断会产生小概率的误判。即如果返回false，则一定不在这个集合内。而如果范围true，则有可能在这个集合内。\n\nBloomFilter索引也是以Block为粒度创建的。每个Block中，指定列的值作为一个集合生成一个BloomFilter索引条目，用于在查询是快速过滤不满足条件的数据。\n\n下面我们通过实例来看看Doris怎么创建BloomFilter索引。\n\n## 创建BloomFilter索引\n\nDoris BloomFilter索引的创建是通过在建表语句的PROPERTIES里加上\"bloom_filter_columns\"=\"k1,k2,k3\",这个属性，k1,k2,k3是你要创建的BloomFilter索引的Key列名称，例如下面我们对表里的saler_id,category_id创建了BloomFilter索引。\n\n```sql\nCREATE TABLE IF NOT EXISTS sale_detail_bloom  (\n    sale_date date NOT NULL COMMENT \"销售时间\",\n    customer_id int NOT NULL COMMENT \"客户编号\",\n    saler_id int NOT NULL COMMENT \"销售员\",\n    sku_id int NOT NULL COMMENT \"商品编号\",\n    category_id int NOT NULL COMMENT \"商品分类\",\n    sale_count int NOT NULL COMMENT \"销售数量\",\n    sale_price DECIMAL(12,2) NOT NULL COMMENT \"单价\",\n    sale_amt DECIMAL(20,2)  COMMENT \"销售总金额\"\n)\nDuplicate  KEY(sale_date, customer_id,saler_id,sku_id,category_id)\nPARTITION BY RANGE(sale_date)\n(\nPARTITION P_202111 VALUES [('2021-11-01'), ('2021-12-01'))\n)\nDISTRIBUTED BY HASH(saler_id) BUCKETS 10\nPROPERTIES (\n\"replication_num\" = \"3\",\n\"bloom_filter_columns\"=\"saler_id,category_id\",\n\"dynamic_partition.enable\" = \"true\",\n\"dynamic_partition.time_unit\" = \"MONTH\",\n\"dynamic_partition.time_zone\" = \"Asia/Shanghai\",\n\"dynamic_partition.start\" = \"-2147483648\",\n\"dynamic_partition.end\" = \"2\",\n\"dynamic_partition.prefix\" = \"P_\",\n\"dynamic_partition.replication_num\" = \"3\",\n\"dynamic_partition.buckets\" = \"3\"\n);\n```\n\n\n\n## 查看BloomFilter索引\n\n查看我们在表上建立的BloomFilter索引是使用:\n\n```sql\nSHOW CREATE TABLE <table_name>\n```\n\n## 删除BloomFilter索引\n\n删除索引即为将索引列从bloom_filter_columns属性中移除：\n\n```sql\nALTER TABLE <db.table_name> SET (\"bloom_filter_columns\" = \"\");\n```\n\n## 修改BloomFilter索引\n\n修改索引即为修改表的bloom_filter_columns属性：\n\n```sql\nALTER TABLE <db.table_name> SET (\"bloom_filter_columns\" = \"k1,k3\");\n```\n\n## **Doris BloomFilter使用场景**\n\n满足以下几个条件时可以考虑对某列建立Bloom Filter 索引：\n\n1. 首先BloomFilter适用于非前缀过滤.\n2. 查询会根据该列高频过滤，而且查询条件大多是in和 = 过滤.\n3. 不同于Bitmap, BloomFilter适用于高基数列。比如UserID。因为如果创建在低基数的列上，比如”性别“列，则每个Block几乎都会包含所有取值，导致BloomFilter索引失去意义\n\n## **Doris BloomFilter使用注意事项**\n\n1. 不支持对Tinyint、Float、Double 类型的列建Bloom Filter索引。\n2. Bloom Filter索引只对in和 = 过滤查询有加速效果。\n3. 如果要查看某个查询是否命中了Bloom Filter索引，可以通过查询的Profile信息查看\n\n\n\n# Bitmap 索引\n\n","source":"_posts/bigdata/Doris中的索引.md","raw":"---\ntitle: Doris中的索引\ntags:\n  - Doris\ncategories:\n  - - bigdata\n    - Doris\nabbrlink: 24352\ndate: 2022-08-07 10:51:08\nupdated: 2022-08-07 10:51:08\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n> **不同于传统的数据库设计，Doris 不支持在任意列上创建索引。**Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。\n>\n> Doris 支持比较丰富的索引结构，来减少数据的扫描：\n>\n> - Sorted Compound Key Index，可以最多指定三个列组成复合排序键，通过该索引，能够有效进行数据裁剪，从而能够更好支持高并发的报表场景\n> - Z-order Index ：使用 Z-order 索引，可以高效对数据模型中的任意字段组合进行范围查询\n> - Min/Max ：有效过滤数值类型的等值和范围查询\n> - Bloom Filter ：对高基数列的等值过滤裁剪非常有效\n> - Invert Index ：能够对任意字段实现快速检索\n\n\n\n# 前缀索引\n\n## 基本概念\n\n不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。\n\n本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。\n\n在 Aggregate、Unique 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表语句中，AGGREGATE KEY、UNIQUE KEY 和 DUPLICATE KEY 中指定的列进行排序存储的。\n\n而前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方式。\n\n## 示例\n\n我们将一行数据的前 **36 个字节** 作为这行数据的前缀索引。当遇到 VARCHAR 类型时，前缀索引会直接截断。我们举例说明：\n\n1. 以下表结构的前缀索引为 user_id(8 Bytes) + age(4 Bytes) + message(prefix 20 Bytes)。\n\n   | ColumnName     | Type         |\n   | -------------- | ------------ |\n   | user_id        | BIGINT       |\n   | age            | INT          |\n   | message        | VARCHAR(100) |\n   | max_dwell_time | DATETIME     |\n   | min_dwell_time | DATETIME     |\n\n2. 以下表结构的前缀索引为 user_name(20 Bytes)。即使没有达到 36 个字节，因为遇到 VARCHAR，所以直接截断，不再往后继续。\n\n   | ColumnName     | Type         |\n   | -------------- | ------------ |\n   | user_name      | VARCHAR(20)  |\n   | age            | INT          |\n   | message        | VARCHAR(100) |\n   | max_dwell_time | DATETIME     |\n   | min_dwell_time | DATETIME     |\n\n当我们的查询条件，是**前缀索引的前缀**时，可以极大的加快查询速度。比如在第一个例子中，我们执行如下查询：\n\n```sql\nSELECT * FROM table WHERE user_id=1829239 and age=20；\n```\n\n该查询的效率会**远高于**如下查询：\n\n```sql\nSELECT * FROM table WHERE age=20；\n```\n\n所以在建表时，**正确的选择列顺序，能够极大地提高查询效率**。\n\n## 通过ROLLUP来调整前缀索引\n\n因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过创建 ROLLUP 来人为的调整列顺序。详情可参考[ROLLUP](https://doris.apache.org/zh-CN/docs/data-table/hit-the-rollup)。\n\n\n\n# BloomFilter索引\n\n## Doris BloomFilter索引及使用使用场景\n\n举个例子：如果要查找一个占用100字节存储空间大小的短行，一个64KB的HFile数据块应该包含(64 * 1024)/100 = 655.53 = ~700行，如果仅能在整个数据块的起始行键上建立索引，那么它是无法给你提供细粒度的索引信息的。因为要查找的行数据可能会落在该数据块的行区间上，也可能行数据没在该数据块上，也可能是表中根本就不存在该行数据，也或者是行数据在另一个HFile里，甚至在MemStore里。以上这几种情况，都会导致从磁盘读取数据块时带来额外的IO开销，也会滥用数据块的缓存，当面对一个巨大的数据集且处于高并发读时，会严重影响性能。\n\n因此，HBase提供了布隆过滤器，它允许你对存储在每个数据块的数据做一个反向测试。当某行被请求时，通过布隆过滤器先检查该行是否不在这个数据块，布隆过滤器要么确定回答该行不在，要么回答它不知道。这就是为什么我们称它是反向测试。布隆过滤器同样也可以应用到行里的单元上，当访问某列标识符时可以先使用同样的反向测试。\n\n但布隆过滤器也不是没有代价。存储这个额外的索引层次会占用额外的空间。布隆过滤器随着它们的索引对象数据增长而增长，所以行级布隆过滤器比列标识符级布隆过滤器占用空间要少。当空间不是问题时，它们可以帮助你榨干系统的性能潜力。 Doris的BloomFilter索引可以通过建表的时候指定，或者通过表的ALTER操作来完成。Bloom Filter本质上是一种位图结构，用于快速的判断一个给定的值是否在一个集合中。这种判断会产生小概率的误判。即如果返回false，则一定不在这个集合内。而如果范围true，则有可能在这个集合内。\n\nBloomFilter索引也是以Block为粒度创建的。每个Block中，指定列的值作为一个集合生成一个BloomFilter索引条目，用于在查询是快速过滤不满足条件的数据。\n\n下面我们通过实例来看看Doris怎么创建BloomFilter索引。\n\n## 创建BloomFilter索引\n\nDoris BloomFilter索引的创建是通过在建表语句的PROPERTIES里加上\"bloom_filter_columns\"=\"k1,k2,k3\",这个属性，k1,k2,k3是你要创建的BloomFilter索引的Key列名称，例如下面我们对表里的saler_id,category_id创建了BloomFilter索引。\n\n```sql\nCREATE TABLE IF NOT EXISTS sale_detail_bloom  (\n    sale_date date NOT NULL COMMENT \"销售时间\",\n    customer_id int NOT NULL COMMENT \"客户编号\",\n    saler_id int NOT NULL COMMENT \"销售员\",\n    sku_id int NOT NULL COMMENT \"商品编号\",\n    category_id int NOT NULL COMMENT \"商品分类\",\n    sale_count int NOT NULL COMMENT \"销售数量\",\n    sale_price DECIMAL(12,2) NOT NULL COMMENT \"单价\",\n    sale_amt DECIMAL(20,2)  COMMENT \"销售总金额\"\n)\nDuplicate  KEY(sale_date, customer_id,saler_id,sku_id,category_id)\nPARTITION BY RANGE(sale_date)\n(\nPARTITION P_202111 VALUES [('2021-11-01'), ('2021-12-01'))\n)\nDISTRIBUTED BY HASH(saler_id) BUCKETS 10\nPROPERTIES (\n\"replication_num\" = \"3\",\n\"bloom_filter_columns\"=\"saler_id,category_id\",\n\"dynamic_partition.enable\" = \"true\",\n\"dynamic_partition.time_unit\" = \"MONTH\",\n\"dynamic_partition.time_zone\" = \"Asia/Shanghai\",\n\"dynamic_partition.start\" = \"-2147483648\",\n\"dynamic_partition.end\" = \"2\",\n\"dynamic_partition.prefix\" = \"P_\",\n\"dynamic_partition.replication_num\" = \"3\",\n\"dynamic_partition.buckets\" = \"3\"\n);\n```\n\n\n\n## 查看BloomFilter索引\n\n查看我们在表上建立的BloomFilter索引是使用:\n\n```sql\nSHOW CREATE TABLE <table_name>\n```\n\n## 删除BloomFilter索引\n\n删除索引即为将索引列从bloom_filter_columns属性中移除：\n\n```sql\nALTER TABLE <db.table_name> SET (\"bloom_filter_columns\" = \"\");\n```\n\n## 修改BloomFilter索引\n\n修改索引即为修改表的bloom_filter_columns属性：\n\n```sql\nALTER TABLE <db.table_name> SET (\"bloom_filter_columns\" = \"k1,k3\");\n```\n\n## **Doris BloomFilter使用场景**\n\n满足以下几个条件时可以考虑对某列建立Bloom Filter 索引：\n\n1. 首先BloomFilter适用于非前缀过滤.\n2. 查询会根据该列高频过滤，而且查询条件大多是in和 = 过滤.\n3. 不同于Bitmap, BloomFilter适用于高基数列。比如UserID。因为如果创建在低基数的列上，比如”性别“列，则每个Block几乎都会包含所有取值，导致BloomFilter索引失去意义\n\n## **Doris BloomFilter使用注意事项**\n\n1. 不支持对Tinyint、Float、Double 类型的列建Bloom Filter索引。\n2. Bloom Filter索引只对in和 = 过滤查询有加速效果。\n3. 如果要查看某个查询是否命中了Bloom Filter索引，可以通过查询的Profile信息查看\n\n\n\n# Bitmap 索引\n\n","slug":"bigdata/Doris中的索引","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksx001e8j5m5q87b2ku","content":"<blockquote>\n<p><strong>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。</strong>Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。</p>\n<p>Doris 支持比较丰富的索引结构，来减少数据的扫描：</p>\n<ul>\n<li>Sorted Compound Key Index，可以最多指定三个列组成复合排序键，通过该索引，能够有效进行数据裁剪，从而能够更好支持高并发的报表场景</li>\n<li>Z-order Index ：使用 Z-order 索引，可以高效对数据模型中的任意字段组合进行范围查询</li>\n<li>Min&#x2F;Max ：有效过滤数值类型的等值和范围查询</li>\n<li>Bloom Filter ：对高基数列的等值过滤裁剪非常有效</li>\n<li>Invert Index ：能够对任意字段实现快速检索</li>\n</ul>\n</blockquote>\n<h1 id=\"前缀索引\"><a href=\"#前缀索引\" class=\"headerlink\" title=\"前缀索引\"></a>前缀索引</h1><h2 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h2><p>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。</p>\n<p>本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。</p>\n<p>在 Aggregate、Unique 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表语句中，AGGREGATE KEY、UNIQUE KEY 和 DUPLICATE KEY 中指定的列进行排序存储的。</p>\n<p>而前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方式。</p>\n<h2 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h2><p>我们将一行数据的前 <strong>36 个字节</strong> 作为这行数据的前缀索引。当遇到 VARCHAR 类型时，前缀索引会直接截断。我们举例说明：</p>\n<ol>\n<li><p>以下表结构的前缀索引为 user_id(8 Bytes) + age(4 Bytes) + message(prefix 20 Bytes)。</p>\n<table>\n<thead>\n<tr>\n<th>ColumnName</th>\n<th>Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>user_id</td>\n<td>BIGINT</td>\n</tr>\n<tr>\n<td>age</td>\n<td>INT</td>\n</tr>\n<tr>\n<td>message</td>\n<td>VARCHAR(100)</td>\n</tr>\n<tr>\n<td>max_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n<tr>\n<td>min_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>以下表结构的前缀索引为 user_name(20 Bytes)。即使没有达到 36 个字节，因为遇到 VARCHAR，所以直接截断，不再往后继续。</p>\n<table>\n<thead>\n<tr>\n<th>ColumnName</th>\n<th>Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>user_name</td>\n<td>VARCHAR(20)</td>\n</tr>\n<tr>\n<td>age</td>\n<td>INT</td>\n</tr>\n<tr>\n<td>message</td>\n<td>VARCHAR(100)</td>\n</tr>\n<tr>\n<td>max_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n<tr>\n<td>min_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n</tbody></table>\n</li>\n</ol>\n<p>当我们的查询条件，是<strong>前缀索引的前缀</strong>时，可以极大的加快查询速度。比如在第一个例子中，我们执行如下查询：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> <span class=\"keyword\">table</span> <span class=\"keyword\">WHERE</span> user_id<span class=\"operator\">=</span><span class=\"number\">1829239</span> <span class=\"keyword\">and</span> age<span class=\"operator\">=</span><span class=\"number\">20</span>；</span><br></pre></td></tr></table></figure>\n\n<p>该查询的效率会<strong>远高于</strong>如下查询：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> <span class=\"keyword\">table</span> <span class=\"keyword\">WHERE</span> age<span class=\"operator\">=</span><span class=\"number\">20</span>；</span><br></pre></td></tr></table></figure>\n\n<p>所以在建表时，<strong>正确的选择列顺序，能够极大地提高查询效率</strong>。</p>\n<h2 id=\"通过ROLLUP来调整前缀索引\"><a href=\"#通过ROLLUP来调整前缀索引\" class=\"headerlink\" title=\"通过ROLLUP来调整前缀索引\"></a>通过ROLLUP来调整前缀索引</h2><p>因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过创建 ROLLUP 来人为的调整列顺序。详情可参考<a href=\"https://doris.apache.org/zh-CN/docs/data-table/hit-the-rollup\">ROLLUP</a>。</p>\n<h1 id=\"BloomFilter索引\"><a href=\"#BloomFilter索引\" class=\"headerlink\" title=\"BloomFilter索引\"></a>BloomFilter索引</h1><h2 id=\"Doris-BloomFilter索引及使用使用场景\"><a href=\"#Doris-BloomFilter索引及使用使用场景\" class=\"headerlink\" title=\"Doris BloomFilter索引及使用使用场景\"></a>Doris BloomFilter索引及使用使用场景</h2><p>举个例子：如果要查找一个占用100字节存储空间大小的短行，一个64KB的HFile数据块应该包含(64 * 1024)&#x2F;100 &#x3D; 655.53 &#x3D; ~700行，如果仅能在整个数据块的起始行键上建立索引，那么它是无法给你提供细粒度的索引信息的。因为要查找的行数据可能会落在该数据块的行区间上，也可能行数据没在该数据块上，也可能是表中根本就不存在该行数据，也或者是行数据在另一个HFile里，甚至在MemStore里。以上这几种情况，都会导致从磁盘读取数据块时带来额外的IO开销，也会滥用数据块的缓存，当面对一个巨大的数据集且处于高并发读时，会严重影响性能。</p>\n<p>因此，HBase提供了布隆过滤器，它允许你对存储在每个数据块的数据做一个反向测试。当某行被请求时，通过布隆过滤器先检查该行是否不在这个数据块，布隆过滤器要么确定回答该行不在，要么回答它不知道。这就是为什么我们称它是反向测试。布隆过滤器同样也可以应用到行里的单元上，当访问某列标识符时可以先使用同样的反向测试。</p>\n<p>但布隆过滤器也不是没有代价。存储这个额外的索引层次会占用额外的空间。布隆过滤器随着它们的索引对象数据增长而增长，所以行级布隆过滤器比列标识符级布隆过滤器占用空间要少。当空间不是问题时，它们可以帮助你榨干系统的性能潜力。 Doris的BloomFilter索引可以通过建表的时候指定，或者通过表的ALTER操作来完成。Bloom Filter本质上是一种位图结构，用于快速的判断一个给定的值是否在一个集合中。这种判断会产生小概率的误判。即如果返回false，则一定不在这个集合内。而如果范围true，则有可能在这个集合内。</p>\n<p>BloomFilter索引也是以Block为粒度创建的。每个Block中，指定列的值作为一个集合生成一个BloomFilter索引条目，用于在查询是快速过滤不满足条件的数据。</p>\n<p>下面我们通过实例来看看Doris怎么创建BloomFilter索引。</p>\n<h2 id=\"创建BloomFilter索引\"><a href=\"#创建BloomFilter索引\" class=\"headerlink\" title=\"创建BloomFilter索引\"></a>创建BloomFilter索引</h2><p>Doris BloomFilter索引的创建是通过在建表语句的PROPERTIES里加上”bloom_filter_columns”&#x3D;”k1,k2,k3”,这个属性，k1,k2,k3是你要创建的BloomFilter索引的Key列名称，例如下面我们对表里的saler_id,category_id创建了BloomFilter索引。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> sale_detail_bloom  (</span><br><span class=\"line\">    sale_date <span class=\"type\">date</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售时间&quot;,</span><br><span class=\"line\">    customer_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;客户编号&quot;,</span><br><span class=\"line\">    saler_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售员&quot;,</span><br><span class=\"line\">    sku_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;商品编号&quot;,</span><br><span class=\"line\">    category_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;商品分类&quot;,</span><br><span class=\"line\">    sale_count <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售数量&quot;,</span><br><span class=\"line\">    sale_price <span class=\"type\">DECIMAL</span>(<span class=\"number\">12</span>,<span class=\"number\">2</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;单价&quot;,</span><br><span class=\"line\">    sale_amt <span class=\"type\">DECIMAL</span>(<span class=\"number\">20</span>,<span class=\"number\">2</span>)  COMMENT &quot;销售总金额&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">Duplicate  KEY(sale_date, customer_id,saler_id,sku_id,category_id)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">RANGE</span>(sale_date)</span><br><span class=\"line\">(</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> P_202111 <span class=\"keyword\">VALUES</span> [(<span class=\"string\">&#x27;2021-11-01&#x27;</span>), (<span class=\"string\">&#x27;2021-12-01&#x27;</span>))</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(saler_id) BUCKETS <span class=\"number\">10</span></span><br><span class=\"line\">PROPERTIES (</span><br><span class=\"line\">&quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\">&quot;bloom_filter_columns&quot;<span class=\"operator\">=</span>&quot;saler_id,category_id&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.enable&quot; <span class=\"operator\">=</span> &quot;true&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.time_unit&quot; <span class=\"operator\">=</span> &quot;MONTH&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.time_zone&quot; <span class=\"operator\">=</span> &quot;Asia/Shanghai&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.start&quot; <span class=\"operator\">=</span> &quot;-2147483648&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.end&quot; <span class=\"operator\">=</span> &quot;2&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.prefix&quot; <span class=\"operator\">=</span> &quot;P_&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.buckets&quot; <span class=\"operator\">=</span> &quot;3&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"查看BloomFilter索引\"><a href=\"#查看BloomFilter索引\" class=\"headerlink\" title=\"查看BloomFilter索引\"></a>查看BloomFilter索引</h2><p>查看我们在表上建立的BloomFilter索引是使用:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SHOW</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>table_name<span class=\"operator\">&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"删除BloomFilter索引\"><a href=\"#删除BloomFilter索引\" class=\"headerlink\" title=\"删除BloomFilter索引\"></a>删除BloomFilter索引</h2><p>删除索引即为将索引列从bloom_filter_columns属性中移除：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>db.table_name<span class=\"operator\">&gt;</span> <span class=\"keyword\">SET</span> (&quot;bloom_filter_columns&quot; <span class=\"operator\">=</span> &quot;&quot;);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"修改BloomFilter索引\"><a href=\"#修改BloomFilter索引\" class=\"headerlink\" title=\"修改BloomFilter索引\"></a>修改BloomFilter索引</h2><p>修改索引即为修改表的bloom_filter_columns属性：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>db.table_name<span class=\"operator\">&gt;</span> <span class=\"keyword\">SET</span> (&quot;bloom_filter_columns&quot; <span class=\"operator\">=</span> &quot;k1,k3&quot;);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Doris-BloomFilter使用场景\"><a href=\"#Doris-BloomFilter使用场景\" class=\"headerlink\" title=\"Doris BloomFilter使用场景\"></a><strong>Doris BloomFilter使用场景</strong></h2><p>满足以下几个条件时可以考虑对某列建立Bloom Filter 索引：</p>\n<ol>\n<li>首先BloomFilter适用于非前缀过滤.</li>\n<li>查询会根据该列高频过滤，而且查询条件大多是in和 &#x3D; 过滤.</li>\n<li>不同于Bitmap, BloomFilter适用于高基数列。比如UserID。因为如果创建在低基数的列上，比如”性别“列，则每个Block几乎都会包含所有取值，导致BloomFilter索引失去意义</li>\n</ol>\n<h2 id=\"Doris-BloomFilter使用注意事项\"><a href=\"#Doris-BloomFilter使用注意事项\" class=\"headerlink\" title=\"Doris BloomFilter使用注意事项\"></a><strong>Doris BloomFilter使用注意事项</strong></h2><ol>\n<li>不支持对Tinyint、Float、Double 类型的列建Bloom Filter索引。</li>\n<li>Bloom Filter索引只对in和 &#x3D; 过滤查询有加速效果。</li>\n<li>如果要查看某个查询是否命中了Bloom Filter索引，可以通过查询的Profile信息查看</li>\n</ol>\n<h1 id=\"Bitmap-索引\"><a href=\"#Bitmap-索引\" class=\"headerlink\" title=\"Bitmap 索引\"></a>Bitmap 索引</h1>","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p><strong>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。</strong>Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。</p>\n<p>Doris 支持比较丰富的索引结构，来减少数据的扫描：</p>\n<ul>\n<li>Sorted Compound Key Index，可以最多指定三个列组成复合排序键，通过该索引，能够有效进行数据裁剪，从而能够更好支持高并发的报表场景</li>\n<li>Z-order Index ：使用 Z-order 索引，可以高效对数据模型中的任意字段组合进行范围查询</li>\n<li>Min&#x2F;Max ：有效过滤数值类型的等值和范围查询</li>\n<li>Bloom Filter ：对高基数列的等值过滤裁剪非常有效</li>\n<li>Invert Index ：能够对任意字段实现快速检索</li>\n</ul>\n</blockquote>\n<h1 id=\"前缀索引\"><a href=\"#前缀索引\" class=\"headerlink\" title=\"前缀索引\"></a>前缀索引</h1><h2 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h2><p>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。</p>\n<p>本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。</p>\n<p>在 Aggregate、Unique 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表语句中，AGGREGATE KEY、UNIQUE KEY 和 DUPLICATE KEY 中指定的列进行排序存储的。</p>\n<p>而前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方式。</p>\n<h2 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h2><p>我们将一行数据的前 <strong>36 个字节</strong> 作为这行数据的前缀索引。当遇到 VARCHAR 类型时，前缀索引会直接截断。我们举例说明：</p>\n<ol>\n<li><p>以下表结构的前缀索引为 user_id(8 Bytes) + age(4 Bytes) + message(prefix 20 Bytes)。</p>\n<table>\n<thead>\n<tr>\n<th>ColumnName</th>\n<th>Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>user_id</td>\n<td>BIGINT</td>\n</tr>\n<tr>\n<td>age</td>\n<td>INT</td>\n</tr>\n<tr>\n<td>message</td>\n<td>VARCHAR(100)</td>\n</tr>\n<tr>\n<td>max_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n<tr>\n<td>min_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>以下表结构的前缀索引为 user_name(20 Bytes)。即使没有达到 36 个字节，因为遇到 VARCHAR，所以直接截断，不再往后继续。</p>\n<table>\n<thead>\n<tr>\n<th>ColumnName</th>\n<th>Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>user_name</td>\n<td>VARCHAR(20)</td>\n</tr>\n<tr>\n<td>age</td>\n<td>INT</td>\n</tr>\n<tr>\n<td>message</td>\n<td>VARCHAR(100)</td>\n</tr>\n<tr>\n<td>max_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n<tr>\n<td>min_dwell_time</td>\n<td>DATETIME</td>\n</tr>\n</tbody></table>\n</li>\n</ol>\n<p>当我们的查询条件，是<strong>前缀索引的前缀</strong>时，可以极大的加快查询速度。比如在第一个例子中，我们执行如下查询：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> <span class=\"keyword\">table</span> <span class=\"keyword\">WHERE</span> user_id<span class=\"operator\">=</span><span class=\"number\">1829239</span> <span class=\"keyword\">and</span> age<span class=\"operator\">=</span><span class=\"number\">20</span>；</span><br></pre></td></tr></table></figure>\n\n<p>该查询的效率会<strong>远高于</strong>如下查询：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> <span class=\"keyword\">table</span> <span class=\"keyword\">WHERE</span> age<span class=\"operator\">=</span><span class=\"number\">20</span>；</span><br></pre></td></tr></table></figure>\n\n<p>所以在建表时，<strong>正确的选择列顺序，能够极大地提高查询效率</strong>。</p>\n<h2 id=\"通过ROLLUP来调整前缀索引\"><a href=\"#通过ROLLUP来调整前缀索引\" class=\"headerlink\" title=\"通过ROLLUP来调整前缀索引\"></a>通过ROLLUP来调整前缀索引</h2><p>因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过创建 ROLLUP 来人为的调整列顺序。详情可参考<a href=\"https://doris.apache.org/zh-CN/docs/data-table/hit-the-rollup\">ROLLUP</a>。</p>\n<h1 id=\"BloomFilter索引\"><a href=\"#BloomFilter索引\" class=\"headerlink\" title=\"BloomFilter索引\"></a>BloomFilter索引</h1><h2 id=\"Doris-BloomFilter索引及使用使用场景\"><a href=\"#Doris-BloomFilter索引及使用使用场景\" class=\"headerlink\" title=\"Doris BloomFilter索引及使用使用场景\"></a>Doris BloomFilter索引及使用使用场景</h2><p>举个例子：如果要查找一个占用100字节存储空间大小的短行，一个64KB的HFile数据块应该包含(64 * 1024)&#x2F;100 &#x3D; 655.53 &#x3D; ~700行，如果仅能在整个数据块的起始行键上建立索引，那么它是无法给你提供细粒度的索引信息的。因为要查找的行数据可能会落在该数据块的行区间上，也可能行数据没在该数据块上，也可能是表中根本就不存在该行数据，也或者是行数据在另一个HFile里，甚至在MemStore里。以上这几种情况，都会导致从磁盘读取数据块时带来额外的IO开销，也会滥用数据块的缓存，当面对一个巨大的数据集且处于高并发读时，会严重影响性能。</p>\n<p>因此，HBase提供了布隆过滤器，它允许你对存储在每个数据块的数据做一个反向测试。当某行被请求时，通过布隆过滤器先检查该行是否不在这个数据块，布隆过滤器要么确定回答该行不在，要么回答它不知道。这就是为什么我们称它是反向测试。布隆过滤器同样也可以应用到行里的单元上，当访问某列标识符时可以先使用同样的反向测试。</p>\n<p>但布隆过滤器也不是没有代价。存储这个额外的索引层次会占用额外的空间。布隆过滤器随着它们的索引对象数据增长而增长，所以行级布隆过滤器比列标识符级布隆过滤器占用空间要少。当空间不是问题时，它们可以帮助你榨干系统的性能潜力。 Doris的BloomFilter索引可以通过建表的时候指定，或者通过表的ALTER操作来完成。Bloom Filter本质上是一种位图结构，用于快速的判断一个给定的值是否在一个集合中。这种判断会产生小概率的误判。即如果返回false，则一定不在这个集合内。而如果范围true，则有可能在这个集合内。</p>\n<p>BloomFilter索引也是以Block为粒度创建的。每个Block中，指定列的值作为一个集合生成一个BloomFilter索引条目，用于在查询是快速过滤不满足条件的数据。</p>\n<p>下面我们通过实例来看看Doris怎么创建BloomFilter索引。</p>\n<h2 id=\"创建BloomFilter索引\"><a href=\"#创建BloomFilter索引\" class=\"headerlink\" title=\"创建BloomFilter索引\"></a>创建BloomFilter索引</h2><p>Doris BloomFilter索引的创建是通过在建表语句的PROPERTIES里加上”bloom_filter_columns”&#x3D;”k1,k2,k3”,这个属性，k1,k2,k3是你要创建的BloomFilter索引的Key列名称，例如下面我们对表里的saler_id,category_id创建了BloomFilter索引。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> sale_detail_bloom  (</span><br><span class=\"line\">    sale_date <span class=\"type\">date</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售时间&quot;,</span><br><span class=\"line\">    customer_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;客户编号&quot;,</span><br><span class=\"line\">    saler_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售员&quot;,</span><br><span class=\"line\">    sku_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;商品编号&quot;,</span><br><span class=\"line\">    category_id <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;商品分类&quot;,</span><br><span class=\"line\">    sale_count <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;销售数量&quot;,</span><br><span class=\"line\">    sale_price <span class=\"type\">DECIMAL</span>(<span class=\"number\">12</span>,<span class=\"number\">2</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;单价&quot;,</span><br><span class=\"line\">    sale_amt <span class=\"type\">DECIMAL</span>(<span class=\"number\">20</span>,<span class=\"number\">2</span>)  COMMENT &quot;销售总金额&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">Duplicate  KEY(sale_date, customer_id,saler_id,sku_id,category_id)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">RANGE</span>(sale_date)</span><br><span class=\"line\">(</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> P_202111 <span class=\"keyword\">VALUES</span> [(<span class=\"string\">&#x27;2021-11-01&#x27;</span>), (<span class=\"string\">&#x27;2021-12-01&#x27;</span>))</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(saler_id) BUCKETS <span class=\"number\">10</span></span><br><span class=\"line\">PROPERTIES (</span><br><span class=\"line\">&quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\">&quot;bloom_filter_columns&quot;<span class=\"operator\">=</span>&quot;saler_id,category_id&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.enable&quot; <span class=\"operator\">=</span> &quot;true&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.time_unit&quot; <span class=\"operator\">=</span> &quot;MONTH&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.time_zone&quot; <span class=\"operator\">=</span> &quot;Asia/Shanghai&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.start&quot; <span class=\"operator\">=</span> &quot;-2147483648&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.end&quot; <span class=\"operator\">=</span> &quot;2&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.prefix&quot; <span class=\"operator\">=</span> &quot;P_&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\">&quot;dynamic_partition.buckets&quot; <span class=\"operator\">=</span> &quot;3&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"查看BloomFilter索引\"><a href=\"#查看BloomFilter索引\" class=\"headerlink\" title=\"查看BloomFilter索引\"></a>查看BloomFilter索引</h2><p>查看我们在表上建立的BloomFilter索引是使用:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SHOW</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>table_name<span class=\"operator\">&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"删除BloomFilter索引\"><a href=\"#删除BloomFilter索引\" class=\"headerlink\" title=\"删除BloomFilter索引\"></a>删除BloomFilter索引</h2><p>删除索引即为将索引列从bloom_filter_columns属性中移除：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>db.table_name<span class=\"operator\">&gt;</span> <span class=\"keyword\">SET</span> (&quot;bloom_filter_columns&quot; <span class=\"operator\">=</span> &quot;&quot;);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"修改BloomFilter索引\"><a href=\"#修改BloomFilter索引\" class=\"headerlink\" title=\"修改BloomFilter索引\"></a>修改BloomFilter索引</h2><p>修改索引即为修改表的bloom_filter_columns属性：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> <span class=\"operator\">&lt;</span>db.table_name<span class=\"operator\">&gt;</span> <span class=\"keyword\">SET</span> (&quot;bloom_filter_columns&quot; <span class=\"operator\">=</span> &quot;k1,k3&quot;);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Doris-BloomFilter使用场景\"><a href=\"#Doris-BloomFilter使用场景\" class=\"headerlink\" title=\"Doris BloomFilter使用场景\"></a><strong>Doris BloomFilter使用场景</strong></h2><p>满足以下几个条件时可以考虑对某列建立Bloom Filter 索引：</p>\n<ol>\n<li>首先BloomFilter适用于非前缀过滤.</li>\n<li>查询会根据该列高频过滤，而且查询条件大多是in和 &#x3D; 过滤.</li>\n<li>不同于Bitmap, BloomFilter适用于高基数列。比如UserID。因为如果创建在低基数的列上，比如”性别“列，则每个Block几乎都会包含所有取值，导致BloomFilter索引失去意义</li>\n</ol>\n<h2 id=\"Doris-BloomFilter使用注意事项\"><a href=\"#Doris-BloomFilter使用注意事项\" class=\"headerlink\" title=\"Doris BloomFilter使用注意事项\"></a><strong>Doris BloomFilter使用注意事项</strong></h2><ol>\n<li>不支持对Tinyint、Float、Double 类型的列建Bloom Filter索引。</li>\n<li>Bloom Filter索引只对in和 &#x3D; 过滤查询有加速效果。</li>\n<li>如果要查看某个查询是否命中了Bloom Filter索引，可以通过查询的Profile信息查看</li>\n</ol>\n<h1 id=\"Bitmap-索引\"><a href=\"#Bitmap-索引\" class=\"headerlink\" title=\"Bitmap 索引\"></a>Bitmap 索引</h1>"},{"title":"HBase如何实现MVCC？","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":6299,"date":"2022-07-05T07:13:53.000Z","updated":"2022-07-05T07:13:53.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## HBase的事务一致性保证\n\n**HBase 是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001617770-e017f8b5-f8c4-4b1e-9721-a934e51df162.png)\n\n> - 每个值只出现在一个 Region\n> - 同一时间一个 Region 只分配给一个 RS\n> - 行内的 mutation 操作都是原子的\n\n**HBase 降低可用性提高了一致性。**\n\n当某台 RS fail 的时候，它管理的 Region failover 到其他 RS 时，需要根据 WAL（Write-Ahead Logging）来 redo (redolog，有一种日志文件叫做重做日志文件)，\n这时候进行 redo 的 Region 应该是不可用的，所以 HBase 降低了可用性，提高了一致性。\n\n设想一下，如果 redo 的 Region 能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为 redo 可能还没完成)，那么 HBase 就降低一致性来提高可用性了。\n\n## HBase MVCC实现流程\n\n数据库为了保证一致性，在执行读写操作时往往会对数据做一些锁操作，比如两个client同时修改一条数据，我们无法确定最终的数据到底是哪一个client执行的结果，所以需要通过加锁来保证数据的一致性。\n\n但是锁操作的代价是比较大的，往往需要对加锁操作进行优化，主流的数据库Mysql，PG等都采用MVCC（多版本并发控制）来尽量避免使用不必要的锁以提高性能。本文主要介绍HBase的MVCC实现机制。\n\n在讲解HBase的MVCC之前，我们先了解一下现有的隔离级别，sql标准定义了4种隔离级别：\n\n> 1.read uncommitted    读未提交\n>\n> 2.read committed        读已提交\n>\n> 3.repeatable read        可重复读\n>\n> 4.serializable               可串行化\n\n**HBase不支持跨行事务，目前只支持单行级别的read uncommitted和read committed隔离级别。下面主要讲解HBase的read committed实现机制。**\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352637-015609d0-a12b-4a30-b262-8869b85c9b85.png)\n\nHBase采用LSM树结构，当client发送数据给regionserver端时，regionserver会将数据写入对应的region中，region是由一个memstore和多个storeFile组成，我们可以将memstore看做是一个skipList（跳表），所有写入的数据首先存放在memstore中，当memstore增大到指定的大小后，memstore中的数据flush到磁盘生成一个新的storeFile。\n\n### HBase的写入主要分两步：\n\n> **1.数据首先写入memstore**\n>\n> **2.数据写入WAL**\n>\n> 写入WAL的目的是为了持久化，防止memstore中的数据还未落盘时宕机造成的数据丢失，只有数据写入WAL成功之后才会认为该数据写入成功。\n>\n\n**下面我们考虑一个问题：**\n\n根据前面的讨论可知，假如数据已经写入memstore，但还没有写入WAL，此时认为该条数据还没有写成功，如果按照read committed隔离界别的定义，用户在进行查询操作时（尤其是查询memstore时），是不应该看到这条数据的，那HBase是如何区分正在写入和写入成功的数据呢？\n\n我们可以简单理解HBase在每次put操作时，都会为该操作分配一个id，可以类比mysql里面的事务id，是本次put的唯一标识，该id是region级别递增的，并且每个region还有一个MVCC控制中心，它还同时维护了两个pos：一个readpoint，一个writepoint。readpoint指向目前已经插入完成的id，当put操作完成时会更新readpoint；而writepoint指向目前正在插入的最大id，可以认为writepoint永远和最新申请的put的事务id是一样的。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352762-41efd7fd-cfbd-4077-b218-c451a0d80e5c.png)\n\n**下面我们画图解释：**\n\n1.client插入数据时（这里的client我们可以理解为是regionserver），首先会向MVCC控制中心（MultiVersionConsistencyControl类）申请最新的事务id，其实就是返回write point++，每一个region各自拥有一个独立MVCC控制中心。\n\n2.假设初始状态read和write point都指向2，表明目前没有正在进行的put操作，新的put请求过来时，该region的MVCC控制中心向它自己维护的队列中插入一个新的entry，表示发起了一个新的put事务，并且第一步中将write point++。\n\n3.向client返回本次事务的id为3.\n\n4.client向memstore中插入数据，并且该数据附带本次事务的id号：3\n\n5.将本次的put操作写入WAL，写入成功后代表数据写入成功\n\n6.此时移动read point至3，表示任何MVCC值小于等于3的数据此时都可以被新创建的scan查询检索到。\n\nscan执行查询操作时，首先会向MVCC控制中心拿到目前的read point，然后对memstore和storeFiles进行查询，并过滤掉MVCC值大于本次scan MVCC的数据，保证了scan不会检索到还未提交成功的数据。这也说明HBase默认即为read committed级别，只不过是单行事务。\n\n\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352817-f8176f99-9cd4-477c-ac8e-153fdc023be7.png)\n\n真正业务场景下是会有很多个client同时写入的，此时不管向MVCC申请事务id还是更新read point都会涉及到多用户竞争的情况。如图client A B C分别写入了数据de/fg/hi，有可能A C已经写入成功了，而B还未执行完，下面我们看一下MVCC控制中心是如何协调并发请求的。\n\n先介绍一下MVCC控制中心–**MultiVersionConsistencyControl**类.\n\n**它包含了三个重要的成员：**\n\n1.memstoreRead：即我们提到的read point，记录可以已执行完毕的事务id\n\n2.memstoreWrite：即我们提到的write point，记录当前正在执行的最大事务id\n\n3.writeQueue：一个LinkedList，每一个元素是一个WriteEntry对象。\n\n**WriteEntry类包含两个属性：**\n\n1.writeNumber：事务id\n\n2.completed： True/False，数据写入成功后，写入线程会将其设置为True\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352718-eea3b630-fac5-4e86-8c07-7629c40cb12e.png)\n\n**下面详细解释MVCC控制中心针对多用户请求是如何做到同步的：**\n\n1.当一个client写入数据时，首先lock住MVCC控制中心的写入队列LinkedList，并向其插入一个新的entry，并将之前的write point+1赋予entry的num（write point+1也是同步操作），表示发起了一个新的写入事务。Flag值此时为False，表名目前事务还未完成，数据还在写入过程中。\n\n2.第二步client将数据写入memstore和WAL，此时认为数据已经持久化，可以结束该事务。\n\n3.client调用MVCC控制中心的completeMemstoreInsert(num)方法，该方法采用synchronized关键字，可以理解就是同步方法，将该num对应的entry的Flag设置为True，表示该entry对应的事务完成。但是单单将Flag设置为True是不够的，我们的最终目的是要让scan能够看到最新写入完成的数据，也就是说还需要更新read point。\n\n4.更新read point：同样在completeMemstoreInsert方法中完成，每一个client将其对应的entry的Flag设置为True后，都会去按照队列顺序，从read point开始遍历，假如遍历到的entry的Flag为True，则将read point更新至此位置，直到遇到Flag为False的位置时停止。也就是说每个client写入之后，都会尽力去将read point更新到目前最大连续的已经完成的事务的点（因为是有可能后开始的事务先于之前的事务完成）。\n\n看到这里，可能大家会想了，那假如事务A先于事务C，事务A还未完成，但事务C已经完成，事务C也只能将read point更新到事务A之前的位置，如果此时事务C返回写入成功，那按道理来说scan是应该能够查到事务C的数据，但是由于read point没有更新到C，就会造成一个现象就是：事务C明明提示执行成功，但是查询的时候却看不到。\n\n所以上面说的第4步其实还并没有完，client在执行completeMemstoreInsert后，还会执行一个waitForRead(entry)方法，参数的entry就是该事务对应的entry，该方法会一直等待read point大于等于该entry的num时才会返回，这样保证了事务有序完成。\n\n以上就是HBase写入时MVCC的工作流程，scan就比较好理解了，每一个scan请求都会申请一个readpoint，保证了该read point之后的事务不会被检索到。\n\n\n\n**说明**：HBase也同样支持read uncommitted级别，也就是我们在查询的时候将scan的mvcc值设置为一个超大的值，大于目前所有申请的MVCC值，那么查询时同样会返回正在写入的数据。\n\n","source":"_posts/bigdata/HBase如何实现MVCC.md","raw":"---\ntitle: HBase如何实现MVCC？\ntags:\n  - HBase\n  - MVCC\ncategories:\n  - - bigdata\n    - HBase\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 6299\ndate: 2022-07-05 15:13:53\nupdated: 2022-07-05 15:13:53\ncover:\ndescription:\nkeywords:\n---\n\n## HBase的事务一致性保证\n\n**HBase 是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001617770-e017f8b5-f8c4-4b1e-9721-a934e51df162.png)\n\n> - 每个值只出现在一个 Region\n> - 同一时间一个 Region 只分配给一个 RS\n> - 行内的 mutation 操作都是原子的\n\n**HBase 降低可用性提高了一致性。**\n\n当某台 RS fail 的时候，它管理的 Region failover 到其他 RS 时，需要根据 WAL（Write-Ahead Logging）来 redo (redolog，有一种日志文件叫做重做日志文件)，\n这时候进行 redo 的 Region 应该是不可用的，所以 HBase 降低了可用性，提高了一致性。\n\n设想一下，如果 redo 的 Region 能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为 redo 可能还没完成)，那么 HBase 就降低一致性来提高可用性了。\n\n## HBase MVCC实现流程\n\n数据库为了保证一致性，在执行读写操作时往往会对数据做一些锁操作，比如两个client同时修改一条数据，我们无法确定最终的数据到底是哪一个client执行的结果，所以需要通过加锁来保证数据的一致性。\n\n但是锁操作的代价是比较大的，往往需要对加锁操作进行优化，主流的数据库Mysql，PG等都采用MVCC（多版本并发控制）来尽量避免使用不必要的锁以提高性能。本文主要介绍HBase的MVCC实现机制。\n\n在讲解HBase的MVCC之前，我们先了解一下现有的隔离级别，sql标准定义了4种隔离级别：\n\n> 1.read uncommitted    读未提交\n>\n> 2.read committed        读已提交\n>\n> 3.repeatable read        可重复读\n>\n> 4.serializable               可串行化\n\n**HBase不支持跨行事务，目前只支持单行级别的read uncommitted和read committed隔离级别。下面主要讲解HBase的read committed实现机制。**\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352637-015609d0-a12b-4a30-b262-8869b85c9b85.png)\n\nHBase采用LSM树结构，当client发送数据给regionserver端时，regionserver会将数据写入对应的region中，region是由一个memstore和多个storeFile组成，我们可以将memstore看做是一个skipList（跳表），所有写入的数据首先存放在memstore中，当memstore增大到指定的大小后，memstore中的数据flush到磁盘生成一个新的storeFile。\n\n### HBase的写入主要分两步：\n\n> **1.数据首先写入memstore**\n>\n> **2.数据写入WAL**\n>\n> 写入WAL的目的是为了持久化，防止memstore中的数据还未落盘时宕机造成的数据丢失，只有数据写入WAL成功之后才会认为该数据写入成功。\n>\n\n**下面我们考虑一个问题：**\n\n根据前面的讨论可知，假如数据已经写入memstore，但还没有写入WAL，此时认为该条数据还没有写成功，如果按照read committed隔离界别的定义，用户在进行查询操作时（尤其是查询memstore时），是不应该看到这条数据的，那HBase是如何区分正在写入和写入成功的数据呢？\n\n我们可以简单理解HBase在每次put操作时，都会为该操作分配一个id，可以类比mysql里面的事务id，是本次put的唯一标识，该id是region级别递增的，并且每个region还有一个MVCC控制中心，它还同时维护了两个pos：一个readpoint，一个writepoint。readpoint指向目前已经插入完成的id，当put操作完成时会更新readpoint；而writepoint指向目前正在插入的最大id，可以认为writepoint永远和最新申请的put的事务id是一样的。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352762-41efd7fd-cfbd-4077-b218-c451a0d80e5c.png)\n\n**下面我们画图解释：**\n\n1.client插入数据时（这里的client我们可以理解为是regionserver），首先会向MVCC控制中心（MultiVersionConsistencyControl类）申请最新的事务id，其实就是返回write point++，每一个region各自拥有一个独立MVCC控制中心。\n\n2.假设初始状态read和write point都指向2，表明目前没有正在进行的put操作，新的put请求过来时，该region的MVCC控制中心向它自己维护的队列中插入一个新的entry，表示发起了一个新的put事务，并且第一步中将write point++。\n\n3.向client返回本次事务的id为3.\n\n4.client向memstore中插入数据，并且该数据附带本次事务的id号：3\n\n5.将本次的put操作写入WAL，写入成功后代表数据写入成功\n\n6.此时移动read point至3，表示任何MVCC值小于等于3的数据此时都可以被新创建的scan查询检索到。\n\nscan执行查询操作时，首先会向MVCC控制中心拿到目前的read point，然后对memstore和storeFiles进行查询，并过滤掉MVCC值大于本次scan MVCC的数据，保证了scan不会检索到还未提交成功的数据。这也说明HBase默认即为read committed级别，只不过是单行事务。\n\n\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352817-f8176f99-9cd4-477c-ac8e-153fdc023be7.png)\n\n真正业务场景下是会有很多个client同时写入的，此时不管向MVCC申请事务id还是更新read point都会涉及到多用户竞争的情况。如图client A B C分别写入了数据de/fg/hi，有可能A C已经写入成功了，而B还未执行完，下面我们看一下MVCC控制中心是如何协调并发请求的。\n\n先介绍一下MVCC控制中心–**MultiVersionConsistencyControl**类.\n\n**它包含了三个重要的成员：**\n\n1.memstoreRead：即我们提到的read point，记录可以已执行完毕的事务id\n\n2.memstoreWrite：即我们提到的write point，记录当前正在执行的最大事务id\n\n3.writeQueue：一个LinkedList，每一个元素是一个WriteEntry对象。\n\n**WriteEntry类包含两个属性：**\n\n1.writeNumber：事务id\n\n2.completed： True/False，数据写入成功后，写入线程会将其设置为True\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352718-eea3b630-fac5-4e86-8c07-7629c40cb12e.png)\n\n**下面详细解释MVCC控制中心针对多用户请求是如何做到同步的：**\n\n1.当一个client写入数据时，首先lock住MVCC控制中心的写入队列LinkedList，并向其插入一个新的entry，并将之前的write point+1赋予entry的num（write point+1也是同步操作），表示发起了一个新的写入事务。Flag值此时为False，表名目前事务还未完成，数据还在写入过程中。\n\n2.第二步client将数据写入memstore和WAL，此时认为数据已经持久化，可以结束该事务。\n\n3.client调用MVCC控制中心的completeMemstoreInsert(num)方法，该方法采用synchronized关键字，可以理解就是同步方法，将该num对应的entry的Flag设置为True，表示该entry对应的事务完成。但是单单将Flag设置为True是不够的，我们的最终目的是要让scan能够看到最新写入完成的数据，也就是说还需要更新read point。\n\n4.更新read point：同样在completeMemstoreInsert方法中完成，每一个client将其对应的entry的Flag设置为True后，都会去按照队列顺序，从read point开始遍历，假如遍历到的entry的Flag为True，则将read point更新至此位置，直到遇到Flag为False的位置时停止。也就是说每个client写入之后，都会尽力去将read point更新到目前最大连续的已经完成的事务的点（因为是有可能后开始的事务先于之前的事务完成）。\n\n看到这里，可能大家会想了，那假如事务A先于事务C，事务A还未完成，但事务C已经完成，事务C也只能将read point更新到事务A之前的位置，如果此时事务C返回写入成功，那按道理来说scan是应该能够查到事务C的数据，但是由于read point没有更新到C，就会造成一个现象就是：事务C明明提示执行成功，但是查询的时候却看不到。\n\n所以上面说的第4步其实还并没有完，client在执行completeMemstoreInsert后，还会执行一个waitForRead(entry)方法，参数的entry就是该事务对应的entry，该方法会一直等待read point大于等于该entry的num时才会返回，这样保证了事务有序完成。\n\n以上就是HBase写入时MVCC的工作流程，scan就比较好理解了，每一个scan请求都会申请一个readpoint，保证了该read point之后的事务不会被检索到。\n\n\n\n**说明**：HBase也同样支持read uncommitted级别，也就是我们在查询的时候将scan的mvcc值设置为一个超大的值，大于目前所有申请的MVCC值，那么查询时同样会返回正在写入的数据。\n\n","slug":"bigdata/HBase如何实现MVCC","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksx001i8j5m1a9c4y0i","content":"<h2 id=\"HBase的事务一致性保证\"><a href=\"#HBase的事务一致性保证\" class=\"headerlink\" title=\"HBase的事务一致性保证\"></a>HBase的事务一致性保证</h2><p><strong>HBase 是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001617770-e017f8b5-f8c4-4b1e-9721-a934e51df162.png\" alt=\"img\"></p>\n<blockquote>\n<ul>\n<li>每个值只出现在一个 Region</li>\n<li>同一时间一个 Region 只分配给一个 RS</li>\n<li>行内的 mutation 操作都是原子的</li>\n</ul>\n</blockquote>\n<p><strong>HBase 降低可用性提高了一致性。</strong></p>\n<p>当某台 RS fail 的时候，它管理的 Region failover 到其他 RS 时，需要根据 WAL（Write-Ahead Logging）来 redo (redolog，有一种日志文件叫做重做日志文件)，<br>这时候进行 redo 的 Region 应该是不可用的，所以 HBase 降低了可用性，提高了一致性。</p>\n<p>设想一下，如果 redo 的 Region 能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为 redo 可能还没完成)，那么 HBase 就降低一致性来提高可用性了。</p>\n<h2 id=\"HBase-MVCC实现流程\"><a href=\"#HBase-MVCC实现流程\" class=\"headerlink\" title=\"HBase MVCC实现流程\"></a>HBase MVCC实现流程</h2><p>数据库为了保证一致性，在执行读写操作时往往会对数据做一些锁操作，比如两个client同时修改一条数据，我们无法确定最终的数据到底是哪一个client执行的结果，所以需要通过加锁来保证数据的一致性。</p>\n<p>但是锁操作的代价是比较大的，往往需要对加锁操作进行优化，主流的数据库Mysql，PG等都采用MVCC（多版本并发控制）来尽量避免使用不必要的锁以提高性能。本文主要介绍HBase的MVCC实现机制。</p>\n<p>在讲解HBase的MVCC之前，我们先了解一下现有的隔离级别，sql标准定义了4种隔离级别：</p>\n<blockquote>\n<p>1.read uncommitted    读未提交</p>\n<p>2.read committed        读已提交</p>\n<p>3.repeatable read        可重复读</p>\n<p>4.serializable               可串行化</p>\n</blockquote>\n<p><strong>HBase不支持跨行事务，目前只支持单行级别的read uncommitted和read committed隔离级别。下面主要讲解HBase的read committed实现机制。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352637-015609d0-a12b-4a30-b262-8869b85c9b85.png\" alt=\"img\"></p>\n<p>HBase采用LSM树结构，当client发送数据给regionserver端时，regionserver会将数据写入对应的region中，region是由一个memstore和多个storeFile组成，我们可以将memstore看做是一个skipList（跳表），所有写入的数据首先存放在memstore中，当memstore增大到指定的大小后，memstore中的数据flush到磁盘生成一个新的storeFile。</p>\n<h3 id=\"HBase的写入主要分两步：\"><a href=\"#HBase的写入主要分两步：\" class=\"headerlink\" title=\"HBase的写入主要分两步：\"></a>HBase的写入主要分两步：</h3><blockquote>\n<p><strong>1.数据首先写入memstore</strong></p>\n<p><strong>2.数据写入WAL</strong></p>\n<p>写入WAL的目的是为了持久化，防止memstore中的数据还未落盘时宕机造成的数据丢失，只有数据写入WAL成功之后才会认为该数据写入成功。</p>\n</blockquote>\n<p><strong>下面我们考虑一个问题：</strong></p>\n<p>根据前面的讨论可知，假如数据已经写入memstore，但还没有写入WAL，此时认为该条数据还没有写成功，如果按照read committed隔离界别的定义，用户在进行查询操作时（尤其是查询memstore时），是不应该看到这条数据的，那HBase是如何区分正在写入和写入成功的数据呢？</p>\n<p>我们可以简单理解HBase在每次put操作时，都会为该操作分配一个id，可以类比mysql里面的事务id，是本次put的唯一标识，该id是region级别递增的，并且每个region还有一个MVCC控制中心，它还同时维护了两个pos：一个readpoint，一个writepoint。readpoint指向目前已经插入完成的id，当put操作完成时会更新readpoint；而writepoint指向目前正在插入的最大id，可以认为writepoint永远和最新申请的put的事务id是一样的。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352762-41efd7fd-cfbd-4077-b218-c451a0d80e5c.png\" alt=\"img\"></p>\n<p><strong>下面我们画图解释：</strong></p>\n<p>1.client插入数据时（这里的client我们可以理解为是regionserver），首先会向MVCC控制中心（MultiVersionConsistencyControl类）申请最新的事务id，其实就是返回write point++，每一个region各自拥有一个独立MVCC控制中心。</p>\n<p>2.假设初始状态read和write point都指向2，表明目前没有正在进行的put操作，新的put请求过来时，该region的MVCC控制中心向它自己维护的队列中插入一个新的entry，表示发起了一个新的put事务，并且第一步中将write point++。</p>\n<p>3.向client返回本次事务的id为3.</p>\n<p>4.client向memstore中插入数据，并且该数据附带本次事务的id号：3</p>\n<p>5.将本次的put操作写入WAL，写入成功后代表数据写入成功</p>\n<p>6.此时移动read point至3，表示任何MVCC值小于等于3的数据此时都可以被新创建的scan查询检索到。</p>\n<p>scan执行查询操作时，首先会向MVCC控制中心拿到目前的read point，然后对memstore和storeFiles进行查询，并过滤掉MVCC值大于本次scan MVCC的数据，保证了scan不会检索到还未提交成功的数据。这也说明HBase默认即为read committed级别，只不过是单行事务。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352817-f8176f99-9cd4-477c-ac8e-153fdc023be7.png\" alt=\"img\"></p>\n<p>真正业务场景下是会有很多个client同时写入的，此时不管向MVCC申请事务id还是更新read point都会涉及到多用户竞争的情况。如图client A B C分别写入了数据de&#x2F;fg&#x2F;hi，有可能A C已经写入成功了，而B还未执行完，下面我们看一下MVCC控制中心是如何协调并发请求的。</p>\n<p>先介绍一下MVCC控制中心–<strong>MultiVersionConsistencyControl</strong>类.</p>\n<p><strong>它包含了三个重要的成员：</strong></p>\n<p>1.memstoreRead：即我们提到的read point，记录可以已执行完毕的事务id</p>\n<p>2.memstoreWrite：即我们提到的write point，记录当前正在执行的最大事务id</p>\n<p>3.writeQueue：一个LinkedList，每一个元素是一个WriteEntry对象。</p>\n<p><strong>WriteEntry类包含两个属性：</strong></p>\n<p>1.writeNumber：事务id</p>\n<p>2.completed： True&#x2F;False，数据写入成功后，写入线程会将其设置为True</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352718-eea3b630-fac5-4e86-8c07-7629c40cb12e.png\" alt=\"img\"></p>\n<p><strong>下面详细解释MVCC控制中心针对多用户请求是如何做到同步的：</strong></p>\n<p>1.当一个client写入数据时，首先lock住MVCC控制中心的写入队列LinkedList，并向其插入一个新的entry，并将之前的write point+1赋予entry的num（write point+1也是同步操作），表示发起了一个新的写入事务。Flag值此时为False，表名目前事务还未完成，数据还在写入过程中。</p>\n<p>2.第二步client将数据写入memstore和WAL，此时认为数据已经持久化，可以结束该事务。</p>\n<p>3.client调用MVCC控制中心的completeMemstoreInsert(num)方法，该方法采用synchronized关键字，可以理解就是同步方法，将该num对应的entry的Flag设置为True，表示该entry对应的事务完成。但是单单将Flag设置为True是不够的，我们的最终目的是要让scan能够看到最新写入完成的数据，也就是说还需要更新read point。</p>\n<p>4.更新read point：同样在completeMemstoreInsert方法中完成，每一个client将其对应的entry的Flag设置为True后，都会去按照队列顺序，从read point开始遍历，假如遍历到的entry的Flag为True，则将read point更新至此位置，直到遇到Flag为False的位置时停止。也就是说每个client写入之后，都会尽力去将read point更新到目前最大连续的已经完成的事务的点（因为是有可能后开始的事务先于之前的事务完成）。</p>\n<p>看到这里，可能大家会想了，那假如事务A先于事务C，事务A还未完成，但事务C已经完成，事务C也只能将read point更新到事务A之前的位置，如果此时事务C返回写入成功，那按道理来说scan是应该能够查到事务C的数据，但是由于read point没有更新到C，就会造成一个现象就是：事务C明明提示执行成功，但是查询的时候却看不到。</p>\n<p>所以上面说的第4步其实还并没有完，client在执行completeMemstoreInsert后，还会执行一个waitForRead(entry)方法，参数的entry就是该事务对应的entry，该方法会一直等待read point大于等于该entry的num时才会返回，这样保证了事务有序完成。</p>\n<p>以上就是HBase写入时MVCC的工作流程，scan就比较好理解了，每一个scan请求都会申请一个readpoint，保证了该read point之后的事务不会被检索到。</p>\n<p><strong>说明</strong>：HBase也同样支持read uncommitted级别，也就是我们在查询的时候将scan的mvcc值设置为一个超大的值，大于目前所有申请的MVCC值，那么查询时同样会返回正在写入的数据。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"HBase的事务一致性保证\"><a href=\"#HBase的事务一致性保证\" class=\"headerlink\" title=\"HBase的事务一致性保证\"></a>HBase的事务一致性保证</h2><p><strong>HBase 是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001617770-e017f8b5-f8c4-4b1e-9721-a934e51df162.png\" alt=\"img\"></p>\n<blockquote>\n<ul>\n<li>每个值只出现在一个 Region</li>\n<li>同一时间一个 Region 只分配给一个 RS</li>\n<li>行内的 mutation 操作都是原子的</li>\n</ul>\n</blockquote>\n<p><strong>HBase 降低可用性提高了一致性。</strong></p>\n<p>当某台 RS fail 的时候，它管理的 Region failover 到其他 RS 时，需要根据 WAL（Write-Ahead Logging）来 redo (redolog，有一种日志文件叫做重做日志文件)，<br>这时候进行 redo 的 Region 应该是不可用的，所以 HBase 降低了可用性，提高了一致性。</p>\n<p>设想一下，如果 redo 的 Region 能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为 redo 可能还没完成)，那么 HBase 就降低一致性来提高可用性了。</p>\n<h2 id=\"HBase-MVCC实现流程\"><a href=\"#HBase-MVCC实现流程\" class=\"headerlink\" title=\"HBase MVCC实现流程\"></a>HBase MVCC实现流程</h2><p>数据库为了保证一致性，在执行读写操作时往往会对数据做一些锁操作，比如两个client同时修改一条数据，我们无法确定最终的数据到底是哪一个client执行的结果，所以需要通过加锁来保证数据的一致性。</p>\n<p>但是锁操作的代价是比较大的，往往需要对加锁操作进行优化，主流的数据库Mysql，PG等都采用MVCC（多版本并发控制）来尽量避免使用不必要的锁以提高性能。本文主要介绍HBase的MVCC实现机制。</p>\n<p>在讲解HBase的MVCC之前，我们先了解一下现有的隔离级别，sql标准定义了4种隔离级别：</p>\n<blockquote>\n<p>1.read uncommitted    读未提交</p>\n<p>2.read committed        读已提交</p>\n<p>3.repeatable read        可重复读</p>\n<p>4.serializable               可串行化</p>\n</blockquote>\n<p><strong>HBase不支持跨行事务，目前只支持单行级别的read uncommitted和read committed隔离级别。下面主要讲解HBase的read committed实现机制。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352637-015609d0-a12b-4a30-b262-8869b85c9b85.png\" alt=\"img\"></p>\n<p>HBase采用LSM树结构，当client发送数据给regionserver端时，regionserver会将数据写入对应的region中，region是由一个memstore和多个storeFile组成，我们可以将memstore看做是一个skipList（跳表），所有写入的数据首先存放在memstore中，当memstore增大到指定的大小后，memstore中的数据flush到磁盘生成一个新的storeFile。</p>\n<h3 id=\"HBase的写入主要分两步：\"><a href=\"#HBase的写入主要分两步：\" class=\"headerlink\" title=\"HBase的写入主要分两步：\"></a>HBase的写入主要分两步：</h3><blockquote>\n<p><strong>1.数据首先写入memstore</strong></p>\n<p><strong>2.数据写入WAL</strong></p>\n<p>写入WAL的目的是为了持久化，防止memstore中的数据还未落盘时宕机造成的数据丢失，只有数据写入WAL成功之后才会认为该数据写入成功。</p>\n</blockquote>\n<p><strong>下面我们考虑一个问题：</strong></p>\n<p>根据前面的讨论可知，假如数据已经写入memstore，但还没有写入WAL，此时认为该条数据还没有写成功，如果按照read committed隔离界别的定义，用户在进行查询操作时（尤其是查询memstore时），是不应该看到这条数据的，那HBase是如何区分正在写入和写入成功的数据呢？</p>\n<p>我们可以简单理解HBase在每次put操作时，都会为该操作分配一个id，可以类比mysql里面的事务id，是本次put的唯一标识，该id是region级别递增的，并且每个region还有一个MVCC控制中心，它还同时维护了两个pos：一个readpoint，一个writepoint。readpoint指向目前已经插入完成的id，当put操作完成时会更新readpoint；而writepoint指向目前正在插入的最大id，可以认为writepoint永远和最新申请的put的事务id是一样的。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352762-41efd7fd-cfbd-4077-b218-c451a0d80e5c.png\" alt=\"img\"></p>\n<p><strong>下面我们画图解释：</strong></p>\n<p>1.client插入数据时（这里的client我们可以理解为是regionserver），首先会向MVCC控制中心（MultiVersionConsistencyControl类）申请最新的事务id，其实就是返回write point++，每一个region各自拥有一个独立MVCC控制中心。</p>\n<p>2.假设初始状态read和write point都指向2，表明目前没有正在进行的put操作，新的put请求过来时，该region的MVCC控制中心向它自己维护的队列中插入一个新的entry，表示发起了一个新的put事务，并且第一步中将write point++。</p>\n<p>3.向client返回本次事务的id为3.</p>\n<p>4.client向memstore中插入数据，并且该数据附带本次事务的id号：3</p>\n<p>5.将本次的put操作写入WAL，写入成功后代表数据写入成功</p>\n<p>6.此时移动read point至3，表示任何MVCC值小于等于3的数据此时都可以被新创建的scan查询检索到。</p>\n<p>scan执行查询操作时，首先会向MVCC控制中心拿到目前的read point，然后对memstore和storeFiles进行查询，并过滤掉MVCC值大于本次scan MVCC的数据，保证了scan不会检索到还未提交成功的数据。这也说明HBase默认即为read committed级别，只不过是单行事务。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352817-f8176f99-9cd4-477c-ac8e-153fdc023be7.png\" alt=\"img\"></p>\n<p>真正业务场景下是会有很多个client同时写入的，此时不管向MVCC申请事务id还是更新read point都会涉及到多用户竞争的情况。如图client A B C分别写入了数据de&#x2F;fg&#x2F;hi，有可能A C已经写入成功了，而B还未执行完，下面我们看一下MVCC控制中心是如何协调并发请求的。</p>\n<p>先介绍一下MVCC控制中心–<strong>MultiVersionConsistencyControl</strong>类.</p>\n<p><strong>它包含了三个重要的成员：</strong></p>\n<p>1.memstoreRead：即我们提到的read point，记录可以已执行完毕的事务id</p>\n<p>2.memstoreWrite：即我们提到的write point，记录当前正在执行的最大事务id</p>\n<p>3.writeQueue：一个LinkedList，每一个元素是一个WriteEntry对象。</p>\n<p><strong>WriteEntry类包含两个属性：</strong></p>\n<p>1.writeNumber：事务id</p>\n<p>2.completed： True&#x2F;False，数据写入成功后，写入线程会将其设置为True</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657001352718-eea3b630-fac5-4e86-8c07-7629c40cb12e.png\" alt=\"img\"></p>\n<p><strong>下面详细解释MVCC控制中心针对多用户请求是如何做到同步的：</strong></p>\n<p>1.当一个client写入数据时，首先lock住MVCC控制中心的写入队列LinkedList，并向其插入一个新的entry，并将之前的write point+1赋予entry的num（write point+1也是同步操作），表示发起了一个新的写入事务。Flag值此时为False，表名目前事务还未完成，数据还在写入过程中。</p>\n<p>2.第二步client将数据写入memstore和WAL，此时认为数据已经持久化，可以结束该事务。</p>\n<p>3.client调用MVCC控制中心的completeMemstoreInsert(num)方法，该方法采用synchronized关键字，可以理解就是同步方法，将该num对应的entry的Flag设置为True，表示该entry对应的事务完成。但是单单将Flag设置为True是不够的，我们的最终目的是要让scan能够看到最新写入完成的数据，也就是说还需要更新read point。</p>\n<p>4.更新read point：同样在completeMemstoreInsert方法中完成，每一个client将其对应的entry的Flag设置为True后，都会去按照队列顺序，从read point开始遍历，假如遍历到的entry的Flag为True，则将read point更新至此位置，直到遇到Flag为False的位置时停止。也就是说每个client写入之后，都会尽力去将read point更新到目前最大连续的已经完成的事务的点（因为是有可能后开始的事务先于之前的事务完成）。</p>\n<p>看到这里，可能大家会想了，那假如事务A先于事务C，事务A还未完成，但事务C已经完成，事务C也只能将read point更新到事务A之前的位置，如果此时事务C返回写入成功，那按道理来说scan是应该能够查到事务C的数据，但是由于read point没有更新到C，就会造成一个现象就是：事务C明明提示执行成功，但是查询的时候却看不到。</p>\n<p>所以上面说的第4步其实还并没有完，client在执行completeMemstoreInsert后，还会执行一个waitForRead(entry)方法，参数的entry就是该事务对应的entry，该方法会一直等待read point大于等于该entry的num时才会返回，这样保证了事务有序完成。</p>\n<p>以上就是HBase写入时MVCC的工作流程，scan就比较好理解了，每一个scan请求都会申请一个readpoint，保证了该read point之后的事务不会被检索到。</p>\n<p><strong>说明</strong>：HBase也同样支持read uncommitted级别，也就是我们在查询的时候将scan的mvcc值设置为一个超大的值，大于目前所有申请的MVCC值，那么查询时同样会返回正在写入的数据。</p>\n"},{"title":"When：何时需要进行Doris Compaction调优","top_img":"/img/bg/banner.gif","abbrlink":65527,"date":"2022-09-03T15:40:51.000Z","updated":"2022-09-03T15:40:51.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n> 本篇将从实际使用场景的角度出发，介绍 Compaction 的调优思路和策略。通过本文将了解到 Compaction 相关的日志分析、参数调整和 API 的使用。\n\n## 什么情况下需要调整 Compaction 参数\n\nCompaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。**并且在这个过程中，Compaction 操作不能占用太多的系统资源。所以我们可以以结果为导向，从以下两个方面反推是否需要调整 Compaction 策略。**\n\n1. 检查数据版本是否有堆积。\n\n2. 检查 IO 和内存资源是否被 Compaction 任务过多的占用。\n\n### 查看数据版本数量变化趋势\n\nDoris 提供数据版本数量的监控数据。如果你部署了 Prometheus + Grafana 的监控，则可以通过 Grafana 仪表盘的 BE Base Compaction Score 和 BE Cumu Compaction Score 图表查看到这个监控数据的趋势图：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662264565446-6e38cc7f-eb01-441c-a6f0-4988b07c4835.png)\n\n> 这个图表展示的是每个 BE 节点，所有 Tablet 中数据版本最多的那个 Tablet 的版本数量，可以反映出当前版本堆积情况。\n> 部署方式参阅：http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html\n\n如果没有安装这个监控，如果你是用的 Palo 0.14.7 版本以上，也可以通过以下命令在命令行查看这个监控数据的趋势图：\n\n```shell\n\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_BASE_COMPACTION_SCORE\", \"time\" = \"last 4 hours\");\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_CUMU_COMPACTION_SCORE\", \"time\" = \"last 4 hours\");\n```\n\n注意这里有两个指标，分别表示 Base Compaction 和 Cumulative Compaction 所对应的版本数量。**在大部分情况下，我们只需要查看 Cumulative Compaction 的指标，即可大致了解集群的数据版本堆积情况。**\n\n**版本是否堆积没有一个明确的界限，而是根据使用场景和查询延迟进行判断的一个经验值。**我们可以按照以下步骤进行简单的推断：\n\n> 1. 观察数据版本数量的趋势，如果趋势平稳，则说明 Compaction 和导入速度基本持平。如果呈上升态势，则说明 Compaction 速度跟不上导入速度了。如果呈下降态势，说明 Compaction 速度超过了导入速度。**如果呈上升态势，或在平稳状态但数值较高，则需要考虑调整 Compaction 参数以加快 Compaction 的进度。**\n>\n> 2. **通常版本数量维持在 100 以内可以视为正常。而在大部分批量导入或低频导入场景下，版本数量通常为10-20甚至更低。**\n\n### 查看Compaction资源占用\nCompaction 资源占用主要是 IO 和 内存。\n\n对于 Compaction 占用的内存，可以在浏览器打开以下链接：http://be_host:webserver_port/mem_tracker在搜索框中输入 AutoCompaction：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662265200095-5cf34b92-10de-4d1f-80c3-4dfcc0fd49fd.png)\n\n则可以查看当前Compaction的内存开销和历史峰值开销。\n\n而对于 IO 操作，目前还没有提供单独的 Compaction 操作的 IO 监控，我们只能根据集群整体的 IO 利用率情况来做判断。我们可以查看监控图 Disk IO util：\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/eGOhXuI8cBaCgNjQSRHmVYlEkMEIH7OGlBmYBlAkJLj3MjeJTTiauBiaYFeia8zf5s2fvImSlPwthGKZSO8oeFzNg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n或者通过命令在命令行查看（Palo 0.14.7 以上版本）：\n\n```shell\n\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_DISK_IO\", \"time\" = \"last 4 hours\");\n```\n\n这个监控展示的是每个 BE 节点上磁盘的 IO util 指标。数值越高表示IO越繁忙。**当然大部分情况下 IO 资源都是查询请求消耗的，这个监控主要用于指导我们是否需要增加或减少 Compaction 任务数。**\n\n## Compaction 调优策略\n\n如果版本数量有上升趋势或者数值较高，则可以从以下两方面优化 Compaction：\n\n1. 修改 Compaction 线程数，使得同时能够执行更多的 Compaction 任务。\n\n2. 优化单个 Compaction 的执行逻辑，使数据版本数量维持在一个合理范围。\n\n### 优化前的准备工作\n\n在优化 Compaction 执行逻辑之前，我们需要使用一些命令来进一步查看一些Compaction的细节信息。\n\n首先，我们通过监控图找到一个版本数量最高的 BE 节点。然后执行以下命令分析日志：\n\n```shell\n\n$> grep \"succeed to do base\" log/be.INFO.log.20210505-142010 |tail -n 100\n$> grep \"succeed to do cumu\" log/be.INFO.log.20210505-142010 |tail -n 100\n```\n\n以上两个命令可以查看最近100个执行完成的 compaction 任务：\n\n```shell\n\nI0505 17:06:56.143455   675 compaction.cpp:135] succeed to do cumulative compaction. tablet=106827682.505347040.d040c1cdf71e5c95-3a002a06127ccd86, output_version=2-2631, current_max_version=2633, disk=/home/disk6/palo.HDD, segments=57. elapsed time=2.29371s. cumulative_compaction_policy=SIZE_BASED.\nI0505 17:06:56.520058   666 compaction.cpp:135] succeed to do cumulative compaction. tablet=106822189.1661856168.654562832a620ea6-46fe84c73ea84795, output_version=2-3247, current_max_version=3250, disk=/home/disk2/palo.HDD, segments=22. elapsed time=2.66858s. cumulative_compaction_policy=SIZE_BASED.\n```\n\n通过日志时间可以判断 Compaction 是否在持续正确的执行，通过 elapsed time 可以观察每个任务的执行时间。\n\n我们还可以执行以下命令展示最近100个 compaction 任务的配额（permits）：\n\n```shell\n\n$> grep \"permits\" log/be.INFO |tail -n 100\n\nI0505 17:04:07.120920   667 compaction.cpp:83] start cumulative compaction. tablet=106827970.777011641.9c474de1b8ba9199-4addeb135d6834ac, output_version=2-2623, permits: 39\nI0505 17:04:13.898777   672 compaction.cpp:83] start cumulative compaction. tablet=106822777.1948936074.a44ac9462e79b76d-4a33ee39559bb0bf, output_version=2-3238, permits: 22\n```\n\n配额和版本数量成正比。\n\n我们可以找到 permits 较大的一个任务对应的 tablet id，如上图permit 为 39 的任务的 tablet id 为 106827970，然后继续分析这个 tablet 的 compaction 情况。\n\n通过 MySQL 客户端连接 Doris 集群后，执行：\n\n```shell\n\nmysql> show tablet 106827970;\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n| DbName                   | TableName | PartitionName | IndexName | DbId    | TableId  | PartitionId | IndexId  | IsSync | DetailCmd                                                                  |\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n| default_cluster:test     | tbl1      | p20210505     | tbl1      | 3828954 | 63708800 | 106826829   | 63709761 | true   | SHOW PROC '/dbs/3828954/63708800/partitions/106826829/63709761/106827970'; |\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n```\n\n然后执行后面的 SHOW PROC 语句，我们可以获得这个 tablet 所有副本的详细信息。其中 VersionCount 列表示对应副本的数据版本数量。我们可以选取一个 VersionCount 较大的副本，在浏览器打开 CompactionStatus 列显示的 URL，得到如下Json结果：\n\n```json\n\n{\n    \"cumulative policy type\": \"SIZE_BASED\",\n    \"cumulative point\": 18438,\n    \"last cumulative failure time\": \"1970-01-01 08:00:00.000\",\n    \"last base failure time\": \"1970-01-01 08:00:00.000\",\n    \"last cumulative success time\": \"2021-05-05 17:18:48.904\",\n    \"last base success time\": \"2021-05-05 16:14:49.786\",\n    \"rowsets\": [\n        \"[0-17444] 13 DATA NONOVERLAPPING 0200000000b1fb8d344f83103113563dd81740036795499d 2.86 GB\",\n        \"[17445-17751] 1 DATA NONOVERLAPPING 0200000000b25183344f83103113563dd81740036795499d 68.61 MB\",\n        \"[17752-18089] 1 DATA NONOVERLAPPING 0200000000b2b9a2344f83103113563dd81740036795499d 74.52 MB\",\n        \"[18090-18437] 1 DATA NONOVERLAPPING 0200000000b32686344f83103113563dd81740036795499d 76.41 MB\",\n        \"[18438-18678] 1 DATA NONOVERLAPPING 0200000000b37084344f83103113563dd81740036795499d 53.07 MB\",\n        \"[18679-18679] 1 DATA NONOVERLAPPING 0200000000b36d87344f83103113563dd81740036795499d 3.11 KB\",\n        \"[18680-18680] 1 DATA NONOVERLAPPING 0200000000b36d70344f83103113563dd81740036795499d 258.40 KB\",\n        \"[18681-18681] 1 DATA NONOVERLAPPING 0200000000b36da0344f83103113563dd81740036795499d 266.98 KB\",\n    ],\n    \"stale_rowsets\": [\n    ],\n    \"stale version path\": [\n    ]\n}\n```\n\n这里我们可以看到一个 tablet 的 Cumulative Point，最近一次成功、失败的 BC/CC 任务时间，以及每个 rowset 的版本信息。如上面这个示例，我们可以得出以下结论：\n\n> 1. 基线数据量大约在2-3GB，增量rowset增长到几十MB后就会晋升到BC任务区。\n>\n> 2. 新增rowset数据量很小，且版本增长较快，说明这是一个高频小批量的导入场景。\n\n我们还可以进一步的通过以下命令分析指定 tablet id 的日志\n\n```shell\n\n# 查看 tablet 48062815 最近十个任务的配额情况\n$> grep permits log/be.INFO |grep 48062815 |tail -n 10\n\n# 查看 tablet 48062815 最近十个执行完成的 compaction 任务\n$> grep \"succeed to do\" log/be.INFO |grep 48062815 |tail -n 10\n```\n\n另外，我们还可以在浏览器打开以下 URL，查看一个 BE 节点当前正在执行的 compaction 任务：be_host:webserver_port/api/compaction/run_status\n\n```json\n\n{\n    \"CumulativeCompaction\": {\n        \"/home/disk2/palo\": [],\n        \"/home/disk1/palo\": [\n            \"48061239\"\n        ]\n    },\n    \"BaseCompaction\": {\n        \"/home/disk2/palo\": [],\n        \"/home/disk1/palo\": [\n            \"48062815\",\n            \"48061276\"\n        ]\n    }\n}\n```\n\n这个接口可以看到每个磁盘上当前正在执行的 compaction 任务。\n\n通过以上一系列的分析，我们应该可以对系统的 Compaction 情况有以下判断：\n\n> 1. Compaction 任务的执行频率、每个任务大致的执行耗时。\n>\n> 2. 指定节点数据版本数量的变化情况。\n>\n> 3. 指定 tablet 数据版本的变化情况，以及 compaction 的频率。\n>\n\n这些结论将指导我们对 Compaction 进行调优。\n\n### 修改 Compaction 线程数\n\n**增加 Compaction 线程数是一个非常直接的加速 Compaction 的方法。**但是更多的任务意味着更大的 IO 和 内存开销。尤其在机械磁盘上，因为随机读写问题，有时可能单线程串行执行的效率会高于多线程并行执行。Doris 默认配置为每块盘两个 Compaction 任务（这也是最小的合法配置），最多 10 个任务。如果磁盘数量多于 5，在内存允许的情况下，可以修改 max_compaction_threads 参数增加总任务数，以保证每块盘可以执行两个 Compaction 任务。\n\n对于机械磁盘，不建议增加每块盘的任务数。对于固态硬盘，可以考虑修改 compaction_task_num_per_disk 参数适当增加每块盘的任务数，如修改为 4。**注意修改这个参数的同时可能还需同步修改 max_compaction_threads，使得 max_compaction_threads 大于等于 compaction_task_num_per_disk * 磁盘数量。**\n\n### 优化单个 Compaction 任务逻辑\n\n这个优化方式比较复杂，我们尝试从几个场景出发来说明：\n\n#### **场景一：基线数据量大，Base Compaction 任务执行时间长。**\n\nBC 任务执行时间长，意味着一个任务会长时间占用 Compaction 工作线程，从而导致其他 tablet 的 compaction 任务时间被挤占。如果是因为 0 号版本的基线数据量较大导致，则我们可以考虑尽量推迟增量rowset 晋升到 BC 任务区的时间。以下两个参数将影响这个逻辑：\n\n> cumulative_size_based_promotion_ratio：默认 0.05，基线数据量乘以这个系数，即晋升阈值。可以调大这个系数来提高晋升阈值。\n>\n> cumulative_size_based_promotion_size_mbytes：默认 1024MB。如果增量rowset的数据量大于这个值，则会忽略第一个参数的阈值直接晋升。因此需要同时调整这个参数来提升晋升阈值。\n\n当然，提升晋升阈值，会导致单个 BC 任务需要处理更大的数据量，耗时更长，但是总体的数据量会减少。举个例子。基线数据大小为 1024GB，假设晋升阈值分别为 100MB 和 200MB。数据导入速度为 100MB/分钟。每5个版本执行一次 BC。那么理论上在10分钟内，阈值为 100MB 时，BC 任务处理的总数据量为 （1024 + 100 * 5）* 2 = 3048MB。阈值为 200MB 是，BC 任务处理的总数据量为 (1024 + 200 * 5) = 2024 MB。\n\n#### **场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。**\n\nmax_cumulative_compaction_num_singleton_deltas 参数控制一个 CC 任务最多合并多少个数据版本，默认值为 1000。我们考虑这样一种场景：针对某一个 tablet，其数据版本的增长速度为 1个/秒。而其 CC 任务的执行时间 + 调度时间是 1000秒（即单个 CC 任务的执行时间加上Compaction再一次调度到这个 tablet 的时间总和）。那么我们可能会看到这个 tablet 的版本数量在 1-1000之间浮动（这里我们忽略基线版本数量）。因为在下一次 CC 任务执行前的 1000 秒内，又会累积 1000 个版本。\n\n这种情况可能导致这个 tablet 的读取效率很不稳定。这时我们可以尝试调小 max_cumulative_compaction_num_singleton_deltas 这个参数，这样一个 CC 所要合并的版本数更少，执行时间更短，执行频率会更高。还是刚才这个场景，假设参数调整到500，而对应的 CC 任务的执行时间 + 调度时间也降低到 500，则理论上这个 tablet 的版本数量将会在 1-500 之间浮动，相比于之前，版本数量更稳定。\n\n当然这个只是理论数值，实际情况还要考虑任务的具体执行时间、调度情况等等。\n\n## 手动 Compaction\n\n某些情况下，自动 Compaction 策略可能无法选取到某些 tablet，这时我们可能需要通过 Compaction 接口来主动触发指定 tablet 的 Compaction。我们以 curl 命令举例：\n\n```shell\n\ncurl -X POST http://192.168.1.1:8040/api/compaction/run?tablet_id=106818600\\&schema_hash=6979334\\&compact_type=cumulative\n```\n\n这里我们指定 id 为 106818600，schema hash 为 6979334 的 tablet 进行 Cumulative Compaction（compact_type参数为 base 则触发 Base Compaction）。其中 schema hash 可以通过 SHOW TABLET tablet_id 命令得到的 SHOW PROC 命令获取。\n如果提交成功，则会返回：\n\n```json\n{\"status\": \"Success\", \"msg\": \"compaction task is successfully triggered.\"}\n```\n\n这是一个异步操作，命令只是提交compaction 任务，之后我们可以通过以下 API 来查看任务是否在运行：\n\n```shell\n\ncurl -X GET http://192.168.1.1:8040/api/compaction/run_status?tablet_id=106818600\\&schema_hash=6979334\n```\n\n返回结果：\n\n```json\n\n{\n    \"status\" : \"Success\",\n    \"run_status\" : false,\n    \"msg\" : \"compaction task for this tablet is running\",\n    \"tablet_id\" : 106818600,\n    \"schema_hash\" : 6979334,\n    \"compact_type\" : \"cumulative\"\n}\n```\n\n当然也可以直接查看 tablet 的版本情况：\n\n```shell\n\ncurl -X GET http://192.168.1.1:8040/api/compaction/show?tablet_id=106818600\\&schema_hash=6979334\n```\n\n## END\n\nCompaction 策略是 Doris 比较复杂的一个数据处理逻辑，需要考虑的状态和情况非常多，因此也在不断完善中，最终希望能够自动的适配各种负载场景，减轻运维压力。","source":"_posts/bigdata/When：何时需要进行Doris Compaction调优.md","raw":"---\ntitle: When：何时需要进行Doris Compaction调优\ntags:\n  - Doris\ncategories:\n  - - Doris\ntop_img: /img/bg/banner.gif\nabbrlink: 65527\ndate: 2022-09-03 23:40:51\nupdated: 2022-09-03 23:40:51\ncover:\ndescription:\nkeywords:\n---\n\n> 本篇将从实际使用场景的角度出发，介绍 Compaction 的调优思路和策略。通过本文将了解到 Compaction 相关的日志分析、参数调整和 API 的使用。\n\n## 什么情况下需要调整 Compaction 参数\n\nCompaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。**并且在这个过程中，Compaction 操作不能占用太多的系统资源。所以我们可以以结果为导向，从以下两个方面反推是否需要调整 Compaction 策略。**\n\n1. 检查数据版本是否有堆积。\n\n2. 检查 IO 和内存资源是否被 Compaction 任务过多的占用。\n\n### 查看数据版本数量变化趋势\n\nDoris 提供数据版本数量的监控数据。如果你部署了 Prometheus + Grafana 的监控，则可以通过 Grafana 仪表盘的 BE Base Compaction Score 和 BE Cumu Compaction Score 图表查看到这个监控数据的趋势图：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662264565446-6e38cc7f-eb01-441c-a6f0-4988b07c4835.png)\n\n> 这个图表展示的是每个 BE 节点，所有 Tablet 中数据版本最多的那个 Tablet 的版本数量，可以反映出当前版本堆积情况。\n> 部署方式参阅：http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html\n\n如果没有安装这个监控，如果你是用的 Palo 0.14.7 版本以上，也可以通过以下命令在命令行查看这个监控数据的趋势图：\n\n```shell\n\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_BASE_COMPACTION_SCORE\", \"time\" = \"last 4 hours\");\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_CUMU_COMPACTION_SCORE\", \"time\" = \"last 4 hours\");\n```\n\n注意这里有两个指标，分别表示 Base Compaction 和 Cumulative Compaction 所对应的版本数量。**在大部分情况下，我们只需要查看 Cumulative Compaction 的指标，即可大致了解集群的数据版本堆积情况。**\n\n**版本是否堆积没有一个明确的界限，而是根据使用场景和查询延迟进行判断的一个经验值。**我们可以按照以下步骤进行简单的推断：\n\n> 1. 观察数据版本数量的趋势，如果趋势平稳，则说明 Compaction 和导入速度基本持平。如果呈上升态势，则说明 Compaction 速度跟不上导入速度了。如果呈下降态势，说明 Compaction 速度超过了导入速度。**如果呈上升态势，或在平稳状态但数值较高，则需要考虑调整 Compaction 参数以加快 Compaction 的进度。**\n>\n> 2. **通常版本数量维持在 100 以内可以视为正常。而在大部分批量导入或低频导入场景下，版本数量通常为10-20甚至更低。**\n\n### 查看Compaction资源占用\nCompaction 资源占用主要是 IO 和 内存。\n\n对于 Compaction 占用的内存，可以在浏览器打开以下链接：http://be_host:webserver_port/mem_tracker在搜索框中输入 AutoCompaction：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1662265200095-5cf34b92-10de-4d1f-80c3-4dfcc0fd49fd.png)\n\n则可以查看当前Compaction的内存开销和历史峰值开销。\n\n而对于 IO 操作，目前还没有提供单独的 Compaction 操作的 IO 监控，我们只能根据集群整体的 IO 利用率情况来做判断。我们可以查看监控图 Disk IO util：\n\n![图片](https://mmbiz.qpic.cn/mmbiz_png/eGOhXuI8cBaCgNjQSRHmVYlEkMEIH7OGlBmYBlAkJLj3MjeJTTiauBiaYFeia8zf5s2fvImSlPwthGKZSO8oeFzNg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n或者通过命令在命令行查看（Palo 0.14.7 以上版本）：\n\n```shell\n\nmysql> ADMIN SHOW BACKEND METRIC (\"nodes\" = \"30746894\", \"metrics\" = \"BE_DISK_IO\", \"time\" = \"last 4 hours\");\n```\n\n这个监控展示的是每个 BE 节点上磁盘的 IO util 指标。数值越高表示IO越繁忙。**当然大部分情况下 IO 资源都是查询请求消耗的，这个监控主要用于指导我们是否需要增加或减少 Compaction 任务数。**\n\n## Compaction 调优策略\n\n如果版本数量有上升趋势或者数值较高，则可以从以下两方面优化 Compaction：\n\n1. 修改 Compaction 线程数，使得同时能够执行更多的 Compaction 任务。\n\n2. 优化单个 Compaction 的执行逻辑，使数据版本数量维持在一个合理范围。\n\n### 优化前的准备工作\n\n在优化 Compaction 执行逻辑之前，我们需要使用一些命令来进一步查看一些Compaction的细节信息。\n\n首先，我们通过监控图找到一个版本数量最高的 BE 节点。然后执行以下命令分析日志：\n\n```shell\n\n$> grep \"succeed to do base\" log/be.INFO.log.20210505-142010 |tail -n 100\n$> grep \"succeed to do cumu\" log/be.INFO.log.20210505-142010 |tail -n 100\n```\n\n以上两个命令可以查看最近100个执行完成的 compaction 任务：\n\n```shell\n\nI0505 17:06:56.143455   675 compaction.cpp:135] succeed to do cumulative compaction. tablet=106827682.505347040.d040c1cdf71e5c95-3a002a06127ccd86, output_version=2-2631, current_max_version=2633, disk=/home/disk6/palo.HDD, segments=57. elapsed time=2.29371s. cumulative_compaction_policy=SIZE_BASED.\nI0505 17:06:56.520058   666 compaction.cpp:135] succeed to do cumulative compaction. tablet=106822189.1661856168.654562832a620ea6-46fe84c73ea84795, output_version=2-3247, current_max_version=3250, disk=/home/disk2/palo.HDD, segments=22. elapsed time=2.66858s. cumulative_compaction_policy=SIZE_BASED.\n```\n\n通过日志时间可以判断 Compaction 是否在持续正确的执行，通过 elapsed time 可以观察每个任务的执行时间。\n\n我们还可以执行以下命令展示最近100个 compaction 任务的配额（permits）：\n\n```shell\n\n$> grep \"permits\" log/be.INFO |tail -n 100\n\nI0505 17:04:07.120920   667 compaction.cpp:83] start cumulative compaction. tablet=106827970.777011641.9c474de1b8ba9199-4addeb135d6834ac, output_version=2-2623, permits: 39\nI0505 17:04:13.898777   672 compaction.cpp:83] start cumulative compaction. tablet=106822777.1948936074.a44ac9462e79b76d-4a33ee39559bb0bf, output_version=2-3238, permits: 22\n```\n\n配额和版本数量成正比。\n\n我们可以找到 permits 较大的一个任务对应的 tablet id，如上图permit 为 39 的任务的 tablet id 为 106827970，然后继续分析这个 tablet 的 compaction 情况。\n\n通过 MySQL 客户端连接 Doris 集群后，执行：\n\n```shell\n\nmysql> show tablet 106827970;\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n| DbName                   | TableName | PartitionName | IndexName | DbId    | TableId  | PartitionId | IndexId  | IsSync | DetailCmd                                                                  |\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n| default_cluster:test     | tbl1      | p20210505     | tbl1      | 3828954 | 63708800 | 106826829   | 63709761 | true   | SHOW PROC '/dbs/3828954/63708800/partitions/106826829/63709761/106827970'; |\n+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+\n```\n\n然后执行后面的 SHOW PROC 语句，我们可以获得这个 tablet 所有副本的详细信息。其中 VersionCount 列表示对应副本的数据版本数量。我们可以选取一个 VersionCount 较大的副本，在浏览器打开 CompactionStatus 列显示的 URL，得到如下Json结果：\n\n```json\n\n{\n    \"cumulative policy type\": \"SIZE_BASED\",\n    \"cumulative point\": 18438,\n    \"last cumulative failure time\": \"1970-01-01 08:00:00.000\",\n    \"last base failure time\": \"1970-01-01 08:00:00.000\",\n    \"last cumulative success time\": \"2021-05-05 17:18:48.904\",\n    \"last base success time\": \"2021-05-05 16:14:49.786\",\n    \"rowsets\": [\n        \"[0-17444] 13 DATA NONOVERLAPPING 0200000000b1fb8d344f83103113563dd81740036795499d 2.86 GB\",\n        \"[17445-17751] 1 DATA NONOVERLAPPING 0200000000b25183344f83103113563dd81740036795499d 68.61 MB\",\n        \"[17752-18089] 1 DATA NONOVERLAPPING 0200000000b2b9a2344f83103113563dd81740036795499d 74.52 MB\",\n        \"[18090-18437] 1 DATA NONOVERLAPPING 0200000000b32686344f83103113563dd81740036795499d 76.41 MB\",\n        \"[18438-18678] 1 DATA NONOVERLAPPING 0200000000b37084344f83103113563dd81740036795499d 53.07 MB\",\n        \"[18679-18679] 1 DATA NONOVERLAPPING 0200000000b36d87344f83103113563dd81740036795499d 3.11 KB\",\n        \"[18680-18680] 1 DATA NONOVERLAPPING 0200000000b36d70344f83103113563dd81740036795499d 258.40 KB\",\n        \"[18681-18681] 1 DATA NONOVERLAPPING 0200000000b36da0344f83103113563dd81740036795499d 266.98 KB\",\n    ],\n    \"stale_rowsets\": [\n    ],\n    \"stale version path\": [\n    ]\n}\n```\n\n这里我们可以看到一个 tablet 的 Cumulative Point，最近一次成功、失败的 BC/CC 任务时间，以及每个 rowset 的版本信息。如上面这个示例，我们可以得出以下结论：\n\n> 1. 基线数据量大约在2-3GB，增量rowset增长到几十MB后就会晋升到BC任务区。\n>\n> 2. 新增rowset数据量很小，且版本增长较快，说明这是一个高频小批量的导入场景。\n\n我们还可以进一步的通过以下命令分析指定 tablet id 的日志\n\n```shell\n\n# 查看 tablet 48062815 最近十个任务的配额情况\n$> grep permits log/be.INFO |grep 48062815 |tail -n 10\n\n# 查看 tablet 48062815 最近十个执行完成的 compaction 任务\n$> grep \"succeed to do\" log/be.INFO |grep 48062815 |tail -n 10\n```\n\n另外，我们还可以在浏览器打开以下 URL，查看一个 BE 节点当前正在执行的 compaction 任务：be_host:webserver_port/api/compaction/run_status\n\n```json\n\n{\n    \"CumulativeCompaction\": {\n        \"/home/disk2/palo\": [],\n        \"/home/disk1/palo\": [\n            \"48061239\"\n        ]\n    },\n    \"BaseCompaction\": {\n        \"/home/disk2/palo\": [],\n        \"/home/disk1/palo\": [\n            \"48062815\",\n            \"48061276\"\n        ]\n    }\n}\n```\n\n这个接口可以看到每个磁盘上当前正在执行的 compaction 任务。\n\n通过以上一系列的分析，我们应该可以对系统的 Compaction 情况有以下判断：\n\n> 1. Compaction 任务的执行频率、每个任务大致的执行耗时。\n>\n> 2. 指定节点数据版本数量的变化情况。\n>\n> 3. 指定 tablet 数据版本的变化情况，以及 compaction 的频率。\n>\n\n这些结论将指导我们对 Compaction 进行调优。\n\n### 修改 Compaction 线程数\n\n**增加 Compaction 线程数是一个非常直接的加速 Compaction 的方法。**但是更多的任务意味着更大的 IO 和 内存开销。尤其在机械磁盘上，因为随机读写问题，有时可能单线程串行执行的效率会高于多线程并行执行。Doris 默认配置为每块盘两个 Compaction 任务（这也是最小的合法配置），最多 10 个任务。如果磁盘数量多于 5，在内存允许的情况下，可以修改 max_compaction_threads 参数增加总任务数，以保证每块盘可以执行两个 Compaction 任务。\n\n对于机械磁盘，不建议增加每块盘的任务数。对于固态硬盘，可以考虑修改 compaction_task_num_per_disk 参数适当增加每块盘的任务数，如修改为 4。**注意修改这个参数的同时可能还需同步修改 max_compaction_threads，使得 max_compaction_threads 大于等于 compaction_task_num_per_disk * 磁盘数量。**\n\n### 优化单个 Compaction 任务逻辑\n\n这个优化方式比较复杂，我们尝试从几个场景出发来说明：\n\n#### **场景一：基线数据量大，Base Compaction 任务执行时间长。**\n\nBC 任务执行时间长，意味着一个任务会长时间占用 Compaction 工作线程，从而导致其他 tablet 的 compaction 任务时间被挤占。如果是因为 0 号版本的基线数据量较大导致，则我们可以考虑尽量推迟增量rowset 晋升到 BC 任务区的时间。以下两个参数将影响这个逻辑：\n\n> cumulative_size_based_promotion_ratio：默认 0.05，基线数据量乘以这个系数，即晋升阈值。可以调大这个系数来提高晋升阈值。\n>\n> cumulative_size_based_promotion_size_mbytes：默认 1024MB。如果增量rowset的数据量大于这个值，则会忽略第一个参数的阈值直接晋升。因此需要同时调整这个参数来提升晋升阈值。\n\n当然，提升晋升阈值，会导致单个 BC 任务需要处理更大的数据量，耗时更长，但是总体的数据量会减少。举个例子。基线数据大小为 1024GB，假设晋升阈值分别为 100MB 和 200MB。数据导入速度为 100MB/分钟。每5个版本执行一次 BC。那么理论上在10分钟内，阈值为 100MB 时，BC 任务处理的总数据量为 （1024 + 100 * 5）* 2 = 3048MB。阈值为 200MB 是，BC 任务处理的总数据量为 (1024 + 200 * 5) = 2024 MB。\n\n#### **场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。**\n\nmax_cumulative_compaction_num_singleton_deltas 参数控制一个 CC 任务最多合并多少个数据版本，默认值为 1000。我们考虑这样一种场景：针对某一个 tablet，其数据版本的增长速度为 1个/秒。而其 CC 任务的执行时间 + 调度时间是 1000秒（即单个 CC 任务的执行时间加上Compaction再一次调度到这个 tablet 的时间总和）。那么我们可能会看到这个 tablet 的版本数量在 1-1000之间浮动（这里我们忽略基线版本数量）。因为在下一次 CC 任务执行前的 1000 秒内，又会累积 1000 个版本。\n\n这种情况可能导致这个 tablet 的读取效率很不稳定。这时我们可以尝试调小 max_cumulative_compaction_num_singleton_deltas 这个参数，这样一个 CC 所要合并的版本数更少，执行时间更短，执行频率会更高。还是刚才这个场景，假设参数调整到500，而对应的 CC 任务的执行时间 + 调度时间也降低到 500，则理论上这个 tablet 的版本数量将会在 1-500 之间浮动，相比于之前，版本数量更稳定。\n\n当然这个只是理论数值，实际情况还要考虑任务的具体执行时间、调度情况等等。\n\n## 手动 Compaction\n\n某些情况下，自动 Compaction 策略可能无法选取到某些 tablet，这时我们可能需要通过 Compaction 接口来主动触发指定 tablet 的 Compaction。我们以 curl 命令举例：\n\n```shell\n\ncurl -X POST http://192.168.1.1:8040/api/compaction/run?tablet_id=106818600\\&schema_hash=6979334\\&compact_type=cumulative\n```\n\n这里我们指定 id 为 106818600，schema hash 为 6979334 的 tablet 进行 Cumulative Compaction（compact_type参数为 base 则触发 Base Compaction）。其中 schema hash 可以通过 SHOW TABLET tablet_id 命令得到的 SHOW PROC 命令获取。\n如果提交成功，则会返回：\n\n```json\n{\"status\": \"Success\", \"msg\": \"compaction task is successfully triggered.\"}\n```\n\n这是一个异步操作，命令只是提交compaction 任务，之后我们可以通过以下 API 来查看任务是否在运行：\n\n```shell\n\ncurl -X GET http://192.168.1.1:8040/api/compaction/run_status?tablet_id=106818600\\&schema_hash=6979334\n```\n\n返回结果：\n\n```json\n\n{\n    \"status\" : \"Success\",\n    \"run_status\" : false,\n    \"msg\" : \"compaction task for this tablet is running\",\n    \"tablet_id\" : 106818600,\n    \"schema_hash\" : 6979334,\n    \"compact_type\" : \"cumulative\"\n}\n```\n\n当然也可以直接查看 tablet 的版本情况：\n\n```shell\n\ncurl -X GET http://192.168.1.1:8040/api/compaction/show?tablet_id=106818600\\&schema_hash=6979334\n```\n\n## END\n\nCompaction 策略是 Doris 比较复杂的一个数据处理逻辑，需要考虑的状态和情况非常多，因此也在不断完善中，最终希望能够自动的适配各种负载场景，减轻运维压力。","slug":"bigdata/When：何时需要进行Doris Compaction调优","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksx001m8j5m989a4kum","content":"<blockquote>\n<p>本篇将从实际使用场景的角度出发，介绍 Compaction 的调优思路和策略。通过本文将了解到 Compaction 相关的日志分析、参数调整和 API 的使用。</p>\n</blockquote>\n<h2 id=\"什么情况下需要调整-Compaction-参数\"><a href=\"#什么情况下需要调整-Compaction-参数\" class=\"headerlink\" title=\"什么情况下需要调整 Compaction 参数\"></a>什么情况下需要调整 Compaction 参数</h2><p>Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。<strong>并且在这个过程中，Compaction 操作不能占用太多的系统资源。所以我们可以以结果为导向，从以下两个方面反推是否需要调整 Compaction 策略。</strong></p>\n<ol>\n<li><p>检查数据版本是否有堆积。</p>\n</li>\n<li><p>检查 IO 和内存资源是否被 Compaction 任务过多的占用。</p>\n</li>\n</ol>\n<h3 id=\"查看数据版本数量变化趋势\"><a href=\"#查看数据版本数量变化趋势\" class=\"headerlink\" title=\"查看数据版本数量变化趋势\"></a>查看数据版本数量变化趋势</h3><p>Doris 提供数据版本数量的监控数据。如果你部署了 Prometheus + Grafana 的监控，则可以通过 Grafana 仪表盘的 BE Base Compaction Score 和 BE Cumu Compaction Score 图表查看到这个监控数据的趋势图：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662264565446-6e38cc7f-eb01-441c-a6f0-4988b07c4835.png\" alt=\"img\"></p>\n<blockquote>\n<p>这个图表展示的是每个 BE 节点，所有 Tablet 中数据版本最多的那个 Tablet 的版本数量，可以反映出当前版本堆积情况。<br>部署方式参阅：<a href=\"http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html\">http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html</a></p>\n</blockquote>\n<p>如果没有安装这个监控，如果你是用的 Palo 0.14.7 版本以上，也可以通过以下命令在命令行查看这个监控数据的趋势图：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_BASE_COMPACTION_SCORE&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_CUMU_COMPACTION_SCORE&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br></pre></td></tr></table></figure>\n\n<p>注意这里有两个指标，分别表示 Base Compaction 和 Cumulative Compaction 所对应的版本数量。<strong>在大部分情况下，我们只需要查看 Cumulative Compaction 的指标，即可大致了解集群的数据版本堆积情况。</strong></p>\n<p><strong>版本是否堆积没有一个明确的界限，而是根据使用场景和查询延迟进行判断的一个经验值。</strong>我们可以按照以下步骤进行简单的推断：</p>\n<blockquote>\n<ol>\n<li><p>观察数据版本数量的趋势，如果趋势平稳，则说明 Compaction 和导入速度基本持平。如果呈上升态势，则说明 Compaction 速度跟不上导入速度了。如果呈下降态势，说明 Compaction 速度超过了导入速度。<strong>如果呈上升态势，或在平稳状态但数值较高，则需要考虑调整 Compaction 参数以加快 Compaction 的进度。</strong></p>\n</li>\n<li><p><strong>通常版本数量维持在 100 以内可以视为正常。而在大部分批量导入或低频导入场景下，版本数量通常为10-20甚至更低。</strong></p>\n</li>\n</ol>\n</blockquote>\n<h3 id=\"查看Compaction资源占用\"><a href=\"#查看Compaction资源占用\" class=\"headerlink\" title=\"查看Compaction资源占用\"></a>查看Compaction资源占用</h3><p>Compaction 资源占用主要是 IO 和 内存。</p>\n<p>对于 Compaction 占用的内存，可以在浏览器打开以下链接：<a href=\"http://be_host:webserver_port/mem_tracker在搜索框中输入\">http://be_host:webserver_port/mem_tracker在搜索框中输入</a> AutoCompaction：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662265200095-5cf34b92-10de-4d1f-80c3-4dfcc0fd49fd.png\" alt=\"img\"></p>\n<p>则可以查看当前Compaction的内存开销和历史峰值开销。</p>\n<p>而对于 IO 操作，目前还没有提供单独的 Compaction 操作的 IO 监控，我们只能根据集群整体的 IO 利用率情况来做判断。我们可以查看监控图 Disk IO util：</p>\n<p><img src=\"https://mmbiz.qpic.cn/mmbiz_png/eGOhXuI8cBaCgNjQSRHmVYlEkMEIH7OGlBmYBlAkJLj3MjeJTTiauBiaYFeia8zf5s2fvImSlPwthGKZSO8oeFzNg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\" alt=\"图片\"></p>\n<p>或者通过命令在命令行查看（Palo 0.14.7 以上版本）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_DISK_IO&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br></pre></td></tr></table></figure>\n\n<p>这个监控展示的是每个 BE 节点上磁盘的 IO util 指标。数值越高表示IO越繁忙。<strong>当然大部分情况下 IO 资源都是查询请求消耗的，这个监控主要用于指导我们是否需要增加或减少 Compaction 任务数。</strong></p>\n<h2 id=\"Compaction-调优策略\"><a href=\"#Compaction-调优策略\" class=\"headerlink\" title=\"Compaction 调优策略\"></a>Compaction 调优策略</h2><p>如果版本数量有上升趋势或者数值较高，则可以从以下两方面优化 Compaction：</p>\n<ol>\n<li><p>修改 Compaction 线程数，使得同时能够执行更多的 Compaction 任务。</p>\n</li>\n<li><p>优化单个 Compaction 的执行逻辑，使数据版本数量维持在一个合理范围。</p>\n</li>\n</ol>\n<h3 id=\"优化前的准备工作\"><a href=\"#优化前的准备工作\" class=\"headerlink\" title=\"优化前的准备工作\"></a>优化前的准备工作</h3><p>在优化 Compaction 执行逻辑之前，我们需要使用一些命令来进一步查看一些Compaction的细节信息。</p>\n<p>首先，我们通过监控图找到一个版本数量最高的 BE 节点。然后执行以下命令分析日志：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do base&quot;</span> <span class=\"built_in\">log</span>/be.INFO.log.20210505-142010 |<span class=\"built_in\">tail</span> -n 100</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do cumu&quot;</span> <span class=\"built_in\">log</span>/be.INFO.log.20210505-142010 |<span class=\"built_in\">tail</span> -n 100</span></span><br></pre></td></tr></table></figure>\n\n<p>以上两个命令可以查看最近100个执行完成的 compaction 任务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">I0505 17:06:56.143455   675 compaction.cpp:135] succeed to do cumulative compaction. tablet=106827682.505347040.d040c1cdf71e5c95-3a002a06127ccd86, output_version=2-2631, current_max_version=2633, disk=/home/disk6/palo.HDD, segments=57. elapsed time=2.29371s. cumulative_compaction_policy=SIZE_BASED.</span><br><span class=\"line\">I0505 17:06:56.520058   666 compaction.cpp:135] succeed to do cumulative compaction. tablet=106822189.1661856168.654562832a620ea6-46fe84c73ea84795, output_version=2-3247, current_max_version=3250, disk=/home/disk2/palo.HDD, segments=22. elapsed time=2.66858s. cumulative_compaction_policy=SIZE_BASED.</span><br></pre></td></tr></table></figure>\n\n<p>通过日志时间可以判断 Compaction 是否在持续正确的执行，通过 elapsed time 可以观察每个任务的执行时间。</p>\n<p>我们还可以执行以下命令展示最近100个 compaction 任务的配额（permits）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;permits&quot;</span> <span class=\"built_in\">log</span>/be.INFO |<span class=\"built_in\">tail</span> -n 100</span></span><br><span class=\"line\"></span><br><span class=\"line\">I0505 17:04:07.120920   667 compaction.cpp:83] start cumulative compaction. tablet=106827970.777011641.9c474de1b8ba9199-4addeb135d6834ac, output_version=2-2623, permits: 39</span><br><span class=\"line\">I0505 17:04:13.898777   672 compaction.cpp:83] start cumulative compaction. tablet=106822777.1948936074.a44ac9462e79b76d-4a33ee39559bb0bf, output_version=2-3238, permits: 22</span><br></pre></td></tr></table></figure>\n\n<p>配额和版本数量成正比。</p>\n<p>我们可以找到 permits 较大的一个任务对应的 tablet id，如上图permit 为 39 的任务的 tablet id 为 106827970，然后继续分析这个 tablet 的 compaction 情况。</p>\n<p>通过 MySQL 客户端连接 Doris 集群后，执行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">show tablet 106827970;</span></span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br><span class=\"line\">| DbName                   | TableName | PartitionName | IndexName | DbId    | TableId  | PartitionId | IndexId  | IsSync | DetailCmd                                                                  |</span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br><span class=\"line\">| default_cluster:test     | tbl1      | p20210505     | tbl1      | 3828954 | 63708800 | 106826829   | 63709761 | true   | SHOW PROC &#x27;/dbs/3828954/63708800/partitions/106826829/63709761/106827970&#x27;; |</span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>\n\n<p>然后执行后面的 SHOW PROC 语句，我们可以获得这个 tablet 所有副本的详细信息。其中 VersionCount 列表示对应副本的数据版本数量。我们可以选取一个 VersionCount 较大的副本，在浏览器打开 CompactionStatus 列显示的 URL，得到如下Json结果：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;cumulative policy type&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;SIZE_BASED&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;cumulative point&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">18438</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last cumulative failure time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;1970-01-01 08:00:00.000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last base failure time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;1970-01-01 08:00:00.000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last cumulative success time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2021-05-05 17:18:48.904&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last base success time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2021-05-05 16:14:49.786&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;rowsets&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[0-17444] 13 DATA NONOVERLAPPING 0200000000b1fb8d344f83103113563dd81740036795499d 2.86 GB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[17445-17751] 1 DATA NONOVERLAPPING 0200000000b25183344f83103113563dd81740036795499d 68.61 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[17752-18089] 1 DATA NONOVERLAPPING 0200000000b2b9a2344f83103113563dd81740036795499d 74.52 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18090-18437] 1 DATA NONOVERLAPPING 0200000000b32686344f83103113563dd81740036795499d 76.41 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18438-18678] 1 DATA NONOVERLAPPING 0200000000b37084344f83103113563dd81740036795499d 53.07 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18679-18679] 1 DATA NONOVERLAPPING 0200000000b36d87344f83103113563dd81740036795499d 3.11 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18680-18680] 1 DATA NONOVERLAPPING 0200000000b36d70344f83103113563dd81740036795499d 258.40 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18681-18681] 1 DATA NONOVERLAPPING 0200000000b36da0344f83103113563dd81740036795499d 266.98 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;stale_rowsets&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;stale version path&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这里我们可以看到一个 tablet 的 Cumulative Point，最近一次成功、失败的 BC&#x2F;CC 任务时间，以及每个 rowset 的版本信息。如上面这个示例，我们可以得出以下结论：</p>\n<blockquote>\n<ol>\n<li><p>基线数据量大约在2-3GB，增量rowset增长到几十MB后就会晋升到BC任务区。</p>\n</li>\n<li><p>新增rowset数据量很小，且版本增长较快，说明这是一个高频小批量的导入场景。</p>\n</li>\n</ol>\n</blockquote>\n<p>我们还可以进一步的通过以下命令分析指定 tablet id 的日志</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查看 tablet 48062815 最近十个任务的配额情况</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep permits <span class=\"built_in\">log</span>/be.INFO |grep 48062815 |<span class=\"built_in\">tail</span> -n 10</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查看 tablet 48062815 最近十个执行完成的 compaction 任务</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do&quot;</span> <span class=\"built_in\">log</span>/be.INFO |grep 48062815 |<span class=\"built_in\">tail</span> -n 10</span></span><br></pre></td></tr></table></figure>\n\n<p>另外，我们还可以在浏览器打开以下 URL，查看一个 BE 节点当前正在执行的 compaction 任务：be_host:webserver_port&#x2F;api&#x2F;compaction&#x2F;run_status</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;CumulativeCompaction&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk2/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk1/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48061239&quot;</span></span><br><span class=\"line\">        <span class=\"punctuation\">]</span></span><br><span class=\"line\">    <span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;BaseCompaction&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk2/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk1/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48062815&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48061276&quot;</span></span><br><span class=\"line\">        <span class=\"punctuation\">]</span></span><br><span class=\"line\">    <span class=\"punctuation\">&#125;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这个接口可以看到每个磁盘上当前正在执行的 compaction 任务。</p>\n<p>通过以上一系列的分析，我们应该可以对系统的 Compaction 情况有以下判断：</p>\n<blockquote>\n<ol>\n<li><p>Compaction 任务的执行频率、每个任务大致的执行耗时。</p>\n</li>\n<li><p>指定节点数据版本数量的变化情况。</p>\n</li>\n<li><p>指定 tablet 数据版本的变化情况，以及 compaction 的频率。</p>\n</li>\n</ol>\n</blockquote>\n<p>这些结论将指导我们对 Compaction 进行调优。</p>\n<h3 id=\"修改-Compaction-线程数\"><a href=\"#修改-Compaction-线程数\" class=\"headerlink\" title=\"修改 Compaction 线程数\"></a>修改 Compaction 线程数</h3><p><strong>增加 Compaction 线程数是一个非常直接的加速 Compaction 的方法。</strong>但是更多的任务意味着更大的 IO 和 内存开销。尤其在机械磁盘上，因为随机读写问题，有时可能单线程串行执行的效率会高于多线程并行执行。Doris 默认配置为每块盘两个 Compaction 任务（这也是最小的合法配置），最多 10 个任务。如果磁盘数量多于 5，在内存允许的情况下，可以修改 max_compaction_threads 参数增加总任务数，以保证每块盘可以执行两个 Compaction 任务。</p>\n<p>对于机械磁盘，不建议增加每块盘的任务数。对于固态硬盘，可以考虑修改 compaction_task_num_per_disk 参数适当增加每块盘的任务数，如修改为 4。<strong>注意修改这个参数的同时可能还需同步修改 max_compaction_threads，使得 max_compaction_threads 大于等于 compaction_task_num_per_disk * 磁盘数量。</strong></p>\n<h3 id=\"优化单个-Compaction-任务逻辑\"><a href=\"#优化单个-Compaction-任务逻辑\" class=\"headerlink\" title=\"优化单个 Compaction 任务逻辑\"></a>优化单个 Compaction 任务逻辑</h3><p>这个优化方式比较复杂，我们尝试从几个场景出发来说明：</p>\n<h4 id=\"场景一：基线数据量大，Base-Compaction-任务执行时间长。\"><a href=\"#场景一：基线数据量大，Base-Compaction-任务执行时间长。\" class=\"headerlink\" title=\"场景一：基线数据量大，Base Compaction 任务执行时间长。\"></a><strong>场景一：基线数据量大，Base Compaction 任务执行时间长。</strong></h4><p>BC 任务执行时间长，意味着一个任务会长时间占用 Compaction 工作线程，从而导致其他 tablet 的 compaction 任务时间被挤占。如果是因为 0 号版本的基线数据量较大导致，则我们可以考虑尽量推迟增量rowset 晋升到 BC 任务区的时间。以下两个参数将影响这个逻辑：</p>\n<blockquote>\n<p>cumulative_size_based_promotion_ratio：默认 0.05，基线数据量乘以这个系数，即晋升阈值。可以调大这个系数来提高晋升阈值。</p>\n<p>cumulative_size_based_promotion_size_mbytes：默认 1024MB。如果增量rowset的数据量大于这个值，则会忽略第一个参数的阈值直接晋升。因此需要同时调整这个参数来提升晋升阈值。</p>\n</blockquote>\n<p>当然，提升晋升阈值，会导致单个 BC 任务需要处理更大的数据量，耗时更长，但是总体的数据量会减少。举个例子。基线数据大小为 1024GB，假设晋升阈值分别为 100MB 和 200MB。数据导入速度为 100MB&#x2F;分钟。每5个版本执行一次 BC。那么理论上在10分钟内，阈值为 100MB 时，BC 任务处理的总数据量为 （1024 + 100 * 5）* 2 &#x3D; 3048MB。阈值为 200MB 是，BC 任务处理的总数据量为 (1024 + 200 * 5) &#x3D; 2024 MB。</p>\n<h4 id=\"场景二：增量数据版本数量增长较快，Cumulative-Compaction-处理过多版本，耗时较长。\"><a href=\"#场景二：增量数据版本数量增长较快，Cumulative-Compaction-处理过多版本，耗时较长。\" class=\"headerlink\" title=\"场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。\"></a><strong>场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。</strong></h4><p>max_cumulative_compaction_num_singleton_deltas 参数控制一个 CC 任务最多合并多少个数据版本，默认值为 1000。我们考虑这样一种场景：针对某一个 tablet，其数据版本的增长速度为 1个&#x2F;秒。而其 CC 任务的执行时间 + 调度时间是 1000秒（即单个 CC 任务的执行时间加上Compaction再一次调度到这个 tablet 的时间总和）。那么我们可能会看到这个 tablet 的版本数量在 1-1000之间浮动（这里我们忽略基线版本数量）。因为在下一次 CC 任务执行前的 1000 秒内，又会累积 1000 个版本。</p>\n<p>这种情况可能导致这个 tablet 的读取效率很不稳定。这时我们可以尝试调小 max_cumulative_compaction_num_singleton_deltas 这个参数，这样一个 CC 所要合并的版本数更少，执行时间更短，执行频率会更高。还是刚才这个场景，假设参数调整到500，而对应的 CC 任务的执行时间 + 调度时间也降低到 500，则理论上这个 tablet 的版本数量将会在 1-500 之间浮动，相比于之前，版本数量更稳定。</p>\n<p>当然这个只是理论数值，实际情况还要考虑任务的具体执行时间、调度情况等等。</p>\n<h2 id=\"手动-Compaction\"><a href=\"#手动-Compaction\" class=\"headerlink\" title=\"手动 Compaction\"></a>手动 Compaction</h2><p>某些情况下，自动 Compaction 策略可能无法选取到某些 tablet，这时我们可能需要通过 Compaction 接口来主动触发指定 tablet 的 Compaction。我们以 curl 命令举例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X POST http://192.168.1.1:8040/api/compaction/run?tablet_id=106818600\\&amp;schema_hash=6979334\\&amp;compact_type=cumulative</span><br></pre></td></tr></table></figure>\n\n<p>这里我们指定 id 为 106818600，schema hash 为 6979334 的 tablet 进行 Cumulative Compaction（compact_type参数为 base 则触发 Base Compaction）。其中 schema hash 可以通过 SHOW TABLET tablet_id 命令得到的 SHOW PROC 命令获取。<br>如果提交成功，则会返回：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span><span class=\"attr\">&quot;status&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Success&quot;</span><span class=\"punctuation\">,</span> <span class=\"attr\">&quot;msg&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;compaction task is successfully triggered.&quot;</span><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这是一个异步操作，命令只是提交compaction 任务，之后我们可以通过以下 API 来查看任务是否在运行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X GET http://192.168.1.1:8040/api/compaction/run_status?tablet_id=106818600\\&amp;schema_hash=6979334</span><br></pre></td></tr></table></figure>\n\n<p>返回结果：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;status&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;Success&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;run_status&quot;</span> <span class=\"punctuation\">:</span> <span class=\"literal\"><span class=\"keyword\">false</span></span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;msg&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;compaction task for this tablet is running&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;tablet_id&quot;</span> <span class=\"punctuation\">:</span> <span class=\"number\">106818600</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;schema_hash&quot;</span> <span class=\"punctuation\">:</span> <span class=\"number\">6979334</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;compact_type&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;cumulative&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>当然也可以直接查看 tablet 的版本情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X GET http://192.168.1.1:8040/api/compaction/show?tablet_id=106818600\\&amp;schema_hash=6979334</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h2><p>Compaction 策略是 Doris 比较复杂的一个数据处理逻辑，需要考虑的状态和情况非常多，因此也在不断完善中，最终希望能够自动的适配各种负载场景，减轻运维压力。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>本篇将从实际使用场景的角度出发，介绍 Compaction 的调优思路和策略。通过本文将了解到 Compaction 相关的日志分析、参数调整和 API 的使用。</p>\n</blockquote>\n<h2 id=\"什么情况下需要调整-Compaction-参数\"><a href=\"#什么情况下需要调整-Compaction-参数\" class=\"headerlink\" title=\"什么情况下需要调整 Compaction 参数\"></a>什么情况下需要调整 Compaction 参数</h2><p>Compaction 的目的是合并多个数据版本，一是避免在读取时大量的 Merge 操作，二是避免大量的数据版本导致的随机IO。<strong>并且在这个过程中，Compaction 操作不能占用太多的系统资源。所以我们可以以结果为导向，从以下两个方面反推是否需要调整 Compaction 策略。</strong></p>\n<ol>\n<li><p>检查数据版本是否有堆积。</p>\n</li>\n<li><p>检查 IO 和内存资源是否被 Compaction 任务过多的占用。</p>\n</li>\n</ol>\n<h3 id=\"查看数据版本数量变化趋势\"><a href=\"#查看数据版本数量变化趋势\" class=\"headerlink\" title=\"查看数据版本数量变化趋势\"></a>查看数据版本数量变化趋势</h3><p>Doris 提供数据版本数量的监控数据。如果你部署了 Prometheus + Grafana 的监控，则可以通过 Grafana 仪表盘的 BE Base Compaction Score 和 BE Cumu Compaction Score 图表查看到这个监控数据的趋势图：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662264565446-6e38cc7f-eb01-441c-a6f0-4988b07c4835.png\" alt=\"img\"></p>\n<blockquote>\n<p>这个图表展示的是每个 BE 节点，所有 Tablet 中数据版本最多的那个 Tablet 的版本数量，可以反映出当前版本堆积情况。<br>部署方式参阅：<a href=\"http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html\">http://doris.incubator.apache.org/master/zh-CN/administrator-guide/operation/monitor-alert.html</a></p>\n</blockquote>\n<p>如果没有安装这个监控，如果你是用的 Palo 0.14.7 版本以上，也可以通过以下命令在命令行查看这个监控数据的趋势图：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_BASE_COMPACTION_SCORE&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_CUMU_COMPACTION_SCORE&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br></pre></td></tr></table></figure>\n\n<p>注意这里有两个指标，分别表示 Base Compaction 和 Cumulative Compaction 所对应的版本数量。<strong>在大部分情况下，我们只需要查看 Cumulative Compaction 的指标，即可大致了解集群的数据版本堆积情况。</strong></p>\n<p><strong>版本是否堆积没有一个明确的界限，而是根据使用场景和查询延迟进行判断的一个经验值。</strong>我们可以按照以下步骤进行简单的推断：</p>\n<blockquote>\n<ol>\n<li><p>观察数据版本数量的趋势，如果趋势平稳，则说明 Compaction 和导入速度基本持平。如果呈上升态势，则说明 Compaction 速度跟不上导入速度了。如果呈下降态势，说明 Compaction 速度超过了导入速度。<strong>如果呈上升态势，或在平稳状态但数值较高，则需要考虑调整 Compaction 参数以加快 Compaction 的进度。</strong></p>\n</li>\n<li><p><strong>通常版本数量维持在 100 以内可以视为正常。而在大部分批量导入或低频导入场景下，版本数量通常为10-20甚至更低。</strong></p>\n</li>\n</ol>\n</blockquote>\n<h3 id=\"查看Compaction资源占用\"><a href=\"#查看Compaction资源占用\" class=\"headerlink\" title=\"查看Compaction资源占用\"></a>查看Compaction资源占用</h3><p>Compaction 资源占用主要是 IO 和 内存。</p>\n<p>对于 Compaction 占用的内存，可以在浏览器打开以下链接：<a href=\"http://be_host:webserver_port/mem_tracker在搜索框中输入\">http://be_host:webserver_port/mem_tracker在搜索框中输入</a> AutoCompaction：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1662265200095-5cf34b92-10de-4d1f-80c3-4dfcc0fd49fd.png\" alt=\"img\"></p>\n<p>则可以查看当前Compaction的内存开销和历史峰值开销。</p>\n<p>而对于 IO 操作，目前还没有提供单独的 Compaction 操作的 IO 监控，我们只能根据集群整体的 IO 利用率情况来做判断。我们可以查看监控图 Disk IO util：</p>\n<p><img src=\"https://mmbiz.qpic.cn/mmbiz_png/eGOhXuI8cBaCgNjQSRHmVYlEkMEIH7OGlBmYBlAkJLj3MjeJTTiauBiaYFeia8zf5s2fvImSlPwthGKZSO8oeFzNg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1\" alt=\"图片\"></p>\n<p>或者通过命令在命令行查看（Palo 0.14.7 以上版本）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">ADMIN SHOW BACKEND METRIC (<span class=\"string\">&quot;nodes&quot;</span> = <span class=\"string\">&quot;30746894&quot;</span>, <span class=\"string\">&quot;metrics&quot;</span> = <span class=\"string\">&quot;BE_DISK_IO&quot;</span>, <span class=\"string\">&quot;time&quot;</span> = <span class=\"string\">&quot;last 4 hours&quot;</span>);</span></span><br></pre></td></tr></table></figure>\n\n<p>这个监控展示的是每个 BE 节点上磁盘的 IO util 指标。数值越高表示IO越繁忙。<strong>当然大部分情况下 IO 资源都是查询请求消耗的，这个监控主要用于指导我们是否需要增加或减少 Compaction 任务数。</strong></p>\n<h2 id=\"Compaction-调优策略\"><a href=\"#Compaction-调优策略\" class=\"headerlink\" title=\"Compaction 调优策略\"></a>Compaction 调优策略</h2><p>如果版本数量有上升趋势或者数值较高，则可以从以下两方面优化 Compaction：</p>\n<ol>\n<li><p>修改 Compaction 线程数，使得同时能够执行更多的 Compaction 任务。</p>\n</li>\n<li><p>优化单个 Compaction 的执行逻辑，使数据版本数量维持在一个合理范围。</p>\n</li>\n</ol>\n<h3 id=\"优化前的准备工作\"><a href=\"#优化前的准备工作\" class=\"headerlink\" title=\"优化前的准备工作\"></a>优化前的准备工作</h3><p>在优化 Compaction 执行逻辑之前，我们需要使用一些命令来进一步查看一些Compaction的细节信息。</p>\n<p>首先，我们通过监控图找到一个版本数量最高的 BE 节点。然后执行以下命令分析日志：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do base&quot;</span> <span class=\"built_in\">log</span>/be.INFO.log.20210505-142010 |<span class=\"built_in\">tail</span> -n 100</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do cumu&quot;</span> <span class=\"built_in\">log</span>/be.INFO.log.20210505-142010 |<span class=\"built_in\">tail</span> -n 100</span></span><br></pre></td></tr></table></figure>\n\n<p>以上两个命令可以查看最近100个执行完成的 compaction 任务：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">I0505 17:06:56.143455   675 compaction.cpp:135] succeed to do cumulative compaction. tablet=106827682.505347040.d040c1cdf71e5c95-3a002a06127ccd86, output_version=2-2631, current_max_version=2633, disk=/home/disk6/palo.HDD, segments=57. elapsed time=2.29371s. cumulative_compaction_policy=SIZE_BASED.</span><br><span class=\"line\">I0505 17:06:56.520058   666 compaction.cpp:135] succeed to do cumulative compaction. tablet=106822189.1661856168.654562832a620ea6-46fe84c73ea84795, output_version=2-3247, current_max_version=3250, disk=/home/disk2/palo.HDD, segments=22. elapsed time=2.66858s. cumulative_compaction_policy=SIZE_BASED.</span><br></pre></td></tr></table></figure>\n\n<p>通过日志时间可以判断 Compaction 是否在持续正确的执行，通过 elapsed time 可以观察每个任务的执行时间。</p>\n<p>我们还可以执行以下命令展示最近100个 compaction 任务的配额（permits）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;permits&quot;</span> <span class=\"built_in\">log</span>/be.INFO |<span class=\"built_in\">tail</span> -n 100</span></span><br><span class=\"line\"></span><br><span class=\"line\">I0505 17:04:07.120920   667 compaction.cpp:83] start cumulative compaction. tablet=106827970.777011641.9c474de1b8ba9199-4addeb135d6834ac, output_version=2-2623, permits: 39</span><br><span class=\"line\">I0505 17:04:13.898777   672 compaction.cpp:83] start cumulative compaction. tablet=106822777.1948936074.a44ac9462e79b76d-4a33ee39559bb0bf, output_version=2-3238, permits: 22</span><br></pre></td></tr></table></figure>\n\n<p>配额和版本数量成正比。</p>\n<p>我们可以找到 permits 较大的一个任务对应的 tablet id，如上图permit 为 39 的任务的 tablet id 为 106827970，然后继续分析这个 tablet 的 compaction 情况。</p>\n<p>通过 MySQL 客户端连接 Doris 集群后，执行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">mysql&gt; </span><span class=\"language-bash\">show tablet 106827970;</span></span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br><span class=\"line\">| DbName                   | TableName | PartitionName | IndexName | DbId    | TableId  | PartitionId | IndexId  | IsSync | DetailCmd                                                                  |</span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br><span class=\"line\">| default_cluster:test     | tbl1      | p20210505     | tbl1      | 3828954 | 63708800 | 106826829   | 63709761 | true   | SHOW PROC &#x27;/dbs/3828954/63708800/partitions/106826829/63709761/106827970&#x27;; |</span><br><span class=\"line\">+--------------------------+-----------+---------------+-----------+---------+----------+-------------+----------+--------+----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>\n\n<p>然后执行后面的 SHOW PROC 语句，我们可以获得这个 tablet 所有副本的详细信息。其中 VersionCount 列表示对应副本的数据版本数量。我们可以选取一个 VersionCount 较大的副本，在浏览器打开 CompactionStatus 列显示的 URL，得到如下Json结果：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;cumulative policy type&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;SIZE_BASED&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;cumulative point&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">18438</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last cumulative failure time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;1970-01-01 08:00:00.000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last base failure time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;1970-01-01 08:00:00.000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last cumulative success time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2021-05-05 17:18:48.904&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;last base success time&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2021-05-05 16:14:49.786&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;rowsets&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[0-17444] 13 DATA NONOVERLAPPING 0200000000b1fb8d344f83103113563dd81740036795499d 2.86 GB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[17445-17751] 1 DATA NONOVERLAPPING 0200000000b25183344f83103113563dd81740036795499d 68.61 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[17752-18089] 1 DATA NONOVERLAPPING 0200000000b2b9a2344f83103113563dd81740036795499d 74.52 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18090-18437] 1 DATA NONOVERLAPPING 0200000000b32686344f83103113563dd81740036795499d 76.41 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18438-18678] 1 DATA NONOVERLAPPING 0200000000b37084344f83103113563dd81740036795499d 53.07 MB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18679-18679] 1 DATA NONOVERLAPPING 0200000000b36d87344f83103113563dd81740036795499d 3.11 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18680-18680] 1 DATA NONOVERLAPPING 0200000000b36d70344f83103113563dd81740036795499d 258.40 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"string\">&quot;[18681-18681] 1 DATA NONOVERLAPPING 0200000000b36da0344f83103113563dd81740036795499d 266.98 KB&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;stale_rowsets&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;stale version path&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">    <span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这里我们可以看到一个 tablet 的 Cumulative Point，最近一次成功、失败的 BC&#x2F;CC 任务时间，以及每个 rowset 的版本信息。如上面这个示例，我们可以得出以下结论：</p>\n<blockquote>\n<ol>\n<li><p>基线数据量大约在2-3GB，增量rowset增长到几十MB后就会晋升到BC任务区。</p>\n</li>\n<li><p>新增rowset数据量很小，且版本增长较快，说明这是一个高频小批量的导入场景。</p>\n</li>\n</ol>\n</blockquote>\n<p>我们还可以进一步的通过以下命令分析指定 tablet id 的日志</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查看 tablet 48062815 最近十个任务的配额情况</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep permits <span class=\"built_in\">log</span>/be.INFO |grep 48062815 |<span class=\"built_in\">tail</span> -n 10</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查看 tablet 48062815 最近十个执行完成的 compaction 任务</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">&gt; grep <span class=\"string\">&quot;succeed to do&quot;</span> <span class=\"built_in\">log</span>/be.INFO |grep 48062815 |<span class=\"built_in\">tail</span> -n 10</span></span><br></pre></td></tr></table></figure>\n\n<p>另外，我们还可以在浏览器打开以下 URL，查看一个 BE 节点当前正在执行的 compaction 任务：be_host:webserver_port&#x2F;api&#x2F;compaction&#x2F;run_status</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;CumulativeCompaction&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk2/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk1/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48061239&quot;</span></span><br><span class=\"line\">        <span class=\"punctuation\">]</span></span><br><span class=\"line\">    <span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;BaseCompaction&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk2/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">        <span class=\"attr\">&quot;/home/disk1/palo&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48062815&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">            <span class=\"string\">&quot;48061276&quot;</span></span><br><span class=\"line\">        <span class=\"punctuation\">]</span></span><br><span class=\"line\">    <span class=\"punctuation\">&#125;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这个接口可以看到每个磁盘上当前正在执行的 compaction 任务。</p>\n<p>通过以上一系列的分析，我们应该可以对系统的 Compaction 情况有以下判断：</p>\n<blockquote>\n<ol>\n<li><p>Compaction 任务的执行频率、每个任务大致的执行耗时。</p>\n</li>\n<li><p>指定节点数据版本数量的变化情况。</p>\n</li>\n<li><p>指定 tablet 数据版本的变化情况，以及 compaction 的频率。</p>\n</li>\n</ol>\n</blockquote>\n<p>这些结论将指导我们对 Compaction 进行调优。</p>\n<h3 id=\"修改-Compaction-线程数\"><a href=\"#修改-Compaction-线程数\" class=\"headerlink\" title=\"修改 Compaction 线程数\"></a>修改 Compaction 线程数</h3><p><strong>增加 Compaction 线程数是一个非常直接的加速 Compaction 的方法。</strong>但是更多的任务意味着更大的 IO 和 内存开销。尤其在机械磁盘上，因为随机读写问题，有时可能单线程串行执行的效率会高于多线程并行执行。Doris 默认配置为每块盘两个 Compaction 任务（这也是最小的合法配置），最多 10 个任务。如果磁盘数量多于 5，在内存允许的情况下，可以修改 max_compaction_threads 参数增加总任务数，以保证每块盘可以执行两个 Compaction 任务。</p>\n<p>对于机械磁盘，不建议增加每块盘的任务数。对于固态硬盘，可以考虑修改 compaction_task_num_per_disk 参数适当增加每块盘的任务数，如修改为 4。<strong>注意修改这个参数的同时可能还需同步修改 max_compaction_threads，使得 max_compaction_threads 大于等于 compaction_task_num_per_disk * 磁盘数量。</strong></p>\n<h3 id=\"优化单个-Compaction-任务逻辑\"><a href=\"#优化单个-Compaction-任务逻辑\" class=\"headerlink\" title=\"优化单个 Compaction 任务逻辑\"></a>优化单个 Compaction 任务逻辑</h3><p>这个优化方式比较复杂，我们尝试从几个场景出发来说明：</p>\n<h4 id=\"场景一：基线数据量大，Base-Compaction-任务执行时间长。\"><a href=\"#场景一：基线数据量大，Base-Compaction-任务执行时间长。\" class=\"headerlink\" title=\"场景一：基线数据量大，Base Compaction 任务执行时间长。\"></a><strong>场景一：基线数据量大，Base Compaction 任务执行时间长。</strong></h4><p>BC 任务执行时间长，意味着一个任务会长时间占用 Compaction 工作线程，从而导致其他 tablet 的 compaction 任务时间被挤占。如果是因为 0 号版本的基线数据量较大导致，则我们可以考虑尽量推迟增量rowset 晋升到 BC 任务区的时间。以下两个参数将影响这个逻辑：</p>\n<blockquote>\n<p>cumulative_size_based_promotion_ratio：默认 0.05，基线数据量乘以这个系数，即晋升阈值。可以调大这个系数来提高晋升阈值。</p>\n<p>cumulative_size_based_promotion_size_mbytes：默认 1024MB。如果增量rowset的数据量大于这个值，则会忽略第一个参数的阈值直接晋升。因此需要同时调整这个参数来提升晋升阈值。</p>\n</blockquote>\n<p>当然，提升晋升阈值，会导致单个 BC 任务需要处理更大的数据量，耗时更长，但是总体的数据量会减少。举个例子。基线数据大小为 1024GB，假设晋升阈值分别为 100MB 和 200MB。数据导入速度为 100MB&#x2F;分钟。每5个版本执行一次 BC。那么理论上在10分钟内，阈值为 100MB 时，BC 任务处理的总数据量为 （1024 + 100 * 5）* 2 &#x3D; 3048MB。阈值为 200MB 是，BC 任务处理的总数据量为 (1024 + 200 * 5) &#x3D; 2024 MB。</p>\n<h4 id=\"场景二：增量数据版本数量增长较快，Cumulative-Compaction-处理过多版本，耗时较长。\"><a href=\"#场景二：增量数据版本数量增长较快，Cumulative-Compaction-处理过多版本，耗时较长。\" class=\"headerlink\" title=\"场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。\"></a><strong>场景二：增量数据版本数量增长较快，Cumulative Compaction 处理过多版本，耗时较长。</strong></h4><p>max_cumulative_compaction_num_singleton_deltas 参数控制一个 CC 任务最多合并多少个数据版本，默认值为 1000。我们考虑这样一种场景：针对某一个 tablet，其数据版本的增长速度为 1个&#x2F;秒。而其 CC 任务的执行时间 + 调度时间是 1000秒（即单个 CC 任务的执行时间加上Compaction再一次调度到这个 tablet 的时间总和）。那么我们可能会看到这个 tablet 的版本数量在 1-1000之间浮动（这里我们忽略基线版本数量）。因为在下一次 CC 任务执行前的 1000 秒内，又会累积 1000 个版本。</p>\n<p>这种情况可能导致这个 tablet 的读取效率很不稳定。这时我们可以尝试调小 max_cumulative_compaction_num_singleton_deltas 这个参数，这样一个 CC 所要合并的版本数更少，执行时间更短，执行频率会更高。还是刚才这个场景，假设参数调整到500，而对应的 CC 任务的执行时间 + 调度时间也降低到 500，则理论上这个 tablet 的版本数量将会在 1-500 之间浮动，相比于之前，版本数量更稳定。</p>\n<p>当然这个只是理论数值，实际情况还要考虑任务的具体执行时间、调度情况等等。</p>\n<h2 id=\"手动-Compaction\"><a href=\"#手动-Compaction\" class=\"headerlink\" title=\"手动 Compaction\"></a>手动 Compaction</h2><p>某些情况下，自动 Compaction 策略可能无法选取到某些 tablet，这时我们可能需要通过 Compaction 接口来主动触发指定 tablet 的 Compaction。我们以 curl 命令举例：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X POST http://192.168.1.1:8040/api/compaction/run?tablet_id=106818600\\&amp;schema_hash=6979334\\&amp;compact_type=cumulative</span><br></pre></td></tr></table></figure>\n\n<p>这里我们指定 id 为 106818600，schema hash 为 6979334 的 tablet 进行 Cumulative Compaction（compact_type参数为 base 则触发 Base Compaction）。其中 schema hash 可以通过 SHOW TABLET tablet_id 命令得到的 SHOW PROC 命令获取。<br>如果提交成功，则会返回：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span><span class=\"attr\">&quot;status&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Success&quot;</span><span class=\"punctuation\">,</span> <span class=\"attr\">&quot;msg&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;compaction task is successfully triggered.&quot;</span><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>这是一个异步操作，命令只是提交compaction 任务，之后我们可以通过以下 API 来查看任务是否在运行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X GET http://192.168.1.1:8040/api/compaction/run_status?tablet_id=106818600\\&amp;schema_hash=6979334</span><br></pre></td></tr></table></figure>\n\n<p>返回结果：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;status&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;Success&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;run_status&quot;</span> <span class=\"punctuation\">:</span> <span class=\"literal\"><span class=\"keyword\">false</span></span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;msg&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;compaction task for this tablet is running&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;tablet_id&quot;</span> <span class=\"punctuation\">:</span> <span class=\"number\">106818600</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;schema_hash&quot;</span> <span class=\"punctuation\">:</span> <span class=\"number\">6979334</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">    <span class=\"attr\">&quot;compact_type&quot;</span> <span class=\"punctuation\">:</span> <span class=\"string\">&quot;cumulative&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>当然也可以直接查看 tablet 的版本情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">curl -X GET http://192.168.1.1:8040/api/compaction/show?tablet_id=106818600\\&amp;schema_hash=6979334</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h2><p>Compaction 策略是 Doris 比较复杂的一个数据处理逻辑，需要考虑的状态和情况非常多，因此也在不断完善中，最终希望能够自动的适配各种负载场景，减轻运维压力。</p>\n"},{"title":"Kyuubi-从入门到跑路","abbrlink":42607,"date":"2022-10-30T08:24:47.000Z","updated":"2022-10-30T08:24:47.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"description":null,"keywords":null,"_content":"\n> Kyuubi 将 Spark ThriftServer 的使用扩展为基于统一接口的多租户模型，并依靠多租户的概念与集群管理器交互，最终获得资源共享/隔离和数据安全的能力。Kyuubi Server 和 Engine 的松耦合架构大大提高了服务本身的并发性和服务稳定性。\n\n## What-Kyuubi是什么\n\nApache Kyuubi (Incubating)，一个分布式和多租户网关，用于在 Lakehouse 上提供 Serverless SQL。\n\n> 简单的来说Kyuubi就是一个SQL网关，用来将用户需要执行的SQL交给对应的计算引擎执行，如Spark、Flink等。作为一个优秀的网关，Kyuubi理所当然的实现了负载均衡、HA、多租户等功能。\n>\n> 正是这些功能，保证了Spark SQL可以真正的在企业内可用、好用、稳定的运行。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1667120616678-362b15b3-89ac-4b49-961f-71d1b0eeda4e.png)\n\n## Why-为什么需要Kyuubi\n\n- 当然是Spark Thrift Server不好用，甚至可以说在生产上不可用（不支持HA和多租户），Spark SQL无法大展拳脚，因此诞生了Kyuubi。\n\n## How\n\n ### How: Kyuubi on Spark最佳实践\n\n- spark-defaults.conf配置\n\n```yaml\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Default system properties included when running spark-submit.\n# This is useful for setting default environmental settings.\n\n# Example:\n# spark.master                     spark://master:7077\n# spark.eventLog.enabled           true\n# spark.eventLog.dir               hdfs://namenode:8021/directory\n# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n# spark.driver.memory              5g\n# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n\n\n## Spark on Yarn config\nspark.master=yarn\nspark.submit.deployMode=cluster\nspark.executor.cores=1\nspark.yarn.am.memory=512m\nspark.driver.memory=1g\nspark.driver.memoryOverheadFactor=0.10\nspark.executor.memory=1g\nspark.executor.memoryOverheadFactor=0.10\n\n## Spark DRA config\nspark.dynamicAllocation.enabled=true\n# false if perfer shuffle tracking than ESS\nspark.shuffle.service.enabled=true\n# 理想情况下，三者的大小关系应为minExecutors<= initialExecutors< maxExecutors\nspark.dynamicAllocation.initialExecutors=10\nspark.dynamicAllocation.minExecutors=10\nspark.dynamicAllocation.maxExecutors=500\n# adjust spark.dynamicAllocation.executorAllocationRatio a bit lower to reduce the number of executors w.r.t. full parallelism.\nspark.dynamicAllocation.executorAllocationRatio=0.5\n# If one executor reached the maximum idle timeout, it will be removed.\nspark.dynamicAllocation.executorIdleTimeout=60s\nspark.dynamicAllocation.cachedExecutorIdleTimeout=30min\n# true if perfer shuffle tracking than ESS\nspark.dynamicAllocation.shuffleTracking.enabled=false\nspark.dynamicAllocation.shuffleTracking.timeout=30min\n# 如果 DRA 发现有待处理的任务积压超过超时，将请求新的执行程序，由以下配置控制。\nspark.dynamicAllocation.schedulerBacklogTimeout=1s\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s\nspark.cleaner.periodicGC.interval=5min\n\n\n## Spark ESS config: DRA依赖于ESS，不过在Spark3后可以启用shuffleTracking后也可以启用DRA\n#  spark.shuffle.service.enabled=true   开启Spark ESS，前面已配置\nspark.shuffle.service.port=7337\nspark.shuffle.useOldFetchProtocol=true\n\n\n## Spark AQE config\nspark.sql.adaptive.enabled=true\nspark.sql.adaptive.forceApply=false\nspark.sql.adaptive.logLevel=info\n# 如果我们用HDFS读写数据，匹配HDFS的块大小应该是最好的选择，即128MB或256MB。\nspark.sql.adaptive.advisoryPartitionSizeInBytes=256m\nspark.sql.adaptive.coalescePartitions.enabled=true\nspark.sql.adaptive.coalescePartitions.minPartitionNum=1\n# 它代表合并之前的洗牌分区的初始数量。最好明确设置它而不是回退到spark.sql.shuffle.partitions.\nspark.sql.adaptive.coalescePartitions.initialPartitionNum=8192\nspark.sql.adaptive.fetchShuffleBlocksInBatch=true\nspark.sql.adaptive.localShuffleReader.enabled=true\nspark.sql.adaptive.skewJoin.enabled=true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor=5\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m\nspark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2\nspark.sql.adaptive.optimizer.excludedRules\nspark.sql.autoBroadcastJoinThreshold=-1\n\n\n## Spark Doc: Tuning Guide\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.yarn.jars=hdfs://hadoop122:9000/spark-yarn/jars/*.jar\n# TODO-Push-based shuffle overview待启用\n```\n\n- kyuubi-defaults.conf配置示例 \n\n\n```yaml\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n## Kyuubi Configurations\n\n#\n# kyuubi.authentication           NONE\n# kyuubi.frontend.bind.host       localhost\n# kyuubi.frontend.bind.port       10009\n#\n\n# Details in https://kyuubi.apache.org/docs/latest/deployment/settings.html\n\n\n# For a user named kent\n___kent___.spark.master=yarn\n___kent___.spark.sql.adaptive.enabled=false\n# hudi conf\n___kent___.spark.serializer=org.apache.spark.serializer.KryoSerializer\n___kent___.spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog\n___kent___.spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n\n\n# For a user named flink\n___flink___.kyuubi.engine.type=FLINK_SQL\n\n\n# For a user named bob\n___bob___.spark.master=spark://master:7077\n___bob___.spark.executor.memory=8g\n\n\n# Fpr a user named doris: doris conf\n___doris___.kyuubi.engine.jdbc.connection.url=jdbc:mysql://xxx:xxx\n___doris___.kyuubi.engine.jdbc.connection.user=***\n___doris___.engine.jdbc.connection.password=***\n___doris___.engine.jdbc.type=doris\n___doris___.engine.jdbc.driver.class=com.mysql.cj.jdbc.Driver\n___doris___.engine.type=jdbc\n```\n\n## Extension\n\n### 基于MySQL自定义认证\n\n```scala\npackage cn.jxau\n\nimport org.apache.kyuubi.service.authentication.PasswdAuthenticationProvider\n\nimport java.sql.{Connection, DriverManager}\nimport javax.security.sasl.AuthenticationException\n\nclass SimpleAuthenticationProvider extends PasswdAuthenticationProvider {\n\n  override def authenticate(user: String, password: String): Unit = {\n\n    val pwd: String = ConnectionFactory().authById(user)\n\n    if (pwd.equals(\"\"))\n      throw new AuthenticationException(s\"auth fail, no user\")\n    else if (!pwd.equals(password))\n      throw new AuthenticationException(s\"auth fail, pwd wrong\")\n  }\n\n}\n\ncase class ConnectionFactory() {\n\n  val database = \"test\"\n  val table = \"tb_score\"\n\n  // 访问本地MySQL服务器，通过3306端口访问mysql数据库\n  val url = s\"jdbc:mysql://172.29.130.156:3306/$database?useUnicode=true&characterEncoding=utf-8&useSSL=false\"\n  //驱动名称\n  val driver = \"com.mysql.cj.jdbc.Driver\"\n\n  //用户名\n  val username = \"root\"\n  //密码\n  val password = \"1234\"\n  //初始化数据连接\n  var connection: Connection = _\n\n  def authById(id: String): String ={\n    var pwd = \"\"\n\n    try {\n      //注册Driver\n      Class.forName(driver)\n      //得到连接\n      connection = DriverManager.getConnection(url, username, password)\n      val statement = connection.createStatement\n\n      //执行查询语句，并返回结果\n      val rs = statement.executeQuery(s\"SELECT subject FROM $table WHERE userid = $id\")\n\n      //打印返回结果\n      while (rs.next) {\n        pwd = rs.getString(\"subject\")\n      }\n\n      pwd match {\n        case \"\" => \"\"\n        case _ => pwd\n      }\n\n    } catch {\n      case exception: Exception => {\n        exception.printStackTrace()\n        throw exception\n      }\n    }finally {\n      if (connection != null){\n        connection.close()\n      }\n    }\n  }\n\n  def apply(): ConnectionFactory = ConnectionFactory()\n\n}\n```\n\n","source":"_posts/bigdata/Kyuubi-从入门到跑路.md","raw":"---\ntitle: Kyuubi-从入门到跑路\ntags:\n  - Kyuubi\ncategories:\n  - - bigdata\n    - Kyuubi\nabbrlink: 42607\ndate: 2022-10-30 16:24:47\nupdated: 2022-10-30 16:24:47\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n> Kyuubi 将 Spark ThriftServer 的使用扩展为基于统一接口的多租户模型，并依靠多租户的概念与集群管理器交互，最终获得资源共享/隔离和数据安全的能力。Kyuubi Server 和 Engine 的松耦合架构大大提高了服务本身的并发性和服务稳定性。\n\n## What-Kyuubi是什么\n\nApache Kyuubi (Incubating)，一个分布式和多租户网关，用于在 Lakehouse 上提供 Serverless SQL。\n\n> 简单的来说Kyuubi就是一个SQL网关，用来将用户需要执行的SQL交给对应的计算引擎执行，如Spark、Flink等。作为一个优秀的网关，Kyuubi理所当然的实现了负载均衡、HA、多租户等功能。\n>\n> 正是这些功能，保证了Spark SQL可以真正的在企业内可用、好用、稳定的运行。\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1667120616678-362b15b3-89ac-4b49-961f-71d1b0eeda4e.png)\n\n## Why-为什么需要Kyuubi\n\n- 当然是Spark Thrift Server不好用，甚至可以说在生产上不可用（不支持HA和多租户），Spark SQL无法大展拳脚，因此诞生了Kyuubi。\n\n## How\n\n ### How: Kyuubi on Spark最佳实践\n\n- spark-defaults.conf配置\n\n```yaml\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Default system properties included when running spark-submit.\n# This is useful for setting default environmental settings.\n\n# Example:\n# spark.master                     spark://master:7077\n# spark.eventLog.enabled           true\n# spark.eventLog.dir               hdfs://namenode:8021/directory\n# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n# spark.driver.memory              5g\n# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n\n\n## Spark on Yarn config\nspark.master=yarn\nspark.submit.deployMode=cluster\nspark.executor.cores=1\nspark.yarn.am.memory=512m\nspark.driver.memory=1g\nspark.driver.memoryOverheadFactor=0.10\nspark.executor.memory=1g\nspark.executor.memoryOverheadFactor=0.10\n\n## Spark DRA config\nspark.dynamicAllocation.enabled=true\n# false if perfer shuffle tracking than ESS\nspark.shuffle.service.enabled=true\n# 理想情况下，三者的大小关系应为minExecutors<= initialExecutors< maxExecutors\nspark.dynamicAllocation.initialExecutors=10\nspark.dynamicAllocation.minExecutors=10\nspark.dynamicAllocation.maxExecutors=500\n# adjust spark.dynamicAllocation.executorAllocationRatio a bit lower to reduce the number of executors w.r.t. full parallelism.\nspark.dynamicAllocation.executorAllocationRatio=0.5\n# If one executor reached the maximum idle timeout, it will be removed.\nspark.dynamicAllocation.executorIdleTimeout=60s\nspark.dynamicAllocation.cachedExecutorIdleTimeout=30min\n# true if perfer shuffle tracking than ESS\nspark.dynamicAllocation.shuffleTracking.enabled=false\nspark.dynamicAllocation.shuffleTracking.timeout=30min\n# 如果 DRA 发现有待处理的任务积压超过超时，将请求新的执行程序，由以下配置控制。\nspark.dynamicAllocation.schedulerBacklogTimeout=1s\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s\nspark.cleaner.periodicGC.interval=5min\n\n\n## Spark ESS config: DRA依赖于ESS，不过在Spark3后可以启用shuffleTracking后也可以启用DRA\n#  spark.shuffle.service.enabled=true   开启Spark ESS，前面已配置\nspark.shuffle.service.port=7337\nspark.shuffle.useOldFetchProtocol=true\n\n\n## Spark AQE config\nspark.sql.adaptive.enabled=true\nspark.sql.adaptive.forceApply=false\nspark.sql.adaptive.logLevel=info\n# 如果我们用HDFS读写数据，匹配HDFS的块大小应该是最好的选择，即128MB或256MB。\nspark.sql.adaptive.advisoryPartitionSizeInBytes=256m\nspark.sql.adaptive.coalescePartitions.enabled=true\nspark.sql.adaptive.coalescePartitions.minPartitionNum=1\n# 它代表合并之前的洗牌分区的初始数量。最好明确设置它而不是回退到spark.sql.shuffle.partitions.\nspark.sql.adaptive.coalescePartitions.initialPartitionNum=8192\nspark.sql.adaptive.fetchShuffleBlocksInBatch=true\nspark.sql.adaptive.localShuffleReader.enabled=true\nspark.sql.adaptive.skewJoin.enabled=true\nspark.sql.adaptive.skewJoin.skewedPartitionFactor=5\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m\nspark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2\nspark.sql.adaptive.optimizer.excludedRules\nspark.sql.autoBroadcastJoinThreshold=-1\n\n\n## Spark Doc: Tuning Guide\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.yarn.jars=hdfs://hadoop122:9000/spark-yarn/jars/*.jar\n# TODO-Push-based shuffle overview待启用\n```\n\n- kyuubi-defaults.conf配置示例 \n\n\n```yaml\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n## Kyuubi Configurations\n\n#\n# kyuubi.authentication           NONE\n# kyuubi.frontend.bind.host       localhost\n# kyuubi.frontend.bind.port       10009\n#\n\n# Details in https://kyuubi.apache.org/docs/latest/deployment/settings.html\n\n\n# For a user named kent\n___kent___.spark.master=yarn\n___kent___.spark.sql.adaptive.enabled=false\n# hudi conf\n___kent___.spark.serializer=org.apache.spark.serializer.KryoSerializer\n___kent___.spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog\n___kent___.spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n\n\n# For a user named flink\n___flink___.kyuubi.engine.type=FLINK_SQL\n\n\n# For a user named bob\n___bob___.spark.master=spark://master:7077\n___bob___.spark.executor.memory=8g\n\n\n# Fpr a user named doris: doris conf\n___doris___.kyuubi.engine.jdbc.connection.url=jdbc:mysql://xxx:xxx\n___doris___.kyuubi.engine.jdbc.connection.user=***\n___doris___.engine.jdbc.connection.password=***\n___doris___.engine.jdbc.type=doris\n___doris___.engine.jdbc.driver.class=com.mysql.cj.jdbc.Driver\n___doris___.engine.type=jdbc\n```\n\n## Extension\n\n### 基于MySQL自定义认证\n\n```scala\npackage cn.jxau\n\nimport org.apache.kyuubi.service.authentication.PasswdAuthenticationProvider\n\nimport java.sql.{Connection, DriverManager}\nimport javax.security.sasl.AuthenticationException\n\nclass SimpleAuthenticationProvider extends PasswdAuthenticationProvider {\n\n  override def authenticate(user: String, password: String): Unit = {\n\n    val pwd: String = ConnectionFactory().authById(user)\n\n    if (pwd.equals(\"\"))\n      throw new AuthenticationException(s\"auth fail, no user\")\n    else if (!pwd.equals(password))\n      throw new AuthenticationException(s\"auth fail, pwd wrong\")\n  }\n\n}\n\ncase class ConnectionFactory() {\n\n  val database = \"test\"\n  val table = \"tb_score\"\n\n  // 访问本地MySQL服务器，通过3306端口访问mysql数据库\n  val url = s\"jdbc:mysql://172.29.130.156:3306/$database?useUnicode=true&characterEncoding=utf-8&useSSL=false\"\n  //驱动名称\n  val driver = \"com.mysql.cj.jdbc.Driver\"\n\n  //用户名\n  val username = \"root\"\n  //密码\n  val password = \"1234\"\n  //初始化数据连接\n  var connection: Connection = _\n\n  def authById(id: String): String ={\n    var pwd = \"\"\n\n    try {\n      //注册Driver\n      Class.forName(driver)\n      //得到连接\n      connection = DriverManager.getConnection(url, username, password)\n      val statement = connection.createStatement\n\n      //执行查询语句，并返回结果\n      val rs = statement.executeQuery(s\"SELECT subject FROM $table WHERE userid = $id\")\n\n      //打印返回结果\n      while (rs.next) {\n        pwd = rs.getString(\"subject\")\n      }\n\n      pwd match {\n        case \"\" => \"\"\n        case _ => pwd\n      }\n\n    } catch {\n      case exception: Exception => {\n        exception.printStackTrace()\n        throw exception\n      }\n    }finally {\n      if (connection != null){\n        connection.close()\n      }\n    }\n  }\n\n  def apply(): ConnectionFactory = ConnectionFactory()\n\n}\n```\n\n","slug":"bigdata/Kyuubi-从入门到跑路","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksy001o8j5m0cpn4c0k","content":"<blockquote>\n<p>Kyuubi 将 Spark ThriftServer 的使用扩展为基于统一接口的多租户模型，并依靠多租户的概念与集群管理器交互，最终获得资源共享&#x2F;隔离和数据安全的能力。Kyuubi Server 和 Engine 的松耦合架构大大提高了服务本身的并发性和服务稳定性。</p>\n</blockquote>\n<h2 id=\"What-Kyuubi是什么\"><a href=\"#What-Kyuubi是什么\" class=\"headerlink\" title=\"What-Kyuubi是什么\"></a>What-Kyuubi是什么</h2><p>Apache Kyuubi (Incubating)，一个分布式和多租户网关，用于在 Lakehouse 上提供 Serverless SQL。</p>\n<blockquote>\n<p>简单的来说Kyuubi就是一个SQL网关，用来将用户需要执行的SQL交给对应的计算引擎执行，如Spark、Flink等。作为一个优秀的网关，Kyuubi理所当然的实现了负载均衡、HA、多租户等功能。</p>\n<p>正是这些功能，保证了Spark SQL可以真正的在企业内可用、好用、稳定的运行。</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1667120616678-362b15b3-89ac-4b49-961f-71d1b0eeda4e.png\" alt=\"image.png\"></p>\n<h2 id=\"Why-为什么需要Kyuubi\"><a href=\"#Why-为什么需要Kyuubi\" class=\"headerlink\" title=\"Why-为什么需要Kyuubi\"></a>Why-为什么需要Kyuubi</h2><ul>\n<li>当然是Spark Thrift Server不好用，甚至可以说在生产上不可用（不支持HA和多租户），Spark SQL无法大展拳脚，因此诞生了Kyuubi。</li>\n</ul>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><h3 id=\"How-Kyuubi-on-Spark最佳实践\"><a href=\"#How-Kyuubi-on-Spark最佳实践\" class=\"headerlink\" title=\"How: Kyuubi on Spark最佳实践\"></a>How: Kyuubi on Spark最佳实践</h3><ul>\n<li>spark-defaults.conf配置</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"># this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"># the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"># See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"># limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Default system properties included when running spark-submit.</span></span><br><span class=\"line\"><span class=\"comment\"># This is useful for setting default environmental settings.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Example:</span></span><br><span class=\"line\"><span class=\"comment\"># spark.master                     spark://master:7077</span></span><br><span class=\"line\"><span class=\"comment\"># spark.eventLog.enabled           true</span></span><br><span class=\"line\"><span class=\"comment\"># spark.eventLog.dir               hdfs://namenode:8021/directory</span></span><br><span class=\"line\"><span class=\"comment\"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"comment\"># spark.driver.memory              5g</span></span><br><span class=\"line\"><span class=\"comment\"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark on Yarn config</span></span><br><span class=\"line\"><span class=\"string\">spark.master=yarn</span></span><br><span class=\"line\"><span class=\"string\">spark.submit.deployMode=cluster</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.cores=1</span></span><br><span class=\"line\"><span class=\"string\">spark.yarn.am.memory=512m</span></span><br><span class=\"line\"><span class=\"string\">spark.driver.memory=1g</span></span><br><span class=\"line\"><span class=\"string\">spark.driver.memoryOverheadFactor=0.10</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.memory=1g</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.memoryOverheadFactor=0.10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark DRA config</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.enabled=true</span></span><br><span class=\"line\"><span class=\"comment\"># false if perfer shuffle tracking than ESS</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.enabled=true</span></span><br><span class=\"line\"><span class=\"comment\"># 理想情况下，三者的大小关系应为minExecutors&lt;= initialExecutors&lt; maxExecutors</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.initialExecutors=10</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.minExecutors=10</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.maxExecutors=500</span></span><br><span class=\"line\"><span class=\"comment\"># adjust spark.dynamicAllocation.executorAllocationRatio a bit lower to reduce the number of executors w.r.t. full parallelism.</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.executorAllocationRatio=0.5</span></span><br><span class=\"line\"><span class=\"comment\"># If one executor reached the maximum idle timeout, it will be removed.</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.executorIdleTimeout=60s</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.cachedExecutorIdleTimeout=30min</span></span><br><span class=\"line\"><span class=\"comment\"># true if perfer shuffle tracking than ESS</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.shuffleTracking.enabled=false</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.shuffleTracking.timeout=30min</span></span><br><span class=\"line\"><span class=\"comment\"># 如果 DRA 发现有待处理的任务积压超过超时，将请求新的执行程序，由以下配置控制。</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.schedulerBacklogTimeout=1s</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s</span></span><br><span class=\"line\"><span class=\"string\">spark.cleaner.periodicGC.interval=5min</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark ESS config: DRA依赖于ESS，不过在Spark3后可以启用shuffleTracking后也可以启用DRA</span></span><br><span class=\"line\"><span class=\"comment\">#  spark.shuffle.service.enabled=true   开启Spark ESS，前面已配置</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.port=7337</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.useOldFetchProtocol=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark AQE config</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.forceApply=false</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.logLevel=info</span></span><br><span class=\"line\"><span class=\"comment\"># 如果我们用HDFS读写数据，匹配HDFS的块大小应该是最好的选择，即128MB或256MB。</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.advisoryPartitionSizeInBytes=256m</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.minPartitionNum=1</span></span><br><span class=\"line\"><span class=\"comment\"># 它代表合并之前的洗牌分区的初始数量。最好明确设置它而不是回退到spark.sql.shuffle.partitions.</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.initialPartitionNum=8192</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.fetchShuffleBlocksInBatch=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.localShuffleReader.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.skewedPartitionFactor=5</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.optimizer.excludedRules</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.autoBroadcastJoinThreshold=-1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark Doc: Tuning Guide</span></span><br><span class=\"line\"><span class=\"string\">spark.serializer=org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"string\">spark.yarn.jars=hdfs://hadoop122:9000/spark-yarn/jars/*.jar</span></span><br><span class=\"line\"><span class=\"comment\"># TODO-Push-based shuffle overview待启用</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>kyuubi-defaults.conf配置示例</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"># this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"># the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"># See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"># limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Kyuubi Configurations</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.authentication           NONE</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.frontend.bind.host       localhost</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.frontend.bind.port       10009</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Details in https://kyuubi.apache.org/docs/latest/deployment/settings.html</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named kent</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.master=yarn</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.adaptive.enabled=false</span></span><br><span class=\"line\"><span class=\"comment\"># hudi conf</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.serializer=org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named flink</span></span><br><span class=\"line\"><span class=\"string\">___flink___.kyuubi.engine.type=FLINK_SQL</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named bob</span></span><br><span class=\"line\"><span class=\"string\">___bob___.spark.master=spark://master:7077</span></span><br><span class=\"line\"><span class=\"string\">___bob___.spark.executor.memory=8g</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Fpr a user named doris: doris conf</span></span><br><span class=\"line\"><span class=\"string\">___doris___.kyuubi.engine.jdbc.connection.url=jdbc:mysql://xxx:xxx</span></span><br><span class=\"line\"><span class=\"string\">___doris___.kyuubi.engine.jdbc.connection.user=***</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.connection.password=***</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.type=doris</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.driver.class=com.mysql.cj.jdbc.Driver</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.type=jdbc</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Extension\"><a href=\"#Extension\" class=\"headerlink\" title=\"Extension\"></a>Extension</h2><h3 id=\"基于MySQL自定义认证\"><a href=\"#基于MySQL自定义认证\" class=\"headerlink\" title=\"基于MySQL自定义认证\"></a>基于MySQL自定义认证</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kyuubi.service.authentication.<span class=\"type\">PasswdAuthenticationProvider</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.&#123;<span class=\"type\">Connection</span>, <span class=\"type\">DriverManager</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.security.sasl.<span class=\"type\">AuthenticationException</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleAuthenticationProvider</span> <span class=\"keyword\">extends</span> <span class=\"title\">PasswdAuthenticationProvider</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">authenticate</span></span>(user: <span class=\"type\">String</span>, password: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> pwd: <span class=\"type\">String</span> = <span class=\"type\">ConnectionFactory</span>().authById(user)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (pwd.equals(<span class=\"string\">&quot;&quot;</span>))</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">AuthenticationException</span>(<span class=\"string\">s&quot;auth fail, no user&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!pwd.equals(password))</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">AuthenticationException</span>(<span class=\"string\">s&quot;auth fail, pwd wrong&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ConnectionFactory</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> database = <span class=\"string\">&quot;test&quot;</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> table = <span class=\"string\">&quot;tb_score&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 访问本地MySQL服务器，通过3306端口访问mysql数据库</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> url = <span class=\"string\">s&quot;jdbc:mysql://172.29.130.156:3306/<span class=\"subst\">$database</span>?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//驱动名称</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> driver = <span class=\"string\">&quot;com.mysql.cj.jdbc.Driver&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//用户名</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> username = <span class=\"string\">&quot;root&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//密码</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> password = <span class=\"string\">&quot;1234&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//初始化数据连接</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> connection: <span class=\"type\">Connection</span> = _</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">authById</span></span>(id: <span class=\"type\">String</span>): <span class=\"type\">String</span> =&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> pwd = <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">//注册Driver</span></span><br><span class=\"line\">      <span class=\"type\">Class</span>.forName(driver)</span><br><span class=\"line\">      <span class=\"comment\">//得到连接</span></span><br><span class=\"line\">      connection = <span class=\"type\">DriverManager</span>.getConnection(url, username, password)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> statement = connection.createStatement</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">//执行查询语句，并返回结果</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> rs = statement.executeQuery(<span class=\"string\">s&quot;SELECT subject FROM <span class=\"subst\">$table</span> WHERE userid = <span class=\"subst\">$id</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">//打印返回结果</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (rs.next) &#123;</span><br><span class=\"line\">        pwd = rs.getString(<span class=\"string\">&quot;subject&quot;</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      pwd <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&quot;&quot;</span> =&gt; <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">case</span> _ =&gt; pwd</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> exception: <span class=\"type\">Exception</span> =&gt; &#123;</span><br><span class=\"line\">        exception.printStackTrace()</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> exception</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (connection != <span class=\"literal\">null</span>)&#123;</span><br><span class=\"line\">        connection.close()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(): <span class=\"type\">ConnectionFactory</span> = <span class=\"type\">ConnectionFactory</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>Kyuubi 将 Spark ThriftServer 的使用扩展为基于统一接口的多租户模型，并依靠多租户的概念与集群管理器交互，最终获得资源共享&#x2F;隔离和数据安全的能力。Kyuubi Server 和 Engine 的松耦合架构大大提高了服务本身的并发性和服务稳定性。</p>\n</blockquote>\n<h2 id=\"What-Kyuubi是什么\"><a href=\"#What-Kyuubi是什么\" class=\"headerlink\" title=\"What-Kyuubi是什么\"></a>What-Kyuubi是什么</h2><p>Apache Kyuubi (Incubating)，一个分布式和多租户网关，用于在 Lakehouse 上提供 Serverless SQL。</p>\n<blockquote>\n<p>简单的来说Kyuubi就是一个SQL网关，用来将用户需要执行的SQL交给对应的计算引擎执行，如Spark、Flink等。作为一个优秀的网关，Kyuubi理所当然的实现了负载均衡、HA、多租户等功能。</p>\n<p>正是这些功能，保证了Spark SQL可以真正的在企业内可用、好用、稳定的运行。</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1667120616678-362b15b3-89ac-4b49-961f-71d1b0eeda4e.png\" alt=\"image.png\"></p>\n<h2 id=\"Why-为什么需要Kyuubi\"><a href=\"#Why-为什么需要Kyuubi\" class=\"headerlink\" title=\"Why-为什么需要Kyuubi\"></a>Why-为什么需要Kyuubi</h2><ul>\n<li>当然是Spark Thrift Server不好用，甚至可以说在生产上不可用（不支持HA和多租户），Spark SQL无法大展拳脚，因此诞生了Kyuubi。</li>\n</ul>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><h3 id=\"How-Kyuubi-on-Spark最佳实践\"><a href=\"#How-Kyuubi-on-Spark最佳实践\" class=\"headerlink\" title=\"How: Kyuubi on Spark最佳实践\"></a>How: Kyuubi on Spark最佳实践</h3><ul>\n<li>spark-defaults.conf配置</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"># this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"># the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"># See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"># limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Default system properties included when running spark-submit.</span></span><br><span class=\"line\"><span class=\"comment\"># This is useful for setting default environmental settings.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Example:</span></span><br><span class=\"line\"><span class=\"comment\"># spark.master                     spark://master:7077</span></span><br><span class=\"line\"><span class=\"comment\"># spark.eventLog.enabled           true</span></span><br><span class=\"line\"><span class=\"comment\"># spark.eventLog.dir               hdfs://namenode:8021/directory</span></span><br><span class=\"line\"><span class=\"comment\"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"comment\"># spark.driver.memory              5g</span></span><br><span class=\"line\"><span class=\"comment\"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark on Yarn config</span></span><br><span class=\"line\"><span class=\"string\">spark.master=yarn</span></span><br><span class=\"line\"><span class=\"string\">spark.submit.deployMode=cluster</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.cores=1</span></span><br><span class=\"line\"><span class=\"string\">spark.yarn.am.memory=512m</span></span><br><span class=\"line\"><span class=\"string\">spark.driver.memory=1g</span></span><br><span class=\"line\"><span class=\"string\">spark.driver.memoryOverheadFactor=0.10</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.memory=1g</span></span><br><span class=\"line\"><span class=\"string\">spark.executor.memoryOverheadFactor=0.10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark DRA config</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.enabled=true</span></span><br><span class=\"line\"><span class=\"comment\"># false if perfer shuffle tracking than ESS</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.enabled=true</span></span><br><span class=\"line\"><span class=\"comment\"># 理想情况下，三者的大小关系应为minExecutors&lt;= initialExecutors&lt; maxExecutors</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.initialExecutors=10</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.minExecutors=10</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.maxExecutors=500</span></span><br><span class=\"line\"><span class=\"comment\"># adjust spark.dynamicAllocation.executorAllocationRatio a bit lower to reduce the number of executors w.r.t. full parallelism.</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.executorAllocationRatio=0.5</span></span><br><span class=\"line\"><span class=\"comment\"># If one executor reached the maximum idle timeout, it will be removed.</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.executorIdleTimeout=60s</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.cachedExecutorIdleTimeout=30min</span></span><br><span class=\"line\"><span class=\"comment\"># true if perfer shuffle tracking than ESS</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.shuffleTracking.enabled=false</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.shuffleTracking.timeout=30min</span></span><br><span class=\"line\"><span class=\"comment\"># 如果 DRA 发现有待处理的任务积压超过超时，将请求新的执行程序，由以下配置控制。</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.schedulerBacklogTimeout=1s</span></span><br><span class=\"line\"><span class=\"string\">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s</span></span><br><span class=\"line\"><span class=\"string\">spark.cleaner.periodicGC.interval=5min</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark ESS config: DRA依赖于ESS，不过在Spark3后可以启用shuffleTracking后也可以启用DRA</span></span><br><span class=\"line\"><span class=\"comment\">#  spark.shuffle.service.enabled=true   开启Spark ESS，前面已配置</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.port=7337</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.useOldFetchProtocol=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark AQE config</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.forceApply=false</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.logLevel=info</span></span><br><span class=\"line\"><span class=\"comment\"># 如果我们用HDFS读写数据，匹配HDFS的块大小应该是最好的选择，即128MB或256MB。</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.advisoryPartitionSizeInBytes=256m</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.minPartitionNum=1</span></span><br><span class=\"line\"><span class=\"comment\"># 它代表合并之前的洗牌分区的初始数量。最好明确设置它而不是回退到spark.sql.shuffle.partitions.</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.initialPartitionNum=8192</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.fetchShuffleBlocksInBatch=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.localShuffleReader.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.skewedPartitionFactor=5</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.optimizer.excludedRules</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.autoBroadcastJoinThreshold=-1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Spark Doc: Tuning Guide</span></span><br><span class=\"line\"><span class=\"string\">spark.serializer=org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"string\">spark.yarn.jars=hdfs://hadoop122:9000/spark-yarn/jars/*.jar</span></span><br><span class=\"line\"><span class=\"comment\"># TODO-Push-based shuffle overview待启用</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>kyuubi-defaults.conf配置示例</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"># this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"># the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"># See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"># limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## Kyuubi Configurations</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.authentication           NONE</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.frontend.bind.host       localhost</span></span><br><span class=\"line\"><span class=\"comment\"># kyuubi.frontend.bind.port       10009</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Details in https://kyuubi.apache.org/docs/latest/deployment/settings.html</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named kent</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.master=yarn</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.adaptive.enabled=false</span></span><br><span class=\"line\"><span class=\"comment\"># hudi conf</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.serializer=org.apache.spark.serializer.KryoSerializer</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog</span></span><br><span class=\"line\"><span class=\"string\">___kent___.spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named flink</span></span><br><span class=\"line\"><span class=\"string\">___flink___.kyuubi.engine.type=FLINK_SQL</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For a user named bob</span></span><br><span class=\"line\"><span class=\"string\">___bob___.spark.master=spark://master:7077</span></span><br><span class=\"line\"><span class=\"string\">___bob___.spark.executor.memory=8g</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Fpr a user named doris: doris conf</span></span><br><span class=\"line\"><span class=\"string\">___doris___.kyuubi.engine.jdbc.connection.url=jdbc:mysql://xxx:xxx</span></span><br><span class=\"line\"><span class=\"string\">___doris___.kyuubi.engine.jdbc.connection.user=***</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.connection.password=***</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.type=doris</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.jdbc.driver.class=com.mysql.cj.jdbc.Driver</span></span><br><span class=\"line\"><span class=\"string\">___doris___.engine.type=jdbc</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Extension\"><a href=\"#Extension\" class=\"headerlink\" title=\"Extension\"></a>Extension</h2><h3 id=\"基于MySQL自定义认证\"><a href=\"#基于MySQL自定义认证\" class=\"headerlink\" title=\"基于MySQL自定义认证\"></a>基于MySQL自定义认证</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.kyuubi.service.authentication.<span class=\"type\">PasswdAuthenticationProvider</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.&#123;<span class=\"type\">Connection</span>, <span class=\"type\">DriverManager</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> javax.security.sasl.<span class=\"type\">AuthenticationException</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SimpleAuthenticationProvider</span> <span class=\"keyword\">extends</span> <span class=\"title\">PasswdAuthenticationProvider</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">authenticate</span></span>(user: <span class=\"type\">String</span>, password: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> pwd: <span class=\"type\">String</span> = <span class=\"type\">ConnectionFactory</span>().authById(user)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (pwd.equals(<span class=\"string\">&quot;&quot;</span>))</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">AuthenticationException</span>(<span class=\"string\">s&quot;auth fail, no user&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!pwd.equals(password))</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">AuthenticationException</span>(<span class=\"string\">s&quot;auth fail, pwd wrong&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ConnectionFactory</span>(<span class=\"params\"></span>) </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> database = <span class=\"string\">&quot;test&quot;</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> table = <span class=\"string\">&quot;tb_score&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// 访问本地MySQL服务器，通过3306端口访问mysql数据库</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> url = <span class=\"string\">s&quot;jdbc:mysql://172.29.130.156:3306/<span class=\"subst\">$database</span>?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//驱动名称</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> driver = <span class=\"string\">&quot;com.mysql.cj.jdbc.Driver&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//用户名</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> username = <span class=\"string\">&quot;root&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//密码</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> password = <span class=\"string\">&quot;1234&quot;</span></span><br><span class=\"line\">  <span class=\"comment\">//初始化数据连接</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> connection: <span class=\"type\">Connection</span> = _</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">authById</span></span>(id: <span class=\"type\">String</span>): <span class=\"type\">String</span> =&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> pwd = <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">//注册Driver</span></span><br><span class=\"line\">      <span class=\"type\">Class</span>.forName(driver)</span><br><span class=\"line\">      <span class=\"comment\">//得到连接</span></span><br><span class=\"line\">      connection = <span class=\"type\">DriverManager</span>.getConnection(url, username, password)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> statement = connection.createStatement</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">//执行查询语句，并返回结果</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> rs = statement.executeQuery(<span class=\"string\">s&quot;SELECT subject FROM <span class=\"subst\">$table</span> WHERE userid = <span class=\"subst\">$id</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">//打印返回结果</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (rs.next) &#123;</span><br><span class=\"line\">        pwd = rs.getString(<span class=\"string\">&quot;subject&quot;</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      pwd <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&quot;&quot;</span> =&gt; <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">case</span> _ =&gt; pwd</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> exception: <span class=\"type\">Exception</span> =&gt; &#123;</span><br><span class=\"line\">        exception.printStackTrace()</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> exception</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (connection != <span class=\"literal\">null</span>)&#123;</span><br><span class=\"line\">        connection.close()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(): <span class=\"type\">ConnectionFactory</span> = <span class=\"type\">ConnectionFactory</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Doris性能优化（一）","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":29811,"date":"2022-08-21T03:36:49.000Z","updated":"2022-08-21T03:36:49.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661053317299-dab15028-4e02-434a-9ba5-46139e3dd653.png)\n\n## 优化的两个基本原则\n\n ![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059787313-f1d0fe57-5d60-46f1-ac16-c43dcbced951.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059855537-2648b01f-7178-4a29-90df-67abf615a084.png)\n\n## Runtime Filter\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059946818-7c38a11c-6224-4a72-9cda-e9ac0b7efc03.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059996873-d7ed6ddc-e561-4f6e-a349-5934c237c4d7.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060032139-d4765d59-b2e2-4d03-b533-df3607280f9b.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060077007-f152de7c-3761-4779-8106-2a81c0c2a90e.png)\n","source":"_posts/bigdata/doris性能优化（一）.md","raw":"---\ntitle: Doris性能优化（一）\ntags:\n  - Doris\ncategories:\n  - - bigdata\n    - Doris\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 29811\ndate: 2022-08-21 11:36:49\nupdated: 2022-08-21 11:36:49\ncover:\ndescription:\nkeywords:\n---\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661053317299-dab15028-4e02-434a-9ba5-46139e3dd653.png)\n\n## 优化的两个基本原则\n\n ![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059787313-f1d0fe57-5d60-46f1-ac16-c43dcbced951.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059855537-2648b01f-7178-4a29-90df-67abf615a084.png)\n\n## Runtime Filter\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059946818-7c38a11c-6224-4a72-9cda-e9ac0b7efc03.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059996873-d7ed6ddc-e561-4f6e-a349-5934c237c4d7.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060032139-d4765d59-b2e2-4d03-b533-df3607280f9b.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060077007-f152de7c-3761-4779-8106-2a81c0c2a90e.png)\n","slug":"bigdata/doris性能优化（一）","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksy001q8j5mcpeb51st","content":"<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661053317299-dab15028-4e02-434a-9ba5-46139e3dd653.png\" alt=\"img\"></p>\n<h2 id=\"优化的两个基本原则\"><a href=\"#优化的两个基本原则\" class=\"headerlink\" title=\"优化的两个基本原则\"></a>优化的两个基本原则</h2><p> <img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059787313-f1d0fe57-5d60-46f1-ac16-c43dcbced951.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059855537-2648b01f-7178-4a29-90df-67abf615a084.png\" alt=\"img\"></p>\n<h2 id=\"Runtime-Filter\"><a href=\"#Runtime-Filter\" class=\"headerlink\" title=\"Runtime Filter\"></a>Runtime Filter</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059946818-7c38a11c-6224-4a72-9cda-e9ac0b7efc03.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059996873-d7ed6ddc-e561-4f6e-a349-5934c237c4d7.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060032139-d4765d59-b2e2-4d03-b533-df3607280f9b.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060077007-f152de7c-3761-4779-8106-2a81c0c2a90e.png\" alt=\"img\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661053317299-dab15028-4e02-434a-9ba5-46139e3dd653.png\" alt=\"img\"></p>\n<h2 id=\"优化的两个基本原则\"><a href=\"#优化的两个基本原则\" class=\"headerlink\" title=\"优化的两个基本原则\"></a>优化的两个基本原则</h2><p> <img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059787313-f1d0fe57-5d60-46f1-ac16-c43dcbced951.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059855537-2648b01f-7178-4a29-90df-67abf615a084.png\" alt=\"img\"></p>\n<h2 id=\"Runtime-Filter\"><a href=\"#Runtime-Filter\" class=\"headerlink\" title=\"Runtime Filter\"></a>Runtime Filter</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059946818-7c38a11c-6224-4a72-9cda-e9ac0b7efc03.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661059996873-d7ed6ddc-e561-4f6e-a349-5934c237c4d7.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060032139-d4765d59-b2e2-4d03-b533-df3607280f9b.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661060077007-f152de7c-3761-4779-8106-2a81c0c2a90e.png\" alt=\"img\"></p>\n"},{"title":"Yarn容量调度器和公平调度器的异同","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":8819,"date":"2022-07-05T10:20:39.000Z","updated":"2022-07-05T10:20:39.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n> 理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。\n>\n> ## Capacity Scheduler(容量调度器)\n>\n> 对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。用这个资源调度器，就可以配置yarn资源队列，这个后面后介绍用到。\n>\n> ## Fair Scheduler(公平调度器)\n>\n> **Fair调度器的设计目标是为所有的应用分配公平的资源**（对公平的定义可以通过参数来设置）。当然，公平调度在也可以在多个队列间工作。\n>\n> 举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。\n>\n> **在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。**当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 \n>\n> a) 公平调度器，就是能够共享整个集群的资源 \n>\n> b) 不用预先占用资源，每一个作业都是共享的 \n>\n> c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源。\n\n# Yarn调度器和调度算法\n\n目前，Hadoop作业调度器主要有三种: FIFO、容量(Capacity Scheduler)和公平(Fair Scheduler)。\n\nApache Hadoop-1.x默认调度器是FIFO；\n\nApache hadoop-2.7.2之后默认调度器是容量调度器Capacity Scheduler\n\nApache hadoop-3.2.2默认调度器是公平调度器Fair Scheduler。\n\nCDH 框架默认调度器是 Fair Scheduler。\n\n## 一、容量调度器(Capacity Scheduler)\n\n### 1. 容量调度器特点\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019460282-1d05ddc1-06a3-4931-b8ac-96ededf60c3b.png)\n\n1. 多队列 ：每个队列可配置一定的资源量，每个队列内部采用FIFO调度策略；\n2. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；\n3. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；\n4. 多租户：\n\na. 支持多用户共享集群和多应用程序同时运行；\n\nb. 为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。\n\n### 2. 容量调度器资源分配算法\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018164262-f5e7e270-fc4c-41e8-ba92-cac2a508bcdb.png)\n\n## 二、公平调度器(Fair Scheduler)\n\n### 1. 公平调度器特点\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019558958-d7f020f1-230e-4855-92f8-11c473379a18.png)\n\n### 与容量调度器相同点:\n\n1. 多队列 ：每个队列可配置一定的资源量。\n2. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；\n3. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；\n4. 多租户：\n\n1. 1.  支持多用户共享集群和多应用程序同时运行；\n   2.  为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。\n\n### 与容量调度器不同点:\n\n#### 核心调度策略不同\n\n容量调度器： 优先选择资源利用率低的队列；\n\n公平调度器：优先选择对资源缺额比例大的。\n\n#### 每个队列可以单独设置资源分配方式\n\n**容量调度器：FIFO、DRF(内存+CPU)；**\n\n**公平调度器：FIFO、FAIR、DRF。**\n\n#### 队列任务平行度不同\n\n公平调度器：多队列，同一时间队列中多任务按照缺额执行，队列并行度大于队列个数\n\n容量调度器：多队列，同一时间队列中只有一个任务执行，队列中按照先进先出分配任务，队列并行度等于队列个数。\n\n## Fair策略\n\nFair策略是公平调度器默认的队列分配方式\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018698231-b7ca6c87-74ef-4128-8c05-89fbbab59881.png)\n","source":"_posts/bigdata/yarn公平和容量调度器的异同.md","raw":"---\ntitle: Yarn容量调度器和公平调度器的异同\ntags:\n  - yarn\n  - hadoop\ncategories:\n  - - bigdata\n    - yarn\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 8819\ndate: 2022-07-05 18:20:39\nupdated: 2022-07-05 18:20:39\ncover:\ndescription:\nkeywords:\n---\n\n> 理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。\n>\n> ## Capacity Scheduler(容量调度器)\n>\n> 对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。用这个资源调度器，就可以配置yarn资源队列，这个后面后介绍用到。\n>\n> ## Fair Scheduler(公平调度器)\n>\n> **Fair调度器的设计目标是为所有的应用分配公平的资源**（对公平的定义可以通过参数来设置）。当然，公平调度在也可以在多个队列间工作。\n>\n> 举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。\n>\n> **在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。**当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 \n>\n> a) 公平调度器，就是能够共享整个集群的资源 \n>\n> b) 不用预先占用资源，每一个作业都是共享的 \n>\n> c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源。\n\n# Yarn调度器和调度算法\n\n目前，Hadoop作业调度器主要有三种: FIFO、容量(Capacity Scheduler)和公平(Fair Scheduler)。\n\nApache Hadoop-1.x默认调度器是FIFO；\n\nApache hadoop-2.7.2之后默认调度器是容量调度器Capacity Scheduler\n\nApache hadoop-3.2.2默认调度器是公平调度器Fair Scheduler。\n\nCDH 框架默认调度器是 Fair Scheduler。\n\n## 一、容量调度器(Capacity Scheduler)\n\n### 1. 容量调度器特点\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019460282-1d05ddc1-06a3-4931-b8ac-96ededf60c3b.png)\n\n1. 多队列 ：每个队列可配置一定的资源量，每个队列内部采用FIFO调度策略；\n2. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；\n3. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；\n4. 多租户：\n\na. 支持多用户共享集群和多应用程序同时运行；\n\nb. 为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。\n\n### 2. 容量调度器资源分配算法\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018164262-f5e7e270-fc4c-41e8-ba92-cac2a508bcdb.png)\n\n## 二、公平调度器(Fair Scheduler)\n\n### 1. 公平调度器特点\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019558958-d7f020f1-230e-4855-92f8-11c473379a18.png)\n\n### 与容量调度器相同点:\n\n1. 多队列 ：每个队列可配置一定的资源量。\n2. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；\n3. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；\n4. 多租户：\n\n1. 1.  支持多用户共享集群和多应用程序同时运行；\n   2.  为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。\n\n### 与容量调度器不同点:\n\n#### 核心调度策略不同\n\n容量调度器： 优先选择资源利用率低的队列；\n\n公平调度器：优先选择对资源缺额比例大的。\n\n#### 每个队列可以单独设置资源分配方式\n\n**容量调度器：FIFO、DRF(内存+CPU)；**\n\n**公平调度器：FIFO、FAIR、DRF。**\n\n#### 队列任务平行度不同\n\n公平调度器：多队列，同一时间队列中多任务按照缺额执行，队列并行度大于队列个数\n\n容量调度器：多队列，同一时间队列中只有一个任务执行，队列中按照先进先出分配任务，队列并行度等于队列个数。\n\n## Fair策略\n\nFair策略是公平调度器默认的队列分配方式\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018698231-b7ca6c87-74ef-4128-8c05-89fbbab59881.png)\n","slug":"bigdata/yarn公平和容量调度器的异同","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksz001t8j5m3g309yc7","content":"<blockquote>\n<p>理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。</p>\n<h2 id=\"Capacity-Scheduler-容量调度器\"><a href=\"#Capacity-Scheduler-容量调度器\" class=\"headerlink\" title=\"Capacity Scheduler(容量调度器)\"></a>Capacity Scheduler(容量调度器)</h2><p>对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。用这个资源调度器，就可以配置yarn资源队列，这个后面后介绍用到。</p>\n<h2 id=\"Fair-Scheduler-公平调度器\"><a href=\"#Fair-Scheduler-公平调度器\" class=\"headerlink\" title=\"Fair Scheduler(公平调度器)\"></a>Fair Scheduler(公平调度器)</h2><p><strong>Fair调度器的设计目标是为所有的应用分配公平的资源</strong>（对公平的定义可以通过参数来设置）。当然，公平调度在也可以在多个队列间工作。</p>\n<p>举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。</p>\n<p><strong>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。</strong>当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 </p>\n<p>a) 公平调度器，就是能够共享整个集群的资源 </p>\n<p>b) 不用预先占用资源，每一个作业都是共享的 </p>\n<p>c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源。</p>\n</blockquote>\n<h1 id=\"Yarn调度器和调度算法\"><a href=\"#Yarn调度器和调度算法\" class=\"headerlink\" title=\"Yarn调度器和调度算法\"></a>Yarn调度器和调度算法</h1><p>目前，Hadoop作业调度器主要有三种: FIFO、容量(Capacity Scheduler)和公平(Fair Scheduler)。</p>\n<p>Apache Hadoop-1.x默认调度器是FIFO；</p>\n<p>Apache hadoop-2.7.2之后默认调度器是容量调度器Capacity Scheduler</p>\n<p>Apache hadoop-3.2.2默认调度器是公平调度器Fair Scheduler。</p>\n<p>CDH 框架默认调度器是 Fair Scheduler。</p>\n<h2 id=\"一、容量调度器-Capacity-Scheduler\"><a href=\"#一、容量调度器-Capacity-Scheduler\" class=\"headerlink\" title=\"一、容量调度器(Capacity Scheduler)\"></a>一、容量调度器(Capacity Scheduler)</h2><h3 id=\"1-容量调度器特点\"><a href=\"#1-容量调度器特点\" class=\"headerlink\" title=\"1. 容量调度器特点\"></a>1. 容量调度器特点</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019460282-1d05ddc1-06a3-4931-b8ac-96ededf60c3b.png\" alt=\"img\"></p>\n<ol>\n<li>多队列 ：每个队列可配置一定的资源量，每个队列内部采用FIFO调度策略；</li>\n<li>容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；</li>\n<li>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；</li>\n<li>多租户：</li>\n</ol>\n<p>a. 支持多用户共享集群和多应用程序同时运行；</p>\n<p>b. 为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。</p>\n<h3 id=\"2-容量调度器资源分配算法\"><a href=\"#2-容量调度器资源分配算法\" class=\"headerlink\" title=\"2. 容量调度器资源分配算法\"></a>2. 容量调度器资源分配算法</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018164262-f5e7e270-fc4c-41e8-ba92-cac2a508bcdb.png\" alt=\"img\"></p>\n<h2 id=\"二、公平调度器-Fair-Scheduler\"><a href=\"#二、公平调度器-Fair-Scheduler\" class=\"headerlink\" title=\"二、公平调度器(Fair Scheduler)\"></a>二、公平调度器(Fair Scheduler)</h2><h3 id=\"1-公平调度器特点\"><a href=\"#1-公平调度器特点\" class=\"headerlink\" title=\"1. 公平调度器特点\"></a>1. 公平调度器特点</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019558958-d7f020f1-230e-4855-92f8-11c473379a18.png\" alt=\"img\"></p>\n<h3 id=\"与容量调度器相同点\"><a href=\"#与容量调度器相同点\" class=\"headerlink\" title=\"与容量调度器相同点:\"></a>与容量调度器相同点:</h3><ol>\n<li><p>多队列 ：每个队列可配置一定的资源量。</p>\n</li>\n<li><p>容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；</p>\n</li>\n<li><p>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；</p>\n</li>\n<li><p>多租户：</p>\n</li>\n<li><ol>\n<li>支持多用户共享集群和多应用程序同时运行；</li>\n<li>为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"与容量调度器不同点\"><a href=\"#与容量调度器不同点\" class=\"headerlink\" title=\"与容量调度器不同点:\"></a>与容量调度器不同点:</h3><h4 id=\"核心调度策略不同\"><a href=\"#核心调度策略不同\" class=\"headerlink\" title=\"核心调度策略不同\"></a>核心调度策略不同</h4><p>容量调度器： 优先选择资源利用率低的队列；</p>\n<p>公平调度器：优先选择对资源缺额比例大的。</p>\n<h4 id=\"每个队列可以单独设置资源分配方式\"><a href=\"#每个队列可以单独设置资源分配方式\" class=\"headerlink\" title=\"每个队列可以单独设置资源分配方式\"></a>每个队列可以单独设置资源分配方式</h4><p><strong>容量调度器：FIFO、DRF(内存+CPU)；</strong></p>\n<p><strong>公平调度器：FIFO、FAIR、DRF。</strong></p>\n<h4 id=\"队列任务平行度不同\"><a href=\"#队列任务平行度不同\" class=\"headerlink\" title=\"队列任务平行度不同\"></a>队列任务平行度不同</h4><p>公平调度器：多队列，同一时间队列中多任务按照缺额执行，队列并行度大于队列个数</p>\n<p>容量调度器：多队列，同一时间队列中只有一个任务执行，队列中按照先进先出分配任务，队列并行度等于队列个数。</p>\n<h2 id=\"Fair策略\"><a href=\"#Fair策略\" class=\"headerlink\" title=\"Fair策略\"></a>Fair策略</h2><p>Fair策略是公平调度器默认的队列分配方式</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018698231-b7ca6c87-74ef-4128-8c05-89fbbab59881.png\" alt=\"img\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。</p>\n<h2 id=\"Capacity-Scheduler-容量调度器\"><a href=\"#Capacity-Scheduler-容量调度器\" class=\"headerlink\" title=\"Capacity Scheduler(容量调度器)\"></a>Capacity Scheduler(容量调度器)</h2><p>对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。用这个资源调度器，就可以配置yarn资源队列，这个后面后介绍用到。</p>\n<h2 id=\"Fair-Scheduler-公平调度器\"><a href=\"#Fair-Scheduler-公平调度器\" class=\"headerlink\" title=\"Fair Scheduler(公平调度器)\"></a>Fair Scheduler(公平调度器)</h2><p><strong>Fair调度器的设计目标是为所有的应用分配公平的资源</strong>（对公平的定义可以通过参数来设置）。当然，公平调度在也可以在多个队列间工作。</p>\n<p>举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。</p>\n<p><strong>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。</strong>当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 </p>\n<p>a) 公平调度器，就是能够共享整个集群的资源 </p>\n<p>b) 不用预先占用资源，每一个作业都是共享的 </p>\n<p>c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源。</p>\n</blockquote>\n<h1 id=\"Yarn调度器和调度算法\"><a href=\"#Yarn调度器和调度算法\" class=\"headerlink\" title=\"Yarn调度器和调度算法\"></a>Yarn调度器和调度算法</h1><p>目前，Hadoop作业调度器主要有三种: FIFO、容量(Capacity Scheduler)和公平(Fair Scheduler)。</p>\n<p>Apache Hadoop-1.x默认调度器是FIFO；</p>\n<p>Apache hadoop-2.7.2之后默认调度器是容量调度器Capacity Scheduler</p>\n<p>Apache hadoop-3.2.2默认调度器是公平调度器Fair Scheduler。</p>\n<p>CDH 框架默认调度器是 Fair Scheduler。</p>\n<h2 id=\"一、容量调度器-Capacity-Scheduler\"><a href=\"#一、容量调度器-Capacity-Scheduler\" class=\"headerlink\" title=\"一、容量调度器(Capacity Scheduler)\"></a>一、容量调度器(Capacity Scheduler)</h2><h3 id=\"1-容量调度器特点\"><a href=\"#1-容量调度器特点\" class=\"headerlink\" title=\"1. 容量调度器特点\"></a>1. 容量调度器特点</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019460282-1d05ddc1-06a3-4931-b8ac-96ededf60c3b.png\" alt=\"img\"></p>\n<ol>\n<li>多队列 ：每个队列可配置一定的资源量，每个队列内部采用FIFO调度策略；</li>\n<li>容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；</li>\n<li>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；</li>\n<li>多租户：</li>\n</ol>\n<p>a. 支持多用户共享集群和多应用程序同时运行；</p>\n<p>b. 为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。</p>\n<h3 id=\"2-容量调度器资源分配算法\"><a href=\"#2-容量调度器资源分配算法\" class=\"headerlink\" title=\"2. 容量调度器资源分配算法\"></a>2. 容量调度器资源分配算法</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018164262-f5e7e270-fc4c-41e8-ba92-cac2a508bcdb.png\" alt=\"img\"></p>\n<h2 id=\"二、公平调度器-Fair-Scheduler\"><a href=\"#二、公平调度器-Fair-Scheduler\" class=\"headerlink\" title=\"二、公平调度器(Fair Scheduler)\"></a>二、公平调度器(Fair Scheduler)</h2><h3 id=\"1-公平调度器特点\"><a href=\"#1-公平调度器特点\" class=\"headerlink\" title=\"1. 公平调度器特点\"></a>1. 公平调度器特点</h3><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657019558958-d7f020f1-230e-4855-92f8-11c473379a18.png\" alt=\"img\"></p>\n<h3 id=\"与容量调度器相同点\"><a href=\"#与容量调度器相同点\" class=\"headerlink\" title=\"与容量调度器相同点:\"></a>与容量调度器相同点:</h3><ol>\n<li><p>多队列 ：每个队列可配置一定的资源量。</p>\n</li>\n<li><p>容量保证：管理员可为每个队列设置资源最低保证和资源使用上线；</p>\n</li>\n<li><p>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列；</p>\n</li>\n<li><p>多租户：</p>\n</li>\n<li><ol>\n<li>支持多用户共享集群和多应用程序同时运行；</li>\n<li>为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源进行限定。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"与容量调度器不同点\"><a href=\"#与容量调度器不同点\" class=\"headerlink\" title=\"与容量调度器不同点:\"></a>与容量调度器不同点:</h3><h4 id=\"核心调度策略不同\"><a href=\"#核心调度策略不同\" class=\"headerlink\" title=\"核心调度策略不同\"></a>核心调度策略不同</h4><p>容量调度器： 优先选择资源利用率低的队列；</p>\n<p>公平调度器：优先选择对资源缺额比例大的。</p>\n<h4 id=\"每个队列可以单独设置资源分配方式\"><a href=\"#每个队列可以单独设置资源分配方式\" class=\"headerlink\" title=\"每个队列可以单独设置资源分配方式\"></a>每个队列可以单独设置资源分配方式</h4><p><strong>容量调度器：FIFO、DRF(内存+CPU)；</strong></p>\n<p><strong>公平调度器：FIFO、FAIR、DRF。</strong></p>\n<h4 id=\"队列任务平行度不同\"><a href=\"#队列任务平行度不同\" class=\"headerlink\" title=\"队列任务平行度不同\"></a>队列任务平行度不同</h4><p>公平调度器：多队列，同一时间队列中多任务按照缺额执行，队列并行度大于队列个数</p>\n<p>容量调度器：多队列，同一时间队列中只有一个任务执行，队列中按照先进先出分配任务，队列并行度等于队列个数。</p>\n<h2 id=\"Fair策略\"><a href=\"#Fair策略\" class=\"headerlink\" title=\"Fair策略\"></a>Fair策略</h2><p>Fair策略是公平调度器默认的队列分配方式</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657018698231-b7ca6c87-74ef-4128-8c05-89fbbab59881.png\" alt=\"img\"></p>\n"},{"title":"初入用户画像","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":2371,"date":"2022-07-16T09:18:03.000Z","updated":"2022-07-16T09:18:03.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n# ![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964933349-cdb78ce8-c129-4bcb-a488-1f7893d60951.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968667409-a69d4ea1-0109-4256-b82c-0c0c9a1140a8.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968859817-eeae0216-172c-4263-ad92-dd54bc9a3e49.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970317537-eeb1a873-b622-4295-bb77-bf647ac8bf82.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657969697929-6449077d-1147-45a9-85ab-e6ba911ed237.png)\n\n# 用户画像\n\n- 应用场景\n\n1. 1. 精准营销\n   2. 数据化运营\n   3. 推荐系统\n   4. 广告投放\n   5. 产品布局\n   6. 行业报告\n   7. 场景间的共性\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964826208-f018b6ec-15cb-45ce-8f61-022a18c56ff0.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964865703-6839d76a-6684-4520-92aa-284db00a5c53.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965034189-2ce05463-eb5e-4269-998e-c23d98c9036e.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965062495-0b7a89f8-5185-4590-a1c8-9f2aa58295d1.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965084978-3bedec8e-2263-4404-ba1b-70715b55fa4d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965117870-d6391669-0922-41fe-953d-4aa57785c0bd.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965147545-19b2219e-f017-4dfe-8454-59928015904d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965165090-939ae364-e19c-455c-8121-04eb94795607.png)\n\n## 平台用户画像（用户群体画像）\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963821022-c48933fc-dafd-4c9b-9db9-af85fc84aa53.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963862801-930ad384-d3d7-4110-804c-441e0b754ebf.png)\n\n\n\n统计整个平台的用户画像，更了解我们的用户\n\n- 性别比例\n- 年龄分布\n- 城市分布\n- 手机系统、型号分布\n\n## 用户个体画像\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967347237-aff38b75-fbdb-4698-a17f-33990ea130b4.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967925952-df8b9a9b-edd8-47af-8a56-732e346a1219.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967976515-4e605319-54b0-430b-83f2-da4655cf61ec.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967057844-1e450af9-5466-4149-97da-f4993b741519.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968173261-a3d676b1-db1b-4d40-b4f1-ae6a6b91fa63.png)\n\n\n\n### 标签的存储\n\n- 用户画像标签表\n\n- - 1、存储到HBase - 根据UserID查询标签，用于个性化广告、推荐系统\n\n> 列族：user、item（商品标签列族）\n>\n> RowKey：userID\n>\n> Cloumn：标签名 + 标签值\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966057558-60b2c61b-2513-42d1-8e2f-2fde427d9e9d.png)\n\n- - 2、存储到Elasticsearch - 倒排索引，根据标签，圈定人群\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966542279-5879e05c-cf3a-4f15-9718-842c02cb729b.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970155892-d692d44c-4ba7-4087-9e64-a218d6a07f54.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966569582-646027f7-ed6d-46d6-9567-b66e083d3a89.png)\n\n- 标签信息表（标签**元数据**）- 存储到MySQL\n\n> TagId,\tTagName,\tTagDesc\n>\n> 384\t\t男\t\t\t用户的性别为男性\n>\n> 385\t\t女\t\t\t用户的性别为女性\n","source":"_posts/bigdata/用户画像介绍.md","raw":"---\ntitle: 初入用户画像\ntags:\n  - 用户画像\ncategories:\n  - - bigdata\n    - User-Profile\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 2371\ndate: 2022-07-16 17:18:03\nupdated: 2022-07-16 17:18:03\ncover:\ndescription:\nkeywords:\n---\n\n# ![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964933349-cdb78ce8-c129-4bcb-a488-1f7893d60951.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968667409-a69d4ea1-0109-4256-b82c-0c0c9a1140a8.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968859817-eeae0216-172c-4263-ad92-dd54bc9a3e49.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970317537-eeb1a873-b622-4295-bb77-bf647ac8bf82.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657969697929-6449077d-1147-45a9-85ab-e6ba911ed237.png)\n\n# 用户画像\n\n- 应用场景\n\n1. 1. 精准营销\n   2. 数据化运营\n   3. 推荐系统\n   4. 广告投放\n   5. 产品布局\n   6. 行业报告\n   7. 场景间的共性\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964826208-f018b6ec-15cb-45ce-8f61-022a18c56ff0.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964865703-6839d76a-6684-4520-92aa-284db00a5c53.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965034189-2ce05463-eb5e-4269-998e-c23d98c9036e.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965062495-0b7a89f8-5185-4590-a1c8-9f2aa58295d1.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965084978-3bedec8e-2263-4404-ba1b-70715b55fa4d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965117870-d6391669-0922-41fe-953d-4aa57785c0bd.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965147545-19b2219e-f017-4dfe-8454-59928015904d.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965165090-939ae364-e19c-455c-8121-04eb94795607.png)\n\n## 平台用户画像（用户群体画像）\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963821022-c48933fc-dafd-4c9b-9db9-af85fc84aa53.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963862801-930ad384-d3d7-4110-804c-441e0b754ebf.png)\n\n\n\n统计整个平台的用户画像，更了解我们的用户\n\n- 性别比例\n- 年龄分布\n- 城市分布\n- 手机系统、型号分布\n\n## 用户个体画像\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967347237-aff38b75-fbdb-4698-a17f-33990ea130b4.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967925952-df8b9a9b-edd8-47af-8a56-732e346a1219.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967976515-4e605319-54b0-430b-83f2-da4655cf61ec.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967057844-1e450af9-5466-4149-97da-f4993b741519.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968173261-a3d676b1-db1b-4d40-b4f1-ae6a6b91fa63.png)\n\n\n\n### 标签的存储\n\n- 用户画像标签表\n\n- - 1、存储到HBase - 根据UserID查询标签，用于个性化广告、推荐系统\n\n> 列族：user、item（商品标签列族）\n>\n> RowKey：userID\n>\n> Cloumn：标签名 + 标签值\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966057558-60b2c61b-2513-42d1-8e2f-2fde427d9e9d.png)\n\n- - 2、存储到Elasticsearch - 倒排索引，根据标签，圈定人群\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966542279-5879e05c-cf3a-4f15-9718-842c02cb729b.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970155892-d692d44c-4ba7-4087-9e64-a218d6a07f54.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966569582-646027f7-ed6d-46d6-9567-b66e083d3a89.png)\n\n- 标签信息表（标签**元数据**）- 存储到MySQL\n\n> TagId,\tTagName,\tTagDesc\n>\n> 384\t\t男\t\t\t用户的性别为男性\n>\n> 385\t\t女\t\t\t用户的性别为女性\n","slug":"bigdata/用户画像介绍","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksz001x8j5m299xeejj","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964933349-cdb78ce8-c129-4bcb-a488-1f7893d60951.png\" alt=\"img\"></h1><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968667409-a69d4ea1-0109-4256-b82c-0c0c9a1140a8.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968859817-eeae0216-172c-4263-ad92-dd54bc9a3e49.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970317537-eeb1a873-b622-4295-bb77-bf647ac8bf82.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657969697929-6449077d-1147-45a9-85ab-e6ba911ed237.png\" alt=\"img\"></p>\n<h1 id=\"用户画像\"><a href=\"#用户画像\" class=\"headerlink\" title=\"用户画像\"></a>用户画像</h1><ul>\n<li>应用场景</li>\n</ul>\n<ol>\n<li><ol>\n<li>精准营销</li>\n<li>数据化运营</li>\n<li>推荐系统</li>\n<li>广告投放</li>\n<li>产品布局</li>\n<li>行业报告</li>\n<li>场景间的共性</li>\n</ol>\n</li>\n</ol>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964826208-f018b6ec-15cb-45ce-8f61-022a18c56ff0.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964865703-6839d76a-6684-4520-92aa-284db00a5c53.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965034189-2ce05463-eb5e-4269-998e-c23d98c9036e.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965062495-0b7a89f8-5185-4590-a1c8-9f2aa58295d1.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965084978-3bedec8e-2263-4404-ba1b-70715b55fa4d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965117870-d6391669-0922-41fe-953d-4aa57785c0bd.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965147545-19b2219e-f017-4dfe-8454-59928015904d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965165090-939ae364-e19c-455c-8121-04eb94795607.png\" alt=\"img\"></p>\n<h2 id=\"平台用户画像（用户群体画像）\"><a href=\"#平台用户画像（用户群体画像）\" class=\"headerlink\" title=\"平台用户画像（用户群体画像）\"></a>平台用户画像（用户群体画像）</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963821022-c48933fc-dafd-4c9b-9db9-af85fc84aa53.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963862801-930ad384-d3d7-4110-804c-441e0b754ebf.png\" alt=\"img\"></p>\n<p>统计整个平台的用户画像，更了解我们的用户</p>\n<ul>\n<li>性别比例</li>\n<li>年龄分布</li>\n<li>城市分布</li>\n<li>手机系统、型号分布</li>\n</ul>\n<h2 id=\"用户个体画像\"><a href=\"#用户个体画像\" class=\"headerlink\" title=\"用户个体画像\"></a>用户个体画像</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967347237-aff38b75-fbdb-4698-a17f-33990ea130b4.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967925952-df8b9a9b-edd8-47af-8a56-732e346a1219.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967976515-4e605319-54b0-430b-83f2-da4655cf61ec.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967057844-1e450af9-5466-4149-97da-f4993b741519.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968173261-a3d676b1-db1b-4d40-b4f1-ae6a6b91fa63.png\" alt=\"img\"></p>\n<h3 id=\"标签的存储\"><a href=\"#标签的存储\" class=\"headerlink\" title=\"标签的存储\"></a>标签的存储</h3><ul>\n<li><p>用户画像标签表</p>\n</li>\n<li><ul>\n<li>1、存储到HBase - 根据UserID查询标签，用于个性化广告、推荐系统</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>列族：user、item（商品标签列族）</p>\n<p>RowKey：userID</p>\n<p>Cloumn：标签名 + 标签值</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966057558-60b2c61b-2513-42d1-8e2f-2fde427d9e9d.png\" alt=\"img\"></p>\n<ul>\n<li><ul>\n<li>2、存储到Elasticsearch - 倒排索引，根据标签，圈定人群</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966542279-5879e05c-cf3a-4f15-9718-842c02cb729b.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970155892-d692d44c-4ba7-4087-9e64-a218d6a07f54.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966569582-646027f7-ed6d-46d6-9567-b66e083d3a89.png\" alt=\"img\"></p>\n<ul>\n<li>标签信息表（标签<strong>元数据</strong>）- 存储到MySQL</li>\n</ul>\n<blockquote>\n<p>TagId,\tTagName,\tTagDesc</p>\n<p>384\t\t男\t\t\t用户的性别为男性</p>\n<p>385\t\t女\t\t\t用户的性别为女性</p>\n</blockquote>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964933349-cdb78ce8-c129-4bcb-a488-1f7893d60951.png\" alt=\"img\"></h1><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968667409-a69d4ea1-0109-4256-b82c-0c0c9a1140a8.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968859817-eeae0216-172c-4263-ad92-dd54bc9a3e49.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970317537-eeb1a873-b622-4295-bb77-bf647ac8bf82.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657969697929-6449077d-1147-45a9-85ab-e6ba911ed237.png\" alt=\"img\"></p>\n<h1 id=\"用户画像\"><a href=\"#用户画像\" class=\"headerlink\" title=\"用户画像\"></a>用户画像</h1><ul>\n<li>应用场景</li>\n</ul>\n<ol>\n<li><ol>\n<li>精准营销</li>\n<li>数据化运营</li>\n<li>推荐系统</li>\n<li>广告投放</li>\n<li>产品布局</li>\n<li>行业报告</li>\n<li>场景间的共性</li>\n</ol>\n</li>\n</ol>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964826208-f018b6ec-15cb-45ce-8f61-022a18c56ff0.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657964865703-6839d76a-6684-4520-92aa-284db00a5c53.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965034189-2ce05463-eb5e-4269-998e-c23d98c9036e.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965062495-0b7a89f8-5185-4590-a1c8-9f2aa58295d1.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965084978-3bedec8e-2263-4404-ba1b-70715b55fa4d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965117870-d6391669-0922-41fe-953d-4aa57785c0bd.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965147545-19b2219e-f017-4dfe-8454-59928015904d.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657965165090-939ae364-e19c-455c-8121-04eb94795607.png\" alt=\"img\"></p>\n<h2 id=\"平台用户画像（用户群体画像）\"><a href=\"#平台用户画像（用户群体画像）\" class=\"headerlink\" title=\"平台用户画像（用户群体画像）\"></a>平台用户画像（用户群体画像）</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963821022-c48933fc-dafd-4c9b-9db9-af85fc84aa53.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657963862801-930ad384-d3d7-4110-804c-441e0b754ebf.png\" alt=\"img\"></p>\n<p>统计整个平台的用户画像，更了解我们的用户</p>\n<ul>\n<li>性别比例</li>\n<li>年龄分布</li>\n<li>城市分布</li>\n<li>手机系统、型号分布</li>\n</ul>\n<h2 id=\"用户个体画像\"><a href=\"#用户个体画像\" class=\"headerlink\" title=\"用户个体画像\"></a>用户个体画像</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967347237-aff38b75-fbdb-4698-a17f-33990ea130b4.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967925952-df8b9a9b-edd8-47af-8a56-732e346a1219.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967976515-4e605319-54b0-430b-83f2-da4655cf61ec.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657967057844-1e450af9-5466-4149-97da-f4993b741519.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657968173261-a3d676b1-db1b-4d40-b4f1-ae6a6b91fa63.png\" alt=\"img\"></p>\n<h3 id=\"标签的存储\"><a href=\"#标签的存储\" class=\"headerlink\" title=\"标签的存储\"></a>标签的存储</h3><ul>\n<li><p>用户画像标签表</p>\n</li>\n<li><ul>\n<li>1、存储到HBase - 根据UserID查询标签，用于个性化广告、推荐系统</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>列族：user、item（商品标签列族）</p>\n<p>RowKey：userID</p>\n<p>Cloumn：标签名 + 标签值</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966057558-60b2c61b-2513-42d1-8e2f-2fde427d9e9d.png\" alt=\"img\"></p>\n<ul>\n<li><ul>\n<li>2、存储到Elasticsearch - 倒排索引，根据标签，圈定人群</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966542279-5879e05c-cf3a-4f15-9718-842c02cb729b.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657970155892-d692d44c-4ba7-4087-9e64-a218d6a07f54.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1657966569582-646027f7-ed6d-46d6-9567-b66e083d3a89.png\" alt=\"img\"></p>\n<ul>\n<li>标签信息表（标签<strong>元数据</strong>）- 存储到MySQL</li>\n</ul>\n<blockquote>\n<p>TagId,\tTagName,\tTagDesc</p>\n<p>384\t\t男\t\t\t用户的性别为男性</p>\n<p>385\t\t女\t\t\t用户的性别为女性</p>\n</blockquote>\n"},{"title":"初识Doris","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":27737,"date":"2022-07-22T22:19:04.000Z","updated":"2022-07-22T22:19:04.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n>- MPP（ Massively Parallel Processing - 大规模并行处理）Based高性能、实时的分析型数据库。\n>\n>- 在**使用接口**方面，Doris 采用 MySQ L 协议，高度兼容 MySQL 语法，支持标准 SQL，用户可以通过各类客户端工具来访问 Doris，并支持与 BI 工具的无缝对接。\n>\n>- 在**存储引擎**方面，Doris 采用列式存储，按列进行数据的编码压缩和读取，能够实现极高的压缩比，同时减少大量非相关数据的扫描，从而更加有效利用 IO 和 CPU 资源。\n>\n>- 在**查询引擎**方面，Doris 采用 MPP 的模型，节点间和节点内都并行执行，也支持多个大表的分布式 Shuffle Join，从而能够更好应对复杂查询。\n>\n>  ![origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g-cd4a2a172ec93222a40231fe1a8d4edd.png)\n>\n>- **Doris 查询引擎是向量化**的查询引擎，所有的内存结构能够按照列式布局，能够达到大幅减少虚函数调用、提升 Cache 命中率，高效利用 SIMD 指令的效果。在宽表聚合场景下性能是非向量化引擎的 5-10 倍。\n>\n>  ![origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g-84273f42ae82408ff09c7af6c5b67022.png)\n>\n>- **Doris 采用了 Adaptive Query Execution 技术，** 可以根据 Runtime Statistics 来动态调整执行计划，比如通过 Runtime Filter 技术能够在运行时生成生成 Filter 推到 Probe 侧，并且能够将 Filter 自动穿透到 Probe 侧最底层的 Scan 节点，从而大幅减少 Probe 的数据量，加速 Join 性能。Doris 的 Runtime Filter 支持 In/Min/Max/Bloom Filter。\n>\n>- 在**优化器**方面 Doris 使用 CBO 和 RBO 结合的优化策略，RBO 支持常量折叠、子查询改写、谓词下推等，CBO 支持 Join Reorder。目前 CBO 还在持续优化中，主要集中在更加精准的统计信息收集和推导，更加精准的代价模型预估等方面。\n>\n>- **Doris 也支持强一致的物化视图**，物化视图的更新和选择都在系统内自动进行，不需要用户手动选择，从而大幅减少了物化视图维护的代价。\n>\n>- **Doris 也支持比较丰富的索引结构，来减少数据的扫描**\n\n------\n\n>- Doris 数据模型上目前分为三类: AGGREGATE KEY, UNIQUE KEY, DUPLICATE KEY。**三种模型中数据都是按KEY进行排序。**\n>\n>- AGGREGATE KEY\n>\n>  AGGREGATE KEY相同时，新旧记录进行聚合，目前支持的聚合函数有SUM, MIN, MAX, REPLACE。\n>\n>  AGGREGATE KEY模型可以提前聚合数据, 适合报表和多维分析业务。\n>\n>- UNIQUE KEY\n>\n>  UNIQUE KEY 相同时，新记录覆盖旧记录。目前 UNIQUE KEY 实现上和 AGGREGATE KEY 的 REPLACE 聚合方法一样，二者本质上相同。适用于有更新需求的分析业务。\n>\n>- DUPLICATE KEY\n>\n>  只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。\n\n------\n\n>Doris**整体架构**如下图所示，Doris 架构非常简单，只有两类进程\n>\n>- **Frontend（FE）**，主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作。\n>- **另一个是 Backend（BE）**，主要负责数据存储、查询计划的执行。\n>\n>这两类进程都是可以横向扩展的，单集群可以支持到数百台机器，数十 PB 的存储容量。并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。\n>\n>![origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g-11e2c3e5c6b6dc26b4f602697a1071a9.png)\n\n## 建表语句\n\n- Range Partition\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS example_db.expamle_range_tbl\n  (\n   `user_id` LARGEINT NOT NULL COMMENT \"用户 id\",\n   `date` DATE NOT NULL COMMENT \"数据灌入日期时间\",\n   `timestamp` DATETIME NOT NULL COMMENT \"数据灌入的时间戳\",\n   `city` VARCHAR(20) COMMENT \"用户所在城市\",\n   `age` SMALLINT COMMENT \"用户年龄\",\n   `sex` TINYINT COMMENT \"用户性别\",\n   `last_visit_date` DATETIME REPLACE DEFAULT \"1970-01-01 \n  00:00:00\" COMMENT \"用户最后一次访问时间\",\n   `cost` BIGINT SUM DEFAULT \"0\" COMMENT \"用户总消费\",\n   `max_dwell_time` INT MAX DEFAULT \"0\" COMMENT \"用户最大停留时间\",\n   `min_dwell_time` INT MIN DEFAULT \"99999\" COMMENT \"用户最小停留时间\"\n  )\n  ENGINE=olap\n  AGGREGATE KEY(`user_id`, `date`, `timestamp`, `city`, `age`, `sex`)\n  PARTITION BY RANGE(`date`)\n  (\n   PARTITION `p201701` VALUES LESS THAN (\"2017-02-01\"),\n   PARTITION `p201702` VALUES LESS THAN (\"2017-03-01\"),\n   PARTITION `p201703` VALUES LESS THAN (\"2017-04-01\")\n  )\n  DISTRIBUTED BY HASH(`user_id`) BUCKETS 16\n  PROPERTIES\n  (\n   \"replication_num\" = \"3\",\n   \"storage_medium\" = \"SSD\",\n   \"storage_cooldown_time\" = \"2018-01-01 12:00:00\"\n  );\n  \n  ```\n\n- List Partition\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS example_db.expamle_list_tbl\n  (\n   `user_id` LARGEINT NOT NULL COMMENT \"用户 id\",\n   `date` DATE NOT NULL COMMENT \"数据灌入日期时间\",\n   `timestamp` DATETIME NOT NULL COMMENT \"数据灌入的时间戳\",\n   `city` VARCHAR(20) COMMENT \"用户所在城市\",\n   `age` SMALLINT COMMENT \"用户年龄\",\n   `sex` TINYINT COMMENT \"用户性别\",\n   `last_visit_date` DATETIME REPLACE DEFAULT \"1970-01-01 \n  00:00:00\" COMMENT \"用户最后一次访问时间\",\n   `cost` BIGINT SUM DEFAULT \"0\" COMMENT \"用户总消费\",\n   `max_dwell_time` INT MAX DEFAULT \"0\" COMMENT \"用户最大停留时间\",\n   `min_dwell_time` INT MIN DEFAULT \"99999\" COMMENT \"用户最小停留时\n  间\"\n  )\n  ENGINE=olap\n  AGGREGATE KEY(`user_id`, `date`, `timestamp`, `city`, `age`, `sex`)\n  PARTITION BY LIST(`city`)\n  (\n   PARTITION `p_cn` VALUES IN (\"Beijing\", \"Shanghai\", \"Hong Kong\"),\n   PARTITION `p_usa` VALUES IN (\"New York\", \"San Francisco\"),\n   PARTITION `p_jp` VALUES IN (\"Tokyo\")\n  )\n  DISTRIBUTED BY HASH(`user_id`) BUCKETS 16\n  PROPERTIES\n  (\n   \"replication_num\" = \"3\",\n   \"storage_medium\" = \"SSD\",\n   \"storage_cooldown_time\" = \"2018-01-01 12:00:00\"\n  );\n  ```\n\n  \n\n## 数据模型\n\nDoris 的数据模型主要分为 3 类：Aggregate、Uniq、Duplicate\n\n### Aggregate 模型\n\n表中的列按照是否设置了 AggregationType，分为 Key（维度列）和 Value（指标列）。没有设置 AggregationType 的称为 Key，设置了 AggregationType 的称为 Value。\n当我们导入数据时，对于 Key 列相同的行会聚合成一行，而 Value 列会按照设置的AggregationType 进行聚合。AggregationType 目前有以下四种聚合方式：\n\n- ➢ SUM：求和，多行的 Value 进行累加。\n- ➢ REPLACE：替代，下一批数据中的 Value 会替换之前导入过的行中的 Value。\n     REPLACE_IF_NOT_NULL ：当遇到 null 值则不更新。\n- ➢ MAX：保留最大值。\n- ➢ MIN：保留最小值。\n\n数据的聚合，在 Doris 中有如下三个阶段发生：\n- （1）每一批次数据导入的 ETL 阶段。该阶段会在每一批次导入的数据内部进行聚合。\n- （2）底层 BE 进行数据 Compaction 的阶段。该阶段，BE 会对已导入的不同批次的数据进行进一步的聚合。\n- （3）数据查询阶段。在数据查询时，对于查询涉及到的数据，会进行对应的聚合。\n\n数据在不同时间，可能聚合的程度不一致。比如一批数据刚导入时，可能还未与之前已存在的数据进行聚合。但是对于用户而言，用户只能查询到聚合后的数据。即不同的聚合程度对于用户查询而言是透明的。用户需始终认为数据以最终的完成的聚合程度存在，而不应假设某些聚合还未发生。（可参阅聚合模型的局限性一节获得更多详情。）\n\n### Uniq 模型\n\n在某些多维分析场景下，用户更关注的是如何保证 Key 的唯一性，即如何获得 Primary  Key 唯一性约束。因此，我们引入了 Uniq 的数据模型。该模型本质上是聚合模型的一个特 例，也是一种简化的表结构表示方式。\n\nUniq 模型完全可以用聚合模型中的 REPLACE 方式替代。其内部的实现方式和数据存 储方式也完全一样。\n\n### Duplicate 模型\n\n在某些多维分析场景下，数据既没有主键，也没有聚合需求。Duplicate 数据模型可以 满足这类需求。数据完全按照导入文件中的数据进行存储，不会有任何聚合。即使两行数据 完全相同，也都会保留。 而在建表语句中指定的 `DUPLICATE KEY`，只是用来指明底层数 据按照那些列进行排序。\n\n### 数据模型的选择建议\n\n因为数据模型在建表时就已经确定，且无法修改。所以，选择一个合适的数据模型非常 重要。 \n\n（1）Aggregate 模型可以通过预聚合，极大地降低聚合查询时所需扫描的数据量和查询 的计算量，非常适合有固定模式的报表类查询场景。但是该模型对 count(*) 查询很不友好。 **同时因为固定了 Value 列上的聚合方式，在进行其他类型的聚合查询时，需要考虑语意正确 性。**\n\n（2）Uniq 模型针对需要唯一主键约束的场景，可以保证主键唯一性约束。**但是无法利 用 ROLLUP 等预聚合带来的查询优势（因为本质是 REPLACE，没有 SUM 这种聚合方式）。** \n\n（3）Duplicate 适合任意维度的 Ad-hoc 查询。虽然同样无法利用预聚合的特性，但是不 受聚合模型的约束，可以发挥列存模型的优势（只读取相关列，而不需要读取所有 Key 列）\n\n## Rollup\n\nROLLUP 在多维分析中是“上卷”的意思，即将数据按某种指定的粒度进行进一步聚 合。\n\n### 基本概念\n\n在 Doris 中，我们将用户通过建表语句创建出来的表称为 Base 表（Base Table）。Base  表中保存着按用户建表语句指定的方式存储的基础数据。 \n\n在 Base 表之上，我们可以创建任意多个 ROLLUP 表。这些 ROLLUP 的数据是基于 Base  表产生的，并且在物理上是独立存储的。 ROLLUP 表的基本作用，在于在 Base 表的基础上，获得更粗粒度的聚合数据\n\n###  Duplicate 模型中的 ROLLUP\n\n因为 Duplicate 模型没有聚合的语意。所以该模型中的 ROLLUP，已经失去了“上卷” 这一层含义。而仅仅是作为调整列顺序，以命中前缀索引的作用。下面详细介绍前缀索引， 以及如何使用 ROLLUP 改变前缀索引，以获得更好的查询效率。\n\n#### 前缀索引\n\n不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。 \n\n本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构 是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作 为条件进行查找，会非常的高效。 \n\n在 Aggregate、Uniq 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表 语句中，AGGREGATE KEY、UNIQ KEY 和 DUPLICATE KEY 中指定的列进行排序存储 的。而**前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方 式。**\n\n#### ROLLUP 调整前缀索引\n\n因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命 中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过 创建 ROLLUP 来人为的调整列顺序。举例说明。\n\n### ROLLUP 的几点说明\n\n⚫ ROLLUP 最根本的作用是提高某些查询的查询效率（无论是通过聚合来减少数据 量，还是修改列顺序以匹配前缀索引）。因此 ROLLUP 的含义已经超出了“上卷” 的范围。这也是为什么在源代码中，将其命名为 Materialized Index（物化索引）的 原因。\n\n⚫ ROLLUP 是附属于 Base 表的，可以看做是 Base 表的一种辅助数据结构。用户可以 在 Base 表的基础上，创建或删除 ROLLUP，但是不能在查询中显式的指定查询某 ROLLUP。是否命中 ROLLUP 完全由 Doris 系统自动决定。 \n\n⚫ ROLLUP 的数据是独立物理存储的。因此，创建的 ROLLUP 越多，占用的磁盘空 间也就越大。同时对导入速度也会有影响（导入的 ETL 阶段会自动产生所有 ROLLUP 的数据），但是不会降低查询效率（只会更好）。 \n\n⚫ ROLLUP 的数据更新与 Base 表是完全同步的。用户无需关心这个问题。\n\n⚫ ROLLUP 中列的聚合方式，与 Base 表完全相同。在创建 ROLLUP 无需指定，也不 能修改。 \n\n⚫ 查询能否命中 ROLLUP 的一个必要条件（非充分条件）是，查询所涉及的所有列 （包括 select list 和 where 中的查询条件列等）都存在于该 ROLLUP 的列中。否 则，查询只能命中 Base 表。 \n\n⚫ 某些类型的查询（如 count(*)）在任何条件下，都无法命中 ROLLUP。具体参见接 下来的聚合模型的局限性一节。\n\n⚫ 可以通过 EXPLAIN your_sql; 命令获得查询执行计划，在执行计划中，查看是否命 中 ROLLUP。\n\n⚫ 可以通过 DESC tbl_name ALL; 语句显示 Base 表和所有已创建完成的 ROLLUP。\n\n\n\n## 物化视图\n\n物化视图就是包含了查询结果的数据库对象，可能是对远程数据的本地 copy，也可能 是一个表或多表 join 后结果的行或列的子集，也可能是聚合后的结果。说白了，就是预先存 储查询结果的一种数据库对象。\n\n在 Doris 中的物化视图，就是查询结果预先存储起来的特殊的表。 \n\n物化视图的出现主要是为了满足用户，既能对原始明细数据的任意维度分析，也能快速 的对固定维度进行分析查询。\n\n### 优势\n\n⚫ 对于那些经常重复的使用相同的子查询结果的查询性能大幅提升。 \n\n⚫ Doris 自动维护物化视图的数据，无论是新的导入，还是删除操作都能保证 base 表 和物化视图表的数据一致性。无需任何额外的人工维护成本。 \n\n⚫ 查询时，会自动匹配到最优物化视图，并直接从物化视图中读取数据。 自动维护物化视图的数据会造成一些维护开销，会在后面的物化视图的局限性中展开说 明。\n\n### 物化视图 VS Rollup\n\n在没有物化视图功能之前，用户一般都是使用 Rollup 功能通过预聚合方式提升查询效 率的。但是 Rollup 具有一定的局限性，他不能基于明细模型做预聚合。 \n\n物化视图则在覆盖了 Rollup 的功能的同时，还能支持更丰富的聚合函数。所以物化视 图其实是 Rollup 的一个超集。 \n\n也就是说，之前 ALTER TABLE ADD ROLLUP 语法支持的功能现在均可以通过 CREATE MATERIALIZED VIEW 实现。\n\n### 物化视图原理\n\nDoris 系统提供了一整套对物化视图的 DDL 语法，包括创建，查看，删除。DDL 的语 法和 PostgreSQL, Oracle 都是一致的。但是 Doris 目前创建物化视图只能在单表操作，不支 持 join。\n","source":"_posts/bigdata/初识Doris.md","raw":"---\ntitle: 初识Doris\ntags:\n  - Doris\ncategories:\n  - - bigdata\n    - Doris\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 27737\ndate: 2022-07-23 06:19:04\nupdated: 2022-07-23 06:19:04\ncover:\ndescription:\nkeywords:\n---\n\n>- MPP（ Massively Parallel Processing - 大规模并行处理）Based高性能、实时的分析型数据库。\n>\n>- 在**使用接口**方面，Doris 采用 MySQ L 协议，高度兼容 MySQL 语法，支持标准 SQL，用户可以通过各类客户端工具来访问 Doris，并支持与 BI 工具的无缝对接。\n>\n>- 在**存储引擎**方面，Doris 采用列式存储，按列进行数据的编码压缩和读取，能够实现极高的压缩比，同时减少大量非相关数据的扫描，从而更加有效利用 IO 和 CPU 资源。\n>\n>- 在**查询引擎**方面，Doris 采用 MPP 的模型，节点间和节点内都并行执行，也支持多个大表的分布式 Shuffle Join，从而能够更好应对复杂查询。\n>\n>  ![origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g-cd4a2a172ec93222a40231fe1a8d4edd.png)\n>\n>- **Doris 查询引擎是向量化**的查询引擎，所有的内存结构能够按照列式布局，能够达到大幅减少虚函数调用、提升 Cache 命中率，高效利用 SIMD 指令的效果。在宽表聚合场景下性能是非向量化引擎的 5-10 倍。\n>\n>  ![origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g-84273f42ae82408ff09c7af6c5b67022.png)\n>\n>- **Doris 采用了 Adaptive Query Execution 技术，** 可以根据 Runtime Statistics 来动态调整执行计划，比如通过 Runtime Filter 技术能够在运行时生成生成 Filter 推到 Probe 侧，并且能够将 Filter 自动穿透到 Probe 侧最底层的 Scan 节点，从而大幅减少 Probe 的数据量，加速 Join 性能。Doris 的 Runtime Filter 支持 In/Min/Max/Bloom Filter。\n>\n>- 在**优化器**方面 Doris 使用 CBO 和 RBO 结合的优化策略，RBO 支持常量折叠、子查询改写、谓词下推等，CBO 支持 Join Reorder。目前 CBO 还在持续优化中，主要集中在更加精准的统计信息收集和推导，更加精准的代价模型预估等方面。\n>\n>- **Doris 也支持强一致的物化视图**，物化视图的更新和选择都在系统内自动进行，不需要用户手动选择，从而大幅减少了物化视图维护的代价。\n>\n>- **Doris 也支持比较丰富的索引结构，来减少数据的扫描**\n\n------\n\n>- Doris 数据模型上目前分为三类: AGGREGATE KEY, UNIQUE KEY, DUPLICATE KEY。**三种模型中数据都是按KEY进行排序。**\n>\n>- AGGREGATE KEY\n>\n>  AGGREGATE KEY相同时，新旧记录进行聚合，目前支持的聚合函数有SUM, MIN, MAX, REPLACE。\n>\n>  AGGREGATE KEY模型可以提前聚合数据, 适合报表和多维分析业务。\n>\n>- UNIQUE KEY\n>\n>  UNIQUE KEY 相同时，新记录覆盖旧记录。目前 UNIQUE KEY 实现上和 AGGREGATE KEY 的 REPLACE 聚合方法一样，二者本质上相同。适用于有更新需求的分析业务。\n>\n>- DUPLICATE KEY\n>\n>  只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。\n\n------\n\n>Doris**整体架构**如下图所示，Doris 架构非常简单，只有两类进程\n>\n>- **Frontend（FE）**，主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作。\n>- **另一个是 Backend（BE）**，主要负责数据存储、查询计划的执行。\n>\n>这两类进程都是可以横向扩展的，单集群可以支持到数百台机器，数十 PB 的存储容量。并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。\n>\n>![origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g](https://doris.apache.org/zh-CN/assets/images/origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g-11e2c3e5c6b6dc26b4f602697a1071a9.png)\n\n## 建表语句\n\n- Range Partition\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS example_db.expamle_range_tbl\n  (\n   `user_id` LARGEINT NOT NULL COMMENT \"用户 id\",\n   `date` DATE NOT NULL COMMENT \"数据灌入日期时间\",\n   `timestamp` DATETIME NOT NULL COMMENT \"数据灌入的时间戳\",\n   `city` VARCHAR(20) COMMENT \"用户所在城市\",\n   `age` SMALLINT COMMENT \"用户年龄\",\n   `sex` TINYINT COMMENT \"用户性别\",\n   `last_visit_date` DATETIME REPLACE DEFAULT \"1970-01-01 \n  00:00:00\" COMMENT \"用户最后一次访问时间\",\n   `cost` BIGINT SUM DEFAULT \"0\" COMMENT \"用户总消费\",\n   `max_dwell_time` INT MAX DEFAULT \"0\" COMMENT \"用户最大停留时间\",\n   `min_dwell_time` INT MIN DEFAULT \"99999\" COMMENT \"用户最小停留时间\"\n  )\n  ENGINE=olap\n  AGGREGATE KEY(`user_id`, `date`, `timestamp`, `city`, `age`, `sex`)\n  PARTITION BY RANGE(`date`)\n  (\n   PARTITION `p201701` VALUES LESS THAN (\"2017-02-01\"),\n   PARTITION `p201702` VALUES LESS THAN (\"2017-03-01\"),\n   PARTITION `p201703` VALUES LESS THAN (\"2017-04-01\")\n  )\n  DISTRIBUTED BY HASH(`user_id`) BUCKETS 16\n  PROPERTIES\n  (\n   \"replication_num\" = \"3\",\n   \"storage_medium\" = \"SSD\",\n   \"storage_cooldown_time\" = \"2018-01-01 12:00:00\"\n  );\n  \n  ```\n\n- List Partition\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS example_db.expamle_list_tbl\n  (\n   `user_id` LARGEINT NOT NULL COMMENT \"用户 id\",\n   `date` DATE NOT NULL COMMENT \"数据灌入日期时间\",\n   `timestamp` DATETIME NOT NULL COMMENT \"数据灌入的时间戳\",\n   `city` VARCHAR(20) COMMENT \"用户所在城市\",\n   `age` SMALLINT COMMENT \"用户年龄\",\n   `sex` TINYINT COMMENT \"用户性别\",\n   `last_visit_date` DATETIME REPLACE DEFAULT \"1970-01-01 \n  00:00:00\" COMMENT \"用户最后一次访问时间\",\n   `cost` BIGINT SUM DEFAULT \"0\" COMMENT \"用户总消费\",\n   `max_dwell_time` INT MAX DEFAULT \"0\" COMMENT \"用户最大停留时间\",\n   `min_dwell_time` INT MIN DEFAULT \"99999\" COMMENT \"用户最小停留时\n  间\"\n  )\n  ENGINE=olap\n  AGGREGATE KEY(`user_id`, `date`, `timestamp`, `city`, `age`, `sex`)\n  PARTITION BY LIST(`city`)\n  (\n   PARTITION `p_cn` VALUES IN (\"Beijing\", \"Shanghai\", \"Hong Kong\"),\n   PARTITION `p_usa` VALUES IN (\"New York\", \"San Francisco\"),\n   PARTITION `p_jp` VALUES IN (\"Tokyo\")\n  )\n  DISTRIBUTED BY HASH(`user_id`) BUCKETS 16\n  PROPERTIES\n  (\n   \"replication_num\" = \"3\",\n   \"storage_medium\" = \"SSD\",\n   \"storage_cooldown_time\" = \"2018-01-01 12:00:00\"\n  );\n  ```\n\n  \n\n## 数据模型\n\nDoris 的数据模型主要分为 3 类：Aggregate、Uniq、Duplicate\n\n### Aggregate 模型\n\n表中的列按照是否设置了 AggregationType，分为 Key（维度列）和 Value（指标列）。没有设置 AggregationType 的称为 Key，设置了 AggregationType 的称为 Value。\n当我们导入数据时，对于 Key 列相同的行会聚合成一行，而 Value 列会按照设置的AggregationType 进行聚合。AggregationType 目前有以下四种聚合方式：\n\n- ➢ SUM：求和，多行的 Value 进行累加。\n- ➢ REPLACE：替代，下一批数据中的 Value 会替换之前导入过的行中的 Value。\n     REPLACE_IF_NOT_NULL ：当遇到 null 值则不更新。\n- ➢ MAX：保留最大值。\n- ➢ MIN：保留最小值。\n\n数据的聚合，在 Doris 中有如下三个阶段发生：\n- （1）每一批次数据导入的 ETL 阶段。该阶段会在每一批次导入的数据内部进行聚合。\n- （2）底层 BE 进行数据 Compaction 的阶段。该阶段，BE 会对已导入的不同批次的数据进行进一步的聚合。\n- （3）数据查询阶段。在数据查询时，对于查询涉及到的数据，会进行对应的聚合。\n\n数据在不同时间，可能聚合的程度不一致。比如一批数据刚导入时，可能还未与之前已存在的数据进行聚合。但是对于用户而言，用户只能查询到聚合后的数据。即不同的聚合程度对于用户查询而言是透明的。用户需始终认为数据以最终的完成的聚合程度存在，而不应假设某些聚合还未发生。（可参阅聚合模型的局限性一节获得更多详情。）\n\n### Uniq 模型\n\n在某些多维分析场景下，用户更关注的是如何保证 Key 的唯一性，即如何获得 Primary  Key 唯一性约束。因此，我们引入了 Uniq 的数据模型。该模型本质上是聚合模型的一个特 例，也是一种简化的表结构表示方式。\n\nUniq 模型完全可以用聚合模型中的 REPLACE 方式替代。其内部的实现方式和数据存 储方式也完全一样。\n\n### Duplicate 模型\n\n在某些多维分析场景下，数据既没有主键，也没有聚合需求。Duplicate 数据模型可以 满足这类需求。数据完全按照导入文件中的数据进行存储，不会有任何聚合。即使两行数据 完全相同，也都会保留。 而在建表语句中指定的 `DUPLICATE KEY`，只是用来指明底层数 据按照那些列进行排序。\n\n### 数据模型的选择建议\n\n因为数据模型在建表时就已经确定，且无法修改。所以，选择一个合适的数据模型非常 重要。 \n\n（1）Aggregate 模型可以通过预聚合，极大地降低聚合查询时所需扫描的数据量和查询 的计算量，非常适合有固定模式的报表类查询场景。但是该模型对 count(*) 查询很不友好。 **同时因为固定了 Value 列上的聚合方式，在进行其他类型的聚合查询时，需要考虑语意正确 性。**\n\n（2）Uniq 模型针对需要唯一主键约束的场景，可以保证主键唯一性约束。**但是无法利 用 ROLLUP 等预聚合带来的查询优势（因为本质是 REPLACE，没有 SUM 这种聚合方式）。** \n\n（3）Duplicate 适合任意维度的 Ad-hoc 查询。虽然同样无法利用预聚合的特性，但是不 受聚合模型的约束，可以发挥列存模型的优势（只读取相关列，而不需要读取所有 Key 列）\n\n## Rollup\n\nROLLUP 在多维分析中是“上卷”的意思，即将数据按某种指定的粒度进行进一步聚 合。\n\n### 基本概念\n\n在 Doris 中，我们将用户通过建表语句创建出来的表称为 Base 表（Base Table）。Base  表中保存着按用户建表语句指定的方式存储的基础数据。 \n\n在 Base 表之上，我们可以创建任意多个 ROLLUP 表。这些 ROLLUP 的数据是基于 Base  表产生的，并且在物理上是独立存储的。 ROLLUP 表的基本作用，在于在 Base 表的基础上，获得更粗粒度的聚合数据\n\n###  Duplicate 模型中的 ROLLUP\n\n因为 Duplicate 模型没有聚合的语意。所以该模型中的 ROLLUP，已经失去了“上卷” 这一层含义。而仅仅是作为调整列顺序，以命中前缀索引的作用。下面详细介绍前缀索引， 以及如何使用 ROLLUP 改变前缀索引，以获得更好的查询效率。\n\n#### 前缀索引\n\n不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。 \n\n本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构 是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作 为条件进行查找，会非常的高效。 \n\n在 Aggregate、Uniq 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表 语句中，AGGREGATE KEY、UNIQ KEY 和 DUPLICATE KEY 中指定的列进行排序存储 的。而**前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方 式。**\n\n#### ROLLUP 调整前缀索引\n\n因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命 中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过 创建 ROLLUP 来人为的调整列顺序。举例说明。\n\n### ROLLUP 的几点说明\n\n⚫ ROLLUP 最根本的作用是提高某些查询的查询效率（无论是通过聚合来减少数据 量，还是修改列顺序以匹配前缀索引）。因此 ROLLUP 的含义已经超出了“上卷” 的范围。这也是为什么在源代码中，将其命名为 Materialized Index（物化索引）的 原因。\n\n⚫ ROLLUP 是附属于 Base 表的，可以看做是 Base 表的一种辅助数据结构。用户可以 在 Base 表的基础上，创建或删除 ROLLUP，但是不能在查询中显式的指定查询某 ROLLUP。是否命中 ROLLUP 完全由 Doris 系统自动决定。 \n\n⚫ ROLLUP 的数据是独立物理存储的。因此，创建的 ROLLUP 越多，占用的磁盘空 间也就越大。同时对导入速度也会有影响（导入的 ETL 阶段会自动产生所有 ROLLUP 的数据），但是不会降低查询效率（只会更好）。 \n\n⚫ ROLLUP 的数据更新与 Base 表是完全同步的。用户无需关心这个问题。\n\n⚫ ROLLUP 中列的聚合方式，与 Base 表完全相同。在创建 ROLLUP 无需指定，也不 能修改。 \n\n⚫ 查询能否命中 ROLLUP 的一个必要条件（非充分条件）是，查询所涉及的所有列 （包括 select list 和 where 中的查询条件列等）都存在于该 ROLLUP 的列中。否 则，查询只能命中 Base 表。 \n\n⚫ 某些类型的查询（如 count(*)）在任何条件下，都无法命中 ROLLUP。具体参见接 下来的聚合模型的局限性一节。\n\n⚫ 可以通过 EXPLAIN your_sql; 命令获得查询执行计划，在执行计划中，查看是否命 中 ROLLUP。\n\n⚫ 可以通过 DESC tbl_name ALL; 语句显示 Base 表和所有已创建完成的 ROLLUP。\n\n\n\n## 物化视图\n\n物化视图就是包含了查询结果的数据库对象，可能是对远程数据的本地 copy，也可能 是一个表或多表 join 后结果的行或列的子集，也可能是聚合后的结果。说白了，就是预先存 储查询结果的一种数据库对象。\n\n在 Doris 中的物化视图，就是查询结果预先存储起来的特殊的表。 \n\n物化视图的出现主要是为了满足用户，既能对原始明细数据的任意维度分析，也能快速 的对固定维度进行分析查询。\n\n### 优势\n\n⚫ 对于那些经常重复的使用相同的子查询结果的查询性能大幅提升。 \n\n⚫ Doris 自动维护物化视图的数据，无论是新的导入，还是删除操作都能保证 base 表 和物化视图表的数据一致性。无需任何额外的人工维护成本。 \n\n⚫ 查询时，会自动匹配到最优物化视图，并直接从物化视图中读取数据。 自动维护物化视图的数据会造成一些维护开销，会在后面的物化视图的局限性中展开说 明。\n\n### 物化视图 VS Rollup\n\n在没有物化视图功能之前，用户一般都是使用 Rollup 功能通过预聚合方式提升查询效 率的。但是 Rollup 具有一定的局限性，他不能基于明细模型做预聚合。 \n\n物化视图则在覆盖了 Rollup 的功能的同时，还能支持更丰富的聚合函数。所以物化视 图其实是 Rollup 的一个超集。 \n\n也就是说，之前 ALTER TABLE ADD ROLLUP 语法支持的功能现在均可以通过 CREATE MATERIALIZED VIEW 实现。\n\n### 物化视图原理\n\nDoris 系统提供了一整套对物化视图的 DDL 语法，包括创建，查看，删除。DDL 的语 法和 PostgreSQL, Oracle 都是一致的。但是 Doris 目前创建物化视图只能在单表操作，不支 持 join。\n","slug":"bigdata/初识Doris","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksz00218j5me1gy329g","content":"<blockquote>\n<ul>\n<li><p>MPP（ Massively Parallel Processing - 大规模并行处理）Based高性能、实时的分析型数据库。</p>\n</li>\n<li><p>在<strong>使用接口</strong>方面，Doris 采用 MySQ L 协议，高度兼容 MySQL 语法，支持标准 SQL，用户可以通过各类客户端工具来访问 Doris，并支持与 BI 工具的无缝对接。</p>\n</li>\n<li><p>在<strong>存储引擎</strong>方面，Doris 采用列式存储，按列进行数据的编码压缩和读取，能够实现极高的压缩比，同时减少大量非相关数据的扫描，从而更加有效利用 IO 和 CPU 资源。</p>\n</li>\n<li><p>在<strong>查询引擎</strong>方面，Doris 采用 MPP 的模型，节点间和节点内都并行执行，也支持多个大表的分布式 Shuffle Join，从而能够更好应对复杂查询。</p>\n</li>\n</ul>\n<p> <img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g-cd4a2a172ec93222a40231fe1a8d4edd.png\" alt=\"origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g\"></p>\n<ul>\n<li><strong>Doris 查询引擎是向量化</strong>的查询引擎，所有的内存结构能够按照列式布局，能够达到大幅减少虚函数调用、提升 Cache 命中率，高效利用 SIMD 指令的效果。在宽表聚合场景下性能是非向量化引擎的 5-10 倍。</li>\n</ul>\n<p> <img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g-84273f42ae82408ff09c7af6c5b67022.png\" alt=\"origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g\"></p>\n<ul>\n<li><p><strong>Doris 采用了 Adaptive Query Execution 技术，</strong> 可以根据 Runtime Statistics 来动态调整执行计划，比如通过 Runtime Filter 技术能够在运行时生成生成 Filter 推到 Probe 侧，并且能够将 Filter 自动穿透到 Probe 侧最底层的 Scan 节点，从而大幅减少 Probe 的数据量，加速 Join 性能。Doris 的 Runtime Filter 支持 In&#x2F;Min&#x2F;Max&#x2F;Bloom Filter。</p>\n</li>\n<li><p>在<strong>优化器</strong>方面 Doris 使用 CBO 和 RBO 结合的优化策略，RBO 支持常量折叠、子查询改写、谓词下推等，CBO 支持 Join Reorder。目前 CBO 还在持续优化中，主要集中在更加精准的统计信息收集和推导，更加精准的代价模型预估等方面。</p>\n</li>\n<li><p><strong>Doris 也支持强一致的物化视图</strong>，物化视图的更新和选择都在系统内自动进行，不需要用户手动选择，从而大幅减少了物化视图维护的代价。</p>\n</li>\n<li><p><strong>Doris 也支持比较丰富的索引结构，来减少数据的扫描</strong></p>\n</li>\n</ul>\n</blockquote>\n<hr>\n<blockquote>\n<ul>\n<li><p>Doris 数据模型上目前分为三类: AGGREGATE KEY, UNIQUE KEY, DUPLICATE KEY。<strong>三种模型中数据都是按KEY进行排序。</strong></p>\n</li>\n<li><p>AGGREGATE KEY</p>\n</li>\n</ul>\n<p> AGGREGATE KEY相同时，新旧记录进行聚合，目前支持的聚合函数有SUM, MIN, MAX, REPLACE。</p>\n<p> AGGREGATE KEY模型可以提前聚合数据, 适合报表和多维分析业务。</p>\n<ul>\n<li>UNIQUE KEY</li>\n</ul>\n<p> UNIQUE KEY 相同时，新记录覆盖旧记录。目前 UNIQUE KEY 实现上和 AGGREGATE KEY 的 REPLACE 聚合方法一样，二者本质上相同。适用于有更新需求的分析业务。</p>\n<ul>\n<li>DUPLICATE KEY</li>\n</ul>\n<p> 只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>Doris<strong>整体架构</strong>如下图所示，Doris 架构非常简单，只有两类进程</p>\n<ul>\n<li><strong>Frontend（FE）</strong>，主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作。</li>\n<li><strong>另一个是 Backend（BE）</strong>，主要负责数据存储、查询计划的执行。</li>\n</ul>\n<p>这两类进程都是可以横向扩展的，单集群可以支持到数百台机器，数十 PB 的存储容量。并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。</p>\n<p><img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g-11e2c3e5c6b6dc26b4f602697a1071a9.png\" alt=\"origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g\"></p>\n</blockquote>\n<h2 id=\"建表语句\"><a href=\"#建表语句\" class=\"headerlink\" title=\"建表语句\"></a>建表语句</h2><ul>\n<li><p>Range Partition</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> example_db.expamle_range_tbl</span><br><span class=\"line\">(</span><br><span class=\"line\"> `user_id` LARGEINT <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;用户 id&quot;,</span><br><span class=\"line\"> `<span class=\"type\">date</span>` <span class=\"type\">DATE</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入日期时间&quot;,</span><br><span class=\"line\"> `<span class=\"type\">timestamp</span>` DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入的时间戳&quot;,</span><br><span class=\"line\"> `city` <span class=\"type\">VARCHAR</span>(<span class=\"number\">20</span>) COMMENT &quot;用户所在城市&quot;,</span><br><span class=\"line\"> `age` <span class=\"type\">SMALLINT</span> COMMENT &quot;用户年龄&quot;,</span><br><span class=\"line\"> `sex` TINYINT COMMENT &quot;用户性别&quot;,</span><br><span class=\"line\"> `last_visit_date` DATETIME REPLACE <span class=\"keyword\">DEFAULT</span> &quot;1970-01-01 </span><br><span class=\"line\">00:00:00&quot; COMMENT &quot;用户最后一次访问时间&quot;,</span><br><span class=\"line\"> `cost` <span class=\"type\">BIGINT</span> SUM <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户总消费&quot;,</span><br><span class=\"line\"> `max_dwell_time` <span class=\"type\">INT</span> MAX <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户最大停留时间&quot;,</span><br><span class=\"line\"> `min_dwell_time` <span class=\"type\">INT</span> MIN <span class=\"keyword\">DEFAULT</span> &quot;99999&quot; COMMENT &quot;用户最小停留时间&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">ENGINE<span class=\"operator\">=</span>olap</span><br><span class=\"line\">AGGREGATE KEY(`user_id`, `<span class=\"type\">date</span>`, `<span class=\"type\">timestamp</span>`, `city`, `age`, `sex`)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">RANGE</span>(`<span class=\"type\">date</span>`)</span><br><span class=\"line\">(</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201701` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-02-01&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201702` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-03-01&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201703` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-04-01&quot;)</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(`user_id`) BUCKETS <span class=\"number\">16</span></span><br><span class=\"line\">PROPERTIES</span><br><span class=\"line\">(</span><br><span class=\"line\"> &quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\"> &quot;storage_medium&quot; <span class=\"operator\">=</span> &quot;SSD&quot;,</span><br><span class=\"line\"> &quot;storage_cooldown_time&quot; <span class=\"operator\">=</span> &quot;2018-01-01 12:00:00&quot;</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>List Partition</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> example_db.expamle_list_tbl</span><br><span class=\"line\">(</span><br><span class=\"line\"> `user_id` LARGEINT <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;用户 id&quot;,</span><br><span class=\"line\"> `<span class=\"type\">date</span>` <span class=\"type\">DATE</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入日期时间&quot;,</span><br><span class=\"line\"> `<span class=\"type\">timestamp</span>` DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入的时间戳&quot;,</span><br><span class=\"line\"> `city` <span class=\"type\">VARCHAR</span>(<span class=\"number\">20</span>) COMMENT &quot;用户所在城市&quot;,</span><br><span class=\"line\"> `age` <span class=\"type\">SMALLINT</span> COMMENT &quot;用户年龄&quot;,</span><br><span class=\"line\"> `sex` TINYINT COMMENT &quot;用户性别&quot;,</span><br><span class=\"line\"> `last_visit_date` DATETIME REPLACE <span class=\"keyword\">DEFAULT</span> &quot;1970-01-01 </span><br><span class=\"line\">00:00:00&quot; COMMENT &quot;用户最后一次访问时间&quot;,</span><br><span class=\"line\"> `cost` <span class=\"type\">BIGINT</span> SUM <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户总消费&quot;,</span><br><span class=\"line\"> `max_dwell_time` <span class=\"type\">INT</span> MAX <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户最大停留时间&quot;,</span><br><span class=\"line\"> `min_dwell_time` <span class=\"type\">INT</span> MIN <span class=\"keyword\">DEFAULT</span> &quot;99999&quot; COMMENT &quot;用户最小停留时</span><br><span class=\"line\">间&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">ENGINE<span class=\"operator\">=</span>olap</span><br><span class=\"line\">AGGREGATE KEY(`user_id`, `<span class=\"type\">date</span>`, `<span class=\"type\">timestamp</span>`, `city`, `age`, `sex`)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> LIST(`city`)</span><br><span class=\"line\">(</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_cn` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Hong Kong&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_usa` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;New York&quot;, &quot;San Francisco&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_jp` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;Tokyo&quot;)</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(`user_id`) BUCKETS <span class=\"number\">16</span></span><br><span class=\"line\">PROPERTIES</span><br><span class=\"line\">(</span><br><span class=\"line\"> &quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\"> &quot;storage_medium&quot; <span class=\"operator\">=</span> &quot;SSD&quot;,</span><br><span class=\"line\"> &quot;storage_cooldown_time&quot; <span class=\"operator\">=</span> &quot;2018-01-01 12:00:00&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"数据模型\"><a href=\"#数据模型\" class=\"headerlink\" title=\"数据模型\"></a>数据模型</h2><p>Doris 的数据模型主要分为 3 类：Aggregate、Uniq、Duplicate</p>\n<h3 id=\"Aggregate-模型\"><a href=\"#Aggregate-模型\" class=\"headerlink\" title=\"Aggregate 模型\"></a>Aggregate 模型</h3><p>表中的列按照是否设置了 AggregationType，分为 Key（维度列）和 Value（指标列）。没有设置 AggregationType 的称为 Key，设置了 AggregationType 的称为 Value。<br>当我们导入数据时，对于 Key 列相同的行会聚合成一行，而 Value 列会按照设置的AggregationType 进行聚合。AggregationType 目前有以下四种聚合方式：</p>\n<ul>\n<li>➢ SUM：求和，多行的 Value 进行累加。</li>\n<li>➢ REPLACE：替代，下一批数据中的 Value 会替换之前导入过的行中的 Value。<br>   REPLACE_IF_NOT_NULL ：当遇到 null 值则不更新。</li>\n<li>➢ MAX：保留最大值。</li>\n<li>➢ MIN：保留最小值。</li>\n</ul>\n<p>数据的聚合，在 Doris 中有如下三个阶段发生：</p>\n<ul>\n<li>（1）每一批次数据导入的 ETL 阶段。该阶段会在每一批次导入的数据内部进行聚合。</li>\n<li>（2）底层 BE 进行数据 Compaction 的阶段。该阶段，BE 会对已导入的不同批次的数据进行进一步的聚合。</li>\n<li>（3）数据查询阶段。在数据查询时，对于查询涉及到的数据，会进行对应的聚合。</li>\n</ul>\n<p>数据在不同时间，可能聚合的程度不一致。比如一批数据刚导入时，可能还未与之前已存在的数据进行聚合。但是对于用户而言，用户只能查询到聚合后的数据。即不同的聚合程度对于用户查询而言是透明的。用户需始终认为数据以最终的完成的聚合程度存在，而不应假设某些聚合还未发生。（可参阅聚合模型的局限性一节获得更多详情。）</p>\n<h3 id=\"Uniq-模型\"><a href=\"#Uniq-模型\" class=\"headerlink\" title=\"Uniq 模型\"></a>Uniq 模型</h3><p>在某些多维分析场景下，用户更关注的是如何保证 Key 的唯一性，即如何获得 Primary  Key 唯一性约束。因此，我们引入了 Uniq 的数据模型。该模型本质上是聚合模型的一个特 例，也是一种简化的表结构表示方式。</p>\n<p>Uniq 模型完全可以用聚合模型中的 REPLACE 方式替代。其内部的实现方式和数据存 储方式也完全一样。</p>\n<h3 id=\"Duplicate-模型\"><a href=\"#Duplicate-模型\" class=\"headerlink\" title=\"Duplicate 模型\"></a>Duplicate 模型</h3><p>在某些多维分析场景下，数据既没有主键，也没有聚合需求。Duplicate 数据模型可以 满足这类需求。数据完全按照导入文件中的数据进行存储，不会有任何聚合。即使两行数据 完全相同，也都会保留。 而在建表语句中指定的 <code>DUPLICATE KEY</code>，只是用来指明底层数 据按照那些列进行排序。</p>\n<h3 id=\"数据模型的选择建议\"><a href=\"#数据模型的选择建议\" class=\"headerlink\" title=\"数据模型的选择建议\"></a>数据模型的选择建议</h3><p>因为数据模型在建表时就已经确定，且无法修改。所以，选择一个合适的数据模型非常 重要。 </p>\n<p>（1）Aggregate 模型可以通过预聚合，极大地降低聚合查询时所需扫描的数据量和查询 的计算量，非常适合有固定模式的报表类查询场景。但是该模型对 count(*) 查询很不友好。 <strong>同时因为固定了 Value 列上的聚合方式，在进行其他类型的聚合查询时，需要考虑语意正确 性。</strong></p>\n<p>（2）Uniq 模型针对需要唯一主键约束的场景，可以保证主键唯一性约束。<strong>但是无法利 用 ROLLUP 等预聚合带来的查询优势（因为本质是 REPLACE，没有 SUM 这种聚合方式）。</strong> </p>\n<p>（3）Duplicate 适合任意维度的 Ad-hoc 查询。虽然同样无法利用预聚合的特性，但是不 受聚合模型的约束，可以发挥列存模型的优势（只读取相关列，而不需要读取所有 Key 列）</p>\n<h2 id=\"Rollup\"><a href=\"#Rollup\" class=\"headerlink\" title=\"Rollup\"></a>Rollup</h2><p>ROLLUP 在多维分析中是“上卷”的意思，即将数据按某种指定的粒度进行进一步聚 合。</p>\n<h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>在 Doris 中，我们将用户通过建表语句创建出来的表称为 Base 表（Base Table）。Base  表中保存着按用户建表语句指定的方式存储的基础数据。 </p>\n<p>在 Base 表之上，我们可以创建任意多个 ROLLUP 表。这些 ROLLUP 的数据是基于 Base  表产生的，并且在物理上是独立存储的。 ROLLUP 表的基本作用，在于在 Base 表的基础上，获得更粗粒度的聚合数据</p>\n<h3 id=\"Duplicate-模型中的-ROLLUP\"><a href=\"#Duplicate-模型中的-ROLLUP\" class=\"headerlink\" title=\"Duplicate 模型中的 ROLLUP\"></a>Duplicate 模型中的 ROLLUP</h3><p>因为 Duplicate 模型没有聚合的语意。所以该模型中的 ROLLUP，已经失去了“上卷” 这一层含义。而仅仅是作为调整列顺序，以命中前缀索引的作用。下面详细介绍前缀索引， 以及如何使用 ROLLUP 改变前缀索引，以获得更好的查询效率。</p>\n<h4 id=\"前缀索引\"><a href=\"#前缀索引\" class=\"headerlink\" title=\"前缀索引\"></a>前缀索引</h4><p>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。 </p>\n<p>本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构 是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作 为条件进行查找，会非常的高效。 </p>\n<p>在 Aggregate、Uniq 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表 语句中，AGGREGATE KEY、UNIQ KEY 和 DUPLICATE KEY 中指定的列进行排序存储 的。而<strong>前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方 式。</strong></p>\n<h4 id=\"ROLLUP-调整前缀索引\"><a href=\"#ROLLUP-调整前缀索引\" class=\"headerlink\" title=\"ROLLUP 调整前缀索引\"></a>ROLLUP 调整前缀索引</h4><p>因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命 中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过 创建 ROLLUP 来人为的调整列顺序。举例说明。</p>\n<h3 id=\"ROLLUP-的几点说明\"><a href=\"#ROLLUP-的几点说明\" class=\"headerlink\" title=\"ROLLUP 的几点说明\"></a>ROLLUP 的几点说明</h3><p>⚫ ROLLUP 最根本的作用是提高某些查询的查询效率（无论是通过聚合来减少数据 量，还是修改列顺序以匹配前缀索引）。因此 ROLLUP 的含义已经超出了“上卷” 的范围。这也是为什么在源代码中，将其命名为 Materialized Index（物化索引）的 原因。</p>\n<p>⚫ ROLLUP 是附属于 Base 表的，可以看做是 Base 表的一种辅助数据结构。用户可以 在 Base 表的基础上，创建或删除 ROLLUP，但是不能在查询中显式的指定查询某 ROLLUP。是否命中 ROLLUP 完全由 Doris 系统自动决定。 </p>\n<p>⚫ ROLLUP 的数据是独立物理存储的。因此，创建的 ROLLUP 越多，占用的磁盘空 间也就越大。同时对导入速度也会有影响（导入的 ETL 阶段会自动产生所有 ROLLUP 的数据），但是不会降低查询效率（只会更好）。 </p>\n<p>⚫ ROLLUP 的数据更新与 Base 表是完全同步的。用户无需关心这个问题。</p>\n<p>⚫ ROLLUP 中列的聚合方式，与 Base 表完全相同。在创建 ROLLUP 无需指定，也不 能修改。 </p>\n<p>⚫ 查询能否命中 ROLLUP 的一个必要条件（非充分条件）是，查询所涉及的所有列 （包括 select list 和 where 中的查询条件列等）都存在于该 ROLLUP 的列中。否 则，查询只能命中 Base 表。 </p>\n<p>⚫ 某些类型的查询（如 count(*)）在任何条件下，都无法命中 ROLLUP。具体参见接 下来的聚合模型的局限性一节。</p>\n<p>⚫ 可以通过 EXPLAIN your_sql; 命令获得查询执行计划，在执行计划中，查看是否命 中 ROLLUP。</p>\n<p>⚫ 可以通过 DESC tbl_name ALL; 语句显示 Base 表和所有已创建完成的 ROLLUP。</p>\n<h2 id=\"物化视图\"><a href=\"#物化视图\" class=\"headerlink\" title=\"物化视图\"></a>物化视图</h2><p>物化视图就是包含了查询结果的数据库对象，可能是对远程数据的本地 copy，也可能 是一个表或多表 join 后结果的行或列的子集，也可能是聚合后的结果。说白了，就是预先存 储查询结果的一种数据库对象。</p>\n<p>在 Doris 中的物化视图，就是查询结果预先存储起来的特殊的表。 </p>\n<p>物化视图的出现主要是为了满足用户，既能对原始明细数据的任意维度分析，也能快速 的对固定维度进行分析查询。</p>\n<h3 id=\"优势\"><a href=\"#优势\" class=\"headerlink\" title=\"优势\"></a>优势</h3><p>⚫ 对于那些经常重复的使用相同的子查询结果的查询性能大幅提升。 </p>\n<p>⚫ Doris 自动维护物化视图的数据，无论是新的导入，还是删除操作都能保证 base 表 和物化视图表的数据一致性。无需任何额外的人工维护成本。 </p>\n<p>⚫ 查询时，会自动匹配到最优物化视图，并直接从物化视图中读取数据。 自动维护物化视图的数据会造成一些维护开销，会在后面的物化视图的局限性中展开说 明。</p>\n<h3 id=\"物化视图-VS-Rollup\"><a href=\"#物化视图-VS-Rollup\" class=\"headerlink\" title=\"物化视图 VS Rollup\"></a>物化视图 VS Rollup</h3><p>在没有物化视图功能之前，用户一般都是使用 Rollup 功能通过预聚合方式提升查询效 率的。但是 Rollup 具有一定的局限性，他不能基于明细模型做预聚合。 </p>\n<p>物化视图则在覆盖了 Rollup 的功能的同时，还能支持更丰富的聚合函数。所以物化视 图其实是 Rollup 的一个超集。 </p>\n<p>也就是说，之前 ALTER TABLE ADD ROLLUP 语法支持的功能现在均可以通过 CREATE MATERIALIZED VIEW 实现。</p>\n<h3 id=\"物化视图原理\"><a href=\"#物化视图原理\" class=\"headerlink\" title=\"物化视图原理\"></a>物化视图原理</h3><p>Doris 系统提供了一整套对物化视图的 DDL 语法，包括创建，查看，删除。DDL 的语 法和 PostgreSQL, Oracle 都是一致的。但是 Doris 目前创建物化视图只能在单表操作，不支 持 join。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<ul>\n<li><p>MPP（ Massively Parallel Processing - 大规模并行处理）Based高性能、实时的分析型数据库。</p>\n</li>\n<li><p>在<strong>使用接口</strong>方面，Doris 采用 MySQ L 协议，高度兼容 MySQL 语法，支持标准 SQL，用户可以通过各类客户端工具来访问 Doris，并支持与 BI 工具的无缝对接。</p>\n</li>\n<li><p>在<strong>存储引擎</strong>方面，Doris 采用列式存储，按列进行数据的编码压缩和读取，能够实现极高的压缩比，同时减少大量非相关数据的扫描，从而更加有效利用 IO 和 CPU 资源。</p>\n</li>\n<li><p>在<strong>查询引擎</strong>方面，Doris 采用 MPP 的模型，节点间和节点内都并行执行，也支持多个大表的分布式 Shuffle Join，从而能够更好应对复杂查询。</p>\n</li>\n</ul>\n<p> <img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g-cd4a2a172ec93222a40231fe1a8d4edd.png\" alt=\"origin_img_v2_cee507bd-d6ed-4359-9e52-51e9b8458f8g\"></p>\n<ul>\n<li><strong>Doris 查询引擎是向量化</strong>的查询引擎，所有的内存结构能够按照列式布局，能够达到大幅减少虚函数调用、提升 Cache 命中率，高效利用 SIMD 指令的效果。在宽表聚合场景下性能是非向量化引擎的 5-10 倍。</li>\n</ul>\n<p> <img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g-84273f42ae82408ff09c7af6c5b67022.png\" alt=\"origin_img_v2_ad65aae9-9ed0-463e-a34c-94e32b092a4g\"></p>\n<ul>\n<li><p><strong>Doris 采用了 Adaptive Query Execution 技术，</strong> 可以根据 Runtime Statistics 来动态调整执行计划，比如通过 Runtime Filter 技术能够在运行时生成生成 Filter 推到 Probe 侧，并且能够将 Filter 自动穿透到 Probe 侧最底层的 Scan 节点，从而大幅减少 Probe 的数据量，加速 Join 性能。Doris 的 Runtime Filter 支持 In&#x2F;Min&#x2F;Max&#x2F;Bloom Filter。</p>\n</li>\n<li><p>在<strong>优化器</strong>方面 Doris 使用 CBO 和 RBO 结合的优化策略，RBO 支持常量折叠、子查询改写、谓词下推等，CBO 支持 Join Reorder。目前 CBO 还在持续优化中，主要集中在更加精准的统计信息收集和推导，更加精准的代价模型预估等方面。</p>\n</li>\n<li><p><strong>Doris 也支持强一致的物化视图</strong>，物化视图的更新和选择都在系统内自动进行，不需要用户手动选择，从而大幅减少了物化视图维护的代价。</p>\n</li>\n<li><p><strong>Doris 也支持比较丰富的索引结构，来减少数据的扫描</strong></p>\n</li>\n</ul>\n</blockquote>\n<hr>\n<blockquote>\n<ul>\n<li><p>Doris 数据模型上目前分为三类: AGGREGATE KEY, UNIQUE KEY, DUPLICATE KEY。<strong>三种模型中数据都是按KEY进行排序。</strong></p>\n</li>\n<li><p>AGGREGATE KEY</p>\n</li>\n</ul>\n<p> AGGREGATE KEY相同时，新旧记录进行聚合，目前支持的聚合函数有SUM, MIN, MAX, REPLACE。</p>\n<p> AGGREGATE KEY模型可以提前聚合数据, 适合报表和多维分析业务。</p>\n<ul>\n<li>UNIQUE KEY</li>\n</ul>\n<p> UNIQUE KEY 相同时，新记录覆盖旧记录。目前 UNIQUE KEY 实现上和 AGGREGATE KEY 的 REPLACE 聚合方法一样，二者本质上相同。适用于有更新需求的分析业务。</p>\n<ul>\n<li>DUPLICATE KEY</li>\n</ul>\n<p> 只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>Doris<strong>整体架构</strong>如下图所示，Doris 架构非常简单，只有两类进程</p>\n<ul>\n<li><strong>Frontend（FE）</strong>，主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作。</li>\n<li><strong>另一个是 Backend（BE）</strong>，主要负责数据存储、查询计划的执行。</li>\n</ul>\n<p>这两类进程都是可以横向扩展的，单集群可以支持到数百台机器，数十 PB 的存储容量。并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。</p>\n<p><img src=\"https://doris.apache.org/zh-CN/assets/images/origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g-11e2c3e5c6b6dc26b4f602697a1071a9.png\" alt=\"origin_img_v2_28d005e1-21d6-4801-956f-0c06373a7a9g\"></p>\n</blockquote>\n<h2 id=\"建表语句\"><a href=\"#建表语句\" class=\"headerlink\" title=\"建表语句\"></a>建表语句</h2><ul>\n<li><p>Range Partition</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> example_db.expamle_range_tbl</span><br><span class=\"line\">(</span><br><span class=\"line\"> `user_id` LARGEINT <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;用户 id&quot;,</span><br><span class=\"line\"> `<span class=\"type\">date</span>` <span class=\"type\">DATE</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入日期时间&quot;,</span><br><span class=\"line\"> `<span class=\"type\">timestamp</span>` DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入的时间戳&quot;,</span><br><span class=\"line\"> `city` <span class=\"type\">VARCHAR</span>(<span class=\"number\">20</span>) COMMENT &quot;用户所在城市&quot;,</span><br><span class=\"line\"> `age` <span class=\"type\">SMALLINT</span> COMMENT &quot;用户年龄&quot;,</span><br><span class=\"line\"> `sex` TINYINT COMMENT &quot;用户性别&quot;,</span><br><span class=\"line\"> `last_visit_date` DATETIME REPLACE <span class=\"keyword\">DEFAULT</span> &quot;1970-01-01 </span><br><span class=\"line\">00:00:00&quot; COMMENT &quot;用户最后一次访问时间&quot;,</span><br><span class=\"line\"> `cost` <span class=\"type\">BIGINT</span> SUM <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户总消费&quot;,</span><br><span class=\"line\"> `max_dwell_time` <span class=\"type\">INT</span> MAX <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户最大停留时间&quot;,</span><br><span class=\"line\"> `min_dwell_time` <span class=\"type\">INT</span> MIN <span class=\"keyword\">DEFAULT</span> &quot;99999&quot; COMMENT &quot;用户最小停留时间&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">ENGINE<span class=\"operator\">=</span>olap</span><br><span class=\"line\">AGGREGATE KEY(`user_id`, `<span class=\"type\">date</span>`, `<span class=\"type\">timestamp</span>`, `city`, `age`, `sex`)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">RANGE</span>(`<span class=\"type\">date</span>`)</span><br><span class=\"line\">(</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201701` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-02-01&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201702` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-03-01&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p201703` <span class=\"keyword\">VALUES</span> LESS THAN (&quot;2017-04-01&quot;)</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(`user_id`) BUCKETS <span class=\"number\">16</span></span><br><span class=\"line\">PROPERTIES</span><br><span class=\"line\">(</span><br><span class=\"line\"> &quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\"> &quot;storage_medium&quot; <span class=\"operator\">=</span> &quot;SSD&quot;,</span><br><span class=\"line\"> &quot;storage_cooldown_time&quot; <span class=\"operator\">=</span> &quot;2018-01-01 12:00:00&quot;</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>List Partition</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> example_db.expamle_list_tbl</span><br><span class=\"line\">(</span><br><span class=\"line\"> `user_id` LARGEINT <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;用户 id&quot;,</span><br><span class=\"line\"> `<span class=\"type\">date</span>` <span class=\"type\">DATE</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入日期时间&quot;,</span><br><span class=\"line\"> `<span class=\"type\">timestamp</span>` DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> COMMENT &quot;数据灌入的时间戳&quot;,</span><br><span class=\"line\"> `city` <span class=\"type\">VARCHAR</span>(<span class=\"number\">20</span>) COMMENT &quot;用户所在城市&quot;,</span><br><span class=\"line\"> `age` <span class=\"type\">SMALLINT</span> COMMENT &quot;用户年龄&quot;,</span><br><span class=\"line\"> `sex` TINYINT COMMENT &quot;用户性别&quot;,</span><br><span class=\"line\"> `last_visit_date` DATETIME REPLACE <span class=\"keyword\">DEFAULT</span> &quot;1970-01-01 </span><br><span class=\"line\">00:00:00&quot; COMMENT &quot;用户最后一次访问时间&quot;,</span><br><span class=\"line\"> `cost` <span class=\"type\">BIGINT</span> SUM <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户总消费&quot;,</span><br><span class=\"line\"> `max_dwell_time` <span class=\"type\">INT</span> MAX <span class=\"keyword\">DEFAULT</span> &quot;0&quot; COMMENT &quot;用户最大停留时间&quot;,</span><br><span class=\"line\"> `min_dwell_time` <span class=\"type\">INT</span> MIN <span class=\"keyword\">DEFAULT</span> &quot;99999&quot; COMMENT &quot;用户最小停留时</span><br><span class=\"line\">间&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\">ENGINE<span class=\"operator\">=</span>olap</span><br><span class=\"line\">AGGREGATE KEY(`user_id`, `<span class=\"type\">date</span>`, `<span class=\"type\">timestamp</span>`, `city`, `age`, `sex`)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> LIST(`city`)</span><br><span class=\"line\">(</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_cn` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;Beijing&quot;, &quot;Shanghai&quot;, &quot;Hong Kong&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_usa` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;New York&quot;, &quot;San Francisco&quot;),</span><br><span class=\"line\"> <span class=\"keyword\">PARTITION</span> `p_jp` <span class=\"keyword\">VALUES</span> <span class=\"keyword\">IN</span> (&quot;Tokyo&quot;)</span><br><span class=\"line\">)</span><br><span class=\"line\">DISTRIBUTED <span class=\"keyword\">BY</span> HASH(`user_id`) BUCKETS <span class=\"number\">16</span></span><br><span class=\"line\">PROPERTIES</span><br><span class=\"line\">(</span><br><span class=\"line\"> &quot;replication_num&quot; <span class=\"operator\">=</span> &quot;3&quot;,</span><br><span class=\"line\"> &quot;storage_medium&quot; <span class=\"operator\">=</span> &quot;SSD&quot;,</span><br><span class=\"line\"> &quot;storage_cooldown_time&quot; <span class=\"operator\">=</span> &quot;2018-01-01 12:00:00&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"数据模型\"><a href=\"#数据模型\" class=\"headerlink\" title=\"数据模型\"></a>数据模型</h2><p>Doris 的数据模型主要分为 3 类：Aggregate、Uniq、Duplicate</p>\n<h3 id=\"Aggregate-模型\"><a href=\"#Aggregate-模型\" class=\"headerlink\" title=\"Aggregate 模型\"></a>Aggregate 模型</h3><p>表中的列按照是否设置了 AggregationType，分为 Key（维度列）和 Value（指标列）。没有设置 AggregationType 的称为 Key，设置了 AggregationType 的称为 Value。<br>当我们导入数据时，对于 Key 列相同的行会聚合成一行，而 Value 列会按照设置的AggregationType 进行聚合。AggregationType 目前有以下四种聚合方式：</p>\n<ul>\n<li>➢ SUM：求和，多行的 Value 进行累加。</li>\n<li>➢ REPLACE：替代，下一批数据中的 Value 会替换之前导入过的行中的 Value。<br>   REPLACE_IF_NOT_NULL ：当遇到 null 值则不更新。</li>\n<li>➢ MAX：保留最大值。</li>\n<li>➢ MIN：保留最小值。</li>\n</ul>\n<p>数据的聚合，在 Doris 中有如下三个阶段发生：</p>\n<ul>\n<li>（1）每一批次数据导入的 ETL 阶段。该阶段会在每一批次导入的数据内部进行聚合。</li>\n<li>（2）底层 BE 进行数据 Compaction 的阶段。该阶段，BE 会对已导入的不同批次的数据进行进一步的聚合。</li>\n<li>（3）数据查询阶段。在数据查询时，对于查询涉及到的数据，会进行对应的聚合。</li>\n</ul>\n<p>数据在不同时间，可能聚合的程度不一致。比如一批数据刚导入时，可能还未与之前已存在的数据进行聚合。但是对于用户而言，用户只能查询到聚合后的数据。即不同的聚合程度对于用户查询而言是透明的。用户需始终认为数据以最终的完成的聚合程度存在，而不应假设某些聚合还未发生。（可参阅聚合模型的局限性一节获得更多详情。）</p>\n<h3 id=\"Uniq-模型\"><a href=\"#Uniq-模型\" class=\"headerlink\" title=\"Uniq 模型\"></a>Uniq 模型</h3><p>在某些多维分析场景下，用户更关注的是如何保证 Key 的唯一性，即如何获得 Primary  Key 唯一性约束。因此，我们引入了 Uniq 的数据模型。该模型本质上是聚合模型的一个特 例，也是一种简化的表结构表示方式。</p>\n<p>Uniq 模型完全可以用聚合模型中的 REPLACE 方式替代。其内部的实现方式和数据存 储方式也完全一样。</p>\n<h3 id=\"Duplicate-模型\"><a href=\"#Duplicate-模型\" class=\"headerlink\" title=\"Duplicate 模型\"></a>Duplicate 模型</h3><p>在某些多维分析场景下，数据既没有主键，也没有聚合需求。Duplicate 数据模型可以 满足这类需求。数据完全按照导入文件中的数据进行存储，不会有任何聚合。即使两行数据 完全相同，也都会保留。 而在建表语句中指定的 <code>DUPLICATE KEY</code>，只是用来指明底层数 据按照那些列进行排序。</p>\n<h3 id=\"数据模型的选择建议\"><a href=\"#数据模型的选择建议\" class=\"headerlink\" title=\"数据模型的选择建议\"></a>数据模型的选择建议</h3><p>因为数据模型在建表时就已经确定，且无法修改。所以，选择一个合适的数据模型非常 重要。 </p>\n<p>（1）Aggregate 模型可以通过预聚合，极大地降低聚合查询时所需扫描的数据量和查询 的计算量，非常适合有固定模式的报表类查询场景。但是该模型对 count(*) 查询很不友好。 <strong>同时因为固定了 Value 列上的聚合方式，在进行其他类型的聚合查询时，需要考虑语意正确 性。</strong></p>\n<p>（2）Uniq 模型针对需要唯一主键约束的场景，可以保证主键唯一性约束。<strong>但是无法利 用 ROLLUP 等预聚合带来的查询优势（因为本质是 REPLACE，没有 SUM 这种聚合方式）。</strong> </p>\n<p>（3）Duplicate 适合任意维度的 Ad-hoc 查询。虽然同样无法利用预聚合的特性，但是不 受聚合模型的约束，可以发挥列存模型的优势（只读取相关列，而不需要读取所有 Key 列）</p>\n<h2 id=\"Rollup\"><a href=\"#Rollup\" class=\"headerlink\" title=\"Rollup\"></a>Rollup</h2><p>ROLLUP 在多维分析中是“上卷”的意思，即将数据按某种指定的粒度进行进一步聚 合。</p>\n<h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>在 Doris 中，我们将用户通过建表语句创建出来的表称为 Base 表（Base Table）。Base  表中保存着按用户建表语句指定的方式存储的基础数据。 </p>\n<p>在 Base 表之上，我们可以创建任意多个 ROLLUP 表。这些 ROLLUP 的数据是基于 Base  表产生的，并且在物理上是独立存储的。 ROLLUP 表的基本作用，在于在 Base 表的基础上，获得更粗粒度的聚合数据</p>\n<h3 id=\"Duplicate-模型中的-ROLLUP\"><a href=\"#Duplicate-模型中的-ROLLUP\" class=\"headerlink\" title=\"Duplicate 模型中的 ROLLUP\"></a>Duplicate 模型中的 ROLLUP</h3><p>因为 Duplicate 模型没有聚合的语意。所以该模型中的 ROLLUP，已经失去了“上卷” 这一层含义。而仅仅是作为调整列顺序，以命中前缀索引的作用。下面详细介绍前缀索引， 以及如何使用 ROLLUP 改变前缀索引，以获得更好的查询效率。</p>\n<h4 id=\"前缀索引\"><a href=\"#前缀索引\" class=\"headerlink\" title=\"前缀索引\"></a>前缀索引</h4><p>不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。 </p>\n<p>本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构 是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作 为条件进行查找，会非常的高效。 </p>\n<p>在 Aggregate、Uniq 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表 语句中，AGGREGATE KEY、UNIQ KEY 和 DUPLICATE KEY 中指定的列进行排序存储 的。而<strong>前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方 式。</strong></p>\n<h4 id=\"ROLLUP-调整前缀索引\"><a href=\"#ROLLUP-调整前缀索引\" class=\"headerlink\" title=\"ROLLUP 调整前缀索引\"></a>ROLLUP 调整前缀索引</h4><p>因为建表时已经指定了列顺序，所以一个表只有一种前缀索引。这对于使用其他不能命 中前缀索引的列作为条件进行的查询来说，效率上可能无法满足需求。因此，我们可以通过 创建 ROLLUP 来人为的调整列顺序。举例说明。</p>\n<h3 id=\"ROLLUP-的几点说明\"><a href=\"#ROLLUP-的几点说明\" class=\"headerlink\" title=\"ROLLUP 的几点说明\"></a>ROLLUP 的几点说明</h3><p>⚫ ROLLUP 最根本的作用是提高某些查询的查询效率（无论是通过聚合来减少数据 量，还是修改列顺序以匹配前缀索引）。因此 ROLLUP 的含义已经超出了“上卷” 的范围。这也是为什么在源代码中，将其命名为 Materialized Index（物化索引）的 原因。</p>\n<p>⚫ ROLLUP 是附属于 Base 表的，可以看做是 Base 表的一种辅助数据结构。用户可以 在 Base 表的基础上，创建或删除 ROLLUP，但是不能在查询中显式的指定查询某 ROLLUP。是否命中 ROLLUP 完全由 Doris 系统自动决定。 </p>\n<p>⚫ ROLLUP 的数据是独立物理存储的。因此，创建的 ROLLUP 越多，占用的磁盘空 间也就越大。同时对导入速度也会有影响（导入的 ETL 阶段会自动产生所有 ROLLUP 的数据），但是不会降低查询效率（只会更好）。 </p>\n<p>⚫ ROLLUP 的数据更新与 Base 表是完全同步的。用户无需关心这个问题。</p>\n<p>⚫ ROLLUP 中列的聚合方式，与 Base 表完全相同。在创建 ROLLUP 无需指定，也不 能修改。 </p>\n<p>⚫ 查询能否命中 ROLLUP 的一个必要条件（非充分条件）是，查询所涉及的所有列 （包括 select list 和 where 中的查询条件列等）都存在于该 ROLLUP 的列中。否 则，查询只能命中 Base 表。 </p>\n<p>⚫ 某些类型的查询（如 count(*)）在任何条件下，都无法命中 ROLLUP。具体参见接 下来的聚合模型的局限性一节。</p>\n<p>⚫ 可以通过 EXPLAIN your_sql; 命令获得查询执行计划，在执行计划中，查看是否命 中 ROLLUP。</p>\n<p>⚫ 可以通过 DESC tbl_name ALL; 语句显示 Base 表和所有已创建完成的 ROLLUP。</p>\n<h2 id=\"物化视图\"><a href=\"#物化视图\" class=\"headerlink\" title=\"物化视图\"></a>物化视图</h2><p>物化视图就是包含了查询结果的数据库对象，可能是对远程数据的本地 copy，也可能 是一个表或多表 join 后结果的行或列的子集，也可能是聚合后的结果。说白了，就是预先存 储查询结果的一种数据库对象。</p>\n<p>在 Doris 中的物化视图，就是查询结果预先存储起来的特殊的表。 </p>\n<p>物化视图的出现主要是为了满足用户，既能对原始明细数据的任意维度分析，也能快速 的对固定维度进行分析查询。</p>\n<h3 id=\"优势\"><a href=\"#优势\" class=\"headerlink\" title=\"优势\"></a>优势</h3><p>⚫ 对于那些经常重复的使用相同的子查询结果的查询性能大幅提升。 </p>\n<p>⚫ Doris 自动维护物化视图的数据，无论是新的导入，还是删除操作都能保证 base 表 和物化视图表的数据一致性。无需任何额外的人工维护成本。 </p>\n<p>⚫ 查询时，会自动匹配到最优物化视图，并直接从物化视图中读取数据。 自动维护物化视图的数据会造成一些维护开销，会在后面的物化视图的局限性中展开说 明。</p>\n<h3 id=\"物化视图-VS-Rollup\"><a href=\"#物化视图-VS-Rollup\" class=\"headerlink\" title=\"物化视图 VS Rollup\"></a>物化视图 VS Rollup</h3><p>在没有物化视图功能之前，用户一般都是使用 Rollup 功能通过预聚合方式提升查询效 率的。但是 Rollup 具有一定的局限性，他不能基于明细模型做预聚合。 </p>\n<p>物化视图则在覆盖了 Rollup 的功能的同时，还能支持更丰富的聚合函数。所以物化视 图其实是 Rollup 的一个超集。 </p>\n<p>也就是说，之前 ALTER TABLE ADD ROLLUP 语法支持的功能现在均可以通过 CREATE MATERIALIZED VIEW 实现。</p>\n<h3 id=\"物化视图原理\"><a href=\"#物化视图原理\" class=\"headerlink\" title=\"物化视图原理\"></a>物化视图原理</h3><p>Doris 系统提供了一整套对物化视图的 DDL 语法，包括创建，查看，删除。DDL 的语 法和 PostgreSQL, Oracle 都是一致的。但是 Doris 目前创建物化视图只能在单表操作，不支 持 join。</p>\n"},{"title":"Linux系统编程-文件与I/O","abbrlink":19576,"date":"2022-10-09T05:47:56.000Z","updated":"2022-10-09T05:47:56.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n##  read/write\n\n读常规文件是不会阻塞的，不管读多少字节，`read`一定会在有限的时间内返回。从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用`read`读终端设备就会阻塞，如果网络上没有接收到数据包，调用`read`从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定。\n\n现在明确一下阻塞（Block）这个概念。当进程调用一个阻塞的系统函数时，该进程被置于睡眠（Sleep）状态，这时内核调度其它进程运行，直到该进程等待的事件发生了（比如网络上接收到数据包，或者调用`sleep`指定的睡眠时间到了）它才有可能继续运行。与睡眠状态相对的是运行（Running）状态，在Linux内核中，处于运行状态的进程分为两种情况：\n\n- 正在被调度执行。CPU处于该进程的上下文环境中，程序计数器（`eip`）里保存着该进程的指令地址，通用寄存器里保存着该进程运算过程的中间结果，正在执行该进程的指令，正在读写该进程的地址空间。\n- 就绪状态。该进程不需要等待什么事件发生，随时都可以执行，但CPU暂时还在执行另一个进程，所以该进程在一个就绪队列中等待被内核调度。系统中可能同时有多个就绪的进程，那么该调度谁执行呢？内核的调度算法是基于优先级和时间片的，而且会根据每个进程的运行情况动态调整它的优先级和时间片，让每个进程都能比较公平地得到机会执行，同时要兼顾用户体验，不能让和用户交互的进程响应太慢。\n\n## mmap函数系统调用\n\n`mmap`可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用指针来做而不需要`read`/`write`函数。\n\n> mmap, munmap - map or unmap files or devices into memory\n>\n> void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);\n> int munmap(void *addr, size_t length);\n>\n> mmap() creates a new mapping in the virtual address space of the calling process.  The starting address for the new mapping is specified in addr.  The length argument specifies the length of the mapping (which must be greater than 0).\n\n```c\n#include <stdlib.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n\nint main(void)\n{\n        int *p;\n        int fd = open(\"hello\", O_RDWR);\n        if (fd < 0) {\n                perror(\"open hello\");\n                exit(1);\n        }\n\n          // 6：文件映射到内存的长度\n          // PROT_WRITE：映射的这段内存可写\n          // MAP_SHARED：多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。\n          // fd：文件描述符\n          // 0：offset\n        p = mmap(NULL, 6, PROT_WRITE, MAP_SHARED, fd, 0);\n        if (p == MAP_FAILED) {\n                perror(\"mmap\");\n                exit(1);\n        }\n        close(fd);\n\n      char *char_point = (char *)p;\n      //p[0] = 0x30313233;\n      // 修改第一个字符为i\n      char_point[0] = 'i';\n\n      // 解除内存映射\n      munmap(p, 6);\n      return 0;\n}\n```\n\n修改后，mmap不会立即将更新同步到文件，可以用msync函数将更新刷到内存。\n\n>  msync - synchronize a file with a memory map \n>\n> int msync(void *addr, size_t length, int flags);\n>\n> msync() flushes changes made to the in-core copy of a file that was mapped into memory using mmap(2) back to the filesystem.  Without use of this call, there is no guarantee that changes are written back before munmap(2) is called.  To be more precise,  the  part  of the file that corresponds to the memory area starting at addr and having length length is updated.\n","source":"_posts/os/文件与IO.md","raw":"---\ntitle: Linux系统编程-文件与I/O\ntags:\n  - os\ncategories:\n  - - os\nabbrlink: 19576\ndate: 2022-10-09 13:47:56\nupdated: 2022-10-09 13:47:56\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n##  read/write\n\n读常规文件是不会阻塞的，不管读多少字节，`read`一定会在有限的时间内返回。从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用`read`读终端设备就会阻塞，如果网络上没有接收到数据包，调用`read`从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定。\n\n现在明确一下阻塞（Block）这个概念。当进程调用一个阻塞的系统函数时，该进程被置于睡眠（Sleep）状态，这时内核调度其它进程运行，直到该进程等待的事件发生了（比如网络上接收到数据包，或者调用`sleep`指定的睡眠时间到了）它才有可能继续运行。与睡眠状态相对的是运行（Running）状态，在Linux内核中，处于运行状态的进程分为两种情况：\n\n- 正在被调度执行。CPU处于该进程的上下文环境中，程序计数器（`eip`）里保存着该进程的指令地址，通用寄存器里保存着该进程运算过程的中间结果，正在执行该进程的指令，正在读写该进程的地址空间。\n- 就绪状态。该进程不需要等待什么事件发生，随时都可以执行，但CPU暂时还在执行另一个进程，所以该进程在一个就绪队列中等待被内核调度。系统中可能同时有多个就绪的进程，那么该调度谁执行呢？内核的调度算法是基于优先级和时间片的，而且会根据每个进程的运行情况动态调整它的优先级和时间片，让每个进程都能比较公平地得到机会执行，同时要兼顾用户体验，不能让和用户交互的进程响应太慢。\n\n## mmap函数系统调用\n\n`mmap`可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用指针来做而不需要`read`/`write`函数。\n\n> mmap, munmap - map or unmap files or devices into memory\n>\n> void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);\n> int munmap(void *addr, size_t length);\n>\n> mmap() creates a new mapping in the virtual address space of the calling process.  The starting address for the new mapping is specified in addr.  The length argument specifies the length of the mapping (which must be greater than 0).\n\n```c\n#include <stdlib.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n\nint main(void)\n{\n        int *p;\n        int fd = open(\"hello\", O_RDWR);\n        if (fd < 0) {\n                perror(\"open hello\");\n                exit(1);\n        }\n\n          // 6：文件映射到内存的长度\n          // PROT_WRITE：映射的这段内存可写\n          // MAP_SHARED：多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。\n          // fd：文件描述符\n          // 0：offset\n        p = mmap(NULL, 6, PROT_WRITE, MAP_SHARED, fd, 0);\n        if (p == MAP_FAILED) {\n                perror(\"mmap\");\n                exit(1);\n        }\n        close(fd);\n\n      char *char_point = (char *)p;\n      //p[0] = 0x30313233;\n      // 修改第一个字符为i\n      char_point[0] = 'i';\n\n      // 解除内存映射\n      munmap(p, 6);\n      return 0;\n}\n```\n\n修改后，mmap不会立即将更新同步到文件，可以用msync函数将更新刷到内存。\n\n>  msync - synchronize a file with a memory map \n>\n> int msync(void *addr, size_t length, int flags);\n>\n> msync() flushes changes made to the in-core copy of a file that was mapped into memory using mmap(2) back to the filesystem.  Without use of this call, there is no guarantee that changes are written back before munmap(2) is called.  To be more precise,  the  part  of the file that corresponds to the memory area starting at addr and having length length is updated.\n","slug":"os/文件与IO","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksz00238j5mb7926llr","content":"<h2 id=\"read-x2F-write\"><a href=\"#read-x2F-write\" class=\"headerlink\" title=\"read&#x2F;write\"></a>read&#x2F;write</h2><p>读常规文件是不会阻塞的，不管读多少字节，<code>read</code>一定会在有限的时间内返回。从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用<code>read</code>读终端设备就会阻塞，如果网络上没有接收到数据包，调用<code>read</code>从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定。</p>\n<p>现在明确一下阻塞（Block）这个概念。当进程调用一个阻塞的系统函数时，该进程被置于睡眠（Sleep）状态，这时内核调度其它进程运行，直到该进程等待的事件发生了（比如网络上接收到数据包，或者调用<code>sleep</code>指定的睡眠时间到了）它才有可能继续运行。与睡眠状态相对的是运行（Running）状态，在Linux内核中，处于运行状态的进程分为两种情况：</p>\n<ul>\n<li>正在被调度执行。CPU处于该进程的上下文环境中，程序计数器（<code>eip</code>）里保存着该进程的指令地址，通用寄存器里保存着该进程运算过程的中间结果，正在执行该进程的指令，正在读写该进程的地址空间。</li>\n<li>就绪状态。该进程不需要等待什么事件发生，随时都可以执行，但CPU暂时还在执行另一个进程，所以该进程在一个就绪队列中等待被内核调度。系统中可能同时有多个就绪的进程，那么该调度谁执行呢？内核的调度算法是基于优先级和时间片的，而且会根据每个进程的运行情况动态调整它的优先级和时间片，让每个进程都能比较公平地得到机会执行，同时要兼顾用户体验，不能让和用户交互的进程响应太慢。</li>\n</ul>\n<h2 id=\"mmap函数系统调用\"><a href=\"#mmap函数系统调用\" class=\"headerlink\" title=\"mmap函数系统调用\"></a>mmap函数系统调用</h2><p><code>mmap</code>可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用指针来做而不需要<code>read</code>&#x2F;<code>write</code>函数。</p>\n<blockquote>\n<p>mmap, munmap - map or unmap files or devices into memory</p>\n<p>void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);<br>int munmap(void *addr, size_t length);</p>\n<p>mmap() creates a new mapping in the virtual address space of the calling process.  The starting address for the new mapping is specified in addr.  The length argument specifies the length of the mapping (which must be greater than 0).</p>\n</blockquote>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;sys/mman.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;fcntl.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">void</span>)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> *p;</span><br><span class=\"line\">        <span class=\"type\">int</span> fd = open(<span class=\"string\">&quot;hello&quot;</span>, O_RDWR);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (fd &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                perror(<span class=\"string\">&quot;open hello&quot;</span>);</span><br><span class=\"line\">                <span class=\"built_in\">exit</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"comment\">// 6：文件映射到内存的长度</span></span><br><span class=\"line\">          <span class=\"comment\">// PROT_WRITE：映射的这段内存可写</span></span><br><span class=\"line\">          <span class=\"comment\">// MAP_SHARED：多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。</span></span><br><span class=\"line\">          <span class=\"comment\">// fd：文件描述符</span></span><br><span class=\"line\">          <span class=\"comment\">// 0：offset</span></span><br><span class=\"line\">        p = mmap(<span class=\"literal\">NULL</span>, <span class=\"number\">6</span>, PROT_WRITE, MAP_SHARED, fd, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == MAP_FAILED) &#123;</span><br><span class=\"line\">                perror(<span class=\"string\">&quot;mmap&quot;</span>);</span><br><span class=\"line\">                <span class=\"built_in\">exit</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        close(fd);</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"type\">char</span> *char_point = (<span class=\"type\">char</span> *)p;</span><br><span class=\"line\">      <span class=\"comment\">//p[0] = 0x30313233;</span></span><br><span class=\"line\">      <span class=\"comment\">// 修改第一个字符为i</span></span><br><span class=\"line\">      char_point[<span class=\"number\">0</span>] = <span class=\"string\">&#x27;i&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 解除内存映射</span></span><br><span class=\"line\">      munmap(p, <span class=\"number\">6</span>);</span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>修改后，mmap不会立即将更新同步到文件，可以用msync函数将更新刷到内存。</p>\n<blockquote>\n<p> msync - synchronize a file with a memory map </p>\n<p>int msync(void *addr, size_t length, int flags);</p>\n<p>msync() flushes changes made to the in-core copy of a file that was mapped into memory using mmap(2) back to the filesystem.  Without use of this call, there is no guarantee that changes are written back before munmap(2) is called.  To be more precise,  the  part  of the file that corresponds to the memory area starting at addr and having length length is updated.</p>\n</blockquote>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"read-x2F-write\"><a href=\"#read-x2F-write\" class=\"headerlink\" title=\"read&#x2F;write\"></a>read&#x2F;write</h2><p>读常规文件是不会阻塞的，不管读多少字节，<code>read</code>一定会在有限的时间内返回。从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用<code>read</code>读终端设备就会阻塞，如果网络上没有接收到数据包，调用<code>read</code>从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定。</p>\n<p>现在明确一下阻塞（Block）这个概念。当进程调用一个阻塞的系统函数时，该进程被置于睡眠（Sleep）状态，这时内核调度其它进程运行，直到该进程等待的事件发生了（比如网络上接收到数据包，或者调用<code>sleep</code>指定的睡眠时间到了）它才有可能继续运行。与睡眠状态相对的是运行（Running）状态，在Linux内核中，处于运行状态的进程分为两种情况：</p>\n<ul>\n<li>正在被调度执行。CPU处于该进程的上下文环境中，程序计数器（<code>eip</code>）里保存着该进程的指令地址，通用寄存器里保存着该进程运算过程的中间结果，正在执行该进程的指令，正在读写该进程的地址空间。</li>\n<li>就绪状态。该进程不需要等待什么事件发生，随时都可以执行，但CPU暂时还在执行另一个进程，所以该进程在一个就绪队列中等待被内核调度。系统中可能同时有多个就绪的进程，那么该调度谁执行呢？内核的调度算法是基于优先级和时间片的，而且会根据每个进程的运行情况动态调整它的优先级和时间片，让每个进程都能比较公平地得到机会执行，同时要兼顾用户体验，不能让和用户交互的进程响应太慢。</li>\n</ul>\n<h2 id=\"mmap函数系统调用\"><a href=\"#mmap函数系统调用\" class=\"headerlink\" title=\"mmap函数系统调用\"></a>mmap函数系统调用</h2><p><code>mmap</code>可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用指针来做而不需要<code>read</code>&#x2F;<code>write</code>函数。</p>\n<blockquote>\n<p>mmap, munmap - map or unmap files or devices into memory</p>\n<p>void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);<br>int munmap(void *addr, size_t length);</p>\n<p>mmap() creates a new mapping in the virtual address space of the calling process.  The starting address for the new mapping is specified in addr.  The length argument specifies the length of the mapping (which must be greater than 0).</p>\n</blockquote>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;sys/mman.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;fcntl.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">void</span>)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> *p;</span><br><span class=\"line\">        <span class=\"type\">int</span> fd = open(<span class=\"string\">&quot;hello&quot;</span>, O_RDWR);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (fd &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                perror(<span class=\"string\">&quot;open hello&quot;</span>);</span><br><span class=\"line\">                <span class=\"built_in\">exit</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          <span class=\"comment\">// 6：文件映射到内存的长度</span></span><br><span class=\"line\">          <span class=\"comment\">// PROT_WRITE：映射的这段内存可写</span></span><br><span class=\"line\">          <span class=\"comment\">// MAP_SHARED：多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。</span></span><br><span class=\"line\">          <span class=\"comment\">// fd：文件描述符</span></span><br><span class=\"line\">          <span class=\"comment\">// 0：offset</span></span><br><span class=\"line\">        p = mmap(<span class=\"literal\">NULL</span>, <span class=\"number\">6</span>, PROT_WRITE, MAP_SHARED, fd, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == MAP_FAILED) &#123;</span><br><span class=\"line\">                perror(<span class=\"string\">&quot;mmap&quot;</span>);</span><br><span class=\"line\">                <span class=\"built_in\">exit</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        close(fd);</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"type\">char</span> *char_point = (<span class=\"type\">char</span> *)p;</span><br><span class=\"line\">      <span class=\"comment\">//p[0] = 0x30313233;</span></span><br><span class=\"line\">      <span class=\"comment\">// 修改第一个字符为i</span></span><br><span class=\"line\">      char_point[<span class=\"number\">0</span>] = <span class=\"string\">&#x27;i&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 解除内存映射</span></span><br><span class=\"line\">      munmap(p, <span class=\"number\">6</span>);</span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>修改后，mmap不会立即将更新同步到文件，可以用msync函数将更新刷到内存。</p>\n<blockquote>\n<p> msync - synchronize a file with a memory map </p>\n<p>int msync(void *addr, size_t length, int flags);</p>\n<p>msync() flushes changes made to the in-core copy of a file that was mapped into memory using mmap(2) back to the filesystem.  Without use of this call, there is no guarantee that changes are written back before munmap(2) is called.  To be more precise,  the  part  of the file that corresponds to the memory area starting at addr and having length length is updated.</p>\n</blockquote>\n"},{"title":"配置hadoop-snappy那些事","abbrlink":27401,"date":"2022-11-01T09:46:42.000Z","updated":"2022-11-01T09:46:42.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 前言\n\n在Apache Hadoop3.x社区二进制发行版中已经包含hadoop-snappy，同时Centos7已经自带snappy本地库。因此Hadoop3.x+Centos7无需配置snappy本地库。可运行`hadoop checknative -a`检查snappy本地库是否可用。\n\n> 看了网上繁琐的Hadoop Snappy配置过程，配置了半天，才发现是白费功夫。原来是Hive的一个Bug。\n\n## Hive3.1.2中orc文件snappy压缩的Bug\n\n### Bug描述\n\n- 创建一个Hive表，存储为orc文件，同时启用snappy压缩。\n\n  ```hive\n  CREATE TABLE `default`.`user_orc` (\n    `tid` INT,\n    `userid` STRING\n  )\n  STORED AS orc\n  TBLPROPERTIES (\n    \"orc.compress\"=\"SNAPPY\"\n  );\n  ```\n  \n- Insert overwrite进一些数据\n\n  ```hive\n  insert overwrite table user_orc select * from user_1;\n  ```\n\n- 会在HDFS生成/user/hadoop/warehouse/user_orc/000000_0文件，文件没有带有.orc后缀\n\n- 使用`hive --orcfiledump /user/hadoop/warehouse/user_orc/000000_0`查看该orc文件信息，发现Compression: ZLIB，即默认的ZLIB压缩算法，snappy压缩算法没有生效。\n\n### 解决\n\n- 1、设置`set hive.exec.orc.default.compress=snappy;`参数，可暂时解决。\n- 2、升级到hive-3.1.3，貌似已经修复该问题。","source":"_posts/bigdata/配置hadoop-snappy那些事.md","raw":"---\ntitle: 配置hadoop-snappy那些事\ntags:\n  - ''\ncategories:\n  - []\nabbrlink: 27401\ndate: 2022-11-01 17:46:42\nupdated: 2022-11-01 17:46:42\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 前言\n\n在Apache Hadoop3.x社区二进制发行版中已经包含hadoop-snappy，同时Centos7已经自带snappy本地库。因此Hadoop3.x+Centos7无需配置snappy本地库。可运行`hadoop checknative -a`检查snappy本地库是否可用。\n\n> 看了网上繁琐的Hadoop Snappy配置过程，配置了半天，才发现是白费功夫。原来是Hive的一个Bug。\n\n## Hive3.1.2中orc文件snappy压缩的Bug\n\n### Bug描述\n\n- 创建一个Hive表，存储为orc文件，同时启用snappy压缩。\n\n  ```hive\n  CREATE TABLE `default`.`user_orc` (\n    `tid` INT,\n    `userid` STRING\n  )\n  STORED AS orc\n  TBLPROPERTIES (\n    \"orc.compress\"=\"SNAPPY\"\n  );\n  ```\n  \n- Insert overwrite进一些数据\n\n  ```hive\n  insert overwrite table user_orc select * from user_1;\n  ```\n\n- 会在HDFS生成/user/hadoop/warehouse/user_orc/000000_0文件，文件没有带有.orc后缀\n\n- 使用`hive --orcfiledump /user/hadoop/warehouse/user_orc/000000_0`查看该orc文件信息，发现Compression: ZLIB，即默认的ZLIB压缩算法，snappy压缩算法没有生效。\n\n### 解决\n\n- 1、设置`set hive.exec.orc.default.compress=snappy;`参数，可暂时解决。\n- 2、升级到hive-3.1.3，貌似已经修复该问题。","slug":"bigdata/配置hadoop-snappy那些事","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vksz00268j5m5t06fmvx","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在Apache Hadoop3.x社区二进制发行版中已经包含hadoop-snappy，同时Centos7已经自带snappy本地库。因此Hadoop3.x+Centos7无需配置snappy本地库。可运行<code>hadoop checknative -a</code>检查snappy本地库是否可用。</p>\n<blockquote>\n<p>看了网上繁琐的Hadoop Snappy配置过程，配置了半天，才发现是白费功夫。原来是Hive的一个Bug。</p>\n</blockquote>\n<h2 id=\"Hive3-1-2中orc文件snappy压缩的Bug\"><a href=\"#Hive3-1-2中orc文件snappy压缩的Bug\" class=\"headerlink\" title=\"Hive3.1.2中orc文件snappy压缩的Bug\"></a>Hive3.1.2中orc文件snappy压缩的Bug</h2><h3 id=\"Bug描述\"><a href=\"#Bug描述\" class=\"headerlink\" title=\"Bug描述\"></a>Bug描述</h3><ul>\n<li><p>创建一个Hive表，存储为orc文件，同时启用snappy压缩。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `default`.`user_orc` (</span><br><span class=\"line\">  `tid` INT,</span><br><span class=\"line\">  `userid` STRING</span><br><span class=\"line\">)</span><br><span class=\"line\">STORED AS orc</span><br><span class=\"line\">TBLPROPERTIES (</span><br><span class=\"line\">  &quot;orc.compress&quot;=&quot;SNAPPY&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Insert overwrite进一些数据</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert overwrite table user_orc select * from user_1;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>会在HDFS生成&#x2F;user&#x2F;hadoop&#x2F;warehouse&#x2F;user_orc&#x2F;000000_0文件，文件没有带有.orc后缀</p>\n</li>\n<li><p>使用<code>hive --orcfiledump /user/hadoop/warehouse/user_orc/000000_0</code>查看该orc文件信息，发现Compression: ZLIB，即默认的ZLIB压缩算法，snappy压缩算法没有生效。</p>\n</li>\n</ul>\n<h3 id=\"解决\"><a href=\"#解决\" class=\"headerlink\" title=\"解决\"></a>解决</h3><ul>\n<li>1、设置<code>set hive.exec.orc.default.compress=snappy;</code>参数，可暂时解决。</li>\n<li>2、升级到hive-3.1.3，貌似已经修复该问题。</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在Apache Hadoop3.x社区二进制发行版中已经包含hadoop-snappy，同时Centos7已经自带snappy本地库。因此Hadoop3.x+Centos7无需配置snappy本地库。可运行<code>hadoop checknative -a</code>检查snappy本地库是否可用。</p>\n<blockquote>\n<p>看了网上繁琐的Hadoop Snappy配置过程，配置了半天，才发现是白费功夫。原来是Hive的一个Bug。</p>\n</blockquote>\n<h2 id=\"Hive3-1-2中orc文件snappy压缩的Bug\"><a href=\"#Hive3-1-2中orc文件snappy压缩的Bug\" class=\"headerlink\" title=\"Hive3.1.2中orc文件snappy压缩的Bug\"></a>Hive3.1.2中orc文件snappy压缩的Bug</h2><h3 id=\"Bug描述\"><a href=\"#Bug描述\" class=\"headerlink\" title=\"Bug描述\"></a>Bug描述</h3><ul>\n<li><p>创建一个Hive表，存储为orc文件，同时启用snappy压缩。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `default`.`user_orc` (</span><br><span class=\"line\">  `tid` INT,</span><br><span class=\"line\">  `userid` STRING</span><br><span class=\"line\">)</span><br><span class=\"line\">STORED AS orc</span><br><span class=\"line\">TBLPROPERTIES (</span><br><span class=\"line\">  &quot;orc.compress&quot;=&quot;SNAPPY&quot;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Insert overwrite进一些数据</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert overwrite table user_orc select * from user_1;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>会在HDFS生成&#x2F;user&#x2F;hadoop&#x2F;warehouse&#x2F;user_orc&#x2F;000000_0文件，文件没有带有.orc后缀</p>\n</li>\n<li><p>使用<code>hive --orcfiledump /user/hadoop/warehouse/user_orc/000000_0</code>查看该orc文件信息，发现Compression: ZLIB，即默认的ZLIB压缩算法，snappy压缩算法没有生效。</p>\n</li>\n</ul>\n<h3 id=\"解决\"><a href=\"#解决\" class=\"headerlink\" title=\"解决\"></a>解决</h3><ul>\n<li>1、设置<code>set hive.exec.orc.default.compress=snappy;</code>参数，可暂时解决。</li>\n<li>2、升级到hive-3.1.3，貌似已经修复该问题。</li>\n</ul>\n"},{"title":"DolphinScheduler RPC框架源码分析","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":6282,"date":"2023-07-09T13:07:13.000Z","updated":"2022-07-09T13:07:13.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## 前言\n\n截至2023-07-09，DolphinScheduler3.x最新版本Dev分支，DolphinScheduler中虽然基于Netty实现了一个简单的RPC框架，但是并没有使用，或者说使用的不是完整版的RPC框架。其中大量直接使用Netty Client发送网络请求，并没有使用动态代理简化或或者说屏蔽掉通信细节，虽然在`org.apache.dolphinscheduler.rpc`包中已经有了完整实现。\n\n本文主要分析`org.apache.dolphinscheduler.rpc`包中完整的RPC实现。虽然在DolphinScheduler中没有被使用，但是代码是共通的。\n\n\n\n## 源码分析\n\n### Rpc通信协议Protocol\n\n定义在org.apache.dolphinscheduler.rpc.protocol.MessageHeader类中，没有什么好说的，差不多的套路。\n\n- 一字节的version\n- 一字节的eventType：HEARTBEAT、REQUEST、RESPONSE\n- 四字节的msgLength\n- ......\n- 一字节的serialization类型：dolphinscheduler目前实现了一种基于ProtoStuff。\n\n```java\npublic class MessageHeader {\n\t\n    private byte version = 1;\n\n    private byte eventType;\n\n    private int msgLength = 0;\n\n    private long requestId = 0L;\n\n    private byte serialization = 0;\n\n    private short magic = RpcProtocolConstants.MAGIC;\n}\n```\n\n\n\n### 基于Netty进行网络通信\n\n编解码，心跳机制属于模板代码，不做介绍。核心业务逻辑集中在Netty的Handler中：`org.apache.dolphinscheduler.rpc.remote.NettyClientHandler`和`org.apache.dolphinscheduler.rpc.remote.NettyServerHandler`。\n\n```java\n    /**\n     * RPC Client实际进行RPC方法调用的地方：\n     * 1、借助Netty进行网络传输、编解码\n     * 2、channel.writeAndFlush(protocol)：\n     *      1、RpcProtocol：先被NettyEncoder进行解码，RpcProtocol -> ByteBuf字节流\n     *      2、然后NettyEncoder会将Encode后的字节流发送给server端\n     * 3、RPC Server接受到Client发送过来的字节流：\n     *      1、先被NettyDecoder进行解码：ByteBuf字节流 -> RpcProtocol对象\n     *      2、NettyServerHandler#readHandler进行反射调用执行实际方法，然后将结果编码返回RPC Client\n     * ##############\n     * Netty Client端channel.writeAndFlush，会直接走Pipeline中的OutboundHandler\n     * 而接受服务端返回的信息会走InboundHandler\n     * @param host\n     * @param protocol\n     * @param async\n     * @return\n     */\n    public RpcResponse sendMsg(Host host, RpcProtocol<RpcRequest> protocol, Boolean async) {\n\n        // 从cache中获取netty channel\n        Channel channel = getChannel(host);\n        assert channel != null;\n\n        RpcRequest request = protocol.getBody();\n        RpcRequestCache rpcRequestCache = new RpcRequestCache();\n        String serviceName = request.getClassName() + request.getMethodName();\n        rpcRequestCache.setServiceName(serviceName);\n        long reqId = protocol.getMsgHeader().getRequestId();\n        RpcFuture future = null;\n        if (Boolean.FALSE.equals(async)) {\n            future = new RpcFuture(request, reqId);\n            rpcRequestCache.setRpcFuture(future);\n        }\n        RpcRequestTable.put(protocol.getMsgHeader().getRequestId(), rpcRequestCache);\n        channel.writeAndFlush(protocol);\n        RpcResponse result = null;\n        if (Boolean.TRUE.equals(async)) {\n            result = new RpcResponse();\n            result.setStatus((byte) 0);\n            result.setResult(true);\n            return result;\n        }\n        try {\n            assert future != null;\n            result = future.get();\n        } catch (InterruptedException e) {\n            log.error(\"send msg error，service name is {}\", serviceName, e);\n            Thread.currentThread().interrupt();\n        }\n        return result;\n    }\n```\n\n\n\n### 动态代理\n\nDolphinScheduler使用ByteBuddy框架进行客户端的动态代理，进行实际的网络请求，屏蔽相关细节。\n\n```java\npublic class RpcClient implements IRpcClient {\n\n    @Override\n    public <T> T create(Class<T> clazz, Host host) throws Exception {\n        return new ByteBuddy()\n                // 指定父类\n                .subclass(clazz)\n                // 匹配由clazz声明的方法\n                .method(isDeclaredBy(clazz))\n                // 将匹配到的方法，交给ConsumerInterceptor进行代理增强：\n                // 增加实际进行RPC调用的逻辑\n                .intercept(MethodDelegation.to(new ConsumerInterceptor(host)))\n                // 产生字节码\n                .make()\n                // 加载类\n                .load(getClass().getClassLoader())\n                .getLoaded()\n                .getDeclaredConstructor().newInstance();\n    }\n}\n```\n\n\n\n```java\n    /**\n     * 动态代理只作用于RPC的Client端\n     *\n     * @param args @AllArguments: 将需要增强的方法的参数绑定于此\n     * @param method @Origin Method: 被调用的原始方法\n     * @return\n     * @throws RemotingException\n     */\n    @RuntimeType\n    public Object intercept(@AllArguments Object[] args, @Origin Method method) throws RemotingException {\n        // 1、构造RpcRequest对象\n        RpcRequest request = buildReq(args, method);\n\n        // serviceName：类名+方法名。例如：IUserServicesay\n        String serviceName = method.getDeclaringClass().getSimpleName() + method.getName();\n\n        // ConsumerConfig: 存储每个被RPC调用方法的配置，比如：重试次数、异步与否\n        ConsumerConfig consumerConfig = ConsumerConfigCache.getConfigByServersName(serviceName);\n        if (null == consumerConfig) {\n            consumerConfig = cacheServiceConfig(method, serviceName);\n        }\n        boolean async = consumerConfig.getAsync();\n\n        int retries = consumerConfig.getRetries();\n\n        // 构建RpcProtocol：RpcRequest + rpc协议相关信息\n        RpcProtocol<RpcRequest> protocol = buildProtocol(request);\n\n        while (retries-- > 0) {\n            RpcResponse rsp;\n            // 调用nettyClient进行网络请求\n            rsp = nettyClient.sendMsg(host, protocol, async);\n            // success\n            if (null != rsp && rsp.getStatus() == 0) {\n                return rsp.getResult();\n            }\n        }\n        // execute fail\n        throw new RemotingException(\"send msg error\");\n\n    }\n```\n\n\n\n### 服务发现\n\nDolphinScheduler定义了两个注解`@RpcService(\"IUserService\")`和`@Rpc(async = true, serviceCallback = UserCallback.class)`，简化Rpc的配置和服务的发现。\n\n### Demo\n\n```java\npublic class RpcTest {\n\n    private NettyServer nettyServer;\n\n    private IUserService userService;\n\n    private Host host;\n\n    @BeforeEach\n    public void before() throws Exception {\n        nettyServer = new NettyServer(new NettyServerConfig());\n        IRpcClient rpcClient = new RpcClient();\n        host = new Host(\"127.0.0.1\", 12346);\n        userService = rpcClient.create(IUserService.class, host);\n    }\n\n    @Test\n    public void callTest(){\n        Boolean hello = userService.say(\"hello\");\n        System.out.printf(\"Rpc Call Result %s\\n\", hello);\n\n        System.out.println(\"###############\");\n        System.out.println(userService.callBackIsFalse(\"hello\"));\n    }\n\n    @AfterEach\n    public void after() {\n        NettyClient.getInstance().close();\n        nettyServer.close();\n    }\n\n}\n```\n\n","source":"_posts/rpc/DolphinScheduler RPC框架源码分析.md","raw":"---\ntitle: DolphinScheduler RPC框架源码分析\ntags:\n  - dolphinscheduler\ncategories:\n  - - RPC\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 6282\ndate: 2023-07-09 21:07:13\nupdated: 2022-07-09 21:07:13\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n截至2023-07-09，DolphinScheduler3.x最新版本Dev分支，DolphinScheduler中虽然基于Netty实现了一个简单的RPC框架，但是并没有使用，或者说使用的不是完整版的RPC框架。其中大量直接使用Netty Client发送网络请求，并没有使用动态代理简化或或者说屏蔽掉通信细节，虽然在`org.apache.dolphinscheduler.rpc`包中已经有了完整实现。\n\n本文主要分析`org.apache.dolphinscheduler.rpc`包中完整的RPC实现。虽然在DolphinScheduler中没有被使用，但是代码是共通的。\n\n\n\n## 源码分析\n\n### Rpc通信协议Protocol\n\n定义在org.apache.dolphinscheduler.rpc.protocol.MessageHeader类中，没有什么好说的，差不多的套路。\n\n- 一字节的version\n- 一字节的eventType：HEARTBEAT、REQUEST、RESPONSE\n- 四字节的msgLength\n- ......\n- 一字节的serialization类型：dolphinscheduler目前实现了一种基于ProtoStuff。\n\n```java\npublic class MessageHeader {\n\t\n    private byte version = 1;\n\n    private byte eventType;\n\n    private int msgLength = 0;\n\n    private long requestId = 0L;\n\n    private byte serialization = 0;\n\n    private short magic = RpcProtocolConstants.MAGIC;\n}\n```\n\n\n\n### 基于Netty进行网络通信\n\n编解码，心跳机制属于模板代码，不做介绍。核心业务逻辑集中在Netty的Handler中：`org.apache.dolphinscheduler.rpc.remote.NettyClientHandler`和`org.apache.dolphinscheduler.rpc.remote.NettyServerHandler`。\n\n```java\n    /**\n     * RPC Client实际进行RPC方法调用的地方：\n     * 1、借助Netty进行网络传输、编解码\n     * 2、channel.writeAndFlush(protocol)：\n     *      1、RpcProtocol：先被NettyEncoder进行解码，RpcProtocol -> ByteBuf字节流\n     *      2、然后NettyEncoder会将Encode后的字节流发送给server端\n     * 3、RPC Server接受到Client发送过来的字节流：\n     *      1、先被NettyDecoder进行解码：ByteBuf字节流 -> RpcProtocol对象\n     *      2、NettyServerHandler#readHandler进行反射调用执行实际方法，然后将结果编码返回RPC Client\n     * ##############\n     * Netty Client端channel.writeAndFlush，会直接走Pipeline中的OutboundHandler\n     * 而接受服务端返回的信息会走InboundHandler\n     * @param host\n     * @param protocol\n     * @param async\n     * @return\n     */\n    public RpcResponse sendMsg(Host host, RpcProtocol<RpcRequest> protocol, Boolean async) {\n\n        // 从cache中获取netty channel\n        Channel channel = getChannel(host);\n        assert channel != null;\n\n        RpcRequest request = protocol.getBody();\n        RpcRequestCache rpcRequestCache = new RpcRequestCache();\n        String serviceName = request.getClassName() + request.getMethodName();\n        rpcRequestCache.setServiceName(serviceName);\n        long reqId = protocol.getMsgHeader().getRequestId();\n        RpcFuture future = null;\n        if (Boolean.FALSE.equals(async)) {\n            future = new RpcFuture(request, reqId);\n            rpcRequestCache.setRpcFuture(future);\n        }\n        RpcRequestTable.put(protocol.getMsgHeader().getRequestId(), rpcRequestCache);\n        channel.writeAndFlush(protocol);\n        RpcResponse result = null;\n        if (Boolean.TRUE.equals(async)) {\n            result = new RpcResponse();\n            result.setStatus((byte) 0);\n            result.setResult(true);\n            return result;\n        }\n        try {\n            assert future != null;\n            result = future.get();\n        } catch (InterruptedException e) {\n            log.error(\"send msg error，service name is {}\", serviceName, e);\n            Thread.currentThread().interrupt();\n        }\n        return result;\n    }\n```\n\n\n\n### 动态代理\n\nDolphinScheduler使用ByteBuddy框架进行客户端的动态代理，进行实际的网络请求，屏蔽相关细节。\n\n```java\npublic class RpcClient implements IRpcClient {\n\n    @Override\n    public <T> T create(Class<T> clazz, Host host) throws Exception {\n        return new ByteBuddy()\n                // 指定父类\n                .subclass(clazz)\n                // 匹配由clazz声明的方法\n                .method(isDeclaredBy(clazz))\n                // 将匹配到的方法，交给ConsumerInterceptor进行代理增强：\n                // 增加实际进行RPC调用的逻辑\n                .intercept(MethodDelegation.to(new ConsumerInterceptor(host)))\n                // 产生字节码\n                .make()\n                // 加载类\n                .load(getClass().getClassLoader())\n                .getLoaded()\n                .getDeclaredConstructor().newInstance();\n    }\n}\n```\n\n\n\n```java\n    /**\n     * 动态代理只作用于RPC的Client端\n     *\n     * @param args @AllArguments: 将需要增强的方法的参数绑定于此\n     * @param method @Origin Method: 被调用的原始方法\n     * @return\n     * @throws RemotingException\n     */\n    @RuntimeType\n    public Object intercept(@AllArguments Object[] args, @Origin Method method) throws RemotingException {\n        // 1、构造RpcRequest对象\n        RpcRequest request = buildReq(args, method);\n\n        // serviceName：类名+方法名。例如：IUserServicesay\n        String serviceName = method.getDeclaringClass().getSimpleName() + method.getName();\n\n        // ConsumerConfig: 存储每个被RPC调用方法的配置，比如：重试次数、异步与否\n        ConsumerConfig consumerConfig = ConsumerConfigCache.getConfigByServersName(serviceName);\n        if (null == consumerConfig) {\n            consumerConfig = cacheServiceConfig(method, serviceName);\n        }\n        boolean async = consumerConfig.getAsync();\n\n        int retries = consumerConfig.getRetries();\n\n        // 构建RpcProtocol：RpcRequest + rpc协议相关信息\n        RpcProtocol<RpcRequest> protocol = buildProtocol(request);\n\n        while (retries-- > 0) {\n            RpcResponse rsp;\n            // 调用nettyClient进行网络请求\n            rsp = nettyClient.sendMsg(host, protocol, async);\n            // success\n            if (null != rsp && rsp.getStatus() == 0) {\n                return rsp.getResult();\n            }\n        }\n        // execute fail\n        throw new RemotingException(\"send msg error\");\n\n    }\n```\n\n\n\n### 服务发现\n\nDolphinScheduler定义了两个注解`@RpcService(\"IUserService\")`和`@Rpc(async = true, serviceCallback = UserCallback.class)`，简化Rpc的配置和服务的发现。\n\n### Demo\n\n```java\npublic class RpcTest {\n\n    private NettyServer nettyServer;\n\n    private IUserService userService;\n\n    private Host host;\n\n    @BeforeEach\n    public void before() throws Exception {\n        nettyServer = new NettyServer(new NettyServerConfig());\n        IRpcClient rpcClient = new RpcClient();\n        host = new Host(\"127.0.0.1\", 12346);\n        userService = rpcClient.create(IUserService.class, host);\n    }\n\n    @Test\n    public void callTest(){\n        Boolean hello = userService.say(\"hello\");\n        System.out.printf(\"Rpc Call Result %s\\n\", hello);\n\n        System.out.println(\"###############\");\n        System.out.println(userService.callBackIsFalse(\"hello\"));\n    }\n\n    @AfterEach\n    public void after() {\n        NettyClient.getInstance().close();\n        nettyServer.close();\n    }\n\n}\n```\n\n","slug":"rpc/DolphinScheduler RPC框架源码分析","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt000288j5m3yzz2rua","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>截至2023-07-09，DolphinScheduler3.x最新版本Dev分支，DolphinScheduler中虽然基于Netty实现了一个简单的RPC框架，但是并没有使用，或者说使用的不是完整版的RPC框架。其中大量直接使用Netty Client发送网络请求，并没有使用动态代理简化或或者说屏蔽掉通信细节，虽然在<code>org.apache.dolphinscheduler.rpc</code>包中已经有了完整实现。</p>\n<p>本文主要分析<code>org.apache.dolphinscheduler.rpc</code>包中完整的RPC实现。虽然在DolphinScheduler中没有被使用，但是代码是共通的。</p>\n<h2 id=\"源码分析\"><a href=\"#源码分析\" class=\"headerlink\" title=\"源码分析\"></a>源码分析</h2><h3 id=\"Rpc通信协议Protocol\"><a href=\"#Rpc通信协议Protocol\" class=\"headerlink\" title=\"Rpc通信协议Protocol\"></a>Rpc通信协议Protocol</h3><p>定义在org.apache.dolphinscheduler.rpc.protocol.MessageHeader类中，没有什么好说的，差不多的套路。</p>\n<ul>\n<li>一字节的version</li>\n<li>一字节的eventType：HEARTBEAT、REQUEST、RESPONSE</li>\n<li>四字节的msgLength</li>\n<li>……</li>\n<li>一字节的serialization类型：dolphinscheduler目前实现了一种基于ProtoStuff。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MessageHeader</span> &#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> <span class=\"variable\">version</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> eventType;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> <span class=\"variable\">msgLength</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> <span class=\"variable\">requestId</span> <span class=\"operator\">=</span> <span class=\"number\">0L</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> <span class=\"variable\">serialization</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">short</span> <span class=\"variable\">magic</span> <span class=\"operator\">=</span> RpcProtocolConstants.MAGIC;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"基于Netty进行网络通信\"><a href=\"#基于Netty进行网络通信\" class=\"headerlink\" title=\"基于Netty进行网络通信\"></a>基于Netty进行网络通信</h3><p>编解码，心跳机制属于模板代码，不做介绍。核心业务逻辑集中在Netty的Handler中：<code>org.apache.dolphinscheduler.rpc.remote.NettyClientHandler</code>和<code>org.apache.dolphinscheduler.rpc.remote.NettyServerHandler</code>。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * RPC Client实际进行RPC方法调用的地方：</span></span><br><span class=\"line\"><span class=\"comment\"> * 1、借助Netty进行网络传输、编解码</span></span><br><span class=\"line\"><span class=\"comment\"> * 2、channel.writeAndFlush(protocol)：</span></span><br><span class=\"line\"><span class=\"comment\"> *      1、RpcProtocol：先被NettyEncoder进行解码，RpcProtocol -&gt; ByteBuf字节流</span></span><br><span class=\"line\"><span class=\"comment\"> *      2、然后NettyEncoder会将Encode后的字节流发送给server端</span></span><br><span class=\"line\"><span class=\"comment\"> * 3、RPC Server接受到Client发送过来的字节流：</span></span><br><span class=\"line\"><span class=\"comment\"> *      1、先被NettyDecoder进行解码：ByteBuf字节流 -&gt; RpcProtocol对象</span></span><br><span class=\"line\"><span class=\"comment\"> *      2、NettyServerHandler#readHandler进行反射调用执行实际方法，然后将结果编码返回RPC Client</span></span><br><span class=\"line\"><span class=\"comment\"> * ##############</span></span><br><span class=\"line\"><span class=\"comment\"> * Netty Client端channel.writeAndFlush，会直接走Pipeline中的OutboundHandler</span></span><br><span class=\"line\"><span class=\"comment\"> * 而接受服务端返回的信息会走InboundHandler</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> host</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> protocol</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> async</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> RpcResponse <span class=\"title function_\">sendMsg</span><span class=\"params\">(Host host, RpcProtocol&lt;RpcRequest&gt; protocol, Boolean async)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 从cache中获取netty channel</span></span><br><span class=\"line\">    <span class=\"type\">Channel</span> <span class=\"variable\">channel</span> <span class=\"operator\">=</span> getChannel(host);</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> channel != <span class=\"literal\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">RpcRequest</span> <span class=\"variable\">request</span> <span class=\"operator\">=</span> protocol.getBody();</span><br><span class=\"line\">    <span class=\"type\">RpcRequestCache</span> <span class=\"variable\">rpcRequestCache</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RpcRequestCache</span>();</span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">serviceName</span> <span class=\"operator\">=</span> request.getClassName() + request.getMethodName();</span><br><span class=\"line\">    rpcRequestCache.setServiceName(serviceName);</span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"variable\">reqId</span> <span class=\"operator\">=</span> protocol.getMsgHeader().getRequestId();</span><br><span class=\"line\">    <span class=\"type\">RpcFuture</span> <span class=\"variable\">future</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Boolean.FALSE.equals(async)) &#123;</span><br><span class=\"line\">        future = <span class=\"keyword\">new</span> <span class=\"title class_\">RpcFuture</span>(request, reqId);</span><br><span class=\"line\">        rpcRequestCache.setRpcFuture(future);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    RpcRequestTable.put(protocol.getMsgHeader().getRequestId(), rpcRequestCache);</span><br><span class=\"line\">    channel.writeAndFlush(protocol);</span><br><span class=\"line\">    <span class=\"type\">RpcResponse</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Boolean.TRUE.equals(async)) &#123;</span><br><span class=\"line\">        result = <span class=\"keyword\">new</span> <span class=\"title class_\">RpcResponse</span>();</span><br><span class=\"line\">        result.setStatus((<span class=\"type\">byte</span>) <span class=\"number\">0</span>);</span><br><span class=\"line\">        result.setResult(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> future != <span class=\"literal\">null</span>;</span><br><span class=\"line\">        result = future.get();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;send msg error，service name is &#123;&#125;&quot;</span>, serviceName, e);</span><br><span class=\"line\">        Thread.currentThread().interrupt();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"动态代理\"><a href=\"#动态代理\" class=\"headerlink\" title=\"动态代理\"></a>动态代理</h3><p>DolphinScheduler使用ByteBuddy框架进行客户端的动态代理，进行实际的网络请求，屏蔽相关细节。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RpcClient</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">IRpcClient</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;T&gt; T <span class=\"title function_\">create</span><span class=\"params\">(Class&lt;T&gt; clazz, Host host)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ByteBuddy</span>()</span><br><span class=\"line\">                <span class=\"comment\">// 指定父类</span></span><br><span class=\"line\">                .subclass(clazz)</span><br><span class=\"line\">                <span class=\"comment\">// 匹配由clazz声明的方法</span></span><br><span class=\"line\">                .method(isDeclaredBy(clazz))</span><br><span class=\"line\">                <span class=\"comment\">// 将匹配到的方法，交给ConsumerInterceptor进行代理增强：</span></span><br><span class=\"line\">                <span class=\"comment\">// 增加实际进行RPC调用的逻辑</span></span><br><span class=\"line\">                .intercept(MethodDelegation.to(<span class=\"keyword\">new</span> <span class=\"title class_\">ConsumerInterceptor</span>(host)))</span><br><span class=\"line\">                <span class=\"comment\">// 产生字节码</span></span><br><span class=\"line\">                .make()</span><br><span class=\"line\">                <span class=\"comment\">// 加载类</span></span><br><span class=\"line\">                .load(getClass().getClassLoader())</span><br><span class=\"line\">                .getLoaded()</span><br><span class=\"line\">                .getDeclaredConstructor().newInstance();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 动态代理只作用于RPC的Client端</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> args <span class=\"doctag\">@AllArguments</span>: 将需要增强的方法的参数绑定于此</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> method <span class=\"doctag\">@Origin</span> Method: 被调用的原始方法</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@throws</span> RemotingException</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@RuntimeType</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> Object <span class=\"title function_\">intercept</span><span class=\"params\">(<span class=\"meta\">@AllArguments</span> Object[] args, <span class=\"meta\">@Origin</span> Method method)</span> <span class=\"keyword\">throws</span> RemotingException &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 1、构造RpcRequest对象</span></span><br><span class=\"line\">    <span class=\"type\">RpcRequest</span> <span class=\"variable\">request</span> <span class=\"operator\">=</span> buildReq(args, method);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// serviceName：类名+方法名。例如：IUserServicesay</span></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">serviceName</span> <span class=\"operator\">=</span> method.getDeclaringClass().getSimpleName() + method.getName();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// ConsumerConfig: 存储每个被RPC调用方法的配置，比如：重试次数、异步与否</span></span><br><span class=\"line\">    <span class=\"type\">ConsumerConfig</span> <span class=\"variable\">consumerConfig</span> <span class=\"operator\">=</span> ConsumerConfigCache.getConfigByServersName(serviceName);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"literal\">null</span> == consumerConfig) &#123;</span><br><span class=\"line\">        consumerConfig = cacheServiceConfig(method, serviceName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">boolean</span> <span class=\"variable\">async</span> <span class=\"operator\">=</span> consumerConfig.getAsync();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">retries</span> <span class=\"operator\">=</span> consumerConfig.getRetries();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构建RpcProtocol：RpcRequest + rpc协议相关信息</span></span><br><span class=\"line\">    RpcProtocol&lt;RpcRequest&gt; protocol = buildProtocol(request);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (retries-- &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        RpcResponse rsp;</span><br><span class=\"line\">        <span class=\"comment\">// 调用nettyClient进行网络请求</span></span><br><span class=\"line\">        rsp = nettyClient.sendMsg(host, protocol, async);</span><br><span class=\"line\">        <span class=\"comment\">// success</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"literal\">null</span> != rsp &amp;&amp; rsp.getStatus() == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> rsp.getResult();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// execute fail</span></span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RemotingException</span>(<span class=\"string\">&quot;send msg error&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"服务发现\"><a href=\"#服务发现\" class=\"headerlink\" title=\"服务发现\"></a>服务发现</h3><p>DolphinScheduler定义了两个注解<code>@RpcService(&quot;IUserService&quot;)</code>和<code>@Rpc(async = true, serviceCallback = UserCallback.class)</code>，简化Rpc的配置和服务的发现。</p>\n<h3 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RpcTest</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> NettyServer nettyServer;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IUserService userService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Host host;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@BeforeEach</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">before</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        nettyServer = <span class=\"keyword\">new</span> <span class=\"title class_\">NettyServer</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">NettyServerConfig</span>());</span><br><span class=\"line\">        <span class=\"type\">IRpcClient</span> <span class=\"variable\">rpcClient</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RpcClient</span>();</span><br><span class=\"line\">        host = <span class=\"keyword\">new</span> <span class=\"title class_\">Host</span>(<span class=\"string\">&quot;127.0.0.1&quot;</span>, <span class=\"number\">12346</span>);</span><br><span class=\"line\">        userService = rpcClient.create(IUserService.class, host);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">callTest</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">Boolean</span> <span class=\"variable\">hello</span> <span class=\"operator\">=</span> userService.say(<span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">        System.out.printf(<span class=\"string\">&quot;Rpc Call Result %s\\n&quot;</span>, hello);</span><br><span class=\"line\"></span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;###############&quot;</span>);</span><br><span class=\"line\">        System.out.println(userService.callBackIsFalse(<span class=\"string\">&quot;hello&quot;</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@AfterEach</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">after</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        NettyClient.getInstance().close();</span><br><span class=\"line\">        nettyServer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>截至2023-07-09，DolphinScheduler3.x最新版本Dev分支，DolphinScheduler中虽然基于Netty实现了一个简单的RPC框架，但是并没有使用，或者说使用的不是完整版的RPC框架。其中大量直接使用Netty Client发送网络请求，并没有使用动态代理简化或或者说屏蔽掉通信细节，虽然在<code>org.apache.dolphinscheduler.rpc</code>包中已经有了完整实现。</p>\n<p>本文主要分析<code>org.apache.dolphinscheduler.rpc</code>包中完整的RPC实现。虽然在DolphinScheduler中没有被使用，但是代码是共通的。</p>\n<h2 id=\"源码分析\"><a href=\"#源码分析\" class=\"headerlink\" title=\"源码分析\"></a>源码分析</h2><h3 id=\"Rpc通信协议Protocol\"><a href=\"#Rpc通信协议Protocol\" class=\"headerlink\" title=\"Rpc通信协议Protocol\"></a>Rpc通信协议Protocol</h3><p>定义在org.apache.dolphinscheduler.rpc.protocol.MessageHeader类中，没有什么好说的，差不多的套路。</p>\n<ul>\n<li>一字节的version</li>\n<li>一字节的eventType：HEARTBEAT、REQUEST、RESPONSE</li>\n<li>四字节的msgLength</li>\n<li>……</li>\n<li>一字节的serialization类型：dolphinscheduler目前实现了一种基于ProtoStuff。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MessageHeader</span> &#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> <span class=\"variable\">version</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> eventType;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">int</span> <span class=\"variable\">msgLength</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">long</span> <span class=\"variable\">requestId</span> <span class=\"operator\">=</span> <span class=\"number\">0L</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">byte</span> <span class=\"variable\">serialization</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">short</span> <span class=\"variable\">magic</span> <span class=\"operator\">=</span> RpcProtocolConstants.MAGIC;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"基于Netty进行网络通信\"><a href=\"#基于Netty进行网络通信\" class=\"headerlink\" title=\"基于Netty进行网络通信\"></a>基于Netty进行网络通信</h3><p>编解码，心跳机制属于模板代码，不做介绍。核心业务逻辑集中在Netty的Handler中：<code>org.apache.dolphinscheduler.rpc.remote.NettyClientHandler</code>和<code>org.apache.dolphinscheduler.rpc.remote.NettyServerHandler</code>。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * RPC Client实际进行RPC方法调用的地方：</span></span><br><span class=\"line\"><span class=\"comment\"> * 1、借助Netty进行网络传输、编解码</span></span><br><span class=\"line\"><span class=\"comment\"> * 2、channel.writeAndFlush(protocol)：</span></span><br><span class=\"line\"><span class=\"comment\"> *      1、RpcProtocol：先被NettyEncoder进行解码，RpcProtocol -&gt; ByteBuf字节流</span></span><br><span class=\"line\"><span class=\"comment\"> *      2、然后NettyEncoder会将Encode后的字节流发送给server端</span></span><br><span class=\"line\"><span class=\"comment\"> * 3、RPC Server接受到Client发送过来的字节流：</span></span><br><span class=\"line\"><span class=\"comment\"> *      1、先被NettyDecoder进行解码：ByteBuf字节流 -&gt; RpcProtocol对象</span></span><br><span class=\"line\"><span class=\"comment\"> *      2、NettyServerHandler#readHandler进行反射调用执行实际方法，然后将结果编码返回RPC Client</span></span><br><span class=\"line\"><span class=\"comment\"> * ##############</span></span><br><span class=\"line\"><span class=\"comment\"> * Netty Client端channel.writeAndFlush，会直接走Pipeline中的OutboundHandler</span></span><br><span class=\"line\"><span class=\"comment\"> * 而接受服务端返回的信息会走InboundHandler</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> host</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> protocol</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> async</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> RpcResponse <span class=\"title function_\">sendMsg</span><span class=\"params\">(Host host, RpcProtocol&lt;RpcRequest&gt; protocol, Boolean async)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 从cache中获取netty channel</span></span><br><span class=\"line\">    <span class=\"type\">Channel</span> <span class=\"variable\">channel</span> <span class=\"operator\">=</span> getChannel(host);</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> channel != <span class=\"literal\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">RpcRequest</span> <span class=\"variable\">request</span> <span class=\"operator\">=</span> protocol.getBody();</span><br><span class=\"line\">    <span class=\"type\">RpcRequestCache</span> <span class=\"variable\">rpcRequestCache</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RpcRequestCache</span>();</span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">serviceName</span> <span class=\"operator\">=</span> request.getClassName() + request.getMethodName();</span><br><span class=\"line\">    rpcRequestCache.setServiceName(serviceName);</span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"variable\">reqId</span> <span class=\"operator\">=</span> protocol.getMsgHeader().getRequestId();</span><br><span class=\"line\">    <span class=\"type\">RpcFuture</span> <span class=\"variable\">future</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Boolean.FALSE.equals(async)) &#123;</span><br><span class=\"line\">        future = <span class=\"keyword\">new</span> <span class=\"title class_\">RpcFuture</span>(request, reqId);</span><br><span class=\"line\">        rpcRequestCache.setRpcFuture(future);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    RpcRequestTable.put(protocol.getMsgHeader().getRequestId(), rpcRequestCache);</span><br><span class=\"line\">    channel.writeAndFlush(protocol);</span><br><span class=\"line\">    <span class=\"type\">RpcResponse</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (Boolean.TRUE.equals(async)) &#123;</span><br><span class=\"line\">        result = <span class=\"keyword\">new</span> <span class=\"title class_\">RpcResponse</span>();</span><br><span class=\"line\">        result.setStatus((<span class=\"type\">byte</span>) <span class=\"number\">0</span>);</span><br><span class=\"line\">        result.setResult(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> future != <span class=\"literal\">null</span>;</span><br><span class=\"line\">        result = future.get();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;send msg error，service name is &#123;&#125;&quot;</span>, serviceName, e);</span><br><span class=\"line\">        Thread.currentThread().interrupt();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"动态代理\"><a href=\"#动态代理\" class=\"headerlink\" title=\"动态代理\"></a>动态代理</h3><p>DolphinScheduler使用ByteBuddy框架进行客户端的动态代理，进行实际的网络请求，屏蔽相关细节。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RpcClient</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">IRpcClient</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;T&gt; T <span class=\"title function_\">create</span><span class=\"params\">(Class&lt;T&gt; clazz, Host host)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ByteBuddy</span>()</span><br><span class=\"line\">                <span class=\"comment\">// 指定父类</span></span><br><span class=\"line\">                .subclass(clazz)</span><br><span class=\"line\">                <span class=\"comment\">// 匹配由clazz声明的方法</span></span><br><span class=\"line\">                .method(isDeclaredBy(clazz))</span><br><span class=\"line\">                <span class=\"comment\">// 将匹配到的方法，交给ConsumerInterceptor进行代理增强：</span></span><br><span class=\"line\">                <span class=\"comment\">// 增加实际进行RPC调用的逻辑</span></span><br><span class=\"line\">                .intercept(MethodDelegation.to(<span class=\"keyword\">new</span> <span class=\"title class_\">ConsumerInterceptor</span>(host)))</span><br><span class=\"line\">                <span class=\"comment\">// 产生字节码</span></span><br><span class=\"line\">                .make()</span><br><span class=\"line\">                <span class=\"comment\">// 加载类</span></span><br><span class=\"line\">                .load(getClass().getClassLoader())</span><br><span class=\"line\">                .getLoaded()</span><br><span class=\"line\">                .getDeclaredConstructor().newInstance();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 动态代理只作用于RPC的Client端</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> args <span class=\"doctag\">@AllArguments</span>: 将需要增强的方法的参数绑定于此</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> method <span class=\"doctag\">@Origin</span> Method: 被调用的原始方法</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@throws</span> RemotingException</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@RuntimeType</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> Object <span class=\"title function_\">intercept</span><span class=\"params\">(<span class=\"meta\">@AllArguments</span> Object[] args, <span class=\"meta\">@Origin</span> Method method)</span> <span class=\"keyword\">throws</span> RemotingException &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 1、构造RpcRequest对象</span></span><br><span class=\"line\">    <span class=\"type\">RpcRequest</span> <span class=\"variable\">request</span> <span class=\"operator\">=</span> buildReq(args, method);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// serviceName：类名+方法名。例如：IUserServicesay</span></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">serviceName</span> <span class=\"operator\">=</span> method.getDeclaringClass().getSimpleName() + method.getName();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// ConsumerConfig: 存储每个被RPC调用方法的配置，比如：重试次数、异步与否</span></span><br><span class=\"line\">    <span class=\"type\">ConsumerConfig</span> <span class=\"variable\">consumerConfig</span> <span class=\"operator\">=</span> ConsumerConfigCache.getConfigByServersName(serviceName);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"literal\">null</span> == consumerConfig) &#123;</span><br><span class=\"line\">        consumerConfig = cacheServiceConfig(method, serviceName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">boolean</span> <span class=\"variable\">async</span> <span class=\"operator\">=</span> consumerConfig.getAsync();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">retries</span> <span class=\"operator\">=</span> consumerConfig.getRetries();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 构建RpcProtocol：RpcRequest + rpc协议相关信息</span></span><br><span class=\"line\">    RpcProtocol&lt;RpcRequest&gt; protocol = buildProtocol(request);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (retries-- &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        RpcResponse rsp;</span><br><span class=\"line\">        <span class=\"comment\">// 调用nettyClient进行网络请求</span></span><br><span class=\"line\">        rsp = nettyClient.sendMsg(host, protocol, async);</span><br><span class=\"line\">        <span class=\"comment\">// success</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"literal\">null</span> != rsp &amp;&amp; rsp.getStatus() == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> rsp.getResult();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// execute fail</span></span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RemotingException</span>(<span class=\"string\">&quot;send msg error&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"服务发现\"><a href=\"#服务发现\" class=\"headerlink\" title=\"服务发现\"></a>服务发现</h3><p>DolphinScheduler定义了两个注解<code>@RpcService(&quot;IUserService&quot;)</code>和<code>@Rpc(async = true, serviceCallback = UserCallback.class)</code>，简化Rpc的配置和服务的发现。</p>\n<h3 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RpcTest</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> NettyServer nettyServer;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IUserService userService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Host host;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@BeforeEach</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">before</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">        nettyServer = <span class=\"keyword\">new</span> <span class=\"title class_\">NettyServer</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">NettyServerConfig</span>());</span><br><span class=\"line\">        <span class=\"type\">IRpcClient</span> <span class=\"variable\">rpcClient</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RpcClient</span>();</span><br><span class=\"line\">        host = <span class=\"keyword\">new</span> <span class=\"title class_\">Host</span>(<span class=\"string\">&quot;127.0.0.1&quot;</span>, <span class=\"number\">12346</span>);</span><br><span class=\"line\">        userService = rpcClient.create(IUserService.class, host);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">callTest</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">Boolean</span> <span class=\"variable\">hello</span> <span class=\"operator\">=</span> userService.say(<span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">        System.out.printf(<span class=\"string\">&quot;Rpc Call Result %s\\n&quot;</span>, hello);</span><br><span class=\"line\"></span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;###############&quot;</span>);</span><br><span class=\"line\">        System.out.println(userService.callBackIsFalse(<span class=\"string\">&quot;hello&quot;</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@AfterEach</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">after</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        NettyClient.getInstance().close();</span><br><span class=\"line\">        nettyServer.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"虚拟内存-从入门到跑路","abbrlink":59616,"date":"2022-10-08T10:41:38.000Z","updated":"2022-10-08T10:41:38.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n>#### 虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\n>\n> - 1、控制物理内存的访问权限。\n>\n>   例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。\n>\n> - 2、让每个进程有独立的地址空间。\n>\n>   所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。\n>\n>#### 虚拟内存 + 共享库可以大大节省内存。\n>\n> 比如`libc`共享库，系统中几乎所有的进程都映射`libc`到自己的进程地址空间，而`libc`的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。    \n>\n>#### 虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\n>\n>#### 虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\n>\n>比如要用`malloc`分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的*连续*空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。\n\n## What-什么是虚拟内存\n\n> **虚拟内存**是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上[物理内存](https://zh.m.wikipedia.org/wiki/物理内存)通常被分隔成多个[内存碎片](https://zh.m.wikipedia.org/wiki/碎片化)，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术使得大型程序的编写变得更容易，对真正的物理内存（例如[RAM](https://zh.m.wikipedia.org/wiki/隨機存取記憶體)）的使用也更有效率。此外，虚拟内存技术可以使多个[进程](https://zh.m.wikipedia.org/wiki/行程)共享同一个[运行库](https://zh.m.wikipedia.org/wiki/函式庫)，并通过分割不同进程的内存空间来提高系统的安全性。\n>\n> 注意：**虚拟内存**不只是“用磁盘空间来扩展物理内存”的意思——这只是扩充[内存级别](https://zh.m.wikipedia.org/wiki/記憶體階層)以使其包含[硬盘驱动器](https://zh.m.wikipedia.org/wiki/硬盘驱动器)而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果，它的作用也可以通过[覆盖](https://zh.m.wikipedia.org/wiki/覆盖_(编程))或者把处于不活动状态的程序以及它们的数据全部交换到磁盘上等方式来实现。对虚拟内存的定义是基于对[地址空间](https://zh.m.wikipedia.org/wiki/地址空间)的重定义的，即把地址空间定义为“连续的虚拟内存地址”，以借此“欺骗”程序，使它们以为自己正在使用一大块的“连续”地址。\n>\n> 那些需要快速存取或者相应时间非常稳定的[嵌入式](https://zh.m.wikipedia.org/wiki/嵌入式)系统，以及其他的具有特殊应用的计算机系统，可能会为了避免让[运算结果的可预测性](https://zh.m.wikipedia.org/wiki/确定性算法)降低，而选择不使用虚拟内存。\n\n## How\n\n> 虚拟内存技术是现代[计算机系统结构](https://zh.m.wikipedia.org/wiki/计算机系统结构)中不可分割的一部分。现代所有用于一般应用的[操作系统](https://zh.m.wikipedia.org/wiki/操作系统)都对普通的应用程序使用虚拟内存技术，例如文字处理软件，电子制表软件，多媒体播放器等等。大部分架构通过[CPU](https://zh.m.wikipedia.org/wiki/CPU)中独立的硬件**内存管理单元**（英语：**memory management unit**，缩写为**MMU**），有时称作**分页内存管理单元**（英语：**paged memory management unit**，缩写为**PMMU**)来辅助实现这一功能。\n\n- 操作系统利用体系结构提供的VA到PA的转换机制实现虚拟内存管理。\n\n  用`ps`命令查看当前终端下的进程，得知`bash`进程的id是27613，然后用`cat /proc/27613/maps`命令查看它的虚拟地址空间。`/proc`目录中的文件并不是真正的磁盘文件，而是由内核虚拟出来的文件系统，当前系统中运行的每个进程在`/proc`下都有一个子目录，目录名就是进程的id，查看目录下的文件可以得到该进程的相关信息。\n\n  ```shell\n  root@DESKTOP-KD33OT8:/home/demo# cat /proc/27613/maps\n  5589e9d2b000-5589e9d42000 r--p 00000000 08:10 225588                     /usr/bin/zsh\n  5589e9d42000-5589e9dd7000 r-xp 00017000 08:10 225588                     /usr/bin/zsh\n  5589e9dd7000-5589e9df9000 r--p 000ac000 08:10 225588                     /usr/bin/zsh\n  5589e9dfa000-5589e9dfc000 r--p 000ce000 08:10 225588                     /usr/bin/zsh\n  5589e9dfc000-5589e9e02000 rw-p 000d0000 08:10 225588                     /usr/bin/zsh\n  5589e9e02000-5589e9e16000 rw-p 00000000 00:00 0\n  5589eac55000-5589eb1af000 rw-p 00000000 00:00 0                          [heap]\n  7f90adfc6000-7f90adfc9000 r--p 00000000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfc9000-7f90adfd6000 r-xp 00003000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd6000-7f90adfd8000 r--p 00010000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd8000-7f90adfd9000 r--p 00011000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd9000-7f90adfda000 rw-p 00012000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfda000-7f90ae25a000 r--s 00000000 08:10 225058                     /usr/share/zsh/functions/Completion/Unix.zwc\n  7f90ae265000-7f90ae28a000 r--s 00000000 08:10 225225                     /usr/share/zsh/functions/Completion/Zsh.zwc\n  7f90ae299000-7f90ae2b2000 r--s 00000000 08:10 225469                     /usr/share/zsh/functions/Zle.zwc\n  7f90ae2b6000-7f90ae2da000 rw-p 00000000 00:00 0\n  7f90ae325000-7f90ae331000 rw-p 00000000 00:00 0\n  7f90ae331000-7f90ae37d000 rw-p 00000000 00:00 0\n  7f90ae37d000-7f90ae381000 r--p 00000000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae381000-7f90ae38d000 r-xp 00004000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae38d000-7f90ae38f000 r--p 00010000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae38f000-7f90ae390000 r--p 00011000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae390000-7f90ae391000 rw-p 00012000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90aecf3000-7f90aecf5000 rw-p 00000000 00:00 0\n  7f90aecf5000-7f90aecf6000 r--p 00000000 08:10 2985                       /usr/lib/locale/C.UTF-8/LC_TELEPHONE\n  7f90aecf6000-7f90aecf7000 r--p 00000000 08:10 2978                       /usr/lib/locale/C.UTF-8/LC_MEASUREMENT\n  7f90aecf7000-7f90aecfe000 r--s 00000000 08:10 264854                     /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\n  7f90aecfe000-7f90aecff000 r--p 00000000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aecff000-7f90aed22000 r-xp 00001000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed22000-7f90aed2a000 r--p 00024000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2a000-7f90aed2b000 r--p 00000000 08:10 2977                       /usr/lib/locale/C.UTF-8/LC_IDENTIFICATION\n  7f90aed2b000-7f90aed2c000 r--p 0002c000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2c000-7f90aed2d000 rw-p 0002d000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2d000-7f90aed2e000 rw-p 00000000 00:00 0\n  7ffeb1e2d000-7ffeb1e7c000 rw-p 00000000 00:00 0                          [stack]\n  7ffeb1f0c000-7ffeb1f10000 r--p 00000000 00:00 0                          [vvar]\n  7ffeb1f10000-7ffeb1f11000 r-xp 00000000 00:00 0                          [vdso]\n  ```\n  \n  \n\n## Why-为什么需要虚拟内存\n\n### 第一，虚拟内存管理可以控制物理内存的访问权限。\n\n**物理内存本身是不限制访问的，任何地址都可以读写，而操作系统要求不同的页面具有不同的访问权限，这是利用CPU模式和MMU的内存保护机制实现的。**例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。\n\n### 第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\n\n**所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。**另一方面，每个进程都认为自己独占整个虚拟地址空间，这样链接器和加载器的实现会比较容易，不必考虑各进程的地址范围是否冲突。\n\n继续前面的实验，再打开一个终端窗口，看一下这个新的`bash`进程的地址空间，可以发现和先前的`bash`进程地址空间的布局差不多：\n\n```shell\n$ ps\n  PID TTY          TIME CMD\n30697 pts/1    00:00:00 bash\n30749 pts/1    00:00:00 ps\n$ cat /proc/30697/maps\n08048000-080f4000 r-xp 00000000 08:15 688142     /bin/bash\n080f4000-080f9000 rw-p 000ac000 08:15 688142     /bin/bash\n080f9000-080fe000 rw-p 080f9000 00:00 0 \n082d7000-084f9000 rw-p 082d7000 00:00 0          [heap]\nb7cf1000-b7cfb000 r-xp 00000000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\nb7cfb000-b7cfc000 r--p 00009000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\nb7cfc000-b7cfd000 rw-p 0000a000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\n...\nb7e5e000-b7fb6000 r-xp 00000000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\nb7fb6000-b7fb8000 r--p 00158000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\nb7fb8000-b7fb9000 rw-p 0015a000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\n...\nb8006000-b8020000 r-xp 00000000 08:15 565466     /lib/ld-2.8.90.so\nb8020000-b8021000 r-xp b8020000 00:00 0          [vdso]\nb8021000-b8022000 r--p 0001a000 08:15 565466     /lib/ld-2.8.90.so\nb8022000-b8023000 rw-p 0001b000 08:15 565466     /lib/ld-2.8.90.so\nbff0e000-bff23000 rw-p bffeb000 00:00 0          [stack]\n```\n\n该进程也占用了0x0000 0000-0xbfff ffff的地址空间，Text Segment也是0x0804 8000-0x080f 4000，Data Segment也是0x080f 4000-0x080f 9000，和先前的进程一模一样，因为这些地址是在编译链接时写进`/bin/bash`这个可执行文件的，两个进程都加载它。这两个进程在同一个系统中同时运行着，它们的Data Segment占用相同的VA，但是两个进程各自干各自的事情，显然Data Segment中的数据应该是不同的，相同的VA怎么会有不同的数据呢？因为它们被映射到不同的PA。如下图所示。\n\n**图 20.5. 进程地址空间是独立的**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229505062-380026ae-9136-4837-ac8c-28e7d794a3d6.png)\n\n从图中还可以看到，两个进程都是`bash`进程，Text Segment是一样的，并且Text Segment是只读的，不会被改写，因此操作系统会安排两个进程的Text Segment共享相同的物理页面。由于每个进程都有自己的一套VA到PA的映射表，整个地址空间中的任何VA都在每个进程自己的映射表中查找相应的PA，因此不可能访问到其它进程的地址，也就没有可能意外改写其它进程的数据。\n\n另外，注意到两个进程的共享库加载地址并不相同，共享库的加载地址是在运行时决定的，而不是写在`/bin/bash`这个可执行文件中。但即使如此，也不影响两个进程共享相同物理页面中的共享库，当然，只有只读的部分是共享的，可读可写的部分不共享。\n\n**使用共享库可以大大节省内存。比如`libc`，系统中几乎所有的进程都映射`libc`到自己的进程地址空间，而`libc`的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。**\n\n现在我们也可以理解为什么共享库必须是位置无关代码了。比如`libc`，不同的进程虽然共享`libc`所在的物理页面，但这些物理页面被映射到各进程的虚拟地址空间时却位于不同的地址，所以要求`libc`的代码不管加载到什么地址都能正确执行。\n\n### 第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\n\n比如要用`malloc`分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的*连续*空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。如下图所示。\n\n**图 20.6. 不连续的PA可以映射为连续的VA**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229512867-30829a8c-dc36-47c3-a300-9afec24b0b0a.png)\n\n### 四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\n\n因为各进程分配的只不过是虚拟内存的页面，这些页面的数据可以映射到物理页面，也可以临时保存到磁盘上而不占用物理页面，在磁盘上临时保存虚拟内存页面的可能是一个磁盘分区，也可能是一个磁盘文件，称为**交换设备（Swap Device）**。当物理内存不够用时，将一些不常用的物理页面中的数据临时保存到交换设备，然后这个物理页面就认为是空闲的了，可以重新分配给进程使用，这个过程称为**换出（Page out）**。如果进程要用到被换出的页面，就从交换设备再加载回物理内存，这称为**换入（Page in）**。换出和换入操作统称为换页（Paging），因此：\n\n**系统中可分配的内存总量 = 物理内存的大小 + 交换设备的大小**\n\n如下图所示。第一张图是换出，将物理页面中的数据保存到磁盘，并解除地址映射，释放物理页面。第二张图是换入，从空闲的物理页面中分配一个，将磁盘暂存的页面加载回内存，并建立地址映射。\n\n\n\n**图 20.7. 换页**\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229520142-3901c155-5c01-4e50-b706-6f68f98236c6.png)\n\n\n\n","source":"_posts/os/虚拟内存.md","raw":"---\ntitle: 虚拟内存-从入门到跑路\ntags:\n  - os\ncategories:\n  - - os\nabbrlink: 59616\ndate: 2022-10-08 18:41:38\nupdated: 2022-10-08 18:41:38\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n>#### 虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\n>\n> - 1、控制物理内存的访问权限。\n>\n>   例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。\n>\n> - 2、让每个进程有独立的地址空间。\n>\n>   所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。\n>\n>#### 虚拟内存 + 共享库可以大大节省内存。\n>\n> 比如`libc`共享库，系统中几乎所有的进程都映射`libc`到自己的进程地址空间，而`libc`的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。    \n>\n>#### 虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\n>\n>#### 虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\n>\n>比如要用`malloc`分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的*连续*空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。\n\n## What-什么是虚拟内存\n\n> **虚拟内存**是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上[物理内存](https://zh.m.wikipedia.org/wiki/物理内存)通常被分隔成多个[内存碎片](https://zh.m.wikipedia.org/wiki/碎片化)，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术使得大型程序的编写变得更容易，对真正的物理内存（例如[RAM](https://zh.m.wikipedia.org/wiki/隨機存取記憶體)）的使用也更有效率。此外，虚拟内存技术可以使多个[进程](https://zh.m.wikipedia.org/wiki/行程)共享同一个[运行库](https://zh.m.wikipedia.org/wiki/函式庫)，并通过分割不同进程的内存空间来提高系统的安全性。\n>\n> 注意：**虚拟内存**不只是“用磁盘空间来扩展物理内存”的意思——这只是扩充[内存级别](https://zh.m.wikipedia.org/wiki/記憶體階層)以使其包含[硬盘驱动器](https://zh.m.wikipedia.org/wiki/硬盘驱动器)而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果，它的作用也可以通过[覆盖](https://zh.m.wikipedia.org/wiki/覆盖_(编程))或者把处于不活动状态的程序以及它们的数据全部交换到磁盘上等方式来实现。对虚拟内存的定义是基于对[地址空间](https://zh.m.wikipedia.org/wiki/地址空间)的重定义的，即把地址空间定义为“连续的虚拟内存地址”，以借此“欺骗”程序，使它们以为自己正在使用一大块的“连续”地址。\n>\n> 那些需要快速存取或者相应时间非常稳定的[嵌入式](https://zh.m.wikipedia.org/wiki/嵌入式)系统，以及其他的具有特殊应用的计算机系统，可能会为了避免让[运算结果的可预测性](https://zh.m.wikipedia.org/wiki/确定性算法)降低，而选择不使用虚拟内存。\n\n## How\n\n> 虚拟内存技术是现代[计算机系统结构](https://zh.m.wikipedia.org/wiki/计算机系统结构)中不可分割的一部分。现代所有用于一般应用的[操作系统](https://zh.m.wikipedia.org/wiki/操作系统)都对普通的应用程序使用虚拟内存技术，例如文字处理软件，电子制表软件，多媒体播放器等等。大部分架构通过[CPU](https://zh.m.wikipedia.org/wiki/CPU)中独立的硬件**内存管理单元**（英语：**memory management unit**，缩写为**MMU**），有时称作**分页内存管理单元**（英语：**paged memory management unit**，缩写为**PMMU**)来辅助实现这一功能。\n\n- 操作系统利用体系结构提供的VA到PA的转换机制实现虚拟内存管理。\n\n  用`ps`命令查看当前终端下的进程，得知`bash`进程的id是27613，然后用`cat /proc/27613/maps`命令查看它的虚拟地址空间。`/proc`目录中的文件并不是真正的磁盘文件，而是由内核虚拟出来的文件系统，当前系统中运行的每个进程在`/proc`下都有一个子目录，目录名就是进程的id，查看目录下的文件可以得到该进程的相关信息。\n\n  ```shell\n  root@DESKTOP-KD33OT8:/home/demo# cat /proc/27613/maps\n  5589e9d2b000-5589e9d42000 r--p 00000000 08:10 225588                     /usr/bin/zsh\n  5589e9d42000-5589e9dd7000 r-xp 00017000 08:10 225588                     /usr/bin/zsh\n  5589e9dd7000-5589e9df9000 r--p 000ac000 08:10 225588                     /usr/bin/zsh\n  5589e9dfa000-5589e9dfc000 r--p 000ce000 08:10 225588                     /usr/bin/zsh\n  5589e9dfc000-5589e9e02000 rw-p 000d0000 08:10 225588                     /usr/bin/zsh\n  5589e9e02000-5589e9e16000 rw-p 00000000 00:00 0\n  5589eac55000-5589eb1af000 rw-p 00000000 00:00 0                          [heap]\n  7f90adfc6000-7f90adfc9000 r--p 00000000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfc9000-7f90adfd6000 r-xp 00003000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd6000-7f90adfd8000 r--p 00010000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd8000-7f90adfd9000 r--p 00011000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfd9000-7f90adfda000 rw-p 00012000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so\n  7f90adfda000-7f90ae25a000 r--s 00000000 08:10 225058                     /usr/share/zsh/functions/Completion/Unix.zwc\n  7f90ae265000-7f90ae28a000 r--s 00000000 08:10 225225                     /usr/share/zsh/functions/Completion/Zsh.zwc\n  7f90ae299000-7f90ae2b2000 r--s 00000000 08:10 225469                     /usr/share/zsh/functions/Zle.zwc\n  7f90ae2b6000-7f90ae2da000 rw-p 00000000 00:00 0\n  7f90ae325000-7f90ae331000 rw-p 00000000 00:00 0\n  7f90ae331000-7f90ae37d000 rw-p 00000000 00:00 0\n  7f90ae37d000-7f90ae381000 r--p 00000000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae381000-7f90ae38d000 r-xp 00004000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae38d000-7f90ae38f000 r--p 00010000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae38f000-7f90ae390000 r--p 00011000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90ae390000-7f90ae391000 rw-p 00012000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so\n  7f90aecf3000-7f90aecf5000 rw-p 00000000 00:00 0\n  7f90aecf5000-7f90aecf6000 r--p 00000000 08:10 2985                       /usr/lib/locale/C.UTF-8/LC_TELEPHONE\n  7f90aecf6000-7f90aecf7000 r--p 00000000 08:10 2978                       /usr/lib/locale/C.UTF-8/LC_MEASUREMENT\n  7f90aecf7000-7f90aecfe000 r--s 00000000 08:10 264854                     /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\n  7f90aecfe000-7f90aecff000 r--p 00000000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aecff000-7f90aed22000 r-xp 00001000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed22000-7f90aed2a000 r--p 00024000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2a000-7f90aed2b000 r--p 00000000 08:10 2977                       /usr/lib/locale/C.UTF-8/LC_IDENTIFICATION\n  7f90aed2b000-7f90aed2c000 r--p 0002c000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2c000-7f90aed2d000 rw-p 0002d000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so\n  7f90aed2d000-7f90aed2e000 rw-p 00000000 00:00 0\n  7ffeb1e2d000-7ffeb1e7c000 rw-p 00000000 00:00 0                          [stack]\n  7ffeb1f0c000-7ffeb1f10000 r--p 00000000 00:00 0                          [vvar]\n  7ffeb1f10000-7ffeb1f11000 r-xp 00000000 00:00 0                          [vdso]\n  ```\n  \n  \n\n## Why-为什么需要虚拟内存\n\n### 第一，虚拟内存管理可以控制物理内存的访问权限。\n\n**物理内存本身是不限制访问的，任何地址都可以读写，而操作系统要求不同的页面具有不同的访问权限，这是利用CPU模式和MMU的内存保护机制实现的。**例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。\n\n### 第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\n\n**所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。**另一方面，每个进程都认为自己独占整个虚拟地址空间，这样链接器和加载器的实现会比较容易，不必考虑各进程的地址范围是否冲突。\n\n继续前面的实验，再打开一个终端窗口，看一下这个新的`bash`进程的地址空间，可以发现和先前的`bash`进程地址空间的布局差不多：\n\n```shell\n$ ps\n  PID TTY          TIME CMD\n30697 pts/1    00:00:00 bash\n30749 pts/1    00:00:00 ps\n$ cat /proc/30697/maps\n08048000-080f4000 r-xp 00000000 08:15 688142     /bin/bash\n080f4000-080f9000 rw-p 000ac000 08:15 688142     /bin/bash\n080f9000-080fe000 rw-p 080f9000 00:00 0 \n082d7000-084f9000 rw-p 082d7000 00:00 0          [heap]\nb7cf1000-b7cfb000 r-xp 00000000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\nb7cfb000-b7cfc000 r--p 00009000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\nb7cfc000-b7cfd000 rw-p 0000a000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so\n...\nb7e5e000-b7fb6000 r-xp 00000000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\nb7fb6000-b7fb8000 r--p 00158000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\nb7fb8000-b7fb9000 rw-p 0015a000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so\n...\nb8006000-b8020000 r-xp 00000000 08:15 565466     /lib/ld-2.8.90.so\nb8020000-b8021000 r-xp b8020000 00:00 0          [vdso]\nb8021000-b8022000 r--p 0001a000 08:15 565466     /lib/ld-2.8.90.so\nb8022000-b8023000 rw-p 0001b000 08:15 565466     /lib/ld-2.8.90.so\nbff0e000-bff23000 rw-p bffeb000 00:00 0          [stack]\n```\n\n该进程也占用了0x0000 0000-0xbfff ffff的地址空间，Text Segment也是0x0804 8000-0x080f 4000，Data Segment也是0x080f 4000-0x080f 9000，和先前的进程一模一样，因为这些地址是在编译链接时写进`/bin/bash`这个可执行文件的，两个进程都加载它。这两个进程在同一个系统中同时运行着，它们的Data Segment占用相同的VA，但是两个进程各自干各自的事情，显然Data Segment中的数据应该是不同的，相同的VA怎么会有不同的数据呢？因为它们被映射到不同的PA。如下图所示。\n\n**图 20.5. 进程地址空间是独立的**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229505062-380026ae-9136-4837-ac8c-28e7d794a3d6.png)\n\n从图中还可以看到，两个进程都是`bash`进程，Text Segment是一样的，并且Text Segment是只读的，不会被改写，因此操作系统会安排两个进程的Text Segment共享相同的物理页面。由于每个进程都有自己的一套VA到PA的映射表，整个地址空间中的任何VA都在每个进程自己的映射表中查找相应的PA，因此不可能访问到其它进程的地址，也就没有可能意外改写其它进程的数据。\n\n另外，注意到两个进程的共享库加载地址并不相同，共享库的加载地址是在运行时决定的，而不是写在`/bin/bash`这个可执行文件中。但即使如此，也不影响两个进程共享相同物理页面中的共享库，当然，只有只读的部分是共享的，可读可写的部分不共享。\n\n**使用共享库可以大大节省内存。比如`libc`，系统中几乎所有的进程都映射`libc`到自己的进程地址空间，而`libc`的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。**\n\n现在我们也可以理解为什么共享库必须是位置无关代码了。比如`libc`，不同的进程虽然共享`libc`所在的物理页面，但这些物理页面被映射到各进程的虚拟地址空间时却位于不同的地址，所以要求`libc`的代码不管加载到什么地址都能正确执行。\n\n### 第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\n\n比如要用`malloc`分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的*连续*空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。如下图所示。\n\n**图 20.6. 不连续的PA可以映射为连续的VA**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229512867-30829a8c-dc36-47c3-a300-9afec24b0b0a.png)\n\n### 四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\n\n因为各进程分配的只不过是虚拟内存的页面，这些页面的数据可以映射到物理页面，也可以临时保存到磁盘上而不占用物理页面，在磁盘上临时保存虚拟内存页面的可能是一个磁盘分区，也可能是一个磁盘文件，称为**交换设备（Swap Device）**。当物理内存不够用时，将一些不常用的物理页面中的数据临时保存到交换设备，然后这个物理页面就认为是空闲的了，可以重新分配给进程使用，这个过程称为**换出（Page out）**。如果进程要用到被换出的页面，就从交换设备再加载回物理内存，这称为**换入（Page in）**。换出和换入操作统称为换页（Paging），因此：\n\n**系统中可分配的内存总量 = 物理内存的大小 + 交换设备的大小**\n\n如下图所示。第一张图是换出，将物理页面中的数据保存到磁盘，并解除地址映射，释放物理页面。第二张图是换入，从空闲的物理页面中分配一个，将磁盘暂存的页面加载回内存，并建立地址映射。\n\n\n\n**图 20.7. 换页**\n\n![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229520142-3901c155-5c01-4e50-b706-6f68f98236c6.png)\n\n\n\n","slug":"os/虚拟内存","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt0002b8j5m7kqo4oj4","content":"<blockquote>\n<h4 id=\"虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\"><a href=\"#虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\" class=\"headerlink\" title=\"虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\"></a>虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：</h4><ul>\n<li><p>1、控制物理内存的访问权限。</p>\n<p>例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。</p>\n</li>\n<li><p>2、让每个进程有独立的地址空间。</p>\n<p>所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。</p>\n</li>\n</ul>\n<h4 id=\"虚拟内存-共享库可以大大节省内存。\"><a href=\"#虚拟内存-共享库可以大大节省内存。\" class=\"headerlink\" title=\"虚拟内存 + 共享库可以大大节省内存。\"></a>虚拟内存 + 共享库可以大大节省内存。</h4><p>比如<code>libc</code>共享库，系统中几乎所有的进程都映射<code>libc</code>到自己的进程地址空间，而<code>libc</code>的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。    </p>\n<h4 id=\"虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"><a href=\"#虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\" class=\"headerlink\" title=\"虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"></a>虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。</h4><h4 id=\"虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\"><a href=\"#虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\" class=\"headerlink\" title=\"虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\"></a>虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。</h4><p>比如要用<code>malloc</code>分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的<em>连续</em>空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。</p>\n</blockquote>\n<h2 id=\"What-什么是虚拟内存\"><a href=\"#What-什么是虚拟内存\" class=\"headerlink\" title=\"What-什么是虚拟内存\"></a>What-什么是虚拟内存</h2><blockquote>\n<p><strong>虚拟内存</strong>是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上<a href=\"https://zh.m.wikipedia.org/wiki/%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98\">物理内存</a>通常被分隔成多个<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A2%8E%E7%89%87%E5%8C%96\">内存碎片</a>，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术使得大型程序的编写变得更容易，对真正的物理内存（例如<a href=\"https://zh.m.wikipedia.org/wiki/%E9%9A%A8%E6%A9%9F%E5%AD%98%E5%8F%96%E8%A8%98%E6%86%B6%E9%AB%94\">RAM</a>）的使用也更有效率。此外，虚拟内存技术可以使多个<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A1%8C%E7%A8%8B\">进程</a>共享同一个<a href=\"https://zh.m.wikipedia.org/wiki/%E5%87%BD%E5%BC%8F%E5%BA%AB\">运行库</a>，并通过分割不同进程的内存空间来提高系统的安全性。</p>\n<p>注意：<strong>虚拟内存</strong>不只是“用磁盘空间来扩展物理内存”的意思——这只是扩充<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A8%98%E6%86%B6%E9%AB%94%E9%9A%8E%E5%B1%A4\">内存级别</a>以使其包含<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A1%AC%E7%9B%98%E9%A9%B1%E5%8A%A8%E5%99%A8\">硬盘驱动器</a>而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果，它的作用也可以通过<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A6%86%E7%9B%96_(%E7%BC%96%E7%A8%8B)\">覆盖</a>或者把处于不活动状态的程序以及它们的数据全部交换到磁盘上等方式来实现。对虚拟内存的定义是基于对<a href=\"https://zh.m.wikipedia.org/wiki/%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4\">地址空间</a>的重定义的，即把地址空间定义为“连续的虚拟内存地址”，以借此“欺骗”程序，使它们以为自己正在使用一大块的“连续”地址。</p>\n<p>那些需要快速存取或者相应时间非常稳定的<a href=\"https://zh.m.wikipedia.org/wiki/%E5%B5%8C%E5%85%A5%E5%BC%8F\">嵌入式</a>系统，以及其他的具有特殊应用的计算机系统，可能会为了避免让<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AE%97%E6%B3%95\">运算结果的可预测性</a>降低，而选择不使用虚拟内存。</p>\n</blockquote>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><blockquote>\n<p>虚拟内存技术是现代<a href=\"https://zh.m.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84\">计算机系统结构</a>中不可分割的一部分。现代所有用于一般应用的<a href=\"https://zh.m.wikipedia.org/wiki/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F\">操作系统</a>都对普通的应用程序使用虚拟内存技术，例如文字处理软件，电子制表软件，多媒体播放器等等。大部分架构通过<a href=\"https://zh.m.wikipedia.org/wiki/CPU\">CPU</a>中独立的硬件<strong>内存管理单元</strong>（英语：<strong>memory management unit</strong>，缩写为<strong>MMU</strong>），有时称作<strong>分页内存管理单元</strong>（英语：<strong>paged memory management unit</strong>，缩写为<strong>PMMU</strong>)来辅助实现这一功能。</p>\n</blockquote>\n<ul>\n<li><p>操作系统利用体系结构提供的VA到PA的转换机制实现虚拟内存管理。</p>\n<p>用<code>ps</code>命令查看当前终端下的进程，得知<code>bash</code>进程的id是27613，然后用<code>cat /proc/27613/maps</code>命令查看它的虚拟地址空间。<code>/proc</code>目录中的文件并不是真正的磁盘文件，而是由内核虚拟出来的文件系统，当前系统中运行的每个进程在<code>/proc</code>下都有一个子目录，目录名就是进程的id，查看目录下的文件可以得到该进程的相关信息。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@DESKTOP-KD33OT8:/home/demo# cat /proc/27613/maps</span><br><span class=\"line\">5589e9d2b000-5589e9d42000 r--p 00000000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9d42000-5589e9dd7000 r-xp 00017000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dd7000-5589e9df9000 r--p 000ac000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dfa000-5589e9dfc000 r--p 000ce000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dfc000-5589e9e02000 rw-p 000d0000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9e02000-5589e9e16000 rw-p 00000000 00:00 0</span><br><span class=\"line\">5589eac55000-5589eb1af000 rw-p 00000000 00:00 0                          [heap]</span><br><span class=\"line\">7f90adfc6000-7f90adfc9000 r--p 00000000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfc9000-7f90adfd6000 r-xp 00003000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd6000-7f90adfd8000 r--p 00010000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd8000-7f90adfd9000 r--p 00011000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd9000-7f90adfda000 rw-p 00012000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfda000-7f90ae25a000 r--s 00000000 08:10 225058                     /usr/share/zsh/functions/Completion/Unix.zwc</span><br><span class=\"line\">7f90ae265000-7f90ae28a000 r--s 00000000 08:10 225225                     /usr/share/zsh/functions/Completion/Zsh.zwc</span><br><span class=\"line\">7f90ae299000-7f90ae2b2000 r--s 00000000 08:10 225469                     /usr/share/zsh/functions/Zle.zwc</span><br><span class=\"line\">7f90ae2b6000-7f90ae2da000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae325000-7f90ae331000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae331000-7f90ae37d000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae37d000-7f90ae381000 r--p 00000000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae381000-7f90ae38d000 r-xp 00004000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae38d000-7f90ae38f000 r--p 00010000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae38f000-7f90ae390000 r--p 00011000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae390000-7f90ae391000 rw-p 00012000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90aecf3000-7f90aecf5000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90aecf5000-7f90aecf6000 r--p 00000000 08:10 2985                       /usr/lib/locale/C.UTF-8/LC_TELEPHONE</span><br><span class=\"line\">7f90aecf6000-7f90aecf7000 r--p 00000000 08:10 2978                       /usr/lib/locale/C.UTF-8/LC_MEASUREMENT</span><br><span class=\"line\">7f90aecf7000-7f90aecfe000 r--s 00000000 08:10 264854                     /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache</span><br><span class=\"line\">7f90aecfe000-7f90aecff000 r--p 00000000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aecff000-7f90aed22000 r-xp 00001000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed22000-7f90aed2a000 r--p 00024000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2a000-7f90aed2b000 r--p 00000000 08:10 2977                       /usr/lib/locale/C.UTF-8/LC_IDENTIFICATION</span><br><span class=\"line\">7f90aed2b000-7f90aed2c000 r--p 0002c000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2c000-7f90aed2d000 rw-p 0002d000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2d000-7f90aed2e000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7ffeb1e2d000-7ffeb1e7c000 rw-p 00000000 00:00 0                          [stack]</span><br><span class=\"line\">7ffeb1f0c000-7ffeb1f10000 r--p 00000000 00:00 0                          [vvar]</span><br><span class=\"line\">7ffeb1f10000-7ffeb1f11000 r-xp 00000000 00:00 0                          [vdso]</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"Why-为什么需要虚拟内存\"><a href=\"#Why-为什么需要虚拟内存\" class=\"headerlink\" title=\"Why-为什么需要虚拟内存\"></a>Why-为什么需要虚拟内存</h2><h3 id=\"第一，虚拟内存管理可以控制物理内存的访问权限。\"><a href=\"#第一，虚拟内存管理可以控制物理内存的访问权限。\" class=\"headerlink\" title=\"第一，虚拟内存管理可以控制物理内存的访问权限。\"></a>第一，虚拟内存管理可以控制物理内存的访问权限。</h3><p><strong>物理内存本身是不限制访问的，任何地址都可以读写，而操作系统要求不同的页面具有不同的访问权限，这是利用CPU模式和MMU的内存保护机制实现的。</strong>例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。</p>\n<h3 id=\"第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\"><a href=\"#第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\" class=\"headerlink\" title=\"第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\"></a>第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。</h3><p><strong>所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。</strong>另一方面，每个进程都认为自己独占整个虚拟地址空间，这样链接器和加载器的实现会比较容易，不必考虑各进程的地址范围是否冲突。</p>\n<p>继续前面的实验，再打开一个终端窗口，看一下这个新的<code>bash</code>进程的地址空间，可以发现和先前的<code>bash</code>进程地址空间的布局差不多：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">ps</span></span><br><span class=\"line\">  PID TTY          TIME CMD</span><br><span class=\"line\">30697 pts/1    00:00:00 bash</span><br><span class=\"line\">30749 pts/1    00:00:00 ps</span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">cat</span> /proc/30697/maps</span></span><br><span class=\"line\">08048000-080f4000 r-xp 00000000 08:15 688142     /bin/bash</span><br><span class=\"line\">080f4000-080f9000 rw-p 000ac000 08:15 688142     /bin/bash</span><br><span class=\"line\">080f9000-080fe000 rw-p 080f9000 00:00 0 </span><br><span class=\"line\">082d7000-084f9000 rw-p 082d7000 00:00 0          [heap]</span><br><span class=\"line\">b7cf1000-b7cfb000 r-xp 00000000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">b7cfb000-b7cfc000 r--p 00009000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">b7cfc000-b7cfd000 rw-p 0000a000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">...</span><br><span class=\"line\">b7e5e000-b7fb6000 r-xp 00000000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">b7fb6000-b7fb8000 r--p 00158000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">b7fb8000-b7fb9000 rw-p 0015a000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">...</span><br><span class=\"line\">b8006000-b8020000 r-xp 00000000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">b8020000-b8021000 r-xp b8020000 00:00 0          [vdso]</span><br><span class=\"line\">b8021000-b8022000 r--p 0001a000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">b8022000-b8023000 rw-p 0001b000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">bff0e000-bff23000 rw-p bffeb000 00:00 0          [stack]</span><br></pre></td></tr></table></figure>\n\n<p>该进程也占用了0x0000 0000-0xbfff ffff的地址空间，Text Segment也是0x0804 8000-0x080f 4000，Data Segment也是0x080f 4000-0x080f 9000，和先前的进程一模一样，因为这些地址是在编译链接时写进<code>/bin/bash</code>这个可执行文件的，两个进程都加载它。这两个进程在同一个系统中同时运行着，它们的Data Segment占用相同的VA，但是两个进程各自干各自的事情，显然Data Segment中的数据应该是不同的，相同的VA怎么会有不同的数据呢？因为它们被映射到不同的PA。如下图所示。</p>\n<p><strong>图 20.5. 进程地址空间是独立的</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229505062-380026ae-9136-4837-ac8c-28e7d794a3d6.png\" alt=\"img\"></p>\n<p>从图中还可以看到，两个进程都是<code>bash</code>进程，Text Segment是一样的，并且Text Segment是只读的，不会被改写，因此操作系统会安排两个进程的Text Segment共享相同的物理页面。由于每个进程都有自己的一套VA到PA的映射表，整个地址空间中的任何VA都在每个进程自己的映射表中查找相应的PA，因此不可能访问到其它进程的地址，也就没有可能意外改写其它进程的数据。</p>\n<p>另外，注意到两个进程的共享库加载地址并不相同，共享库的加载地址是在运行时决定的，而不是写在<code>/bin/bash</code>这个可执行文件中。但即使如此，也不影响两个进程共享相同物理页面中的共享库，当然，只有只读的部分是共享的，可读可写的部分不共享。</p>\n<p><strong>使用共享库可以大大节省内存。比如<code>libc</code>，系统中几乎所有的进程都映射<code>libc</code>到自己的进程地址空间，而<code>libc</code>的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。</strong></p>\n<p>现在我们也可以理解为什么共享库必须是位置无关代码了。比如<code>libc</code>，不同的进程虽然共享<code>libc</code>所在的物理页面，但这些物理页面被映射到各进程的虚拟地址空间时却位于不同的地址，所以要求<code>libc</code>的代码不管加载到什么地址都能正确执行。</p>\n<h3 id=\"第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"><a href=\"#第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\" class=\"headerlink\" title=\"第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"></a>第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。</h3><p>比如要用<code>malloc</code>分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的<em>连续</em>空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。如下图所示。</p>\n<p><strong>图 20.6. 不连续的PA可以映射为连续的VA</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229512867-30829a8c-dc36-47c3-a300-9afec24b0b0a.png\" alt=\"img\"></p>\n<h3 id=\"四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\"><a href=\"#四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\" class=\"headerlink\" title=\"四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\"></a>四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。</h3><p>因为各进程分配的只不过是虚拟内存的页面，这些页面的数据可以映射到物理页面，也可以临时保存到磁盘上而不占用物理页面，在磁盘上临时保存虚拟内存页面的可能是一个磁盘分区，也可能是一个磁盘文件，称为<strong>交换设备（Swap Device）</strong>。当物理内存不够用时，将一些不常用的物理页面中的数据临时保存到交换设备，然后这个物理页面就认为是空闲的了，可以重新分配给进程使用，这个过程称为<strong>换出（Page out）</strong>。如果进程要用到被换出的页面，就从交换设备再加载回物理内存，这称为<strong>换入（Page in）</strong>。换出和换入操作统称为换页（Paging），因此：</p>\n<p><strong>系统中可分配的内存总量 &#x3D; 物理内存的大小 + 交换设备的大小</strong></p>\n<p>如下图所示。第一张图是换出，将物理页面中的数据保存到磁盘，并解除地址映射，释放物理页面。第二张图是换入，从空闲的物理页面中分配一个，将磁盘暂存的页面加载回内存，并建立地址映射。</p>\n<p><strong>图 20.7. 换页</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229520142-3901c155-5c01-4e50-b706-6f68f98236c6.png\" alt=\"image.png\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<h4 id=\"虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\"><a href=\"#虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\" class=\"headerlink\" title=\"虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：\"></a>虚拟内存可以提高系统的稳定性和安全性，主要通过以下两点：</h4><ul>\n<li><p>1、控制物理内存的访问权限。</p>\n<p>例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。</p>\n</li>\n<li><p>2、让每个进程有独立的地址空间。</p>\n<p>所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。</p>\n</li>\n</ul>\n<h4 id=\"虚拟内存-共享库可以大大节省内存。\"><a href=\"#虚拟内存-共享库可以大大节省内存。\" class=\"headerlink\" title=\"虚拟内存 + 共享库可以大大节省内存。\"></a>虚拟内存 + 共享库可以大大节省内存。</h4><p>比如<code>libc</code>共享库，系统中几乎所有的进程都映射<code>libc</code>到自己的进程地址空间，而<code>libc</code>的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。    </p>\n<h4 id=\"虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"><a href=\"#虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\" class=\"headerlink\" title=\"虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"></a>虚拟内存方便给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。</h4><h4 id=\"虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\"><a href=\"#虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\" class=\"headerlink\" title=\"虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。\"></a>虚拟内存带来了交换分区。这样就提高了系统中可分配的内存总量，可以运行更多进程。</h4><p>比如要用<code>malloc</code>分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的<em>连续</em>空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。</p>\n</blockquote>\n<h2 id=\"What-什么是虚拟内存\"><a href=\"#What-什么是虚拟内存\" class=\"headerlink\" title=\"What-什么是虚拟内存\"></a>What-什么是虚拟内存</h2><blockquote>\n<p><strong>虚拟内存</strong>是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上<a href=\"https://zh.m.wikipedia.org/wiki/%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98\">物理内存</a>通常被分隔成多个<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A2%8E%E7%89%87%E5%8C%96\">内存碎片</a>，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术使得大型程序的编写变得更容易，对真正的物理内存（例如<a href=\"https://zh.m.wikipedia.org/wiki/%E9%9A%A8%E6%A9%9F%E5%AD%98%E5%8F%96%E8%A8%98%E6%86%B6%E9%AB%94\">RAM</a>）的使用也更有效率。此外，虚拟内存技术可以使多个<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A1%8C%E7%A8%8B\">进程</a>共享同一个<a href=\"https://zh.m.wikipedia.org/wiki/%E5%87%BD%E5%BC%8F%E5%BA%AB\">运行库</a>，并通过分割不同进程的内存空间来提高系统的安全性。</p>\n<p>注意：<strong>虚拟内存</strong>不只是“用磁盘空间来扩展物理内存”的意思——这只是扩充<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A8%98%E6%86%B6%E9%AB%94%E9%9A%8E%E5%B1%A4\">内存级别</a>以使其包含<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A1%AC%E7%9B%98%E9%A9%B1%E5%8A%A8%E5%99%A8\">硬盘驱动器</a>而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果，它的作用也可以通过<a href=\"https://zh.m.wikipedia.org/wiki/%E8%A6%86%E7%9B%96_(%E7%BC%96%E7%A8%8B)\">覆盖</a>或者把处于不活动状态的程序以及它们的数据全部交换到磁盘上等方式来实现。对虚拟内存的定义是基于对<a href=\"https://zh.m.wikipedia.org/wiki/%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4\">地址空间</a>的重定义的，即把地址空间定义为“连续的虚拟内存地址”，以借此“欺骗”程序，使它们以为自己正在使用一大块的“连续”地址。</p>\n<p>那些需要快速存取或者相应时间非常稳定的<a href=\"https://zh.m.wikipedia.org/wiki/%E5%B5%8C%E5%85%A5%E5%BC%8F\">嵌入式</a>系统，以及其他的具有特殊应用的计算机系统，可能会为了避免让<a href=\"https://zh.m.wikipedia.org/wiki/%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AE%97%E6%B3%95\">运算结果的可预测性</a>降低，而选择不使用虚拟内存。</p>\n</blockquote>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><blockquote>\n<p>虚拟内存技术是现代<a href=\"https://zh.m.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84\">计算机系统结构</a>中不可分割的一部分。现代所有用于一般应用的<a href=\"https://zh.m.wikipedia.org/wiki/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F\">操作系统</a>都对普通的应用程序使用虚拟内存技术，例如文字处理软件，电子制表软件，多媒体播放器等等。大部分架构通过<a href=\"https://zh.m.wikipedia.org/wiki/CPU\">CPU</a>中独立的硬件<strong>内存管理单元</strong>（英语：<strong>memory management unit</strong>，缩写为<strong>MMU</strong>），有时称作<strong>分页内存管理单元</strong>（英语：<strong>paged memory management unit</strong>，缩写为<strong>PMMU</strong>)来辅助实现这一功能。</p>\n</blockquote>\n<ul>\n<li><p>操作系统利用体系结构提供的VA到PA的转换机制实现虚拟内存管理。</p>\n<p>用<code>ps</code>命令查看当前终端下的进程，得知<code>bash</code>进程的id是27613，然后用<code>cat /proc/27613/maps</code>命令查看它的虚拟地址空间。<code>/proc</code>目录中的文件并不是真正的磁盘文件，而是由内核虚拟出来的文件系统，当前系统中运行的每个进程在<code>/proc</code>下都有一个子目录，目录名就是进程的id，查看目录下的文件可以得到该进程的相关信息。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@DESKTOP-KD33OT8:/home/demo# cat /proc/27613/maps</span><br><span class=\"line\">5589e9d2b000-5589e9d42000 r--p 00000000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9d42000-5589e9dd7000 r-xp 00017000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dd7000-5589e9df9000 r--p 000ac000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dfa000-5589e9dfc000 r--p 000ce000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9dfc000-5589e9e02000 rw-p 000d0000 08:10 225588                     /usr/bin/zsh</span><br><span class=\"line\">5589e9e02000-5589e9e16000 rw-p 00000000 00:00 0</span><br><span class=\"line\">5589eac55000-5589eb1af000 rw-p 00000000 00:00 0                          [heap]</span><br><span class=\"line\">7f90adfc6000-7f90adfc9000 r--p 00000000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfc9000-7f90adfd6000 r-xp 00003000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd6000-7f90adfd8000 r--p 00010000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd8000-7f90adfd9000 r--p 00011000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfd9000-7f90adfda000 rw-p 00012000 08:10 225599                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/computil.so</span><br><span class=\"line\">7f90adfda000-7f90ae25a000 r--s 00000000 08:10 225058                     /usr/share/zsh/functions/Completion/Unix.zwc</span><br><span class=\"line\">7f90ae265000-7f90ae28a000 r--s 00000000 08:10 225225                     /usr/share/zsh/functions/Completion/Zsh.zwc</span><br><span class=\"line\">7f90ae299000-7f90ae2b2000 r--s 00000000 08:10 225469                     /usr/share/zsh/functions/Zle.zwc</span><br><span class=\"line\">7f90ae2b6000-7f90ae2da000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae325000-7f90ae331000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae331000-7f90ae37d000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90ae37d000-7f90ae381000 r--p 00000000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae381000-7f90ae38d000 r-xp 00004000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae38d000-7f90ae38f000 r--p 00010000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae38f000-7f90ae390000 r--p 00011000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90ae390000-7f90ae391000 rw-p 00012000 08:10 225596                     /usr/lib/x86_64-linux-gnu/zsh/5.8/zsh/compctl.so</span><br><span class=\"line\">7f90aecf3000-7f90aecf5000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7f90aecf5000-7f90aecf6000 r--p 00000000 08:10 2985                       /usr/lib/locale/C.UTF-8/LC_TELEPHONE</span><br><span class=\"line\">7f90aecf6000-7f90aecf7000 r--p 00000000 08:10 2978                       /usr/lib/locale/C.UTF-8/LC_MEASUREMENT</span><br><span class=\"line\">7f90aecf7000-7f90aecfe000 r--s 00000000 08:10 264854                     /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache</span><br><span class=\"line\">7f90aecfe000-7f90aecff000 r--p 00000000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aecff000-7f90aed22000 r-xp 00001000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed22000-7f90aed2a000 r--p 00024000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2a000-7f90aed2b000 r--p 00000000 08:10 2977                       /usr/lib/locale/C.UTF-8/LC_IDENTIFICATION</span><br><span class=\"line\">7f90aed2b000-7f90aed2c000 r--p 0002c000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2c000-7f90aed2d000 rw-p 0002d000 08:10 38549                      /usr/lib/x86_64-linux-gnu/ld-2.31.so</span><br><span class=\"line\">7f90aed2d000-7f90aed2e000 rw-p 00000000 00:00 0</span><br><span class=\"line\">7ffeb1e2d000-7ffeb1e7c000 rw-p 00000000 00:00 0                          [stack]</span><br><span class=\"line\">7ffeb1f0c000-7ffeb1f10000 r--p 00000000 00:00 0                          [vvar]</span><br><span class=\"line\">7ffeb1f10000-7ffeb1f11000 r-xp 00000000 00:00 0                          [vdso]</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"Why-为什么需要虚拟内存\"><a href=\"#Why-为什么需要虚拟内存\" class=\"headerlink\" title=\"Why-为什么需要虚拟内存\"></a>Why-为什么需要虚拟内存</h2><h3 id=\"第一，虚拟内存管理可以控制物理内存的访问权限。\"><a href=\"#第一，虚拟内存管理可以控制物理内存的访问权限。\" class=\"headerlink\" title=\"第一，虚拟内存管理可以控制物理内存的访问权限。\"></a>第一，虚拟内存管理可以控制物理内存的访问权限。</h3><p><strong>物理内存本身是不限制访问的，任何地址都可以读写，而操作系统要求不同的页面具有不同的访问权限，这是利用CPU模式和MMU的内存保护机制实现的。</strong>例如，Text Segment被只读保护起来，防止被错误的指令意外改写，内核地址空间也被保护起来，防止在用户模式下执行错误的指令意外改写内核数据。这样，执行错误指令或恶意代码的破坏能力受到了限制，顶多使当前进程因段错误终止，而不会影响整个系统的稳定性。</p>\n<h3 id=\"第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\"><a href=\"#第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\" class=\"headerlink\" title=\"第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。\"></a>第二，虚拟内存管理最主要的作用是让每个进程有独立的地址空间。</h3><p><strong>所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。</strong>另一方面，每个进程都认为自己独占整个虚拟地址空间，这样链接器和加载器的实现会比较容易，不必考虑各进程的地址范围是否冲突。</p>\n<p>继续前面的实验，再打开一个终端窗口，看一下这个新的<code>bash</code>进程的地址空间，可以发现和先前的<code>bash</code>进程地址空间的布局差不多：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">ps</span></span><br><span class=\"line\">  PID TTY          TIME CMD</span><br><span class=\"line\">30697 pts/1    00:00:00 bash</span><br><span class=\"line\">30749 pts/1    00:00:00 ps</span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">cat</span> /proc/30697/maps</span></span><br><span class=\"line\">08048000-080f4000 r-xp 00000000 08:15 688142     /bin/bash</span><br><span class=\"line\">080f4000-080f9000 rw-p 000ac000 08:15 688142     /bin/bash</span><br><span class=\"line\">080f9000-080fe000 rw-p 080f9000 00:00 0 </span><br><span class=\"line\">082d7000-084f9000 rw-p 082d7000 00:00 0          [heap]</span><br><span class=\"line\">b7cf1000-b7cfb000 r-xp 00000000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">b7cfb000-b7cfc000 r--p 00009000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">b7cfc000-b7cfd000 rw-p 0000a000 08:15 581665     /lib/tls/i686/cmov/libnss_files-2.8.90.so</span><br><span class=\"line\">...</span><br><span class=\"line\">b7e5e000-b7fb6000 r-xp 00000000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">b7fb6000-b7fb8000 r--p 00158000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">b7fb8000-b7fb9000 rw-p 0015a000 08:15 581656     /lib/tls/i686/cmov/libc-2.8.90.so</span><br><span class=\"line\">...</span><br><span class=\"line\">b8006000-b8020000 r-xp 00000000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">b8020000-b8021000 r-xp b8020000 00:00 0          [vdso]</span><br><span class=\"line\">b8021000-b8022000 r--p 0001a000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">b8022000-b8023000 rw-p 0001b000 08:15 565466     /lib/ld-2.8.90.so</span><br><span class=\"line\">bff0e000-bff23000 rw-p bffeb000 00:00 0          [stack]</span><br></pre></td></tr></table></figure>\n\n<p>该进程也占用了0x0000 0000-0xbfff ffff的地址空间，Text Segment也是0x0804 8000-0x080f 4000，Data Segment也是0x080f 4000-0x080f 9000，和先前的进程一模一样，因为这些地址是在编译链接时写进<code>/bin/bash</code>这个可执行文件的，两个进程都加载它。这两个进程在同一个系统中同时运行着，它们的Data Segment占用相同的VA，但是两个进程各自干各自的事情，显然Data Segment中的数据应该是不同的，相同的VA怎么会有不同的数据呢？因为它们被映射到不同的PA。如下图所示。</p>\n<p><strong>图 20.5. 进程地址空间是独立的</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229505062-380026ae-9136-4837-ac8c-28e7d794a3d6.png\" alt=\"img\"></p>\n<p>从图中还可以看到，两个进程都是<code>bash</code>进程，Text Segment是一样的，并且Text Segment是只读的，不会被改写，因此操作系统会安排两个进程的Text Segment共享相同的物理页面。由于每个进程都有自己的一套VA到PA的映射表，整个地址空间中的任何VA都在每个进程自己的映射表中查找相应的PA，因此不可能访问到其它进程的地址，也就没有可能意外改写其它进程的数据。</p>\n<p>另外，注意到两个进程的共享库加载地址并不相同，共享库的加载地址是在运行时决定的，而不是写在<code>/bin/bash</code>这个可执行文件中。但即使如此，也不影响两个进程共享相同物理页面中的共享库，当然，只有只读的部分是共享的，可读可写的部分不共享。</p>\n<p><strong>使用共享库可以大大节省内存。比如<code>libc</code>，系统中几乎所有的进程都映射<code>libc</code>到自己的进程地址空间，而<code>libc</code>的只读部分在物理内存中只需要存在一份，就可以被所有进程共享，这就是“共享库”这个名称的由来了。</strong></p>\n<p>现在我们也可以理解为什么共享库必须是位置无关代码了。比如<code>libc</code>，不同的进程虽然共享<code>libc</code>所在的物理页面，但这些物理页面被映射到各进程的虚拟地址空间时却位于不同的地址，所以要求<code>libc</code>的代码不管加载到什么地址都能正确执行。</p>\n<h3 id=\"第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"><a href=\"#第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\" class=\"headerlink\" title=\"第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。\"></a>第三，VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。</h3><p>比如要用<code>malloc</code>分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的<em>连续</em>空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。如下图所示。</p>\n<p><strong>图 20.6. 不连续的PA可以映射为连续的VA</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229512867-30829a8c-dc36-47c3-a300-9afec24b0b0a.png\" alt=\"img\"></p>\n<h3 id=\"四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\"><a href=\"#四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\" class=\"headerlink\" title=\"四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。\"></a>四，一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。</h3><p>因为各进程分配的只不过是虚拟内存的页面，这些页面的数据可以映射到物理页面，也可以临时保存到磁盘上而不占用物理页面，在磁盘上临时保存虚拟内存页面的可能是一个磁盘分区，也可能是一个磁盘文件，称为<strong>交换设备（Swap Device）</strong>。当物理内存不够用时，将一些不常用的物理页面中的数据临时保存到交换设备，然后这个物理页面就认为是空闲的了，可以重新分配给进程使用，这个过程称为<strong>换出（Page out）</strong>。如果进程要用到被换出的页面，就从交换设备再加载回物理内存，这称为<strong>换入（Page in）</strong>。换出和换入操作统称为换页（Paging），因此：</p>\n<p><strong>系统中可分配的内存总量 &#x3D; 物理内存的大小 + 交换设备的大小</strong></p>\n<p>如下图所示。第一张图是换出，将物理页面中的数据保存到磁盘，并解除地址映射，释放物理页面。第二张图是换入，从空闲的物理页面中分配一个，将磁盘暂存的页面加载回内存，并建立地址映射。</p>\n<p><strong>图 20.7. 换页</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1665229520142-3901c155-5c01-4e50-b706-6f68f98236c6.png\" alt=\"image.png\"></p>\n"},{"title":"一文搞懂Kudu的整体架构","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":28143,"date":"2022-07-01T12:31:02.000Z","updated":"2022-07-01T12:31:02.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n\n> Kudu是典型的Master-Slave架构，基于LSM优化写入性能，但同时读性能会低（相较于Parquet）。Kudu基于Raft协议实现了Master和Slave Tablet节点的数据的一致性，以及选举功能，保证了容错性和高可用。\n>\n> Kudu是完全的列式存储引擎，可以针对性的编码和压缩，提高了IO性能。HBase是基于列族的，No Schema的NoSQL、KV数据库，无法进行针对性的编码和压缩，同时一般情况只会用一个列族，其实HBase退化为行存储引擎。\n>\n> Kudu通过WAL和Raft保证了分布式数据的一致性。\n>\n> kudu相对于HBase，牺牲了一定的写入性能--->Kudu在写入数据的时候，需要先检查一遍唯一主键是否存在，如果存在会报错，同样更新数据的时候，同样需要先查找主键是否存在。因此Insert和Update等所有操作比HBase多了，`先读一次`的开销，而HBase所有的操作都是转化为直接写入，因此写的性能相较于HBase有一定的劣势。\n>\n> Kudu牺牲写的性能，但是保证了一个主键，只会存在于一个RowSet中，而HBase的RowKey可能会在多个HFlie中。减少了IO，提升了读性能，特别是在大量写入，少量更新的情况下。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605849-a27b0f24-9a73-486d-9797-0009ce3bc3dc.png?x-oss-process=image%2Fresize%2Cw_1080%2Climit_0)\n\n**Table：**具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。\n**Tablet：**Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。\n**TabletServer：**简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。\n**Catalog Table：**目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。\n**Master：**负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。\n**WAL：**一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。\n**逻辑复制：**Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670281157-814d5745-2416-468f-8c68-44c63244069f.png)\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605725-aa6706b6-8453-45b7-b084-18a05394f969.png?x-oss-process=image%2Fresize%2Cw_829%2Climit_0)\n\n\n\n如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。\n**MemRowSet：**插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。\n**DiskRowSet：**DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。\n**BaseData：**DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。\n**BloomFile：**根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。\n**AdhocIndex：**存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。\n**DeltaMemStore：**每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。\n**DeltaFile：**DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。\n**RedoFile：**重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。\n**UndoFile：**撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。\n\nDeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式：\n\n- Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。\n- Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。\n  补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。\n\n**Kudu读流程：**\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670543950-cf2edd8b-55d4-4eb2-b224-78d3c1d9aa31.png)\n\n1. Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。\n2. Client找到所需Tablet所在TServer，TServer接受读请求。\n3. 如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。\n4. 如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。\n\n**Kudu写流程：**\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544082-37d8b7e9-de97-46c0-9ded-20ad7ae15c16.png)\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544010-1315c073-bd11-488c-a780-ffbf553002bb.png)\n\n1. Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。\n2. 因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。\n3. 如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。\n4. 如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。\n5. 写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。\n6. MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。\n\n**Kudu更新流程：**\n\n1. Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。\n2. Kudu检查是否符合表结构。\n3. 如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。\n4. 如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605651-12519d77-90d0-4e92-8def-a832ad06c4b5.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346609884-6c6502ed-d74e-4796-b91c-875a4792c2e9.png?x-oss-process=image%2Fresize%2Cw_913%2Climit_0)\n\n\n\n","source":"_posts/bigdata/一文搞懂Kudu的整体架构.md","raw":"---\ntitle: 一文搞懂Kudu的整体架构\ntags:\n  - kudu\ncategories:\n  - - bigdata\n    - kudu\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 28143\ndate: 2022-07-01 20:31:02\nupdated: 2022-07-01 20:31:02\ncover:\ndescription:\nkeywords:\n---\n\n\n> Kudu是典型的Master-Slave架构，基于LSM优化写入性能，但同时读性能会低（相较于Parquet）。Kudu基于Raft协议实现了Master和Slave Tablet节点的数据的一致性，以及选举功能，保证了容错性和高可用。\n>\n> Kudu是完全的列式存储引擎，可以针对性的编码和压缩，提高了IO性能。HBase是基于列族的，No Schema的NoSQL、KV数据库，无法进行针对性的编码和压缩，同时一般情况只会用一个列族，其实HBase退化为行存储引擎。\n>\n> Kudu通过WAL和Raft保证了分布式数据的一致性。\n>\n> kudu相对于HBase，牺牲了一定的写入性能--->Kudu在写入数据的时候，需要先检查一遍唯一主键是否存在，如果存在会报错，同样更新数据的时候，同样需要先查找主键是否存在。因此Insert和Update等所有操作比HBase多了，`先读一次`的开销，而HBase所有的操作都是转化为直接写入，因此写的性能相较于HBase有一定的劣势。\n>\n> Kudu牺牲写的性能，但是保证了一个主键，只会存在于一个RowSet中，而HBase的RowKey可能会在多个HFlie中。减少了IO，提升了读性能，特别是在大量写入，少量更新的情况下。\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605849-a27b0f24-9a73-486d-9797-0009ce3bc3dc.png?x-oss-process=image%2Fresize%2Cw_1080%2Climit_0)\n\n**Table：**具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。\n**Tablet：**Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。\n**TabletServer：**简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。\n**Catalog Table：**目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。\n**Master：**负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。\n**WAL：**一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。\n**逻辑复制：**Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670281157-814d5745-2416-468f-8c68-44c63244069f.png)\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605725-aa6706b6-8453-45b7-b084-18a05394f969.png?x-oss-process=image%2Fresize%2Cw_829%2Climit_0)\n\n\n\n如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。\n**MemRowSet：**插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。\n**DiskRowSet：**DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。\n**BaseData：**DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。\n**BloomFile：**根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。\n**AdhocIndex：**存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。\n**DeltaMemStore：**每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。\n**DeltaFile：**DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。\n**RedoFile：**重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。\n**UndoFile：**撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。\n\nDeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式：\n\n- Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。\n- Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。\n  补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。\n\n**Kudu读流程：**\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670543950-cf2edd8b-55d4-4eb2-b224-78d3c1d9aa31.png)\n\n1. Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。\n2. Client找到所需Tablet所在TServer，TServer接受读请求。\n3. 如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。\n4. 如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。\n\n**Kudu写流程：**\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544082-37d8b7e9-de97-46c0-9ded-20ad7ae15c16.png)\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544010-1315c073-bd11-488c-a780-ffbf553002bb.png)\n\n1. Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。\n2. 因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。\n3. 如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。\n4. 如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。\n5. 写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。\n6. MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。\n\n**Kudu更新流程：**\n\n1. Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。\n2. Kudu检查是否符合表结构。\n3. 如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。\n4. 如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605651-12519d77-90d0-4e92-8def-a832ad06c4b5.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346609884-6c6502ed-d74e-4796-b91c-875a4792c2e9.png?x-oss-process=image%2Fresize%2Cw_913%2Climit_0)\n\n\n\n","slug":"bigdata/一文搞懂Kudu的整体架构","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt0002e8j5mhv92hnn3","content":"<blockquote>\n<p>Kudu是典型的Master-Slave架构，基于LSM优化写入性能，但同时读性能会低（相较于Parquet）。Kudu基于Raft协议实现了Master和Slave Tablet节点的数据的一致性，以及选举功能，保证了容错性和高可用。</p>\n<p>Kudu是完全的列式存储引擎，可以针对性的编码和压缩，提高了IO性能。HBase是基于列族的，No Schema的NoSQL、KV数据库，无法进行针对性的编码和压缩，同时一般情况只会用一个列族，其实HBase退化为行存储引擎。</p>\n<p>Kudu通过WAL和Raft保证了分布式数据的一致性。</p>\n<p>kudu相对于HBase，牺牲了一定的写入性能—&gt;Kudu在写入数据的时候，需要先检查一遍唯一主键是否存在，如果存在会报错，同样更新数据的时候，同样需要先查找主键是否存在。因此Insert和Update等所有操作比HBase多了，<code>先读一次</code>的开销，而HBase所有的操作都是转化为直接写入，因此写的性能相较于HBase有一定的劣势。</p>\n<p>Kudu牺牲写的性能，但是保证了一个主键，只会存在于一个RowSet中，而HBase的RowKey可能会在多个HFlie中。减少了IO，提升了读性能，特别是在大量写入，少量更新的情况下。</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605849-a27b0f24-9a73-486d-9797-0009ce3bc3dc.png?x-oss-process=image/resize,w_1080,limit_0\" alt=\"img\"></p>\n<p><strong>Table：</strong>具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。<br><strong>Tablet：</strong>Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。<br><strong>TabletServer：</strong>简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。<br><strong>Catalog Table：</strong>目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。<br><strong>Master：</strong>负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。<br><strong>WAL：</strong>一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。<br><strong>逻辑复制：</strong>Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670281157-814d5745-2416-468f-8c68-44c63244069f.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605725-aa6706b6-8453-45b7-b084-18a05394f969.png?x-oss-process=image/resize,w_829,limit_0\" alt=\"img\"></p>\n<p>如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。<br><strong>MemRowSet：</strong>插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。<br><strong>DiskRowSet：</strong>DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。<br><strong>BaseData：</strong>DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。<br><strong>BloomFile：</strong>根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。<br><strong>AdhocIndex：</strong>存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。<br><strong>DeltaMemStore：</strong>每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。<br><strong>DeltaFile：</strong>DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。<br><strong>RedoFile：</strong>重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。<br><strong>UndoFile：</strong>撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。</p>\n<p>DeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式：</p>\n<ul>\n<li>Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。</li>\n<li>Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。<br>补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。</li>\n</ul>\n<p><strong>Kudu读流程：</strong><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670543950-cf2edd8b-55d4-4eb2-b224-78d3c1d9aa31.png\" alt=\"img\"></p>\n<ol>\n<li>Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。</li>\n<li>Client找到所需Tablet所在TServer，TServer接受读请求。</li>\n<li>如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。</li>\n<li>如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。</li>\n</ol>\n<p><strong>Kudu写流程：</strong><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544082-37d8b7e9-de97-46c0-9ded-20ad7ae15c16.png\" alt=\"img\"><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544010-1315c073-bd11-488c-a780-ffbf553002bb.png\" alt=\"img\"></p>\n<ol>\n<li>Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。</li>\n<li>因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。</li>\n<li>如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。</li>\n<li>如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。</li>\n<li>写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。</li>\n<li>MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。</li>\n</ol>\n<p><strong>Kudu更新流程：</strong></p>\n<ol>\n<li>Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。</li>\n<li>Kudu检查是否符合表结构。</li>\n<li>如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。</li>\n<li>如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。</li>\n</ol>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605651-12519d77-90d0-4e92-8def-a832ad06c4b5.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346609884-6c6502ed-d74e-4796-b91c-875a4792c2e9.png?x-oss-process=image/resize,w_913,limit_0\" alt=\"img\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>Kudu是典型的Master-Slave架构，基于LSM优化写入性能，但同时读性能会低（相较于Parquet）。Kudu基于Raft协议实现了Master和Slave Tablet节点的数据的一致性，以及选举功能，保证了容错性和高可用。</p>\n<p>Kudu是完全的列式存储引擎，可以针对性的编码和压缩，提高了IO性能。HBase是基于列族的，No Schema的NoSQL、KV数据库，无法进行针对性的编码和压缩，同时一般情况只会用一个列族，其实HBase退化为行存储引擎。</p>\n<p>Kudu通过WAL和Raft保证了分布式数据的一致性。</p>\n<p>kudu相对于HBase，牺牲了一定的写入性能—&gt;Kudu在写入数据的时候，需要先检查一遍唯一主键是否存在，如果存在会报错，同样更新数据的时候，同样需要先查找主键是否存在。因此Insert和Update等所有操作比HBase多了，<code>先读一次</code>的开销，而HBase所有的操作都是转化为直接写入，因此写的性能相较于HBase有一定的劣势。</p>\n<p>Kudu牺牲写的性能，但是保证了一个主键，只会存在于一个RowSet中，而HBase的RowKey可能会在多个HFlie中。减少了IO，提升了读性能，特别是在大量写入，少量更新的情况下。</p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605849-a27b0f24-9a73-486d-9797-0009ce3bc3dc.png?x-oss-process=image/resize,w_1080,limit_0\" alt=\"img\"></p>\n<p><strong>Table：</strong>具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。<br><strong>Tablet：</strong>Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。<br><strong>TabletServer：</strong>简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。<br><strong>Catalog Table：</strong>目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。<br><strong>Master：</strong>负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。<br><strong>WAL：</strong>一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。<br><strong>逻辑复制：</strong>Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670281157-814d5745-2416-468f-8c68-44c63244069f.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605725-aa6706b6-8453-45b7-b084-18a05394f969.png?x-oss-process=image/resize,w_829,limit_0\" alt=\"img\"></p>\n<p>如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。<br><strong>MemRowSet：</strong>插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。<br><strong>DiskRowSet：</strong>DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。<br><strong>BaseData：</strong>DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。<br><strong>BloomFile：</strong>根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。<br><strong>AdhocIndex：</strong>存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。<br><strong>DeltaMemStore：</strong>每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。<br><strong>DeltaFile：</strong>DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。<br><strong>RedoFile：</strong>重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。<br><strong>UndoFile：</strong>撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。</p>\n<p>DeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式：</p>\n<ul>\n<li>Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。</li>\n<li>Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。<br>补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。</li>\n</ul>\n<p><strong>Kudu读流程：</strong><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670543950-cf2edd8b-55d4-4eb2-b224-78d3c1d9aa31.png\" alt=\"img\"></p>\n<ol>\n<li>Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。</li>\n<li>Client找到所需Tablet所在TServer，TServer接受读请求。</li>\n<li>如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。</li>\n<li>如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。</li>\n</ol>\n<p><strong>Kudu写流程：</strong><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544082-37d8b7e9-de97-46c0-9ded-20ad7ae15c16.png\" alt=\"img\"><br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1656670544010-1315c073-bd11-488c-a780-ffbf553002bb.png\" alt=\"img\"></p>\n<ol>\n<li>Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。</li>\n<li>因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。</li>\n<li>如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。</li>\n<li>如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。</li>\n<li>写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。</li>\n<li>MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。</li>\n</ol>\n<p><strong>Kudu更新流程：</strong></p>\n<ol>\n<li>Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。</li>\n<li>Kudu检查是否符合表结构。</li>\n<li>如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。</li>\n<li>如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。</li>\n</ol>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346605651-12519d77-90d0-4e92-8def-a832ad06c4b5.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1655346609884-6c6502ed-d74e-4796-b91c-875a4792c2e9.png?x-oss-process=image/resize,w_913,limit_0\" alt=\"img\"></p>\n"},{"title":"基于Netty的RPC","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":46224,"date":"2022-08-31T13:07:13.000Z","updated":"2022-08-31T13:07:13.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"","source":"_posts/rpc/基于Netty的简单RPC实现.md","raw":"---\ntitle: 基于Netty的RPC\ntags:\n  - RPC\ncategories:\n  - - RPC\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 46224\ndate: 2022-08-31 21:07:13\nupdated: 2022-08-31 21:07:13\ncover:\ndescription:\nkeywords:\n---\n","slug":"rpc/基于Netty的简单RPC实现","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt0002i8j5m614z84ny","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":""},{"title":"Netty与Reactor模型","abbrlink":22006,"date":"2022-11-29T11:35:27.000Z","updated":"2022-11-29T11:35:27.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","top_img":null,"description":null,"keywords":null,"_content":"\n> Netty is *an asynchronous event-driven network application framework* for rapid development of maintainable high performance protocol servers & clients.\n\n## 基本概念：\n\n- 1、Netty是对JDK NIO进行的一系列封装，使得更容易更快速的编写出高性能的安全的网络应用程序。\n\n- 2、Java NIO与BIO一个重要的不同点是非阻塞，在Linux中，Java NIO依赖于Linux的epoll实现.\n\n- 3、epoll是Linux中的专有名词或实现：epoll是一种I/O事件通知机制，是linux 内核实现IO多路复用的一个实现。\n\n  > Linu主要通过暴漏三个系统调用供上层应用使用epoll：int epoll_create(int size)、 int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)、int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);\n  >\n  > \n  >\n  > IO多路复用是指，在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。\n\n\n\n## Reactor模型\n\n> NIO 服务端编程采用的是 Reactor 模式（也叫做 Dispatcher 模式，分派模式），Reactor 模式有两个要义：\n>\n> 1）基于 IO 多路复用技术，多个连接共用一个多路复用器，应用程序的线程无需阻塞等待所有连接，只需阻塞等待多路复用器即可。当某个连接上有新数据可以处理时，应用程序的线程从阻塞状态返回，开始处理这个连接上的业务。\n>\n> 2）基于线程池技术复用线程资源，不必为每个连接创建专用的线程，应用程序将连接上的业务处理任务分配给线程池中的线程进行处理，一个线程可以处理多个连接的业务。\n\n### 主从 Reactor 多线程模式\n\n针对单 Reactor 多线程模型中，Reactor 在单个线程中运行，面对高并发的场景易成为性能瓶颈的缺陷，主从 Reactor 多线程模式让 Reactor 在多个线程中运行（分成 MainReactor 线程与 SubReactor 线程）。这种模式的基本工作流程为：\n\n- 1）Reactor 主线程 MainReactor 对象通过 select 监听客户端连接事件，收到事件后，通过 Acceptor 处理客户端连接事件。\n- 2）当 Acceptor 处理完客户端连接事件之后（与客户端建立好 Socket 连接），MainReactor 将连接分配给 SubReactor。（即：MainReactor 只负责监听客户端连接请求，和客户端建立连接之后将连接交由 SubReactor 监听后面的 IO 事件。）\n- 3）SubReactor 将连接加入到自己的连接队列进行监听，并创建 Handler 对各种事件进行处理。\n- 4）当连接上有新事件发生的时候，SubReactor 就会调用对应的 Handler 处理。\n- 5）Handler 通过 read 从连接上读取请求数据，将请求数据分发给 Worker 线程池进行业务处理。\n- 6）Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。\n- 7）一个 MainReactor 可以对应多个 SubReactor，即一个 MainReactor 线程可以对应多个 SubReactor 线程。\n\n这种模式的优点是：\n\n- 1）MainReactor 线程与 SubReactor 线程的数据交互简单职责明确，MainReactor 线程只需要接收新连接，SubReactor 线程完成后续的业务处理。\n- 2）MainReactor 线程与 SubReactor 线程的数据交互简单， MainReactor 线程只需要把新连接传给 SubReactor 线程，SubReactor 线程无需返回数据。\n- 3）多个 SubReactor 线程能够应对更高的并发请求。\n\n这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括 Nginx、Memcached、Netty 等。\n\n这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理线程。这是业界成熟的服务器程序设计模式。\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152576136-995fd1ff-596e-4fb3-ba45-f0e4d1acb066.png)\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152583079-a32e69fe-81e7-40a3-9858-74322d9839e6.png)\n\n### Netty 的模样\n\nNetty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。本节将使用一种渐进式的描述方式展示 Netty 的模样，即先给出 Netty 的简单版本，然后逐渐丰富其细节，直至展示出 Netty 的全貌。\n\n简单版本的 Netty 的模样如下：\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152847905-7f0fdd30-a8f1-40e9-92ed-5bea54a51e78.png)\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152884186-ed746066-7cfd-4cab-a58a-ca42eed7e0c1.png)\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673153242717-36ef2c61-2c76-4aae-9480-d2b5a5324778.png)\n\n关于这张图，作以下几点说明：\n\n- 1）Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。\n- 2）NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。\n- 3）NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。\n- 4）NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。\n- 5）每个 BossNioEventLoop 中循环执行以下三个步骤：\n- 5.1）**select**：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）\n- 5.2）**processSelectedKeys**：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上\n- 5.3）**runAllTasks**：再去以此循环处理任务队列中的其他任务\n- 6）每个 WorkerNioEventLoop 中循环执行以下三个步骤：\n- 6.1）**select**：轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件）\n- 6.2）**processSelectedKeys**：在对应的 NioSocketChannel 上处理 read/write 事件\n- 6.3）**runAllTasks**：再去以此循环处理任务队列中的其他任务\n- 7）在以上两个**processSelectedKeys**步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673163809861-7044728c-de2f-4a93-9d19-062318927467.png)","source":"_posts/netty/Netty小记.md","raw":"---\ntitle: Netty与Reactor模型\ntags:\n  - Netty\ncategories:\n  - - Netty\nabbrlink: 22006\ndate: 2022-11-29 19:35:27\nupdated: 2022-11-29 19:35:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n> Netty is *an asynchronous event-driven network application framework* for rapid development of maintainable high performance protocol servers & clients.\n\n## 基本概念：\n\n- 1、Netty是对JDK NIO进行的一系列封装，使得更容易更快速的编写出高性能的安全的网络应用程序。\n\n- 2、Java NIO与BIO一个重要的不同点是非阻塞，在Linux中，Java NIO依赖于Linux的epoll实现.\n\n- 3、epoll是Linux中的专有名词或实现：epoll是一种I/O事件通知机制，是linux 内核实现IO多路复用的一个实现。\n\n  > Linu主要通过暴漏三个系统调用供上层应用使用epoll：int epoll_create(int size)、 int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)、int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);\n  >\n  > \n  >\n  > IO多路复用是指，在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。\n\n\n\n## Reactor模型\n\n> NIO 服务端编程采用的是 Reactor 模式（也叫做 Dispatcher 模式，分派模式），Reactor 模式有两个要义：\n>\n> 1）基于 IO 多路复用技术，多个连接共用一个多路复用器，应用程序的线程无需阻塞等待所有连接，只需阻塞等待多路复用器即可。当某个连接上有新数据可以处理时，应用程序的线程从阻塞状态返回，开始处理这个连接上的业务。\n>\n> 2）基于线程池技术复用线程资源，不必为每个连接创建专用的线程，应用程序将连接上的业务处理任务分配给线程池中的线程进行处理，一个线程可以处理多个连接的业务。\n\n### 主从 Reactor 多线程模式\n\n针对单 Reactor 多线程模型中，Reactor 在单个线程中运行，面对高并发的场景易成为性能瓶颈的缺陷，主从 Reactor 多线程模式让 Reactor 在多个线程中运行（分成 MainReactor 线程与 SubReactor 线程）。这种模式的基本工作流程为：\n\n- 1）Reactor 主线程 MainReactor 对象通过 select 监听客户端连接事件，收到事件后，通过 Acceptor 处理客户端连接事件。\n- 2）当 Acceptor 处理完客户端连接事件之后（与客户端建立好 Socket 连接），MainReactor 将连接分配给 SubReactor。（即：MainReactor 只负责监听客户端连接请求，和客户端建立连接之后将连接交由 SubReactor 监听后面的 IO 事件。）\n- 3）SubReactor 将连接加入到自己的连接队列进行监听，并创建 Handler 对各种事件进行处理。\n- 4）当连接上有新事件发生的时候，SubReactor 就会调用对应的 Handler 处理。\n- 5）Handler 通过 read 从连接上读取请求数据，将请求数据分发给 Worker 线程池进行业务处理。\n- 6）Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。\n- 7）一个 MainReactor 可以对应多个 SubReactor，即一个 MainReactor 线程可以对应多个 SubReactor 线程。\n\n这种模式的优点是：\n\n- 1）MainReactor 线程与 SubReactor 线程的数据交互简单职责明确，MainReactor 线程只需要接收新连接，SubReactor 线程完成后续的业务处理。\n- 2）MainReactor 线程与 SubReactor 线程的数据交互简单， MainReactor 线程只需要把新连接传给 SubReactor 线程，SubReactor 线程无需返回数据。\n- 3）多个 SubReactor 线程能够应对更高的并发请求。\n\n这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括 Nginx、Memcached、Netty 等。\n\n这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理线程。这是业界成熟的服务器程序设计模式。\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152576136-995fd1ff-596e-4fb3-ba45-f0e4d1acb066.png)\n\n\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152583079-a32e69fe-81e7-40a3-9858-74322d9839e6.png)\n\n### Netty 的模样\n\nNetty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。本节将使用一种渐进式的描述方式展示 Netty 的模样，即先给出 Netty 的简单版本，然后逐渐丰富其细节，直至展示出 Netty 的全貌。\n\n简单版本的 Netty 的模样如下：\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152847905-7f0fdd30-a8f1-40e9-92ed-5bea54a51e78.png)\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152884186-ed746066-7cfd-4cab-a58a-ca42eed7e0c1.png)\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673153242717-36ef2c61-2c76-4aae-9480-d2b5a5324778.png)\n\n关于这张图，作以下几点说明：\n\n- 1）Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。\n- 2）NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。\n- 3）NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。\n- 4）NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。\n- 5）每个 BossNioEventLoop 中循环执行以下三个步骤：\n- 5.1）**select**：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）\n- 5.2）**processSelectedKeys**：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上\n- 5.3）**runAllTasks**：再去以此循环处理任务队列中的其他任务\n- 6）每个 WorkerNioEventLoop 中循环执行以下三个步骤：\n- 6.1）**select**：轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件）\n- 6.2）**processSelectedKeys**：在对应的 NioSocketChannel 上处理 read/write 事件\n- 6.3）**runAllTasks**：再去以此循环处理任务队列中的其他任务\n- 7）在以上两个**processSelectedKeys**步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。\n\n![img](https://cdn.nlark.com/yuque/0/2023/png/2500465/1673163809861-7044728c-de2f-4a93-9d19-062318927467.png)","slug":"netty/Netty小记","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt0002m8j5m2r65euoi","content":"<blockquote>\n<p>Netty is <em>an asynchronous event-driven network application framework</em> for rapid development of maintainable high performance protocol servers &amp; clients.</p>\n</blockquote>\n<h2 id=\"基本概念：\"><a href=\"#基本概念：\" class=\"headerlink\" title=\"基本概念：\"></a>基本概念：</h2><ul>\n<li><p>1、Netty是对JDK NIO进行的一系列封装，使得更容易更快速的编写出高性能的安全的网络应用程序。</p>\n</li>\n<li><p>2、Java NIO与BIO一个重要的不同点是非阻塞，在Linux中，Java NIO依赖于Linux的epoll实现.</p>\n</li>\n<li><p>3、epoll是Linux中的专有名词或实现：epoll是一种I&#x2F;O事件通知机制，是linux 内核实现IO多路复用的一个实现。</p>\n<blockquote>\n<p>Linu主要通过暴漏三个系统调用供上层应用使用epoll：int epoll_create(int size)、 int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)、int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);</p>\n<p>IO多路复用是指，在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Reactor模型\"><a href=\"#Reactor模型\" class=\"headerlink\" title=\"Reactor模型\"></a>Reactor模型</h2><blockquote>\n<p>NIO 服务端编程采用的是 Reactor 模式（也叫做 Dispatcher 模式，分派模式），Reactor 模式有两个要义：</p>\n<p>1）基于 IO 多路复用技术，多个连接共用一个多路复用器，应用程序的线程无需阻塞等待所有连接，只需阻塞等待多路复用器即可。当某个连接上有新数据可以处理时，应用程序的线程从阻塞状态返回，开始处理这个连接上的业务。</p>\n<p>2）基于线程池技术复用线程资源，不必为每个连接创建专用的线程，应用程序将连接上的业务处理任务分配给线程池中的线程进行处理，一个线程可以处理多个连接的业务。</p>\n</blockquote>\n<h3 id=\"主从-Reactor-多线程模式\"><a href=\"#主从-Reactor-多线程模式\" class=\"headerlink\" title=\"主从 Reactor 多线程模式\"></a>主从 Reactor 多线程模式</h3><p>针对单 Reactor 多线程模型中，Reactor 在单个线程中运行，面对高并发的场景易成为性能瓶颈的缺陷，主从 Reactor 多线程模式让 Reactor 在多个线程中运行（分成 MainReactor 线程与 SubReactor 线程）。这种模式的基本工作流程为：</p>\n<ul>\n<li>1）Reactor 主线程 MainReactor 对象通过 select 监听客户端连接事件，收到事件后，通过 Acceptor 处理客户端连接事件。</li>\n<li>2）当 Acceptor 处理完客户端连接事件之后（与客户端建立好 Socket 连接），MainReactor 将连接分配给 SubReactor。（即：MainReactor 只负责监听客户端连接请求，和客户端建立连接之后将连接交由 SubReactor 监听后面的 IO 事件。）</li>\n<li>3）SubReactor 将连接加入到自己的连接队列进行监听，并创建 Handler 对各种事件进行处理。</li>\n<li>4）当连接上有新事件发生的时候，SubReactor 就会调用对应的 Handler 处理。</li>\n<li>5）Handler 通过 read 从连接上读取请求数据，将请求数据分发给 Worker 线程池进行业务处理。</li>\n<li>6）Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。</li>\n<li>7）一个 MainReactor 可以对应多个 SubReactor，即一个 MainReactor 线程可以对应多个 SubReactor 线程。</li>\n</ul>\n<p>这种模式的优点是：</p>\n<ul>\n<li>1）MainReactor 线程与 SubReactor 线程的数据交互简单职责明确，MainReactor 线程只需要接收新连接，SubReactor 线程完成后续的业务处理。</li>\n<li>2）MainReactor 线程与 SubReactor 线程的数据交互简单， MainReactor 线程只需要把新连接传给 SubReactor 线程，SubReactor 线程无需返回数据。</li>\n<li>3）多个 SubReactor 线程能够应对更高的并发请求。</li>\n</ul>\n<p>这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括 Nginx、Memcached、Netty 等。</p>\n<p>这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理线程。这是业界成熟的服务器程序设计模式。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152576136-995fd1ff-596e-4fb3-ba45-f0e4d1acb066.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152583079-a32e69fe-81e7-40a3-9858-74322d9839e6.png\" alt=\"img\"></p>\n<h3 id=\"Netty-的模样\"><a href=\"#Netty-的模样\" class=\"headerlink\" title=\"Netty 的模样\"></a>Netty 的模样</h3><p>Netty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。本节将使用一种渐进式的描述方式展示 Netty 的模样，即先给出 Netty 的简单版本，然后逐渐丰富其细节，直至展示出 Netty 的全貌。</p>\n<p>简单版本的 Netty 的模样如下：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152847905-7f0fdd30-a8f1-40e9-92ed-5bea54a51e78.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152884186-ed746066-7cfd-4cab-a58a-ca42eed7e0c1.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673153242717-36ef2c61-2c76-4aae-9480-d2b5a5324778.png\" alt=\"img\"></p>\n<p>关于这张图，作以下几点说明：</p>\n<ul>\n<li>1）Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。</li>\n<li>2）NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。</li>\n<li>3）NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。</li>\n<li>4）NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。</li>\n<li>5）每个 BossNioEventLoop 中循环执行以下三个步骤：</li>\n<li>5.1）<strong>select</strong>：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）</li>\n<li>5.2）<strong>processSelectedKeys</strong>：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上</li>\n<li>5.3）<strong>runAllTasks</strong>：再去以此循环处理任务队列中的其他任务</li>\n<li>6）每个 WorkerNioEventLoop 中循环执行以下三个步骤：</li>\n<li>6.1）<strong>select</strong>：轮训注册在其上的 NioSocketChannel 的 read&#x2F;write 事件（OP_READ&#x2F;OP_WRITE 事件）</li>\n<li>6.2）<strong>processSelectedKeys</strong>：在对应的 NioSocketChannel 上处理 read&#x2F;write 事件</li>\n<li>6.3）<strong>runAllTasks</strong>：再去以此循环处理任务队列中的其他任务</li>\n<li>7）在以上两个<strong>processSelectedKeys</strong>步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673163809861-7044728c-de2f-4a93-9d19-062318927467.png\" alt=\"img\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>Netty is <em>an asynchronous event-driven network application framework</em> for rapid development of maintainable high performance protocol servers &amp; clients.</p>\n</blockquote>\n<h2 id=\"基本概念：\"><a href=\"#基本概念：\" class=\"headerlink\" title=\"基本概念：\"></a>基本概念：</h2><ul>\n<li><p>1、Netty是对JDK NIO进行的一系列封装，使得更容易更快速的编写出高性能的安全的网络应用程序。</p>\n</li>\n<li><p>2、Java NIO与BIO一个重要的不同点是非阻塞，在Linux中，Java NIO依赖于Linux的epoll实现.</p>\n</li>\n<li><p>3、epoll是Linux中的专有名词或实现：epoll是一种I&#x2F;O事件通知机制，是linux 内核实现IO多路复用的一个实现。</p>\n<blockquote>\n<p>Linu主要通过暴漏三个系统调用供上层应用使用epoll：int epoll_create(int size)、 int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)、int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);</p>\n<p>IO多路复用是指，在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Reactor模型\"><a href=\"#Reactor模型\" class=\"headerlink\" title=\"Reactor模型\"></a>Reactor模型</h2><blockquote>\n<p>NIO 服务端编程采用的是 Reactor 模式（也叫做 Dispatcher 模式，分派模式），Reactor 模式有两个要义：</p>\n<p>1）基于 IO 多路复用技术，多个连接共用一个多路复用器，应用程序的线程无需阻塞等待所有连接，只需阻塞等待多路复用器即可。当某个连接上有新数据可以处理时，应用程序的线程从阻塞状态返回，开始处理这个连接上的业务。</p>\n<p>2）基于线程池技术复用线程资源，不必为每个连接创建专用的线程，应用程序将连接上的业务处理任务分配给线程池中的线程进行处理，一个线程可以处理多个连接的业务。</p>\n</blockquote>\n<h3 id=\"主从-Reactor-多线程模式\"><a href=\"#主从-Reactor-多线程模式\" class=\"headerlink\" title=\"主从 Reactor 多线程模式\"></a>主从 Reactor 多线程模式</h3><p>针对单 Reactor 多线程模型中，Reactor 在单个线程中运行，面对高并发的场景易成为性能瓶颈的缺陷，主从 Reactor 多线程模式让 Reactor 在多个线程中运行（分成 MainReactor 线程与 SubReactor 线程）。这种模式的基本工作流程为：</p>\n<ul>\n<li>1）Reactor 主线程 MainReactor 对象通过 select 监听客户端连接事件，收到事件后，通过 Acceptor 处理客户端连接事件。</li>\n<li>2）当 Acceptor 处理完客户端连接事件之后（与客户端建立好 Socket 连接），MainReactor 将连接分配给 SubReactor。（即：MainReactor 只负责监听客户端连接请求，和客户端建立连接之后将连接交由 SubReactor 监听后面的 IO 事件。）</li>\n<li>3）SubReactor 将连接加入到自己的连接队列进行监听，并创建 Handler 对各种事件进行处理。</li>\n<li>4）当连接上有新事件发生的时候，SubReactor 就会调用对应的 Handler 处理。</li>\n<li>5）Handler 通过 read 从连接上读取请求数据，将请求数据分发给 Worker 线程池进行业务处理。</li>\n<li>6）Worker 线程池会分配独立线程来完成真正的业务处理，并将处理结果返回给 Handler。Handler 通过 send 向客户端发送响应数据。</li>\n<li>7）一个 MainReactor 可以对应多个 SubReactor，即一个 MainReactor 线程可以对应多个 SubReactor 线程。</li>\n</ul>\n<p>这种模式的优点是：</p>\n<ul>\n<li>1）MainReactor 线程与 SubReactor 线程的数据交互简单职责明确，MainReactor 线程只需要接收新连接，SubReactor 线程完成后续的业务处理。</li>\n<li>2）MainReactor 线程与 SubReactor 线程的数据交互简单， MainReactor 线程只需要把新连接传给 SubReactor 线程，SubReactor 线程无需返回数据。</li>\n<li>3）多个 SubReactor 线程能够应对更高的并发请求。</li>\n</ul>\n<p>这种模式的缺点是编程复杂度较高。但是由于其优点明显，在许多项目中被广泛使用，包括 Nginx、Memcached、Netty 等。</p>\n<p>这种模式也被叫做服务器的 1+M+N 线程模式，即使用该模式开发的服务器包含一个（或多个，1 只是表示相对较少）连接建立线程+M 个 IO 线程+N 个业务处理线程。这是业界成熟的服务器程序设计模式。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152576136-995fd1ff-596e-4fb3-ba45-f0e4d1acb066.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152583079-a32e69fe-81e7-40a3-9858-74322d9839e6.png\" alt=\"img\"></p>\n<h3 id=\"Netty-的模样\"><a href=\"#Netty-的模样\" class=\"headerlink\" title=\"Netty 的模样\"></a>Netty 的模样</h3><p>Netty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进。本节将使用一种渐进式的描述方式展示 Netty 的模样，即先给出 Netty 的简单版本，然后逐渐丰富其细节，直至展示出 Netty 的全貌。</p>\n<p>简单版本的 Netty 的模样如下：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152847905-7f0fdd30-a8f1-40e9-92ed-5bea54a51e78.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673152884186-ed746066-7cfd-4cab-a58a-ca42eed7e0c1.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673153242717-36ef2c61-2c76-4aae-9480-d2b5a5324778.png\" alt=\"img\"></p>\n<p>关于这张图，作以下几点说明：</p>\n<ul>\n<li>1）Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。</li>\n<li>2）NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。</li>\n<li>3）NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。</li>\n<li>4）NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。</li>\n<li>5）每个 BossNioEventLoop 中循环执行以下三个步骤：</li>\n<li>5.1）<strong>select</strong>：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）</li>\n<li>5.2）<strong>processSelectedKeys</strong>：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上</li>\n<li>5.3）<strong>runAllTasks</strong>：再去以此循环处理任务队列中的其他任务</li>\n<li>6）每个 WorkerNioEventLoop 中循环执行以下三个步骤：</li>\n<li>6.1）<strong>select</strong>：轮训注册在其上的 NioSocketChannel 的 read&#x2F;write 事件（OP_READ&#x2F;OP_WRITE 事件）</li>\n<li>6.2）<strong>processSelectedKeys</strong>：在对应的 NioSocketChannel 上处理 read&#x2F;write 事件</li>\n<li>6.3）<strong>runAllTasks</strong>：再去以此循环处理任务队列中的其他任务</li>\n<li>7）在以上两个<strong>processSelectedKeys</strong>步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。</li>\n</ul>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2023/png/2500465/1673163809861-7044728c-de2f-4a93-9d19-062318927467.png\" alt=\"img\"></p>\n"},{"title":"记一次大量TCP连接CLOSE_WAIT问题排查","abbrlink":10009,"date":"2023-06-11T09:57:25.000Z","updated":"2023-06-12T09:57:25.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## 问题\n\n今天突然发现Spark SQL任务启动不起来，报下面的错误，`'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on a random free port. You may check whether configuring an appropriate binding address. 2023-05-18 13:57:40,952 WARN util.Utils: Service`，看到这段日志后，表明服务器大量端口被占用，Spark申请不到端口，尝试了100次后，抛出了下面的异常。\n\n```\n20/12/21 12:55:18 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n20/12/21 12:55:18 ERROR CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\njava.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:745)\nEnd of LogType:stderr\n```\n\n## 排查\n\n执行ss命令，发现大量连接处于**CLOSE_WAIT**，状态，这非常不正常。ESTABLISHED表示连接已被建立，可以通信了，大量连接处于**ESTABLISHED**状态才有可能正常。然后执行`netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'`统计TCP连接状态，发现绝大部份的链接处于**CLOSE_WAIT**状态，这是非常不可思议情况。\n\n\n### 第一步\n\n执行`netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'`统计TCP连接状态。\n\n### 第二部\n\n用`netstat -tnap`命令进行检查。\n\n### 第三步：查看tcp队列当前情况\n```sh\nss -lntp\nState       Recv-Q Send-Q  Local Address:Port  Peer Address:Port             \nLISTEN      101    100  \n```\n\nRecv-Q代表当前全连接队列的大小，也就是三次握手完成，目前在全连接队列中等待被应用程序accept的socket个数。\n\nSend-Q代表全连接队列的最大值，应用程序可以在创建ServerSocket的时候指定，tomcat默认为100，但是这个值不能超过系统的/proc/sys/net/core/somaxconn，看看jdk中关于这个值的解释，专业名词叫backlog。\n\n从上面的输出可以发现Recv-Q已经大于Send-Q，而且这个数量长时间不变，可以得出两个结论：\n\n1.部分socket一直堆积在队列中没有被accept；\n\n2.由于tcp全连接队列已满，所以新的socket自然是进不来了。\n\n\n\n## 结论\n\n服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。\n\n那怎么解决呢？看看代码为啥耗时长吧。\n\n另外，如果代码不规范的话，说不定在收到对方发起的fin后，自己根本就不会给人家发fin。（比如netty自己开发的框架那种）\n\n没啥好说的，检查自己的代码吧，反正close_wait基本就是自己这边的问题了。\n\n\n\n### 补充TCP知识\n\n![](https://raw.githubusercontent.com/yuanoOo/learngit/b6713af0a1b426be22a510bcd51cb0cddef43ea6/jpg/tcp01.jpeg)\n\n用中文来描述下这个过程：\n\nClient: `服务端大哥，我事情都干完了，准备撤了`，这里对应的就是客户端发了一个**FIN**\n\nServer：`知道了，但是你等等我，我还要收收尾`，这里对应的就是服务端收到 **FIN** 后回应的 **ACK**\n\n经过上面两步之后，服务端就会处于 **CLOSE_WAIT** 状态。过了一段时间 **Server** 收尾完了\n\nServer：`小弟，哥哥我做完了，撤吧`，服务端发送了**FIN**\n\nClient：`大哥，再见啊`，这里是客户端对服务端的一个 **ACK**\n\n到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 **2MSL** 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 **ACK** 服务端没有收到，服务端重发 **FIN** 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。\n\n> **Maximum Segment Lifetime** 报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃\n\n这里一定不要被图里的 **client／server** 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 **FIN** 包（Client），被动关闭（Server）的一方响应 **ACK** 包，此时，被动关闭的一方就进入了 **CLOSE_WAIT** 状态。如果一切正常，稍后被动关闭的一方也会发出 **FIN** 包，然后迁移到 **LAST_ACK** 状态。\n\n## Apache Paimon相关issue\n\nhttps://github.com/apache/incubator-paimon/issues/1277\n\n没有关闭`ParquetFileReader reader = getParquetReader(fileIO, path)`,导致TCP泄露，这种bug非常难以排查，需要对源码非常熟悉。\n\npaimon-format/src/main/java/org/apache/paimon/format/parquet/ParquetUtil.java\n\n```java\n    public static Map<String, Statistics<?>> extractColumnStats(FileIO fileIO, Path path)\n            throws IOException {\n        ParquetMetadata parquetMetadata = getParquetReader(fileIO, path).getFooter();\n        List<BlockMetaData> blockMetaDataList = parquetMetadata.getBlocks();\n        Map<String, Statistics<?>> resultStats = new HashMap<>();\n        for (BlockMetaData blockMetaData : blockMetaDataList) {\n            List<ColumnChunkMetaData> columnChunkMetaDataList = blockMetaData.getColumns();\n            for (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) {\n                Statistics<?> stats = columnChunkMetaData.getStatistics();\n                String columnName = columnChunkMetaData.getPath().toDotString();\n                Statistics<?> midStats;\n                if (!resultStats.containsKey(columnName)) {\n                    midStats = stats;\n                } else {\n                    midStats = resultStats.get(columnName);\n                    midStats.mergeStatistics(stats);\n        try (ParquetFileReader reader = getParquetReader(fileIO, path)) {\n            ParquetMetadata parquetMetadata = reader.getFooter();\n            List<BlockMetaData> blockMetaDataList = parquetMetadata.getBlocks();\n            Map<String, Statistics<?>> resultStats = new HashMap<>();\n            for (BlockMetaData blockMetaData : blockMetaDataList) {\n                List<ColumnChunkMetaData> columnChunkMetaDataList = blockMetaData.getColumns();\n                for (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) {\n                    Statistics<?> stats = columnChunkMetaData.getStatistics();\n                    String columnName = columnChunkMetaData.getPath().toDotString();\n                    Statistics<?> midStats;\n                    if (!resultStats.containsKey(columnName)) {\n                        midStats = stats;\n                    } else {\n                        midStats = resultStats.get(columnName);\n                        midStats.mergeStatistics(stats);\n                    }\n                    resultStats.put(columnName, midStats);\n                }\n                resultStats.put(columnName, midStats);\n            }\n            return resultStats;\n        }\n        return resultStats;\n    }\n\n```\n\n","source":"_posts/troubleshooting/CLOSE_WAIT问题排查.md","raw":"---\ntitle: 记一次大量TCP连接CLOSE_WAIT问题排查\ntags:\n  - Troubleshooting\n  - paimon\ncategories:\n  - - Troubleshooting\nabbrlink: 10009\ndate: 2023-06-11 17:57:25\nupdated: 2023-06-12 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 问题\n\n今天突然发现Spark SQL任务启动不起来，报下面的错误，`'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on a random free port. You may check whether configuring an appropriate binding address. 2023-05-18 13:57:40,952 WARN util.Utils: Service`，看到这段日志后，表明服务器大量端口被占用，Spark申请不到端口，尝试了100次后，抛出了下面的异常。\n\n```\n20/12/21 12:55:18 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n20/12/21 12:55:18 ERROR CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\njava.net.BindException: Address already in use: Service 'org.apache.spark.network.netty.NettyBlockTransferService' failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'org.apache.spark.network.netty.NettyBlockTransferService' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:745)\nEnd of LogType:stderr\n```\n\n## 排查\n\n执行ss命令，发现大量连接处于**CLOSE_WAIT**，状态，这非常不正常。ESTABLISHED表示连接已被建立，可以通信了，大量连接处于**ESTABLISHED**状态才有可能正常。然后执行`netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'`统计TCP连接状态，发现绝大部份的链接处于**CLOSE_WAIT**状态，这是非常不可思议情况。\n\n\n### 第一步\n\n执行`netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'`统计TCP连接状态。\n\n### 第二部\n\n用`netstat -tnap`命令进行检查。\n\n### 第三步：查看tcp队列当前情况\n```sh\nss -lntp\nState       Recv-Q Send-Q  Local Address:Port  Peer Address:Port             \nLISTEN      101    100  \n```\n\nRecv-Q代表当前全连接队列的大小，也就是三次握手完成，目前在全连接队列中等待被应用程序accept的socket个数。\n\nSend-Q代表全连接队列的最大值，应用程序可以在创建ServerSocket的时候指定，tomcat默认为100，但是这个值不能超过系统的/proc/sys/net/core/somaxconn，看看jdk中关于这个值的解释，专业名词叫backlog。\n\n从上面的输出可以发现Recv-Q已经大于Send-Q，而且这个数量长时间不变，可以得出两个结论：\n\n1.部分socket一直堆积在队列中没有被accept；\n\n2.由于tcp全连接队列已满，所以新的socket自然是进不来了。\n\n\n\n## 结论\n\n服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。\n\n那怎么解决呢？看看代码为啥耗时长吧。\n\n另外，如果代码不规范的话，说不定在收到对方发起的fin后，自己根本就不会给人家发fin。（比如netty自己开发的框架那种）\n\n没啥好说的，检查自己的代码吧，反正close_wait基本就是自己这边的问题了。\n\n\n\n### 补充TCP知识\n\n![](https://raw.githubusercontent.com/yuanoOo/learngit/b6713af0a1b426be22a510bcd51cb0cddef43ea6/jpg/tcp01.jpeg)\n\n用中文来描述下这个过程：\n\nClient: `服务端大哥，我事情都干完了，准备撤了`，这里对应的就是客户端发了一个**FIN**\n\nServer：`知道了，但是你等等我，我还要收收尾`，这里对应的就是服务端收到 **FIN** 后回应的 **ACK**\n\n经过上面两步之后，服务端就会处于 **CLOSE_WAIT** 状态。过了一段时间 **Server** 收尾完了\n\nServer：`小弟，哥哥我做完了，撤吧`，服务端发送了**FIN**\n\nClient：`大哥，再见啊`，这里是客户端对服务端的一个 **ACK**\n\n到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 **2MSL** 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 **ACK** 服务端没有收到，服务端重发 **FIN** 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。\n\n> **Maximum Segment Lifetime** 报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃\n\n这里一定不要被图里的 **client／server** 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 **FIN** 包（Client），被动关闭（Server）的一方响应 **ACK** 包，此时，被动关闭的一方就进入了 **CLOSE_WAIT** 状态。如果一切正常，稍后被动关闭的一方也会发出 **FIN** 包，然后迁移到 **LAST_ACK** 状态。\n\n## Apache Paimon相关issue\n\nhttps://github.com/apache/incubator-paimon/issues/1277\n\n没有关闭`ParquetFileReader reader = getParquetReader(fileIO, path)`,导致TCP泄露，这种bug非常难以排查，需要对源码非常熟悉。\n\npaimon-format/src/main/java/org/apache/paimon/format/parquet/ParquetUtil.java\n\n```java\n    public static Map<String, Statistics<?>> extractColumnStats(FileIO fileIO, Path path)\n            throws IOException {\n        ParquetMetadata parquetMetadata = getParquetReader(fileIO, path).getFooter();\n        List<BlockMetaData> blockMetaDataList = parquetMetadata.getBlocks();\n        Map<String, Statistics<?>> resultStats = new HashMap<>();\n        for (BlockMetaData blockMetaData : blockMetaDataList) {\n            List<ColumnChunkMetaData> columnChunkMetaDataList = blockMetaData.getColumns();\n            for (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) {\n                Statistics<?> stats = columnChunkMetaData.getStatistics();\n                String columnName = columnChunkMetaData.getPath().toDotString();\n                Statistics<?> midStats;\n                if (!resultStats.containsKey(columnName)) {\n                    midStats = stats;\n                } else {\n                    midStats = resultStats.get(columnName);\n                    midStats.mergeStatistics(stats);\n        try (ParquetFileReader reader = getParquetReader(fileIO, path)) {\n            ParquetMetadata parquetMetadata = reader.getFooter();\n            List<BlockMetaData> blockMetaDataList = parquetMetadata.getBlocks();\n            Map<String, Statistics<?>> resultStats = new HashMap<>();\n            for (BlockMetaData blockMetaData : blockMetaDataList) {\n                List<ColumnChunkMetaData> columnChunkMetaDataList = blockMetaData.getColumns();\n                for (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) {\n                    Statistics<?> stats = columnChunkMetaData.getStatistics();\n                    String columnName = columnChunkMetaData.getPath().toDotString();\n                    Statistics<?> midStats;\n                    if (!resultStats.containsKey(columnName)) {\n                        midStats = stats;\n                    } else {\n                        midStats = resultStats.get(columnName);\n                        midStats.mergeStatistics(stats);\n                    }\n                    resultStats.put(columnName, midStats);\n                }\n                resultStats.put(columnName, midStats);\n            }\n            return resultStats;\n        }\n        return resultStats;\n    }\n\n```\n\n","slug":"troubleshooting/CLOSE_WAIT问题排查","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt1002q8j5m18xlb7le","content":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>今天突然发现Spark SQL任务启动不起来，报下面的错误，<code>&#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; could not bind on a random free port. You may check whether configuring an appropriate binding address. 2023-05-18 13:57:40,952 WARN util.Utils: Service</code>，看到这段日志后，表明服务器大量端口被占用，Spark申请不到端口，尝试了100次后，抛出了下面的异常。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20/12/21 12:55:18 WARN Utils: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; could not bind on a random free port. You may check whether configuring an appropriate binding address.</span><br><span class=\"line\">20/12/21 12:55:18 ERROR CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.</span><br><span class=\"line\">java.net.BindException: Address already in use: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind0(Native Method)</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind(Net.java:433)</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind(Net.java:425)</span><br><span class=\"line\">\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)</span><br><span class=\"line\">\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)</span><br><span class=\"line\">\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)</span><br><span class=\"line\">\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)</span><br><span class=\"line\">\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)</span><br><span class=\"line\">\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)</span><br><span class=\"line\">\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)</span><br><span class=\"line\">\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)</span><br><span class=\"line\">\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)</span><br><span class=\"line\">\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)</span><br><span class=\"line\">\tat java.lang.Thread.run(Thread.java:745)</span><br><span class=\"line\">End of LogType:stderr</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"排查\"><a href=\"#排查\" class=\"headerlink\" title=\"排查\"></a>排查</h2><p>执行ss命令，发现大量连接处于<strong>CLOSE_WAIT</strong>，状态，这非常不正常。ESTABLISHED表示连接已被建立，可以通信了，大量连接处于<strong>ESTABLISHED</strong>状态才有可能正常。然后执行<code>netstat -na | awk &#39;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#39;</code>统计TCP连接状态，发现绝大部份的链接处于<strong>CLOSE_WAIT</strong>状态，这是非常不可思议情况。</p>\n<h3 id=\"第一步\"><a href=\"#第一步\" class=\"headerlink\" title=\"第一步\"></a>第一步</h3><p>执行<code>netstat -na | awk &#39;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#39;</code>统计TCP连接状态。</p>\n<h3 id=\"第二部\"><a href=\"#第二部\" class=\"headerlink\" title=\"第二部\"></a>第二部</h3><p>用<code>netstat -tnap</code>命令进行检查。</p>\n<h3 id=\"第三步：查看tcp队列当前情况\"><a href=\"#第三步：查看tcp队列当前情况\" class=\"headerlink\" title=\"第三步：查看tcp队列当前情况\"></a>第三步：查看tcp队列当前情况</h3><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ss -lntp</span><br><span class=\"line\">State       Recv-Q Send-Q  Local Address:Port  Peer Address:Port             </span><br><span class=\"line\">LISTEN      101    100  </span><br></pre></td></tr></table></figure>\n\n<p>Recv-Q代表当前全连接队列的大小，也就是三次握手完成，目前在全连接队列中等待被应用程序accept的socket个数。</p>\n<p>Send-Q代表全连接队列的最大值，应用程序可以在创建ServerSocket的时候指定，tomcat默认为100，但是这个值不能超过系统的&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn，看看jdk中关于这个值的解释，专业名词叫backlog。</p>\n<p>从上面的输出可以发现Recv-Q已经大于Send-Q，而且这个数量长时间不变，可以得出两个结论：</p>\n<p>1.部分socket一直堆积在队列中没有被accept；</p>\n<p>2.由于tcp全连接队列已满，所以新的socket自然是进不来了。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。</p>\n<p>那怎么解决呢？看看代码为啥耗时长吧。</p>\n<p>另外，如果代码不规范的话，说不定在收到对方发起的fin后，自己根本就不会给人家发fin。（比如netty自己开发的框架那种）</p>\n<p>没啥好说的，检查自己的代码吧，反正close_wait基本就是自己这边的问题了。</p>\n<h3 id=\"补充TCP知识\"><a href=\"#补充TCP知识\" class=\"headerlink\" title=\"补充TCP知识\"></a>补充TCP知识</h3><p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/b6713af0a1b426be22a510bcd51cb0cddef43ea6/jpg/tcp01.jpeg\"></p>\n<p>用中文来描述下这个过程：</p>\n<p>Client: <code>服务端大哥，我事情都干完了，准备撤了</code>，这里对应的就是客户端发了一个<strong>FIN</strong></p>\n<p>Server：<code>知道了，但是你等等我，我还要收收尾</code>，这里对应的就是服务端收到 <strong>FIN</strong> 后回应的 <strong>ACK</strong></p>\n<p>经过上面两步之后，服务端就会处于 <strong>CLOSE_WAIT</strong> 状态。过了一段时间 <strong>Server</strong> 收尾完了</p>\n<p>Server：<code>小弟，哥哥我做完了，撤吧</code>，服务端发送了<strong>FIN</strong></p>\n<p>Client：<code>大哥，再见啊</code>，这里是客户端对服务端的一个 <strong>ACK</strong></p>\n<p>到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 <strong>2MSL</strong> 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 <strong>ACK</strong> 服务端没有收到，服务端重发 <strong>FIN</strong> 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。</p>\n<blockquote>\n<p><strong>Maximum Segment Lifetime</strong> 报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃</p>\n</blockquote>\n<p>这里一定不要被图里的 <strong>client／server</strong> 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 <strong>FIN</strong> 包（Client），被动关闭（Server）的一方响应 <strong>ACK</strong> 包，此时，被动关闭的一方就进入了 <strong>CLOSE_WAIT</strong> 状态。如果一切正常，稍后被动关闭的一方也会发出 <strong>FIN</strong> 包，然后迁移到 <strong>LAST_ACK</strong> 状态。</p>\n<h2 id=\"Apache-Paimon相关issue\"><a href=\"#Apache-Paimon相关issue\" class=\"headerlink\" title=\"Apache Paimon相关issue\"></a>Apache Paimon相关issue</h2><p><a href=\"https://github.com/apache/incubator-paimon/issues/1277\">https://github.com/apache/incubator-paimon/issues/1277</a></p>\n<p>没有关闭<code>ParquetFileReader reader = getParquetReader(fileIO, path)</code>,导致TCP泄露，这种bug非常难以排查，需要对源码非常熟悉。</p>\n<p>paimon-format&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;paimon&#x2F;format&#x2F;parquet&#x2F;ParquetUtil.java</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Map&lt;String, Statistics&lt;?&gt;&gt; extractColumnStats(FileIO fileIO, Path path)</span><br><span class=\"line\">        <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">    <span class=\"type\">ParquetMetadata</span> <span class=\"variable\">parquetMetadata</span> <span class=\"operator\">=</span> getParquetReader(fileIO, path).getFooter();</span><br><span class=\"line\">    List&lt;BlockMetaData&gt; blockMetaDataList = parquetMetadata.getBlocks();</span><br><span class=\"line\">    Map&lt;String, Statistics&lt;?&gt;&gt; resultStats = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (BlockMetaData blockMetaData : blockMetaDataList) &#123;</span><br><span class=\"line\">        List&lt;ColumnChunkMetaData&gt; columnChunkMetaDataList = blockMetaData.getColumns();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) &#123;</span><br><span class=\"line\">            Statistics&lt;?&gt; stats = columnChunkMetaData.getStatistics();</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">columnName</span> <span class=\"operator\">=</span> columnChunkMetaData.getPath().toDotString();</span><br><span class=\"line\">            Statistics&lt;?&gt; midStats;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!resultStats.containsKey(columnName)) &#123;</span><br><span class=\"line\">                midStats = stats;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                midStats = resultStats.get(columnName);</span><br><span class=\"line\">                midStats.mergeStatistics(stats);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> (<span class=\"type\">ParquetFileReader</span> <span class=\"variable\">reader</span> <span class=\"operator\">=</span> getParquetReader(fileIO, path)) &#123;</span><br><span class=\"line\">        <span class=\"type\">ParquetMetadata</span> <span class=\"variable\">parquetMetadata</span> <span class=\"operator\">=</span> reader.getFooter();</span><br><span class=\"line\">        List&lt;BlockMetaData&gt; blockMetaDataList = parquetMetadata.getBlocks();</span><br><span class=\"line\">        Map&lt;String, Statistics&lt;?&gt;&gt; resultStats = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (BlockMetaData blockMetaData : blockMetaDataList) &#123;</span><br><span class=\"line\">            List&lt;ColumnChunkMetaData&gt; columnChunkMetaDataList = blockMetaData.getColumns();</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) &#123;</span><br><span class=\"line\">                Statistics&lt;?&gt; stats = columnChunkMetaData.getStatistics();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">columnName</span> <span class=\"operator\">=</span> columnChunkMetaData.getPath().toDotString();</span><br><span class=\"line\">                Statistics&lt;?&gt; midStats;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!resultStats.containsKey(columnName)) &#123;</span><br><span class=\"line\">                    midStats = stats;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    midStats = resultStats.get(columnName);</span><br><span class=\"line\">                    midStats.mergeStatistics(stats);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                resultStats.put(columnName, midStats);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            resultStats.put(columnName, midStats);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> resultStats;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> resultStats;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>今天突然发现Spark SQL任务启动不起来，报下面的错误，<code>&#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; could not bind on a random free port. You may check whether configuring an appropriate binding address. 2023-05-18 13:57:40,952 WARN util.Utils: Service</code>，看到这段日志后，表明服务器大量端口被占用，Spark申请不到端口，尝试了100次后，抛出了下面的异常。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20/12/21 12:55:18 WARN Utils: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; could not bind on a random free port. You may check whether configuring an appropriate binding address.</span><br><span class=\"line\">20/12/21 12:55:18 ERROR CoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Address already in use: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.</span><br><span class=\"line\">java.net.BindException: Address already in use: Service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; failed after 100 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service &#x27;org.apache.spark.network.netty.NettyBlockTransferService&#x27; (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind0(Native Method)</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind(Net.java:433)</span><br><span class=\"line\">\tat sun.nio.ch.Net.bind(Net.java:425)</span><br><span class=\"line\">\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)</span><br><span class=\"line\">\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)</span><br><span class=\"line\">\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)</span><br><span class=\"line\">\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)</span><br><span class=\"line\">\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)</span><br><span class=\"line\">\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)</span><br><span class=\"line\">\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)</span><br><span class=\"line\">\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)</span><br><span class=\"line\">\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)</span><br><span class=\"line\">\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)</span><br><span class=\"line\">\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)</span><br><span class=\"line\">\tat java.lang.Thread.run(Thread.java:745)</span><br><span class=\"line\">End of LogType:stderr</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"排查\"><a href=\"#排查\" class=\"headerlink\" title=\"排查\"></a>排查</h2><p>执行ss命令，发现大量连接处于<strong>CLOSE_WAIT</strong>，状态，这非常不正常。ESTABLISHED表示连接已被建立，可以通信了，大量连接处于<strong>ESTABLISHED</strong>状态才有可能正常。然后执行<code>netstat -na | awk &#39;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#39;</code>统计TCP连接状态，发现绝大部份的链接处于<strong>CLOSE_WAIT</strong>状态，这是非常不可思议情况。</p>\n<h3 id=\"第一步\"><a href=\"#第一步\" class=\"headerlink\" title=\"第一步\"></a>第一步</h3><p>执行<code>netstat -na | awk &#39;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#39;</code>统计TCP连接状态。</p>\n<h3 id=\"第二部\"><a href=\"#第二部\" class=\"headerlink\" title=\"第二部\"></a>第二部</h3><p>用<code>netstat -tnap</code>命令进行检查。</p>\n<h3 id=\"第三步：查看tcp队列当前情况\"><a href=\"#第三步：查看tcp队列当前情况\" class=\"headerlink\" title=\"第三步：查看tcp队列当前情况\"></a>第三步：查看tcp队列当前情况</h3><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ss -lntp</span><br><span class=\"line\">State       Recv-Q Send-Q  Local Address:Port  Peer Address:Port             </span><br><span class=\"line\">LISTEN      101    100  </span><br></pre></td></tr></table></figure>\n\n<p>Recv-Q代表当前全连接队列的大小，也就是三次握手完成，目前在全连接队列中等待被应用程序accept的socket个数。</p>\n<p>Send-Q代表全连接队列的最大值，应用程序可以在创建ServerSocket的时候指定，tomcat默认为100，但是这个值不能超过系统的&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn，看看jdk中关于这个值的解释，专业名词叫backlog。</p>\n<p>从上面的输出可以发现Recv-Q已经大于Send-Q，而且这个数量长时间不变，可以得出两个结论：</p>\n<p>1.部分socket一直堆积在队列中没有被accept；</p>\n<p>2.由于tcp全连接队列已满，所以新的socket自然是进不来了。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>服务端接口耗时较长，客户端主动断开了连接，此时，服务端就会出现 close_wait。</p>\n<p>那怎么解决呢？看看代码为啥耗时长吧。</p>\n<p>另外，如果代码不规范的话，说不定在收到对方发起的fin后，自己根本就不会给人家发fin。（比如netty自己开发的框架那种）</p>\n<p>没啥好说的，检查自己的代码吧，反正close_wait基本就是自己这边的问题了。</p>\n<h3 id=\"补充TCP知识\"><a href=\"#补充TCP知识\" class=\"headerlink\" title=\"补充TCP知识\"></a>补充TCP知识</h3><p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/b6713af0a1b426be22a510bcd51cb0cddef43ea6/jpg/tcp01.jpeg\"></p>\n<p>用中文来描述下这个过程：</p>\n<p>Client: <code>服务端大哥，我事情都干完了，准备撤了</code>，这里对应的就是客户端发了一个<strong>FIN</strong></p>\n<p>Server：<code>知道了，但是你等等我，我还要收收尾</code>，这里对应的就是服务端收到 <strong>FIN</strong> 后回应的 <strong>ACK</strong></p>\n<p>经过上面两步之后，服务端就会处于 <strong>CLOSE_WAIT</strong> 状态。过了一段时间 <strong>Server</strong> 收尾完了</p>\n<p>Server：<code>小弟，哥哥我做完了，撤吧</code>，服务端发送了<strong>FIN</strong></p>\n<p>Client：<code>大哥，再见啊</code>，这里是客户端对服务端的一个 <strong>ACK</strong></p>\n<p>到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 <strong>2MSL</strong> 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 <strong>ACK</strong> 服务端没有收到，服务端重发 <strong>FIN</strong> 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。</p>\n<blockquote>\n<p><strong>Maximum Segment Lifetime</strong> 报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃</p>\n</blockquote>\n<p>这里一定不要被图里的 <strong>client／server</strong> 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 <strong>FIN</strong> 包（Client），被动关闭（Server）的一方响应 <strong>ACK</strong> 包，此时，被动关闭的一方就进入了 <strong>CLOSE_WAIT</strong> 状态。如果一切正常，稍后被动关闭的一方也会发出 <strong>FIN</strong> 包，然后迁移到 <strong>LAST_ACK</strong> 状态。</p>\n<h2 id=\"Apache-Paimon相关issue\"><a href=\"#Apache-Paimon相关issue\" class=\"headerlink\" title=\"Apache Paimon相关issue\"></a>Apache Paimon相关issue</h2><p><a href=\"https://github.com/apache/incubator-paimon/issues/1277\">https://github.com/apache/incubator-paimon/issues/1277</a></p>\n<p>没有关闭<code>ParquetFileReader reader = getParquetReader(fileIO, path)</code>,导致TCP泄露，这种bug非常难以排查，需要对源码非常熟悉。</p>\n<p>paimon-format&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;paimon&#x2F;format&#x2F;parquet&#x2F;ParquetUtil.java</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Map&lt;String, Statistics&lt;?&gt;&gt; extractColumnStats(FileIO fileIO, Path path)</span><br><span class=\"line\">        <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">    <span class=\"type\">ParquetMetadata</span> <span class=\"variable\">parquetMetadata</span> <span class=\"operator\">=</span> getParquetReader(fileIO, path).getFooter();</span><br><span class=\"line\">    List&lt;BlockMetaData&gt; blockMetaDataList = parquetMetadata.getBlocks();</span><br><span class=\"line\">    Map&lt;String, Statistics&lt;?&gt;&gt; resultStats = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (BlockMetaData blockMetaData : blockMetaDataList) &#123;</span><br><span class=\"line\">        List&lt;ColumnChunkMetaData&gt; columnChunkMetaDataList = blockMetaData.getColumns();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) &#123;</span><br><span class=\"line\">            Statistics&lt;?&gt; stats = columnChunkMetaData.getStatistics();</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">columnName</span> <span class=\"operator\">=</span> columnChunkMetaData.getPath().toDotString();</span><br><span class=\"line\">            Statistics&lt;?&gt; midStats;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!resultStats.containsKey(columnName)) &#123;</span><br><span class=\"line\">                midStats = stats;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                midStats = resultStats.get(columnName);</span><br><span class=\"line\">                midStats.mergeStatistics(stats);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> (<span class=\"type\">ParquetFileReader</span> <span class=\"variable\">reader</span> <span class=\"operator\">=</span> getParquetReader(fileIO, path)) &#123;</span><br><span class=\"line\">        <span class=\"type\">ParquetMetadata</span> <span class=\"variable\">parquetMetadata</span> <span class=\"operator\">=</span> reader.getFooter();</span><br><span class=\"line\">        List&lt;BlockMetaData&gt; blockMetaDataList = parquetMetadata.getBlocks();</span><br><span class=\"line\">        Map&lt;String, Statistics&lt;?&gt;&gt; resultStats = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (BlockMetaData blockMetaData : blockMetaDataList) &#123;</span><br><span class=\"line\">            List&lt;ColumnChunkMetaData&gt; columnChunkMetaDataList = blockMetaData.getColumns();</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ColumnChunkMetaData columnChunkMetaData : columnChunkMetaDataList) &#123;</span><br><span class=\"line\">                Statistics&lt;?&gt; stats = columnChunkMetaData.getStatistics();</span><br><span class=\"line\">                <span class=\"type\">String</span> <span class=\"variable\">columnName</span> <span class=\"operator\">=</span> columnChunkMetaData.getPath().toDotString();</span><br><span class=\"line\">                Statistics&lt;?&gt; midStats;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!resultStats.containsKey(columnName)) &#123;</span><br><span class=\"line\">                    midStats = stats;</span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    midStats = resultStats.get(columnName);</span><br><span class=\"line\">                    midStats.mergeStatistics(stats);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                resultStats.put(columnName, midStats);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            resultStats.put(columnName, midStats);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> resultStats;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> resultStats;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n"},{"title":"修复dolphinscheduler Hive SQL数据源连接Kyuubi偶现Read timed out错误","abbrlink":3025,"date":"2023-06-14T09:57:25.000Z","updated":"2023-06-14T09:57:25.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## 前言\n\ndolphinscheduler中利用Hive SQL数据源连接Kyuubi偶现Read timed out错误，错误日志如下：\n\n```text\n[ERROR] 2023-06-08 17:39:06.166 +0800 - Task execute failed, due to meet an exception\norg.apache.dolphinscheduler.plugin.task.api.TaskException: Execute sql task failed\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:168)\n\tat org.apache.dolphinscheduler.server.worker.runner.DefaultWorkerDelayTaskExecuteRunnable.executeTask(DefaultWorkerDelayTaskExecuteRunnable.java:49)\n\tat org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.run(WorkerTaskExecuteRunnable.java:174)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\n\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:323)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)\n\tat org.apache.hive.jdbc.HiveStatement.executeUpdate(HiveStatement.java:490)\n\tat org.apache.hive.jdbc.HivePreparedStatement.executeUpdate(HivePreparedStatement.java:122)\n\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\n\tat com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeUpdate(SqlTask.java:312)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeFuncAndSql(SqlTask.java:210)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:161)\n\t... 9 common frames omitted\nCaused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)\n\tat org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)\n\tat org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)\n\tat org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_ExecuteStatement(TCLIService.java:237)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:224)\n\tat sun.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524)\n\tat com.sun.proxy.$Proxy183.ExecuteStatement(Unknown Source)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:312)\n\t... 17 common frames omitted\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\n\t... 35 common frames omitted\n```\n\n经过一系列的问题排查，发现这个Bug在如下场景下稳定复现：当Kyuubi需要为这个连接启动新的Backend Engine的时候。即当Kyuubi的某个用户连接被释放，或者share level为connection级别时，会触发这个Bug。本质还是连接超时导致的。于是翻阅DolphinScheduler源码，发现DolphinScheduler的Hive SQL数据源SQL Task连接HiveServer是用的Hikari数据库连接池。而这个Bug就是由于Hikari数据库连接池不正确的配置的引起的。\n\n本质就是Kyuubi向Yarn申请启动Backend Engine太耗时了，导致Hikari数据库连接池默认的超时配置不适用于Kyuubi，而其他的数据库基本不会有这类问题。\n\n吐槽一下：DolphinScheduler SQL Task的数据库连接池是不支持配置文件配置的，只能通过修改源码修改配置，然后编译打包重新部署。这一块的代码需要重构了...\n\n```java\n// 防止建立连接超时\ndataSource.setConnectionTimeout(300_000L);\n\n// 连接池不保留空闲连接，以尽快释放掉所有连接，保证Spark Kyuubi yarn app可以被快速的释放掉，节省资源\n// ,同时保证了usercache大量的shuffle中间数据可以被及时清理掉。\ndataSource.setMinimumIdle(0);\ndataSource.setIdleTimeout(60_000L);\n\n// 保证用户有充足的连接使用\ndataSource.setMaximumPoolSize(50);\n```\n\n\n\n### 上代码\n\n这里只修改了Hive SQL DataSource数据库连接池的配置，同时也支持了通过JDBC URL参数调整Hikari数据库连接池参数，这下调整参数方便多了。也可以修改为对所有种类的SQL DataSource生效...\n\n\n\n```java\n    org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient#initClient\n        \n    @Override\n    protected void initClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        logger.info(\"Create Configuration for hive configuration.\");\n        this.hadoopConf = createHadoopConf();\n        logger.info(\"Create Configuration success.\");\n\n        logger.info(\"Create UserGroupInformation.\");\n        this.ugi = createUserGroupInformation(baseConnectionParam.getUser());\n        logger.info(\"Create ugi success.\");\n\n        this.dataSource = JDBCDataSourceProvider.createOneSessionJdbcDataSource(baseConnectionParam, dbType);\n\n        this.dataSource.setConnectionTimeout(300_000L);\n\n        // no save idle connection to clean usercache(shuffle) file qucikly\n        dataSource.setMinimumIdle(0);\n        dataSource.setMaximumPoolSize(50);\n\n        String baseConnParamOther = baseConnectionParam.getOther();\n        if (JSONUtils.checkJsonValid(baseConnParamOther)) {\n            Map<String, String> paramMap = JSONUtils.toMap(baseConnParamOther);\n            if (paramMap.containsKey(HIKARI_CONN_TIMEOUT)){\n                String connectionTimeout = paramMap.get(HIKARI_CONN_TIMEOUT);\n                if (StringUtils.isNumeric(connectionTimeout)){\n                    this.dataSource.setConnectionTimeout(Long.parseLong(connectionTimeout));\n                }\n            }\n\n            // now support config HIKARI_MAXIMUM_POOL_SIZE by ConnectionParam\n            if (paramMap.containsKey(HIKARI_MAXIMUM_POOL_SIZE)){\n                String maximumPoolSize = paramMap.get(HIKARI_MAXIMUM_POOL_SIZE);\n                if (StringUtils.isNotBlank(maximumPoolSize)\n                        && StringUtils.isNumeric(maximumPoolSize)){\n\n                    int poolSize = Integer.parseInt(maximumPoolSize);\n                    if (poolSize > 0) {\n                        dataSource.setMaximumPoolSize(poolSize);\n                    }\n                }\n            }\n        }\n\n        // for quick release Kyuubi connection to reduce yarn resource usage\n        // reset DriverManager#setLoginTimeout to\n        dataSource.setIdleTimeout(60_000L);\n        try {\n            dataSource.setLoginTimeout(300);\n        } catch (SQLException e) {\n            logger.info(\"set LoginTimeout fail for hive datasource\");\n        }\n\n        this.jdbcTemplate = new JdbcTemplate(dataSource);\n        logger.info(\"Init {} success.\", getClass().getName());\n    }\n```\n\n\n\n## Final\n\n上述代码改动后，试运行一段时间后，特别是在多任务并发执行的场景下，会发生各种莫名奇妙的问题，经排查是因为连接池的问题，常常用于OLTP场景的连接池HikariCP，在OLAP ETL场景下表现的非常糟糕。\n\n因此我们决定对DolphinScheduler HiveSQL Datasource进行重构，不在使用连接池，而是直接使用原生的`DriverManager`。经过一段时间的测试后，发现运行的非常稳定，再也没有出现过莫名奇妙的问题。\n\n- 由于Spring自带的`org.springframework.jdbc.datasource.DriverManagerDataSource`，不支持设置超时时间`DriverManager.setLoginTimeout(300)`，因此我们Copy出来，自己实现了HiveDriverManagerDataSource，并在里面设置了超时时间。调大这个参数在连接Kyuubi时非常重要，保证了Kyuubi不会因为在启动Backend Engine时发生Timeout。\n\n```java\n/*\n * Copyright 2002-2020 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.dolphinscheduler.plugin.datasource.hive;\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.util.Properties;\n\nimport org.springframework.jdbc.datasource.AbstractDriverBasedDataSource;\nimport org.springframework.util.Assert;\nimport org.springframework.util.ClassUtils;\n\n/**\n * Simple implementation of the standard JDBC {@link javax.sql.DataSource} interface,\n * configuring the plain old JDBC {@link java.sql.DriverManager} via bean properties, and\n * returning a new {@link java.sql.Connection} from every {@code getConnection} call.\n *\n * <p><b>NOTE: This class is not an actual connection pool; it does not actually\n * pool Connections.</b> It just serves as simple replacement for a full-blown\n * connection pool, implementing the same standard interface, but creating new\n * Connections on every call.\n *\n * <p>Useful for test or standalone environments outside of a Java EE container, either\n * as a DataSource bean in a corresponding ApplicationContext or in conjunction with\n * a simple JNDI environment. Pool-assuming {@code Connection.close()} calls will\n * simply close the Connection, so any DataSource-aware persistence code should work.\n *\n * <p><b>NOTE: Within special class loading environments such as OSGi, this class\n * is effectively superseded by {@link SimpleDriverDataSource} due to general class\n * loading issues with the JDBC DriverManager that be resolved through direct Driver\n * usage (which is exactly what SimpleDriverDataSource does).</b>\n *\n * <p>In a Java EE container, it is recommended to use a JNDI DataSource provided by\n * the container. Such a DataSource can be exposed as a DataSource bean in a Spring\n * ApplicationContext via {@link org.springframework.jndi.JndiObjectFactoryBean},\n * for seamless switching to and from a local DataSource bean like this class.\n * For tests, you can then either set up a mock JNDI environment through Spring's\n * {@link org.springframework.mock.jndi.SimpleNamingContextBuilder}, or switch the\n * bean definition to a local DataSource (which is simpler and thus recommended).\n *\n * <p>This {@code DriverManagerDataSource} class was originally designed alongside\n * <a href=\"https://commons.apache.org/proper/commons-dbcp\">Apache Commons DBCP</a>\n * and <a href=\"https://sourceforge.net/projects/c3p0\">C3P0</a>, featuring bean-style\n * {@code BasicDataSource}/{@code ComboPooledDataSource} classes with configuration\n * properties for local resource setups. For a modern JDBC connection pool, consider\n * <a href=\"https://github.com/brettwooldridge/HikariCP\">HikariCP</a> instead,\n * exposing a corresponding {@code HikariDataSource} instance to the application.\n *\n * @author Juergen Hoeller\n * @since 14.03.2003\n * @see SimpleDriverDataSource\n */\npublic class HiveDriverManagerDataSource extends AbstractDriverBasedDataSource {\n\n\t/**\n\t * Constructor for bean-style configuration.\n\t */\n\tpublic HiveDriverManagerDataSource() {\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given JDBC URL,\n\t * not specifying a username or password for JDBC access.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @see java.sql.DriverManager#getConnection(String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url) {\n\t\tsetUrl(url);\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given standard\n\t * DriverManager parameters.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @param username the JDBC username to use for accessing the DriverManager\n\t * @param password the JDBC password to use for accessing the DriverManager\n\t * @see java.sql.DriverManager#getConnection(String, String, String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url, String username, String password) {\n\t\tsetUrl(url);\n\t\tsetUsername(username);\n\t\tsetPassword(password);\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given JDBC URL,\n\t * not specifying a username or password for JDBC access.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @param conProps the JDBC connection properties\n\t * @see java.sql.DriverManager#getConnection(String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url, Properties conProps) {\n\t\tsetUrl(url);\n\t\tsetConnectionProperties(conProps);\n\t}\n\n\n\t/**\n\t * Set the JDBC driver class name. This driver will get initialized\n\t * on startup, registering itself with the JDK's DriverManager.\n\t * <p><b>NOTE: DriverManagerDataSource is primarily intended for accessing\n\t * <i>pre-registered</i> JDBC drivers.</b> If you need to register a new driver,\n\t * consider using {@link SimpleDriverDataSource} instead. Alternatively, consider\n\t * initializing the JDBC driver yourself before instantiating this DataSource.\n\t * The \"driverClassName\" property is mainly preserved for backwards compatibility,\n\t * as well as for migrating between Commons DBCP and this DataSource.\n\t * @see java.sql.DriverManager#registerDriver(java.sql.Driver)\n\t * @see SimpleDriverDataSource\n\t */\n\tpublic void setDriverClassName(String driverClassName) {\n\t\tAssert.hasText(driverClassName, \"Property 'driverClassName' must not be empty\");\n\t\tString driverClassNameToUse = driverClassName.trim();\n\t\ttry {\n\t\t\tClass.forName(driverClassNameToUse, true, ClassUtils.getDefaultClassLoader());\n\t\t}\n\t\tcatch (ClassNotFoundException ex) {\n\t\t\tthrow new IllegalStateException(\"Could not load JDBC driver class [\" + driverClassNameToUse + \"]\", ex);\n\t\t}\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"Loaded JDBC driver: \" + driverClassNameToUse);\n\t\t}\n\t}\n\n\n\t@Override\n\tprotected Connection getConnectionFromDriver(Properties props) throws SQLException {\n\t\tString url = getUrl();\n\t\tAssert.state(url != null, \"'url' not set\");\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"Creating new JDBC DriverManager Connection to [\" + url + \"]\");\n\t\t}\n\t\treturn getConnectionFromDriverManager(url, props);\n\t}\n\n\t/**\n\t * Getting a Connection using the nasty static from DriverManager is extracted\n\t * into a protected method to allow for easy unit testing.\n\t * @see java.sql.DriverManager#getConnection(String, java.util.Properties)\n\t */\n\tprotected Connection getConnectionFromDriverManager(String url, Properties props) throws SQLException {\n\t\tDriverManager.setLoginTimeout(300);\n\t\treturn DriverManager.getConnection(url, props);\n\t}\n\n}\n\n```\n\n- 修改`org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient`源码，使用`HiveDriverManagerDataSource`作为数据源。\n\n```java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.dolphinscheduler.plugin.datasource.hive;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.dolphinscheduler.common.constants.Constants;\nimport org.apache.dolphinscheduler.common.utils.PropertyUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.api.client.CommonDataSourceClient;\nimport org.apache.dolphinscheduler.plugin.datasource.api.utils.DataSourceUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.api.utils.PasswordUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.hive.utils.CommonUtil;\nimport org.apache.dolphinscheduler.spi.datasource.BaseConnectionParam;\nimport org.apache.dolphinscheduler.spi.enums.DbType;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport sun.security.krb5.Config;\n\nimport java.io.IOException;\nimport java.lang.reflect.Field;\nimport java.sql.Connection;\nimport java.sql.SQLException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport static org.apache.dolphinscheduler.plugin.task.api.TaskConstants.*;\n\npublic class HiveDataSourceClient extends CommonDataSourceClient {\n\n    private static final Logger logger = LoggerFactory.getLogger(HiveDataSourceClient.class);\n\n    private ScheduledExecutorService kerberosRenewalService;\n\n    private Configuration hadoopConf;\n    private UserGroupInformation ugi;\n    private boolean retryGetConnection = true;\n\n    private static final String HIKARI_CONN_TIMEOUT = \"connectionTimeout\";\n\n    private static final String HIKARI_MAXIMUM_POOL_SIZE = \"hiveOneSessionEnable\";\n    private HiveDriverManagerDataSource driverManagerDataSource;\n\n\n    public HiveDataSourceClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        super(baseConnectionParam, dbType);\n    }\n\n    @Override\n    protected void preInit() {\n        logger.info(\"PreInit in {}\", getClass().getName());\n        this.kerberosRenewalService = Executors.newSingleThreadScheduledExecutor(\n                new ThreadFactoryBuilder().setNameFormat(\"Hive-Kerberos-Renewal-Thread-\").setDaemon(true).build());\n    }\n\n    @Override\n    protected void initClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        this.driverManagerDataSource =\n                new HiveDriverManagerDataSource(DataSourceUtils.getJdbcUrl(DbType.HIVE, baseConnectionParam),\n                        baseConnectionParam.getUser(), PasswordUtils.decodePassword(baseConnectionParam.getPassword()));\n        driverManagerDataSource.setDriverClassName(baseConnectionParam.getDriverClassName());\n\n        this.jdbcTemplate = new JdbcTemplate(driverManagerDataSource);\n        logger.info(\"Init {} success.\", getClass().getName());\n    }\n\n    @Override\n    protected void checkEnv(BaseConnectionParam baseConnectionParam) {\n        super.checkEnv(baseConnectionParam);\n        checkKerberosEnv();\n    }\n\n    private void checkKerberosEnv() {\n        String krb5File = PropertyUtils.getString(JAVA_SECURITY_KRB5_CONF_PATH);\n        Boolean kerberosStartupState = PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, false);\n        if (kerberosStartupState && StringUtils.isNotBlank(krb5File)) {\n            System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5File);\n            try {\n                Config.refresh();\n                Class<?> kerberosName = Class.forName(\"org.apache.hadoop.security.authentication.util.KerberosName\");\n                Field field = kerberosName.getDeclaredField(\"defaultRealm\");\n                field.setAccessible(true);\n                field.set(null, Config.getInstance().getDefaultRealm());\n            } catch (Exception e) {\n                throw new RuntimeException(\"Update Kerberos environment failed.\", e);\n            }\n        }\n    }\n\n    private UserGroupInformation createUserGroupInformation(String username) {\n        String krb5File = PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH);\n        String keytab = PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH);\n        String principal = PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME);\n\n        try {\n            UserGroupInformation ugi = CommonUtil.createUGI(getHadoopConf(), principal, keytab, krb5File, username);\n            try {\n                Field isKeytabField = ugi.getClass().getDeclaredField(\"isKeytab\");\n                isKeytabField.setAccessible(true);\n                isKeytabField.set(ugi, true);\n            } catch (NoSuchFieldException | IllegalAccessException e) {\n                logger.warn(e.getMessage());\n            }\n\n            kerberosRenewalService.scheduleWithFixedDelay(() -> {\n                try {\n                    ugi.checkTGTAndReloginFromKeytab();\n                } catch (IOException e) {\n                    logger.error(\"Check TGT and Renewal from Keytab error\", e);\n                }\n            }, 5, 5, TimeUnit.MINUTES);\n            return ugi;\n        } catch (IOException e) {\n            throw new RuntimeException(\"createUserGroupInformation fail. \", e);\n        }\n    }\n\n    protected Configuration createHadoopConf() {\n        Configuration hadoopConf = new Configuration();\n        hadoopConf.setBoolean(\"ipc.client.fallback-to-simple-auth-allowed\", true);\n        return hadoopConf;\n    }\n\n    protected Configuration getHadoopConf() {\n        return this.hadoopConf;\n    }\n\n    @Override\n    public Connection getConnection() {\n        try {\n            return driverManagerDataSource.getConnection();\n        } catch (SQLException e) {\n            boolean kerberosStartupState = PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, false);\n            if (retryGetConnection && kerberosStartupState) {\n                retryGetConnection = false;\n                createUserGroupInformation(baseConnectionParam.getUser());\n                Connection connection = getConnection();\n                retryGetConnection = true;\n                return connection;\n            }\n            logger.error(\"get oneSessionDataSource Connection fail SQLException: {}\", e.getMessage(), e);\n            return null;\n        }\n    }\n\n    @Override\n    public void close() {\n        try {\n            super.close();\n        } finally {\n            kerberosRenewalService.shutdown();\n            this.ugi = null;\n        }\n        logger.info(\"Closed Hive datasource client.\");\n\n    }\n}\n\n```\n\n\n\n## 附件\n\n其他DolphinScheduler SQL DataSource和数据库连接池相关的PR，或许去掉数据库连接池是一个更好的选择！\n\nhttps://github.com/apache/dolphinscheduler/pull/14305\n\nhttps://github.com/apache/dolphinscheduler/pull/14190","source":"_posts/troubleshooting/dolphinscheduler中Hive SQL Task连接Kyuubi read timeout.md","raw":"---\ntitle: 修复dolphinscheduler Hive SQL数据源连接Kyuubi偶现Read timed out错误\ntags:\n  - Troubleshooting\n  - dolphinscheduler\ncategories:\n  - - Troubleshooting\nabbrlink: 3025\ndate: 2023-06-14 17:57:25\nupdated: 2023-06-14 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\ndolphinscheduler中利用Hive SQL数据源连接Kyuubi偶现Read timed out错误，错误日志如下：\n\n```text\n[ERROR] 2023-06-08 17:39:06.166 +0800 - Task execute failed, due to meet an exception\norg.apache.dolphinscheduler.plugin.task.api.TaskException: Execute sql task failed\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:168)\n\tat org.apache.dolphinscheduler.server.worker.runner.DefaultWorkerDelayTaskExecuteRunnable.executeTask(DefaultWorkerDelayTaskExecuteRunnable.java:49)\n\tat org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.run(WorkerTaskExecuteRunnable.java:174)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\n\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:323)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)\n\tat org.apache.hive.jdbc.HiveStatement.executeUpdate(HiveStatement.java:490)\n\tat org.apache.hive.jdbc.HivePreparedStatement.executeUpdate(HivePreparedStatement.java:122)\n\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\n\tat com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeUpdate(SqlTask.java:312)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeFuncAndSql(SqlTask.java:210)\n\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:161)\n\t... 9 common frames omitted\nCaused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)\n\tat org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)\n\tat org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)\n\tat org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_ExecuteStatement(TCLIService.java:237)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:224)\n\tat sun.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524)\n\tat com.sun.proxy.$Proxy183.ExecuteStatement(Unknown Source)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:312)\n\t... 17 common frames omitted\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\n\t... 35 common frames omitted\n```\n\n经过一系列的问题排查，发现这个Bug在如下场景下稳定复现：当Kyuubi需要为这个连接启动新的Backend Engine的时候。即当Kyuubi的某个用户连接被释放，或者share level为connection级别时，会触发这个Bug。本质还是连接超时导致的。于是翻阅DolphinScheduler源码，发现DolphinScheduler的Hive SQL数据源SQL Task连接HiveServer是用的Hikari数据库连接池。而这个Bug就是由于Hikari数据库连接池不正确的配置的引起的。\n\n本质就是Kyuubi向Yarn申请启动Backend Engine太耗时了，导致Hikari数据库连接池默认的超时配置不适用于Kyuubi，而其他的数据库基本不会有这类问题。\n\n吐槽一下：DolphinScheduler SQL Task的数据库连接池是不支持配置文件配置的，只能通过修改源码修改配置，然后编译打包重新部署。这一块的代码需要重构了...\n\n```java\n// 防止建立连接超时\ndataSource.setConnectionTimeout(300_000L);\n\n// 连接池不保留空闲连接，以尽快释放掉所有连接，保证Spark Kyuubi yarn app可以被快速的释放掉，节省资源\n// ,同时保证了usercache大量的shuffle中间数据可以被及时清理掉。\ndataSource.setMinimumIdle(0);\ndataSource.setIdleTimeout(60_000L);\n\n// 保证用户有充足的连接使用\ndataSource.setMaximumPoolSize(50);\n```\n\n\n\n### 上代码\n\n这里只修改了Hive SQL DataSource数据库连接池的配置，同时也支持了通过JDBC URL参数调整Hikari数据库连接池参数，这下调整参数方便多了。也可以修改为对所有种类的SQL DataSource生效...\n\n\n\n```java\n    org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient#initClient\n        \n    @Override\n    protected void initClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        logger.info(\"Create Configuration for hive configuration.\");\n        this.hadoopConf = createHadoopConf();\n        logger.info(\"Create Configuration success.\");\n\n        logger.info(\"Create UserGroupInformation.\");\n        this.ugi = createUserGroupInformation(baseConnectionParam.getUser());\n        logger.info(\"Create ugi success.\");\n\n        this.dataSource = JDBCDataSourceProvider.createOneSessionJdbcDataSource(baseConnectionParam, dbType);\n\n        this.dataSource.setConnectionTimeout(300_000L);\n\n        // no save idle connection to clean usercache(shuffle) file qucikly\n        dataSource.setMinimumIdle(0);\n        dataSource.setMaximumPoolSize(50);\n\n        String baseConnParamOther = baseConnectionParam.getOther();\n        if (JSONUtils.checkJsonValid(baseConnParamOther)) {\n            Map<String, String> paramMap = JSONUtils.toMap(baseConnParamOther);\n            if (paramMap.containsKey(HIKARI_CONN_TIMEOUT)){\n                String connectionTimeout = paramMap.get(HIKARI_CONN_TIMEOUT);\n                if (StringUtils.isNumeric(connectionTimeout)){\n                    this.dataSource.setConnectionTimeout(Long.parseLong(connectionTimeout));\n                }\n            }\n\n            // now support config HIKARI_MAXIMUM_POOL_SIZE by ConnectionParam\n            if (paramMap.containsKey(HIKARI_MAXIMUM_POOL_SIZE)){\n                String maximumPoolSize = paramMap.get(HIKARI_MAXIMUM_POOL_SIZE);\n                if (StringUtils.isNotBlank(maximumPoolSize)\n                        && StringUtils.isNumeric(maximumPoolSize)){\n\n                    int poolSize = Integer.parseInt(maximumPoolSize);\n                    if (poolSize > 0) {\n                        dataSource.setMaximumPoolSize(poolSize);\n                    }\n                }\n            }\n        }\n\n        // for quick release Kyuubi connection to reduce yarn resource usage\n        // reset DriverManager#setLoginTimeout to\n        dataSource.setIdleTimeout(60_000L);\n        try {\n            dataSource.setLoginTimeout(300);\n        } catch (SQLException e) {\n            logger.info(\"set LoginTimeout fail for hive datasource\");\n        }\n\n        this.jdbcTemplate = new JdbcTemplate(dataSource);\n        logger.info(\"Init {} success.\", getClass().getName());\n    }\n```\n\n\n\n## Final\n\n上述代码改动后，试运行一段时间后，特别是在多任务并发执行的场景下，会发生各种莫名奇妙的问题，经排查是因为连接池的问题，常常用于OLTP场景的连接池HikariCP，在OLAP ETL场景下表现的非常糟糕。\n\n因此我们决定对DolphinScheduler HiveSQL Datasource进行重构，不在使用连接池，而是直接使用原生的`DriverManager`。经过一段时间的测试后，发现运行的非常稳定，再也没有出现过莫名奇妙的问题。\n\n- 由于Spring自带的`org.springframework.jdbc.datasource.DriverManagerDataSource`，不支持设置超时时间`DriverManager.setLoginTimeout(300)`，因此我们Copy出来，自己实现了HiveDriverManagerDataSource，并在里面设置了超时时间。调大这个参数在连接Kyuubi时非常重要，保证了Kyuubi不会因为在启动Backend Engine时发生Timeout。\n\n```java\n/*\n * Copyright 2002-2020 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.dolphinscheduler.plugin.datasource.hive;\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.util.Properties;\n\nimport org.springframework.jdbc.datasource.AbstractDriverBasedDataSource;\nimport org.springframework.util.Assert;\nimport org.springframework.util.ClassUtils;\n\n/**\n * Simple implementation of the standard JDBC {@link javax.sql.DataSource} interface,\n * configuring the plain old JDBC {@link java.sql.DriverManager} via bean properties, and\n * returning a new {@link java.sql.Connection} from every {@code getConnection} call.\n *\n * <p><b>NOTE: This class is not an actual connection pool; it does not actually\n * pool Connections.</b> It just serves as simple replacement for a full-blown\n * connection pool, implementing the same standard interface, but creating new\n * Connections on every call.\n *\n * <p>Useful for test or standalone environments outside of a Java EE container, either\n * as a DataSource bean in a corresponding ApplicationContext or in conjunction with\n * a simple JNDI environment. Pool-assuming {@code Connection.close()} calls will\n * simply close the Connection, so any DataSource-aware persistence code should work.\n *\n * <p><b>NOTE: Within special class loading environments such as OSGi, this class\n * is effectively superseded by {@link SimpleDriverDataSource} due to general class\n * loading issues with the JDBC DriverManager that be resolved through direct Driver\n * usage (which is exactly what SimpleDriverDataSource does).</b>\n *\n * <p>In a Java EE container, it is recommended to use a JNDI DataSource provided by\n * the container. Such a DataSource can be exposed as a DataSource bean in a Spring\n * ApplicationContext via {@link org.springframework.jndi.JndiObjectFactoryBean},\n * for seamless switching to and from a local DataSource bean like this class.\n * For tests, you can then either set up a mock JNDI environment through Spring's\n * {@link org.springframework.mock.jndi.SimpleNamingContextBuilder}, or switch the\n * bean definition to a local DataSource (which is simpler and thus recommended).\n *\n * <p>This {@code DriverManagerDataSource} class was originally designed alongside\n * <a href=\"https://commons.apache.org/proper/commons-dbcp\">Apache Commons DBCP</a>\n * and <a href=\"https://sourceforge.net/projects/c3p0\">C3P0</a>, featuring bean-style\n * {@code BasicDataSource}/{@code ComboPooledDataSource} classes with configuration\n * properties for local resource setups. For a modern JDBC connection pool, consider\n * <a href=\"https://github.com/brettwooldridge/HikariCP\">HikariCP</a> instead,\n * exposing a corresponding {@code HikariDataSource} instance to the application.\n *\n * @author Juergen Hoeller\n * @since 14.03.2003\n * @see SimpleDriverDataSource\n */\npublic class HiveDriverManagerDataSource extends AbstractDriverBasedDataSource {\n\n\t/**\n\t * Constructor for bean-style configuration.\n\t */\n\tpublic HiveDriverManagerDataSource() {\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given JDBC URL,\n\t * not specifying a username or password for JDBC access.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @see java.sql.DriverManager#getConnection(String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url) {\n\t\tsetUrl(url);\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given standard\n\t * DriverManager parameters.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @param username the JDBC username to use for accessing the DriverManager\n\t * @param password the JDBC password to use for accessing the DriverManager\n\t * @see java.sql.DriverManager#getConnection(String, String, String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url, String username, String password) {\n\t\tsetUrl(url);\n\t\tsetUsername(username);\n\t\tsetPassword(password);\n\t}\n\n\t/**\n\t * Create a new DriverManagerDataSource with the given JDBC URL,\n\t * not specifying a username or password for JDBC access.\n\t * @param url the JDBC URL to use for accessing the DriverManager\n\t * @param conProps the JDBC connection properties\n\t * @see java.sql.DriverManager#getConnection(String)\n\t */\n\tpublic HiveDriverManagerDataSource(String url, Properties conProps) {\n\t\tsetUrl(url);\n\t\tsetConnectionProperties(conProps);\n\t}\n\n\n\t/**\n\t * Set the JDBC driver class name. This driver will get initialized\n\t * on startup, registering itself with the JDK's DriverManager.\n\t * <p><b>NOTE: DriverManagerDataSource is primarily intended for accessing\n\t * <i>pre-registered</i> JDBC drivers.</b> If you need to register a new driver,\n\t * consider using {@link SimpleDriverDataSource} instead. Alternatively, consider\n\t * initializing the JDBC driver yourself before instantiating this DataSource.\n\t * The \"driverClassName\" property is mainly preserved for backwards compatibility,\n\t * as well as for migrating between Commons DBCP and this DataSource.\n\t * @see java.sql.DriverManager#registerDriver(java.sql.Driver)\n\t * @see SimpleDriverDataSource\n\t */\n\tpublic void setDriverClassName(String driverClassName) {\n\t\tAssert.hasText(driverClassName, \"Property 'driverClassName' must not be empty\");\n\t\tString driverClassNameToUse = driverClassName.trim();\n\t\ttry {\n\t\t\tClass.forName(driverClassNameToUse, true, ClassUtils.getDefaultClassLoader());\n\t\t}\n\t\tcatch (ClassNotFoundException ex) {\n\t\t\tthrow new IllegalStateException(\"Could not load JDBC driver class [\" + driverClassNameToUse + \"]\", ex);\n\t\t}\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"Loaded JDBC driver: \" + driverClassNameToUse);\n\t\t}\n\t}\n\n\n\t@Override\n\tprotected Connection getConnectionFromDriver(Properties props) throws SQLException {\n\t\tString url = getUrl();\n\t\tAssert.state(url != null, \"'url' not set\");\n\t\tif (logger.isDebugEnabled()) {\n\t\t\tlogger.debug(\"Creating new JDBC DriverManager Connection to [\" + url + \"]\");\n\t\t}\n\t\treturn getConnectionFromDriverManager(url, props);\n\t}\n\n\t/**\n\t * Getting a Connection using the nasty static from DriverManager is extracted\n\t * into a protected method to allow for easy unit testing.\n\t * @see java.sql.DriverManager#getConnection(String, java.util.Properties)\n\t */\n\tprotected Connection getConnectionFromDriverManager(String url, Properties props) throws SQLException {\n\t\tDriverManager.setLoginTimeout(300);\n\t\treturn DriverManager.getConnection(url, props);\n\t}\n\n}\n\n```\n\n- 修改`org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient`源码，使用`HiveDriverManagerDataSource`作为数据源。\n\n```java\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.dolphinscheduler.plugin.datasource.hive;\n\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.dolphinscheduler.common.constants.Constants;\nimport org.apache.dolphinscheduler.common.utils.PropertyUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.api.client.CommonDataSourceClient;\nimport org.apache.dolphinscheduler.plugin.datasource.api.utils.DataSourceUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.api.utils.PasswordUtils;\nimport org.apache.dolphinscheduler.plugin.datasource.hive.utils.CommonUtil;\nimport org.apache.dolphinscheduler.spi.datasource.BaseConnectionParam;\nimport org.apache.dolphinscheduler.spi.enums.DbType;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport sun.security.krb5.Config;\n\nimport java.io.IOException;\nimport java.lang.reflect.Field;\nimport java.sql.Connection;\nimport java.sql.SQLException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport static org.apache.dolphinscheduler.plugin.task.api.TaskConstants.*;\n\npublic class HiveDataSourceClient extends CommonDataSourceClient {\n\n    private static final Logger logger = LoggerFactory.getLogger(HiveDataSourceClient.class);\n\n    private ScheduledExecutorService kerberosRenewalService;\n\n    private Configuration hadoopConf;\n    private UserGroupInformation ugi;\n    private boolean retryGetConnection = true;\n\n    private static final String HIKARI_CONN_TIMEOUT = \"connectionTimeout\";\n\n    private static final String HIKARI_MAXIMUM_POOL_SIZE = \"hiveOneSessionEnable\";\n    private HiveDriverManagerDataSource driverManagerDataSource;\n\n\n    public HiveDataSourceClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        super(baseConnectionParam, dbType);\n    }\n\n    @Override\n    protected void preInit() {\n        logger.info(\"PreInit in {}\", getClass().getName());\n        this.kerberosRenewalService = Executors.newSingleThreadScheduledExecutor(\n                new ThreadFactoryBuilder().setNameFormat(\"Hive-Kerberos-Renewal-Thread-\").setDaemon(true).build());\n    }\n\n    @Override\n    protected void initClient(BaseConnectionParam baseConnectionParam, DbType dbType) {\n        this.driverManagerDataSource =\n                new HiveDriverManagerDataSource(DataSourceUtils.getJdbcUrl(DbType.HIVE, baseConnectionParam),\n                        baseConnectionParam.getUser(), PasswordUtils.decodePassword(baseConnectionParam.getPassword()));\n        driverManagerDataSource.setDriverClassName(baseConnectionParam.getDriverClassName());\n\n        this.jdbcTemplate = new JdbcTemplate(driverManagerDataSource);\n        logger.info(\"Init {} success.\", getClass().getName());\n    }\n\n    @Override\n    protected void checkEnv(BaseConnectionParam baseConnectionParam) {\n        super.checkEnv(baseConnectionParam);\n        checkKerberosEnv();\n    }\n\n    private void checkKerberosEnv() {\n        String krb5File = PropertyUtils.getString(JAVA_SECURITY_KRB5_CONF_PATH);\n        Boolean kerberosStartupState = PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, false);\n        if (kerberosStartupState && StringUtils.isNotBlank(krb5File)) {\n            System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5File);\n            try {\n                Config.refresh();\n                Class<?> kerberosName = Class.forName(\"org.apache.hadoop.security.authentication.util.KerberosName\");\n                Field field = kerberosName.getDeclaredField(\"defaultRealm\");\n                field.setAccessible(true);\n                field.set(null, Config.getInstance().getDefaultRealm());\n            } catch (Exception e) {\n                throw new RuntimeException(\"Update Kerberos environment failed.\", e);\n            }\n        }\n    }\n\n    private UserGroupInformation createUserGroupInformation(String username) {\n        String krb5File = PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH);\n        String keytab = PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH);\n        String principal = PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME);\n\n        try {\n            UserGroupInformation ugi = CommonUtil.createUGI(getHadoopConf(), principal, keytab, krb5File, username);\n            try {\n                Field isKeytabField = ugi.getClass().getDeclaredField(\"isKeytab\");\n                isKeytabField.setAccessible(true);\n                isKeytabField.set(ugi, true);\n            } catch (NoSuchFieldException | IllegalAccessException e) {\n                logger.warn(e.getMessage());\n            }\n\n            kerberosRenewalService.scheduleWithFixedDelay(() -> {\n                try {\n                    ugi.checkTGTAndReloginFromKeytab();\n                } catch (IOException e) {\n                    logger.error(\"Check TGT and Renewal from Keytab error\", e);\n                }\n            }, 5, 5, TimeUnit.MINUTES);\n            return ugi;\n        } catch (IOException e) {\n            throw new RuntimeException(\"createUserGroupInformation fail. \", e);\n        }\n    }\n\n    protected Configuration createHadoopConf() {\n        Configuration hadoopConf = new Configuration();\n        hadoopConf.setBoolean(\"ipc.client.fallback-to-simple-auth-allowed\", true);\n        return hadoopConf;\n    }\n\n    protected Configuration getHadoopConf() {\n        return this.hadoopConf;\n    }\n\n    @Override\n    public Connection getConnection() {\n        try {\n            return driverManagerDataSource.getConnection();\n        } catch (SQLException e) {\n            boolean kerberosStartupState = PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, false);\n            if (retryGetConnection && kerberosStartupState) {\n                retryGetConnection = false;\n                createUserGroupInformation(baseConnectionParam.getUser());\n                Connection connection = getConnection();\n                retryGetConnection = true;\n                return connection;\n            }\n            logger.error(\"get oneSessionDataSource Connection fail SQLException: {}\", e.getMessage(), e);\n            return null;\n        }\n    }\n\n    @Override\n    public void close() {\n        try {\n            super.close();\n        } finally {\n            kerberosRenewalService.shutdown();\n            this.ugi = null;\n        }\n        logger.info(\"Closed Hive datasource client.\");\n\n    }\n}\n\n```\n\n\n\n## 附件\n\n其他DolphinScheduler SQL DataSource和数据库连接池相关的PR，或许去掉数据库连接池是一个更好的选择！\n\nhttps://github.com/apache/dolphinscheduler/pull/14305\n\nhttps://github.com/apache/dolphinscheduler/pull/14190","slug":"troubleshooting/dolphinscheduler中Hive SQL Task连接Kyuubi read timeout","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt1002u8j5mclucawt5","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>dolphinscheduler中利用Hive SQL数据源连接Kyuubi偶现Read timed out错误，错误日志如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ERROR] 2023-06-08 17:39:06.166 +0800 - Task execute failed, due to meet an exception</span><br><span class=\"line\">org.apache.dolphinscheduler.plugin.task.api.TaskException: Execute sql task failed</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:168)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.server.worker.runner.DefaultWorkerDelayTaskExecuteRunnable.executeTask(DefaultWorkerDelayTaskExecuteRunnable.java:49)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.run(WorkerTaskExecuteRunnable.java:174)</span><br><span class=\"line\">\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)</span><br><span class=\"line\">\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class=\"line\">\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class=\"line\">\tat java.lang.Thread.run(Thread.java:748)</span><br><span class=\"line\">Caused by: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:323)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.executeUpdate(HiveStatement.java:490)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HivePreparedStatement.executeUpdate(HivePreparedStatement.java:122)</span><br><span class=\"line\">\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)</span><br><span class=\"line\">\tat com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeUpdate(SqlTask.java:312)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeFuncAndSql(SqlTask.java:210)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:161)</span><br><span class=\"line\">\t... 9 common frames omitted</span><br><span class=\"line\">Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)</span><br><span class=\"line\">\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)</span><br><span class=\"line\">\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_ExecuteStatement(TCLIService.java:237)</span><br><span class=\"line\">\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:224)</span><br><span class=\"line\">\tat sun.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)</span><br><span class=\"line\">\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class=\"line\">\tat java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524)</span><br><span class=\"line\">\tat com.sun.proxy.$Proxy183.ExecuteStatement(Unknown Source)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:312)</span><br><span class=\"line\">\t... 17 common frames omitted</span><br><span class=\"line\">Caused by: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class=\"line\">\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class=\"line\">\tat java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class=\"line\">\tat java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)</span><br><span class=\"line\">\t... 35 common frames omitted</span><br></pre></td></tr></table></figure>\n\n<p>经过一系列的问题排查，发现这个Bug在如下场景下稳定复现：当Kyuubi需要为这个连接启动新的Backend Engine的时候。即当Kyuubi的某个用户连接被释放，或者share level为connection级别时，会触发这个Bug。本质还是连接超时导致的。于是翻阅DolphinScheduler源码，发现DolphinScheduler的Hive SQL数据源SQL Task连接HiveServer是用的Hikari数据库连接池。而这个Bug就是由于Hikari数据库连接池不正确的配置的引起的。</p>\n<p>本质就是Kyuubi向Yarn申请启动Backend Engine太耗时了，导致Hikari数据库连接池默认的超时配置不适用于Kyuubi，而其他的数据库基本不会有这类问题。</p>\n<p>吐槽一下：DolphinScheduler SQL Task的数据库连接池是不支持配置文件配置的，只能通过修改源码修改配置，然后编译打包重新部署。这一块的代码需要重构了…</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 防止建立连接超时</span></span><br><span class=\"line\">dataSource.setConnectionTimeout(<span class=\"number\">300_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 连接池不保留空闲连接，以尽快释放掉所有连接，保证Spark Kyuubi yarn app可以被快速的释放掉，节省资源</span></span><br><span class=\"line\"><span class=\"comment\">// ,同时保证了usercache大量的shuffle中间数据可以被及时清理掉。</span></span><br><span class=\"line\">dataSource.setMinimumIdle(<span class=\"number\">0</span>);</span><br><span class=\"line\">dataSource.setIdleTimeout(<span class=\"number\">60_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 保证用户有充足的连接使用</span></span><br><span class=\"line\">dataSource.setMaximumPoolSize(<span class=\"number\">50</span>);</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"上代码\"><a href=\"#上代码\" class=\"headerlink\" title=\"上代码\"></a>上代码</h3><p>这里只修改了Hive SQL DataSource数据库连接池的配置，同时也支持了通过JDBC URL参数调整Hikari数据库连接池参数，这下调整参数方便多了。也可以修改为对所有种类的SQL DataSource生效…</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient#initClient</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create Configuration for hive configuration.&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.hadoopConf = createHadoopConf();</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create Configuration success.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create UserGroupInformation.&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.ugi = createUserGroupInformation(baseConnectionParam.getUser());</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create ugi success.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.dataSource = JDBCDataSourceProvider.createOneSessionJdbcDataSource(baseConnectionParam, dbType);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.dataSource.setConnectionTimeout(<span class=\"number\">300_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// no save idle connection to clean usercache(shuffle) file qucikly</span></span><br><span class=\"line\">    dataSource.setMinimumIdle(<span class=\"number\">0</span>);</span><br><span class=\"line\">    dataSource.setMaximumPoolSize(<span class=\"number\">50</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">baseConnParamOther</span> <span class=\"operator\">=</span> baseConnectionParam.getOther();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (JSONUtils.checkJsonValid(baseConnParamOther)) &#123;</span><br><span class=\"line\">        Map&lt;String, String&gt; paramMap = JSONUtils.toMap(baseConnParamOther);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (paramMap.containsKey(HIKARI_CONN_TIMEOUT))&#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">connectionTimeout</span> <span class=\"operator\">=</span> paramMap.get(HIKARI_CONN_TIMEOUT);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.isNumeric(connectionTimeout))&#123;</span><br><span class=\"line\">                <span class=\"built_in\">this</span>.dataSource.setConnectionTimeout(Long.parseLong(connectionTimeout));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// now support config HIKARI_MAXIMUM_POOL_SIZE by ConnectionParam</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (paramMap.containsKey(HIKARI_MAXIMUM_POOL_SIZE))&#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">maximumPoolSize</span> <span class=\"operator\">=</span> paramMap.get(HIKARI_MAXIMUM_POOL_SIZE);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.isNotBlank(maximumPoolSize)</span><br><span class=\"line\">                    &amp;&amp; StringUtils.isNumeric(maximumPoolSize))&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> <span class=\"variable\">poolSize</span> <span class=\"operator\">=</span> Integer.parseInt(maximumPoolSize);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (poolSize &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    dataSource.setMaximumPoolSize(poolSize);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// for quick release Kyuubi connection to reduce yarn resource usage</span></span><br><span class=\"line\">    <span class=\"comment\">// reset DriverManager#setLoginTimeout to</span></span><br><span class=\"line\">    dataSource.setIdleTimeout(<span class=\"number\">60_000L</span>);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        dataSource.setLoginTimeout(<span class=\"number\">300</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;set LoginTimeout fail for hive datasource&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.jdbcTemplate = <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcTemplate</span>(dataSource);</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Init &#123;&#125; success.&quot;</span>, getClass().getName());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"Final\"><a href=\"#Final\" class=\"headerlink\" title=\"Final\"></a>Final</h2><p>上述代码改动后，试运行一段时间后，特别是在多任务并发执行的场景下，会发生各种莫名奇妙的问题，经排查是因为连接池的问题，常常用于OLTP场景的连接池HikariCP，在OLAP ETL场景下表现的非常糟糕。</p>\n<p>因此我们决定对DolphinScheduler HiveSQL Datasource进行重构，不在使用连接池，而是直接使用原生的<code>DriverManager</code>。经过一段时间的测试后，发现运行的非常稳定，再也没有出现过莫名奇妙的问题。</p>\n<ul>\n<li>由于Spring自带的<code>org.springframework.jdbc.datasource.DriverManagerDataSource</code>，不支持设置超时时间<code>DriverManager.setLoginTimeout(300)</code>，因此我们Copy出来，自己实现了HiveDriverManagerDataSource，并在里面设置了超时时间。调大这个参数在连接Kyuubi时非常重要，保证了Kyuubi不会因为在启动Backend Engine时发生Timeout。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> * Copyright 2002-2020 the original author or authors.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class=\"line\"><span class=\"comment\"> * you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\"> * You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> *      https://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"> * See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"> * limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.apache.dolphinscheduler.plugin.datasource.hive;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.Connection;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.DriverManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Properties;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.datasource.AbstractDriverBasedDataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.Assert;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.ClassUtils;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Simple implementation of the standard JDBC &#123;<span class=\"doctag\">@link</span> javax.sql.DataSource&#125; interface,</span></span><br><span class=\"line\"><span class=\"comment\"> * configuring the plain old JDBC &#123;<span class=\"doctag\">@link</span> java.sql.DriverManager&#125; via bean properties, and</span></span><br><span class=\"line\"><span class=\"comment\"> * returning a new &#123;<span class=\"doctag\">@link</span> java.sql.Connection&#125; from every &#123;<span class=\"doctag\">@code</span> getConnection&#125; call.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> This class is not an actual connection pool; it does not actually</span></span><br><span class=\"line\"><span class=\"comment\"> * pool Connections.&lt;/b&gt; It just serves as simple replacement for a full-blown</span></span><br><span class=\"line\"><span class=\"comment\"> * connection pool, implementing the same standard interface, but creating new</span></span><br><span class=\"line\"><span class=\"comment\"> * Connections on every call.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Useful for test or standalone environments outside of a Java EE container, either</span></span><br><span class=\"line\"><span class=\"comment\"> * as a DataSource bean in a corresponding ApplicationContext or in conjunction with</span></span><br><span class=\"line\"><span class=\"comment\"> * a simple JNDI environment. Pool-assuming &#123;<span class=\"doctag\">@code</span> Connection.close()&#125; calls will</span></span><br><span class=\"line\"><span class=\"comment\"> * simply close the Connection, so any DataSource-aware persistence code should work.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> Within special class loading environments such as OSGi, this class</span></span><br><span class=\"line\"><span class=\"comment\"> * is effectively superseded by &#123;<span class=\"doctag\">@link</span> SimpleDriverDataSource&#125; due to general class</span></span><br><span class=\"line\"><span class=\"comment\"> * loading issues with the JDBC DriverManager that be resolved through direct Driver</span></span><br><span class=\"line\"><span class=\"comment\"> * usage (which is exactly what SimpleDriverDataSource does).&lt;/b&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;In a Java EE container, it is recommended to use a JNDI DataSource provided by</span></span><br><span class=\"line\"><span class=\"comment\"> * the container. Such a DataSource can be exposed as a DataSource bean in a Spring</span></span><br><span class=\"line\"><span class=\"comment\"> * ApplicationContext via &#123;<span class=\"doctag\">@link</span> org.springframework.jndi.JndiObjectFactoryBean&#125;,</span></span><br><span class=\"line\"><span class=\"comment\"> * for seamless switching to and from a local DataSource bean like this class.</span></span><br><span class=\"line\"><span class=\"comment\"> * For tests, you can then either set up a mock JNDI environment through Spring&#x27;s</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;<span class=\"doctag\">@link</span> org.springframework.mock.jndi.SimpleNamingContextBuilder&#125;, or switch the</span></span><br><span class=\"line\"><span class=\"comment\"> * bean definition to a local DataSource (which is simpler and thus recommended).</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;This &#123;<span class=\"doctag\">@code</span> DriverManagerDataSource&#125; class was originally designed alongside</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;a href=&quot;https://commons.apache.org/proper/commons-dbcp&quot;&gt;Apache Commons DBCP&lt;/a&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * and &lt;a href=&quot;https://sourceforge.net/projects/c3p0&quot;&gt;C3P0&lt;/a&gt;, featuring bean-style</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;<span class=\"doctag\">@code</span> BasicDataSource&#125;/&#123;<span class=\"doctag\">@code</span> ComboPooledDataSource&#125; classes with configuration</span></span><br><span class=\"line\"><span class=\"comment\"> * properties for local resource setups. For a modern JDBC connection pool, consider</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;a href=&quot;https://github.com/brettwooldridge/HikariCP&quot;&gt;HikariCP&lt;/a&gt; instead,</span></span><br><span class=\"line\"><span class=\"comment\"> * exposing a corresponding &#123;<span class=\"doctag\">@code</span> HikariDataSource&#125; instance to the application.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@author</span> Juergen Hoeller</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@since</span> 14.03.2003</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@see</span> SimpleDriverDataSource</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HiveDriverManagerDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">AbstractDriverBasedDataSource</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Constructor for bean-style configuration.</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given JDBC URL,</span></span><br><span class=\"line\"><span class=\"comment\">\t * not specifying a username or password for JDBC access.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given standard</span></span><br><span class=\"line\"><span class=\"comment\">\t * DriverManager parameters.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> username the JDBC username to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> password the JDBC password to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String, String, String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url, String username, String password)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t\tsetUsername(username);</span><br><span class=\"line\">\t\tsetPassword(password);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given JDBC URL,</span></span><br><span class=\"line\"><span class=\"comment\">\t * not specifying a username or password for JDBC access.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> conProps the JDBC connection properties</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url, Properties conProps)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t\tsetConnectionProperties(conProps);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Set the JDBC driver class name. This driver will get initialized</span></span><br><span class=\"line\"><span class=\"comment\">\t * on startup, registering itself with the JDK&#x27;s DriverManager.</span></span><br><span class=\"line\"><span class=\"comment\">\t * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> DriverManagerDataSource is primarily intended for accessing</span></span><br><span class=\"line\"><span class=\"comment\">\t * &lt;i&gt;pre-registered&lt;/i&gt; JDBC drivers.&lt;/b&gt; If you need to register a new driver,</span></span><br><span class=\"line\"><span class=\"comment\">\t * consider using &#123;<span class=\"doctag\">@link</span> SimpleDriverDataSource&#125; instead. Alternatively, consider</span></span><br><span class=\"line\"><span class=\"comment\">\t * initializing the JDBC driver yourself before instantiating this DataSource.</span></span><br><span class=\"line\"><span class=\"comment\">\t * The &quot;driverClassName&quot; property is mainly preserved for backwards compatibility,</span></span><br><span class=\"line\"><span class=\"comment\">\t * as well as for migrating between Commons DBCP and this DataSource.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#registerDriver(java.sql.Driver)</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> SimpleDriverDataSource</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDriverClassName</span><span class=\"params\">(String driverClassName)</span> &#123;</span><br><span class=\"line\">\t\tAssert.hasText(driverClassName, <span class=\"string\">&quot;Property &#x27;driverClassName&#x27; must not be empty&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">driverClassNameToUse</span> <span class=\"operator\">=</span> driverClassName.trim();</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\tClass.forName(driverClassNameToUse, <span class=\"literal\">true</span>, ClassUtils.getDefaultClassLoader());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">catch</span> (ClassNotFoundException ex) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(<span class=\"string\">&quot;Could not load JDBC driver class [&quot;</span> + driverClassNameToUse + <span class=\"string\">&quot;]&quot;</span>, ex);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</span><br><span class=\"line\">\t\t\tlogger.debug(<span class=\"string\">&quot;Loaded JDBC driver: &quot;</span> + driverClassNameToUse);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Connection <span class=\"title function_\">getConnectionFromDriver</span><span class=\"params\">(Properties props)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">url</span> <span class=\"operator\">=</span> getUrl();</span><br><span class=\"line\">\t\tAssert.state(url != <span class=\"literal\">null</span>, <span class=\"string\">&quot;&#x27;url&#x27; not set&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</span><br><span class=\"line\">\t\t\tlogger.debug(<span class=\"string\">&quot;Creating new JDBC DriverManager Connection to [&quot;</span> + url + <span class=\"string\">&quot;]&quot;</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> getConnectionFromDriverManager(url, props);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Getting a Connection using the nasty static from DriverManager is extracted</span></span><br><span class=\"line\"><span class=\"comment\">\t * into a protected method to allow for easy unit testing.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String, java.util.Properties)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Connection <span class=\"title function_\">getConnectionFromDriverManager</span><span class=\"params\">(String url, Properties props)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\tDriverManager.setLoginTimeout(<span class=\"number\">300</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> DriverManager.getConnection(url, props);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>修改<code>org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient</code>源码，使用<code>HiveDriverManagerDataSource</code>作为数据源。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"> * this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"> * (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"> * the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"> * See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"> * limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.apache.dolphinscheduler.plugin.datasource.hive;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.google.common.util.concurrent.ThreadFactoryBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.common.constants.Constants;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.common.utils.PropertyUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.client.CommonDataSourceClient;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.utils.DataSourceUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.utils.PasswordUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.hive.utils.CommonUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.spi.datasource.BaseConnectionParam;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.spi.enums.DbType;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.Logger;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.LoggerFactory;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.JdbcTemplate;</span><br><span class=\"line\"><span class=\"keyword\">import</span> sun.security.krb5.Config;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.lang.reflect.Field;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.Connection;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.Executors;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.ScheduledExecutorService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.TimeUnit;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.apache.dolphinscheduler.plugin.task.api.TaskConstants.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HiveDataSourceClient</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">CommonDataSourceClient</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">Logger</span> <span class=\"variable\">logger</span> <span class=\"operator\">=</span> LoggerFactory.getLogger(HiveDataSourceClient.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ScheduledExecutorService kerberosRenewalService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Configuration hadoopConf;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserGroupInformation ugi;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">boolean</span> <span class=\"variable\">retryGetConnection</span> <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">HIKARI_CONN_TIMEOUT</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;connectionTimeout&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">HIKARI_MAXIMUM_POOL_SIZE</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;hiveOneSessionEnable&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> HiveDriverManagerDataSource driverManagerDataSource;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">HiveDataSourceClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(baseConnectionParam, dbType);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">preInit</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;PreInit in &#123;&#125;&quot;</span>, getClass().getName());</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.kerberosRenewalService = Executors.newSingleThreadScheduledExecutor(</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadFactoryBuilder</span>().setNameFormat(<span class=\"string\">&quot;Hive-Kerberos-Renewal-Thread-&quot;</span>).setDaemon(<span class=\"literal\">true</span>).build());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.driverManagerDataSource =</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">HiveDriverManagerDataSource</span>(DataSourceUtils.getJdbcUrl(DbType.HIVE, baseConnectionParam),</span><br><span class=\"line\">                        baseConnectionParam.getUser(), PasswordUtils.decodePassword(baseConnectionParam.getPassword()));</span><br><span class=\"line\">        driverManagerDataSource.setDriverClassName(baseConnectionParam.getDriverClassName());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">this</span>.jdbcTemplate = <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcTemplate</span>(driverManagerDataSource);</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;Init &#123;&#125; success.&quot;</span>, getClass().getName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">checkEnv</span><span class=\"params\">(BaseConnectionParam baseConnectionParam)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.checkEnv(baseConnectionParam);</span><br><span class=\"line\">        checkKerberosEnv();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">checkKerberosEnv</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">krb5File</span> <span class=\"operator\">=</span> PropertyUtils.getString(JAVA_SECURITY_KRB5_CONF_PATH);</span><br><span class=\"line\">        <span class=\"type\">Boolean</span> <span class=\"variable\">kerberosStartupState</span> <span class=\"operator\">=</span> PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, <span class=\"literal\">false</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (kerberosStartupState &amp;&amp; StringUtils.isNotBlank(krb5File)) &#123;</span><br><span class=\"line\">            System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5File);</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                Config.refresh();</span><br><span class=\"line\">                Class&lt;?&gt; kerberosName = Class.forName(<span class=\"string\">&quot;org.apache.hadoop.security.authentication.util.KerberosName&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">Field</span> <span class=\"variable\">field</span> <span class=\"operator\">=</span> kerberosName.getDeclaredField(<span class=\"string\">&quot;defaultRealm&quot;</span>);</span><br><span class=\"line\">                field.setAccessible(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                field.set(<span class=\"literal\">null</span>, Config.getInstance().getDefaultRealm());</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(<span class=\"string\">&quot;Update Kerberos environment failed.&quot;</span>, e);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserGroupInformation <span class=\"title function_\">createUserGroupInformation</span><span class=\"params\">(String username)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">krb5File</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">keytab</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">principal</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">UserGroupInformation</span> <span class=\"variable\">ugi</span> <span class=\"operator\">=</span> CommonUtil.createUGI(getHadoopConf(), principal, keytab, krb5File, username);</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">Field</span> <span class=\"variable\">isKeytabField</span> <span class=\"operator\">=</span> ugi.getClass().getDeclaredField(<span class=\"string\">&quot;isKeytab&quot;</span>);</span><br><span class=\"line\">                isKeytabField.setAccessible(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                isKeytabField.set(ugi, <span class=\"literal\">true</span>);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (NoSuchFieldException | IllegalAccessException e) &#123;</span><br><span class=\"line\">                logger.warn(e.getMessage());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            kerberosRenewalService.scheduleWithFixedDelay(() -&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    ugi.checkTGTAndReloginFromKeytab();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    logger.error(<span class=\"string\">&quot;Check TGT and Renewal from Keytab error&quot;</span>, e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;, <span class=\"number\">5</span>, <span class=\"number\">5</span>, TimeUnit.MINUTES);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ugi;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(<span class=\"string\">&quot;createUserGroupInformation fail. &quot;</span>, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Configuration <span class=\"title function_\">createHadoopConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Configuration</span> <span class=\"variable\">hadoopConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">        hadoopConf.setBoolean(<span class=\"string\">&quot;ipc.client.fallback-to-simple-auth-allowed&quot;</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hadoopConf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Configuration <span class=\"title function_\">getHadoopConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>.hadoopConf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Connection <span class=\"title function_\">getConnection</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> driverManagerDataSource.getConnection();</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">            <span class=\"type\">boolean</span> <span class=\"variable\">kerberosStartupState</span> <span class=\"operator\">=</span> PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, <span class=\"literal\">false</span>);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (retryGetConnection &amp;&amp; kerberosStartupState) &#123;</span><br><span class=\"line\">                retryGetConnection = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                createUserGroupInformation(baseConnectionParam.getUser());</span><br><span class=\"line\">                <span class=\"type\">Connection</span> <span class=\"variable\">connection</span> <span class=\"operator\">=</span> getConnection();</span><br><span class=\"line\">                retryGetConnection = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> connection;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            logger.error(<span class=\"string\">&quot;get oneSessionDataSource Connection fail SQLException: &#123;&#125;&quot;</span>, e.getMessage(), e);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">close</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.close();</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            kerberosRenewalService.shutdown();</span><br><span class=\"line\">            <span class=\"built_in\">this</span>.ugi = <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;Closed Hive datasource client.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"附件\"><a href=\"#附件\" class=\"headerlink\" title=\"附件\"></a>附件</h2><p>其他DolphinScheduler SQL DataSource和数据库连接池相关的PR，或许去掉数据库连接池是一个更好的选择！</p>\n<p><a href=\"https://github.com/apache/dolphinscheduler/pull/14305\">https://github.com/apache/dolphinscheduler/pull/14305</a></p>\n<p><a href=\"https://github.com/apache/dolphinscheduler/pull/14190\">https://github.com/apache/dolphinscheduler/pull/14190</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>dolphinscheduler中利用Hive SQL数据源连接Kyuubi偶现Read timed out错误，错误日志如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ERROR] 2023-06-08 17:39:06.166 +0800 - Task execute failed, due to meet an exception</span><br><span class=\"line\">org.apache.dolphinscheduler.plugin.task.api.TaskException: Execute sql task failed</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:168)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.server.worker.runner.DefaultWorkerDelayTaskExecuteRunnable.executeTask(DefaultWorkerDelayTaskExecuteRunnable.java:49)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.run(WorkerTaskExecuteRunnable.java:174)</span><br><span class=\"line\">\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)</span><br><span class=\"line\">\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)</span><br><span class=\"line\">\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class=\"line\">\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class=\"line\">\tat java.lang.Thread.run(Thread.java:748)</span><br><span class=\"line\">Caused by: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:323)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.executeUpdate(HiveStatement.java:490)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HivePreparedStatement.executeUpdate(HivePreparedStatement.java:122)</span><br><span class=\"line\">\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)</span><br><span class=\"line\">\tat com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeUpdate(SqlTask.java:312)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.executeFuncAndSql(SqlTask.java:210)</span><br><span class=\"line\">\tat org.apache.dolphinscheduler.plugin.task.sql.SqlTask.handle(SqlTask.java:161)</span><br><span class=\"line\">\t... 9 common frames omitted</span><br><span class=\"line\">Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)</span><br><span class=\"line\">\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)</span><br><span class=\"line\">\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)</span><br><span class=\"line\">\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_ExecuteStatement(TCLIService.java:237)</span><br><span class=\"line\">\tat org.apache.hive.service.rpc.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:224)</span><br><span class=\"line\">\tat sun.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)</span><br><span class=\"line\">\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class=\"line\">\tat java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524)</span><br><span class=\"line\">\tat com.sun.proxy.$Proxy183.ExecuteStatement(Unknown Source)</span><br><span class=\"line\">\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:312)</span><br><span class=\"line\">\t... 17 common frames omitted</span><br><span class=\"line\">Caused by: java.net.SocketTimeoutException: Read timed out</span><br><span class=\"line\">\tat java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class=\"line\">\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class=\"line\">\tat java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class=\"line\">\tat java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)</span><br><span class=\"line\">\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)</span><br><span class=\"line\">\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)</span><br><span class=\"line\">\t... 35 common frames omitted</span><br></pre></td></tr></table></figure>\n\n<p>经过一系列的问题排查，发现这个Bug在如下场景下稳定复现：当Kyuubi需要为这个连接启动新的Backend Engine的时候。即当Kyuubi的某个用户连接被释放，或者share level为connection级别时，会触发这个Bug。本质还是连接超时导致的。于是翻阅DolphinScheduler源码，发现DolphinScheduler的Hive SQL数据源SQL Task连接HiveServer是用的Hikari数据库连接池。而这个Bug就是由于Hikari数据库连接池不正确的配置的引起的。</p>\n<p>本质就是Kyuubi向Yarn申请启动Backend Engine太耗时了，导致Hikari数据库连接池默认的超时配置不适用于Kyuubi，而其他的数据库基本不会有这类问题。</p>\n<p>吐槽一下：DolphinScheduler SQL Task的数据库连接池是不支持配置文件配置的，只能通过修改源码修改配置，然后编译打包重新部署。这一块的代码需要重构了…</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 防止建立连接超时</span></span><br><span class=\"line\">dataSource.setConnectionTimeout(<span class=\"number\">300_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 连接池不保留空闲连接，以尽快释放掉所有连接，保证Spark Kyuubi yarn app可以被快速的释放掉，节省资源</span></span><br><span class=\"line\"><span class=\"comment\">// ,同时保证了usercache大量的shuffle中间数据可以被及时清理掉。</span></span><br><span class=\"line\">dataSource.setMinimumIdle(<span class=\"number\">0</span>);</span><br><span class=\"line\">dataSource.setIdleTimeout(<span class=\"number\">60_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 保证用户有充足的连接使用</span></span><br><span class=\"line\">dataSource.setMaximumPoolSize(<span class=\"number\">50</span>);</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"上代码\"><a href=\"#上代码\" class=\"headerlink\" title=\"上代码\"></a>上代码</h3><p>这里只修改了Hive SQL DataSource数据库连接池的配置，同时也支持了通过JDBC URL参数调整Hikari数据库连接池参数，这下调整参数方便多了。也可以修改为对所有种类的SQL DataSource生效…</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient#initClient</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create Configuration for hive configuration.&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.hadoopConf = createHadoopConf();</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create Configuration success.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create UserGroupInformation.&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.ugi = createUserGroupInformation(baseConnectionParam.getUser());</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Create ugi success.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.dataSource = JDBCDataSourceProvider.createOneSessionJdbcDataSource(baseConnectionParam, dbType);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.dataSource.setConnectionTimeout(<span class=\"number\">300_000L</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// no save idle connection to clean usercache(shuffle) file qucikly</span></span><br><span class=\"line\">    dataSource.setMinimumIdle(<span class=\"number\">0</span>);</span><br><span class=\"line\">    dataSource.setMaximumPoolSize(<span class=\"number\">50</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">baseConnParamOther</span> <span class=\"operator\">=</span> baseConnectionParam.getOther();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (JSONUtils.checkJsonValid(baseConnParamOther)) &#123;</span><br><span class=\"line\">        Map&lt;String, String&gt; paramMap = JSONUtils.toMap(baseConnParamOther);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (paramMap.containsKey(HIKARI_CONN_TIMEOUT))&#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">connectionTimeout</span> <span class=\"operator\">=</span> paramMap.get(HIKARI_CONN_TIMEOUT);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.isNumeric(connectionTimeout))&#123;</span><br><span class=\"line\">                <span class=\"built_in\">this</span>.dataSource.setConnectionTimeout(Long.parseLong(connectionTimeout));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// now support config HIKARI_MAXIMUM_POOL_SIZE by ConnectionParam</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (paramMap.containsKey(HIKARI_MAXIMUM_POOL_SIZE))&#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">maximumPoolSize</span> <span class=\"operator\">=</span> paramMap.get(HIKARI_MAXIMUM_POOL_SIZE);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (StringUtils.isNotBlank(maximumPoolSize)</span><br><span class=\"line\">                    &amp;&amp; StringUtils.isNumeric(maximumPoolSize))&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> <span class=\"variable\">poolSize</span> <span class=\"operator\">=</span> Integer.parseInt(maximumPoolSize);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (poolSize &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    dataSource.setMaximumPoolSize(poolSize);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// for quick release Kyuubi connection to reduce yarn resource usage</span></span><br><span class=\"line\">    <span class=\"comment\">// reset DriverManager#setLoginTimeout to</span></span><br><span class=\"line\">    dataSource.setIdleTimeout(<span class=\"number\">60_000L</span>);</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        dataSource.setLoginTimeout(<span class=\"number\">300</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;set LoginTimeout fail for hive datasource&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.jdbcTemplate = <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcTemplate</span>(dataSource);</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;Init &#123;&#125; success.&quot;</span>, getClass().getName());</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"Final\"><a href=\"#Final\" class=\"headerlink\" title=\"Final\"></a>Final</h2><p>上述代码改动后，试运行一段时间后，特别是在多任务并发执行的场景下，会发生各种莫名奇妙的问题，经排查是因为连接池的问题，常常用于OLTP场景的连接池HikariCP，在OLAP ETL场景下表现的非常糟糕。</p>\n<p>因此我们决定对DolphinScheduler HiveSQL Datasource进行重构，不在使用连接池，而是直接使用原生的<code>DriverManager</code>。经过一段时间的测试后，发现运行的非常稳定，再也没有出现过莫名奇妙的问题。</p>\n<ul>\n<li>由于Spring自带的<code>org.springframework.jdbc.datasource.DriverManagerDataSource</code>，不支持设置超时时间<code>DriverManager.setLoginTimeout(300)</code>，因此我们Copy出来，自己实现了HiveDriverManagerDataSource，并在里面设置了超时时间。调大这个参数在连接Kyuubi时非常重要，保证了Kyuubi不会因为在启动Backend Engine时发生Timeout。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> * Copyright 2002-2020 the original author or authors.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class=\"line\"><span class=\"comment\"> * you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\"> * You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> *      https://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"> * See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"> * limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.apache.dolphinscheduler.plugin.datasource.hive;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.Connection;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.DriverManager;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Properties;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.datasource.AbstractDriverBasedDataSource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.Assert;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.util.ClassUtils;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Simple implementation of the standard JDBC &#123;<span class=\"doctag\">@link</span> javax.sql.DataSource&#125; interface,</span></span><br><span class=\"line\"><span class=\"comment\"> * configuring the plain old JDBC &#123;<span class=\"doctag\">@link</span> java.sql.DriverManager&#125; via bean properties, and</span></span><br><span class=\"line\"><span class=\"comment\"> * returning a new &#123;<span class=\"doctag\">@link</span> java.sql.Connection&#125; from every &#123;<span class=\"doctag\">@code</span> getConnection&#125; call.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> This class is not an actual connection pool; it does not actually</span></span><br><span class=\"line\"><span class=\"comment\"> * pool Connections.&lt;/b&gt; It just serves as simple replacement for a full-blown</span></span><br><span class=\"line\"><span class=\"comment\"> * connection pool, implementing the same standard interface, but creating new</span></span><br><span class=\"line\"><span class=\"comment\"> * Connections on every call.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Useful for test or standalone environments outside of a Java EE container, either</span></span><br><span class=\"line\"><span class=\"comment\"> * as a DataSource bean in a corresponding ApplicationContext or in conjunction with</span></span><br><span class=\"line\"><span class=\"comment\"> * a simple JNDI environment. Pool-assuming &#123;<span class=\"doctag\">@code</span> Connection.close()&#125; calls will</span></span><br><span class=\"line\"><span class=\"comment\"> * simply close the Connection, so any DataSource-aware persistence code should work.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> Within special class loading environments such as OSGi, this class</span></span><br><span class=\"line\"><span class=\"comment\"> * is effectively superseded by &#123;<span class=\"doctag\">@link</span> SimpleDriverDataSource&#125; due to general class</span></span><br><span class=\"line\"><span class=\"comment\"> * loading issues with the JDBC DriverManager that be resolved through direct Driver</span></span><br><span class=\"line\"><span class=\"comment\"> * usage (which is exactly what SimpleDriverDataSource does).&lt;/b&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;In a Java EE container, it is recommended to use a JNDI DataSource provided by</span></span><br><span class=\"line\"><span class=\"comment\"> * the container. Such a DataSource can be exposed as a DataSource bean in a Spring</span></span><br><span class=\"line\"><span class=\"comment\"> * ApplicationContext via &#123;<span class=\"doctag\">@link</span> org.springframework.jndi.JndiObjectFactoryBean&#125;,</span></span><br><span class=\"line\"><span class=\"comment\"> * for seamless switching to and from a local DataSource bean like this class.</span></span><br><span class=\"line\"><span class=\"comment\"> * For tests, you can then either set up a mock JNDI environment through Spring&#x27;s</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;<span class=\"doctag\">@link</span> org.springframework.mock.jndi.SimpleNamingContextBuilder&#125;, or switch the</span></span><br><span class=\"line\"><span class=\"comment\"> * bean definition to a local DataSource (which is simpler and thus recommended).</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;This &#123;<span class=\"doctag\">@code</span> DriverManagerDataSource&#125; class was originally designed alongside</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;a href=&quot;https://commons.apache.org/proper/commons-dbcp&quot;&gt;Apache Commons DBCP&lt;/a&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * and &lt;a href=&quot;https://sourceforge.net/projects/c3p0&quot;&gt;C3P0&lt;/a&gt;, featuring bean-style</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;<span class=\"doctag\">@code</span> BasicDataSource&#125;/&#123;<span class=\"doctag\">@code</span> ComboPooledDataSource&#125; classes with configuration</span></span><br><span class=\"line\"><span class=\"comment\"> * properties for local resource setups. For a modern JDBC connection pool, consider</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;a href=&quot;https://github.com/brettwooldridge/HikariCP&quot;&gt;HikariCP&lt;/a&gt; instead,</span></span><br><span class=\"line\"><span class=\"comment\"> * exposing a corresponding &#123;<span class=\"doctag\">@code</span> HikariDataSource&#125; instance to the application.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@author</span> Juergen Hoeller</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@since</span> 14.03.2003</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@see</span> SimpleDriverDataSource</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HiveDriverManagerDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">AbstractDriverBasedDataSource</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Constructor for bean-style configuration.</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given JDBC URL,</span></span><br><span class=\"line\"><span class=\"comment\">\t * not specifying a username or password for JDBC access.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given standard</span></span><br><span class=\"line\"><span class=\"comment\">\t * DriverManager parameters.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> username the JDBC username to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> password the JDBC password to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String, String, String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url, String username, String password)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t\tsetUsername(username);</span><br><span class=\"line\">\t\tsetPassword(password);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Create a new DriverManagerDataSource with the given JDBC URL,</span></span><br><span class=\"line\"><span class=\"comment\">\t * not specifying a username or password for JDBC access.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> url the JDBC URL to use for accessing the DriverManager</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> conProps the JDBC connection properties</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"title function_\">HiveDriverManagerDataSource</span><span class=\"params\">(String url, Properties conProps)</span> &#123;</span><br><span class=\"line\">\t\tsetUrl(url);</span><br><span class=\"line\">\t\tsetConnectionProperties(conProps);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Set the JDBC driver class name. This driver will get initialized</span></span><br><span class=\"line\"><span class=\"comment\">\t * on startup, registering itself with the JDK&#x27;s DriverManager.</span></span><br><span class=\"line\"><span class=\"comment\">\t * &lt;p&gt;&lt;b&gt;<span class=\"doctag\">NOTE:</span> DriverManagerDataSource is primarily intended for accessing</span></span><br><span class=\"line\"><span class=\"comment\">\t * &lt;i&gt;pre-registered&lt;/i&gt; JDBC drivers.&lt;/b&gt; If you need to register a new driver,</span></span><br><span class=\"line\"><span class=\"comment\">\t * consider using &#123;<span class=\"doctag\">@link</span> SimpleDriverDataSource&#125; instead. Alternatively, consider</span></span><br><span class=\"line\"><span class=\"comment\">\t * initializing the JDBC driver yourself before instantiating this DataSource.</span></span><br><span class=\"line\"><span class=\"comment\">\t * The &quot;driverClassName&quot; property is mainly preserved for backwards compatibility,</span></span><br><span class=\"line\"><span class=\"comment\">\t * as well as for migrating between Commons DBCP and this DataSource.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#registerDriver(java.sql.Driver)</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> SimpleDriverDataSource</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDriverClassName</span><span class=\"params\">(String driverClassName)</span> &#123;</span><br><span class=\"line\">\t\tAssert.hasText(driverClassName, <span class=\"string\">&quot;Property &#x27;driverClassName&#x27; must not be empty&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">driverClassNameToUse</span> <span class=\"operator\">=</span> driverClassName.trim();</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\tClass.forName(driverClassNameToUse, <span class=\"literal\">true</span>, ClassUtils.getDefaultClassLoader());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">catch</span> (ClassNotFoundException ex) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(<span class=\"string\">&quot;Could not load JDBC driver class [&quot;</span> + driverClassNameToUse + <span class=\"string\">&quot;]&quot;</span>, ex);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</span><br><span class=\"line\">\t\t\tlogger.debug(<span class=\"string\">&quot;Loaded JDBC driver: &quot;</span> + driverClassNameToUse);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Connection <span class=\"title function_\">getConnectionFromDriver</span><span class=\"params\">(Properties props)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">url</span> <span class=\"operator\">=</span> getUrl();</span><br><span class=\"line\">\t\tAssert.state(url != <span class=\"literal\">null</span>, <span class=\"string\">&quot;&#x27;url&#x27; not set&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</span><br><span class=\"line\">\t\t\tlogger.debug(<span class=\"string\">&quot;Creating new JDBC DriverManager Connection to [&quot;</span> + url + <span class=\"string\">&quot;]&quot;</span>);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> getConnectionFromDriverManager(url, props);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Getting a Connection using the nasty static from DriverManager is extracted</span></span><br><span class=\"line\"><span class=\"comment\">\t * into a protected method to allow for easy unit testing.</span></span><br><span class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@see</span> java.sql.DriverManager#getConnection(String, java.util.Properties)</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">protected</span> Connection <span class=\"title function_\">getConnectionFromDriverManager</span><span class=\"params\">(String url, Properties props)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\tDriverManager.setLoginTimeout(<span class=\"number\">300</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> DriverManager.getConnection(url, props);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>修改<code>org.apache.dolphinscheduler.plugin.datasource.hive.HiveDataSourceClient</code>源码，使用<code>HiveDriverManagerDataSource</code>作为数据源。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class=\"line\"><span class=\"comment\"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class=\"line\"><span class=\"comment\"> * this work for additional information regarding copyright ownership.</span></span><br><span class=\"line\"><span class=\"comment\"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class=\"line\"><span class=\"comment\"> * (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class=\"line\"><span class=\"comment\"> * the License.  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class=\"line\"><span class=\"comment\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\"> * See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\"> * limitations under the License.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">package</span> org.apache.dolphinscheduler.plugin.datasource.hive;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.google.common.util.concurrent.ThreadFactoryBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.common.constants.Constants;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.common.utils.PropertyUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.client.CommonDataSourceClient;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.utils.DataSourceUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.api.utils.PasswordUtils;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.plugin.datasource.hive.utils.CommonUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.spi.datasource.BaseConnectionParam;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.dolphinscheduler.spi.enums.DbType;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.Logger;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.LoggerFactory;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.JdbcTemplate;</span><br><span class=\"line\"><span class=\"keyword\">import</span> sun.security.krb5.Config;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.lang.reflect.Field;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.Connection;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.Executors;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.ScheduledExecutorService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.concurrent.TimeUnit;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"keyword\">static</span> org.apache.dolphinscheduler.plugin.task.api.TaskConstants.*;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HiveDataSourceClient</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">CommonDataSourceClient</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">Logger</span> <span class=\"variable\">logger</span> <span class=\"operator\">=</span> LoggerFactory.getLogger(HiveDataSourceClient.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ScheduledExecutorService kerberosRenewalService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Configuration hadoopConf;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserGroupInformation ugi;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"type\">boolean</span> <span class=\"variable\">retryGetConnection</span> <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">HIKARI_CONN_TIMEOUT</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;connectionTimeout&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">HIKARI_MAXIMUM_POOL_SIZE</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;hiveOneSessionEnable&quot;</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> HiveDriverManagerDataSource driverManagerDataSource;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">HiveDataSourceClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(baseConnectionParam, dbType);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">preInit</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;PreInit in &#123;&#125;&quot;</span>, getClass().getName());</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.kerberosRenewalService = Executors.newSingleThreadScheduledExecutor(</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadFactoryBuilder</span>().setNameFormat(<span class=\"string\">&quot;Hive-Kerberos-Renewal-Thread-&quot;</span>).setDaemon(<span class=\"literal\">true</span>).build());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initClient</span><span class=\"params\">(BaseConnectionParam baseConnectionParam, DbType dbType)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.driverManagerDataSource =</span><br><span class=\"line\">                <span class=\"keyword\">new</span> <span class=\"title class_\">HiveDriverManagerDataSource</span>(DataSourceUtils.getJdbcUrl(DbType.HIVE, baseConnectionParam),</span><br><span class=\"line\">                        baseConnectionParam.getUser(), PasswordUtils.decodePassword(baseConnectionParam.getPassword()));</span><br><span class=\"line\">        driverManagerDataSource.setDriverClassName(baseConnectionParam.getDriverClassName());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">this</span>.jdbcTemplate = <span class=\"keyword\">new</span> <span class=\"title class_\">JdbcTemplate</span>(driverManagerDataSource);</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;Init &#123;&#125; success.&quot;</span>, getClass().getName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title function_\">checkEnv</span><span class=\"params\">(BaseConnectionParam baseConnectionParam)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.checkEnv(baseConnectionParam);</span><br><span class=\"line\">        checkKerberosEnv();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">checkKerberosEnv</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">krb5File</span> <span class=\"operator\">=</span> PropertyUtils.getString(JAVA_SECURITY_KRB5_CONF_PATH);</span><br><span class=\"line\">        <span class=\"type\">Boolean</span> <span class=\"variable\">kerberosStartupState</span> <span class=\"operator\">=</span> PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, <span class=\"literal\">false</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (kerberosStartupState &amp;&amp; StringUtils.isNotBlank(krb5File)) &#123;</span><br><span class=\"line\">            System.setProperty(JAVA_SECURITY_KRB5_CONF, krb5File);</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                Config.refresh();</span><br><span class=\"line\">                Class&lt;?&gt; kerberosName = Class.forName(<span class=\"string\">&quot;org.apache.hadoop.security.authentication.util.KerberosName&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">Field</span> <span class=\"variable\">field</span> <span class=\"operator\">=</span> kerberosName.getDeclaredField(<span class=\"string\">&quot;defaultRealm&quot;</span>);</span><br><span class=\"line\">                field.setAccessible(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                field.set(<span class=\"literal\">null</span>, Config.getInstance().getDefaultRealm());</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(<span class=\"string\">&quot;Update Kerberos environment failed.&quot;</span>, e);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserGroupInformation <span class=\"title function_\">createUserGroupInformation</span><span class=\"params\">(String username)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">krb5File</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">keytab</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">principal</span> <span class=\"operator\">=</span> PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">UserGroupInformation</span> <span class=\"variable\">ugi</span> <span class=\"operator\">=</span> CommonUtil.createUGI(getHadoopConf(), principal, keytab, krb5File, username);</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">Field</span> <span class=\"variable\">isKeytabField</span> <span class=\"operator\">=</span> ugi.getClass().getDeclaredField(<span class=\"string\">&quot;isKeytab&quot;</span>);</span><br><span class=\"line\">                isKeytabField.setAccessible(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                isKeytabField.set(ugi, <span class=\"literal\">true</span>);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (NoSuchFieldException | IllegalAccessException e) &#123;</span><br><span class=\"line\">                logger.warn(e.getMessage());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            kerberosRenewalService.scheduleWithFixedDelay(() -&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    ugi.checkTGTAndReloginFromKeytab();</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    logger.error(<span class=\"string\">&quot;Check TGT and Renewal from Keytab error&quot;</span>, e);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;, <span class=\"number\">5</span>, <span class=\"number\">5</span>, TimeUnit.MINUTES);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ugi;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(<span class=\"string\">&quot;createUserGroupInformation fail. &quot;</span>, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Configuration <span class=\"title function_\">createHadoopConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Configuration</span> <span class=\"variable\">hadoopConf</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Configuration</span>();</span><br><span class=\"line\">        hadoopConf.setBoolean(<span class=\"string\">&quot;ipc.client.fallback-to-simple-auth-allowed&quot;</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hadoopConf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Configuration <span class=\"title function_\">getHadoopConf</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>.hadoopConf;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Connection <span class=\"title function_\">getConnection</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> driverManagerDataSource.getConnection();</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">            <span class=\"type\">boolean</span> <span class=\"variable\">kerberosStartupState</span> <span class=\"operator\">=</span> PropertyUtils.getBoolean(HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE, <span class=\"literal\">false</span>);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (retryGetConnection &amp;&amp; kerberosStartupState) &#123;</span><br><span class=\"line\">                retryGetConnection = <span class=\"literal\">false</span>;</span><br><span class=\"line\">                createUserGroupInformation(baseConnectionParam.getUser());</span><br><span class=\"line\">                <span class=\"type\">Connection</span> <span class=\"variable\">connection</span> <span class=\"operator\">=</span> getConnection();</span><br><span class=\"line\">                retryGetConnection = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> connection;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            logger.error(<span class=\"string\">&quot;get oneSessionDataSource Connection fail SQLException: &#123;&#125;&quot;</span>, e.getMessage(), e);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">close</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.close();</span><br><span class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            kerberosRenewalService.shutdown();</span><br><span class=\"line\">            <span class=\"built_in\">this</span>.ugi = <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;Closed Hive datasource client.&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"附件\"><a href=\"#附件\" class=\"headerlink\" title=\"附件\"></a>附件</h2><p>其他DolphinScheduler SQL DataSource和数据库连接池相关的PR，或许去掉数据库连接池是一个更好的选择！</p>\n<p><a href=\"https://github.com/apache/dolphinscheduler/pull/14305\">https://github.com/apache/dolphinscheduler/pull/14305</a></p>\n<p><a href=\"https://github.com/apache/dolphinscheduler/pull/14190\">https://github.com/apache/dolphinscheduler/pull/14190</a></p>\n"},{"title":"记一次Kylin Server JVM OOM事故排查复盘","abbrlink":45838,"date":"2022-09-23T11:35:27.000Z","updated":"2022-09-23T11:35:27.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 问题\n\n在使用Tableau连接Kylin进行多维分析的时候，偶现查询失败，连接超时的问题，由此开始排查事故出现的原因。\n\n- 1、在Kylin的logs里面定位到有JVM OOM的情况，然后查看Kylin Server的JVM GC日志，发现在某个时间段会频繁的发生Full GC，Full GC可以持续十几分钟，同时GC后，老年代空间没有任何变化，这次Full GC无法回收一个对象，开始怀疑发生了内存泄漏。\n- 2、生产中部署了三个Kylin Server节点，前面由Nginx进行反向代理，进行负载均衡。查看Nginx日志，发现只有其中一个节点有Timeout的错误日志，于是怀疑Nginx负载均衡配置有问题，大部分流量分到一个节点，导致OOM。但是查看nginx配置，发现没有任何问题，同时Count Nginx请求日志，发现流量也不大，因此和Nginx与大流量高负载可能没有关系。\n- 3、到这里，只能利用MAT对JVM的Heap Dump文件*.hprof进行分析，文件有20G大，分析过程中一度导致服务器CPU狂飙至99%，顺利完成后，得到三个zip压缩文件，查看后，检测到内存泄漏，是由两个超大List大对象引起的。\n- 4、查看这两个超大List对象的全类名，发现是Kylin hold在内存中的查询结果集，由于查询结果集太大，Kylin Server JVM装不下，直接导致OOM的发生。\n\n\n\n> 正常情况下，查询Kylin的都应该是Group By语句，因此结果集应该很小，不会出现大对象，因此排查是哪一个查询语句查出来如此巨大的结果集。\n>\n> 根据Query id，查找日志，发现是Tableau发出的类似`select tmp.a from tmp;`的这种奇怪且没有意义的查询语句。但是Tableau是闭源的，完全搞不懂为什么会发出这种查询，在Tableau各种实验，也没有办法拖拽出这种查询SQL，但是确实又会时不时发出，导致Kylin Server OOM。\n\n\n\n- 5、为了快速的解决问题，保证服务的SLA，因此决定利用Arthas直接线上不停机运行时修改Kylin的源码，为不是Group By的每一条SQL加上limit，保证不会有大的结果集导致OOM。\n\n## 解决方法\n\n> 利用Arthas attach到线上Kylin Server的JVM\n>\n> 1、利用`jad命令`将QueryUtil类进行反编译，并保存下来，然后用vim修改源码中添加limit的判断逻辑，为不是Group By的每一条SQL加上limit，修改完以后需要将类重新加载到JVM\n>\n> $ jad --source--only com.example.demo.DemoApplication > /data/DemoApplication.java\n>\n> 2、`SC命令` 查找QueryUtil类是哪个classLoader加载的\n>\n> $ sc -d *DemoApplication | grep classLoader\n>\n> classLoaderHash   20ad9418 #类加载器  编号   \n>\n> 3、`MC命令` 用指定的classloader将修改后类在内存中编译（MC：内存编译器）\n>\n> $ mc -c 20ad9418 /data/DemoApplication.java -d /data  \n>\n> Memory compiler output: /data/com/example/demo/DemoApplication.class\n>\n> 4、`redefine命令` 将编译后的类加载到JVM\n>\n> $ redefine /data/com/example/demo/DemoApplication.class   redefine success, size: 1\n\n- 一顿操作猛如虎后，Kylin的源码已经在不停机、运行时完成了更改，最后问题解决，用户完全无感。\n","source":"_posts/troubleshooting/记一次Kylin-Server的JVM-OOM事故排查复盘.md","raw":"---\ntitle: 记一次Kylin Server JVM OOM事故排查复盘\ntags:\n  - Kylin\n  - JVM\n  - MAT\n  - Arthas\ncategories:\n  - - bigdata\n    - Kylin\nabbrlink: 45838\ndate: 2022-09-23 19:35:27\nupdated: 2022-09-23 19:35:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 问题\n\n在使用Tableau连接Kylin进行多维分析的时候，偶现查询失败，连接超时的问题，由此开始排查事故出现的原因。\n\n- 1、在Kylin的logs里面定位到有JVM OOM的情况，然后查看Kylin Server的JVM GC日志，发现在某个时间段会频繁的发生Full GC，Full GC可以持续十几分钟，同时GC后，老年代空间没有任何变化，这次Full GC无法回收一个对象，开始怀疑发生了内存泄漏。\n- 2、生产中部署了三个Kylin Server节点，前面由Nginx进行反向代理，进行负载均衡。查看Nginx日志，发现只有其中一个节点有Timeout的错误日志，于是怀疑Nginx负载均衡配置有问题，大部分流量分到一个节点，导致OOM。但是查看nginx配置，发现没有任何问题，同时Count Nginx请求日志，发现流量也不大，因此和Nginx与大流量高负载可能没有关系。\n- 3、到这里，只能利用MAT对JVM的Heap Dump文件*.hprof进行分析，文件有20G大，分析过程中一度导致服务器CPU狂飙至99%，顺利完成后，得到三个zip压缩文件，查看后，检测到内存泄漏，是由两个超大List大对象引起的。\n- 4、查看这两个超大List对象的全类名，发现是Kylin hold在内存中的查询结果集，由于查询结果集太大，Kylin Server JVM装不下，直接导致OOM的发生。\n\n\n\n> 正常情况下，查询Kylin的都应该是Group By语句，因此结果集应该很小，不会出现大对象，因此排查是哪一个查询语句查出来如此巨大的结果集。\n>\n> 根据Query id，查找日志，发现是Tableau发出的类似`select tmp.a from tmp;`的这种奇怪且没有意义的查询语句。但是Tableau是闭源的，完全搞不懂为什么会发出这种查询，在Tableau各种实验，也没有办法拖拽出这种查询SQL，但是确实又会时不时发出，导致Kylin Server OOM。\n\n\n\n- 5、为了快速的解决问题，保证服务的SLA，因此决定利用Arthas直接线上不停机运行时修改Kylin的源码，为不是Group By的每一条SQL加上limit，保证不会有大的结果集导致OOM。\n\n## 解决方法\n\n> 利用Arthas attach到线上Kylin Server的JVM\n>\n> 1、利用`jad命令`将QueryUtil类进行反编译，并保存下来，然后用vim修改源码中添加limit的判断逻辑，为不是Group By的每一条SQL加上limit，修改完以后需要将类重新加载到JVM\n>\n> $ jad --source--only com.example.demo.DemoApplication > /data/DemoApplication.java\n>\n> 2、`SC命令` 查找QueryUtil类是哪个classLoader加载的\n>\n> $ sc -d *DemoApplication | grep classLoader\n>\n> classLoaderHash   20ad9418 #类加载器  编号   \n>\n> 3、`MC命令` 用指定的classloader将修改后类在内存中编译（MC：内存编译器）\n>\n> $ mc -c 20ad9418 /data/DemoApplication.java -d /data  \n>\n> Memory compiler output: /data/com/example/demo/DemoApplication.class\n>\n> 4、`redefine命令` 将编译后的类加载到JVM\n>\n> $ redefine /data/com/example/demo/DemoApplication.class   redefine success, size: 1\n\n- 一顿操作猛如虎后，Kylin的源码已经在不停机、运行时完成了更改，最后问题解决，用户完全无感。\n","slug":"troubleshooting/记一次Kylin-Server的JVM-OOM事故排查复盘","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt1002y8j5m07w56q10","content":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>在使用Tableau连接Kylin进行多维分析的时候，偶现查询失败，连接超时的问题，由此开始排查事故出现的原因。</p>\n<ul>\n<li>1、在Kylin的logs里面定位到有JVM OOM的情况，然后查看Kylin Server的JVM GC日志，发现在某个时间段会频繁的发生Full GC，Full GC可以持续十几分钟，同时GC后，老年代空间没有任何变化，这次Full GC无法回收一个对象，开始怀疑发生了内存泄漏。</li>\n<li>2、生产中部署了三个Kylin Server节点，前面由Nginx进行反向代理，进行负载均衡。查看Nginx日志，发现只有其中一个节点有Timeout的错误日志，于是怀疑Nginx负载均衡配置有问题，大部分流量分到一个节点，导致OOM。但是查看nginx配置，发现没有任何问题，同时Count Nginx请求日志，发现流量也不大，因此和Nginx与大流量高负载可能没有关系。</li>\n<li>3、到这里，只能利用MAT对JVM的Heap Dump文件*.hprof进行分析，文件有20G大，分析过程中一度导致服务器CPU狂飙至99%，顺利完成后，得到三个zip压缩文件，查看后，检测到内存泄漏，是由两个超大List大对象引起的。</li>\n<li>4、查看这两个超大List对象的全类名，发现是Kylin hold在内存中的查询结果集，由于查询结果集太大，Kylin Server JVM装不下，直接导致OOM的发生。</li>\n</ul>\n<blockquote>\n<p>正常情况下，查询Kylin的都应该是Group By语句，因此结果集应该很小，不会出现大对象，因此排查是哪一个查询语句查出来如此巨大的结果集。</p>\n<p>根据Query id，查找日志，发现是Tableau发出的类似<code>select tmp.a from tmp;</code>的这种奇怪且没有意义的查询语句。但是Tableau是闭源的，完全搞不懂为什么会发出这种查询，在Tableau各种实验，也没有办法拖拽出这种查询SQL，但是确实又会时不时发出，导致Kylin Server OOM。</p>\n</blockquote>\n<ul>\n<li>5、为了快速的解决问题，保证服务的SLA，因此决定利用Arthas直接线上不停机运行时修改Kylin的源码，为不是Group By的每一条SQL加上limit，保证不会有大的结果集导致OOM。</li>\n</ul>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><blockquote>\n<p>利用Arthas attach到线上Kylin Server的JVM</p>\n<p>1、利用<code>jad命令</code>将QueryUtil类进行反编译，并保存下来，然后用vim修改源码中添加limit的判断逻辑，为不是Group By的每一条SQL加上limit，修改完以后需要将类重新加载到JVM</p>\n<p>$ jad –source–only com.example.demo.DemoApplication &gt; &#x2F;data&#x2F;DemoApplication.java</p>\n<p>2、<code>SC命令</code> 查找QueryUtil类是哪个classLoader加载的</p>\n<p>$ sc -d *DemoApplication | grep classLoader</p>\n<p>classLoaderHash   20ad9418 #类加载器  编号   </p>\n<p>3、<code>MC命令</code> 用指定的classloader将修改后类在内存中编译（MC：内存编译器）</p>\n<p>$ mc -c 20ad9418 &#x2F;data&#x2F;DemoApplication.java -d &#x2F;data  </p>\n<p>Memory compiler output: &#x2F;data&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplication.class</p>\n<p>4、<code>redefine命令</code> 将编译后的类加载到JVM</p>\n<p>$ redefine &#x2F;data&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplication.class   redefine success, size: 1</p>\n</blockquote>\n<ul>\n<li>一顿操作猛如虎后，Kylin的源码已经在不停机、运行时完成了更改，最后问题解决，用户完全无感。</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>在使用Tableau连接Kylin进行多维分析的时候，偶现查询失败，连接超时的问题，由此开始排查事故出现的原因。</p>\n<ul>\n<li>1、在Kylin的logs里面定位到有JVM OOM的情况，然后查看Kylin Server的JVM GC日志，发现在某个时间段会频繁的发生Full GC，Full GC可以持续十几分钟，同时GC后，老年代空间没有任何变化，这次Full GC无法回收一个对象，开始怀疑发生了内存泄漏。</li>\n<li>2、生产中部署了三个Kylin Server节点，前面由Nginx进行反向代理，进行负载均衡。查看Nginx日志，发现只有其中一个节点有Timeout的错误日志，于是怀疑Nginx负载均衡配置有问题，大部分流量分到一个节点，导致OOM。但是查看nginx配置，发现没有任何问题，同时Count Nginx请求日志，发现流量也不大，因此和Nginx与大流量高负载可能没有关系。</li>\n<li>3、到这里，只能利用MAT对JVM的Heap Dump文件*.hprof进行分析，文件有20G大，分析过程中一度导致服务器CPU狂飙至99%，顺利完成后，得到三个zip压缩文件，查看后，检测到内存泄漏，是由两个超大List大对象引起的。</li>\n<li>4、查看这两个超大List对象的全类名，发现是Kylin hold在内存中的查询结果集，由于查询结果集太大，Kylin Server JVM装不下，直接导致OOM的发生。</li>\n</ul>\n<blockquote>\n<p>正常情况下，查询Kylin的都应该是Group By语句，因此结果集应该很小，不会出现大对象，因此排查是哪一个查询语句查出来如此巨大的结果集。</p>\n<p>根据Query id，查找日志，发现是Tableau发出的类似<code>select tmp.a from tmp;</code>的这种奇怪且没有意义的查询语句。但是Tableau是闭源的，完全搞不懂为什么会发出这种查询，在Tableau各种实验，也没有办法拖拽出这种查询SQL，但是确实又会时不时发出，导致Kylin Server OOM。</p>\n</blockquote>\n<ul>\n<li>5、为了快速的解决问题，保证服务的SLA，因此决定利用Arthas直接线上不停机运行时修改Kylin的源码，为不是Group By的每一条SQL加上limit，保证不会有大的结果集导致OOM。</li>\n</ul>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><blockquote>\n<p>利用Arthas attach到线上Kylin Server的JVM</p>\n<p>1、利用<code>jad命令</code>将QueryUtil类进行反编译，并保存下来，然后用vim修改源码中添加limit的判断逻辑，为不是Group By的每一条SQL加上limit，修改完以后需要将类重新加载到JVM</p>\n<p>$ jad –source–only com.example.demo.DemoApplication &gt; &#x2F;data&#x2F;DemoApplication.java</p>\n<p>2、<code>SC命令</code> 查找QueryUtil类是哪个classLoader加载的</p>\n<p>$ sc -d *DemoApplication | grep classLoader</p>\n<p>classLoaderHash   20ad9418 #类加载器  编号   </p>\n<p>3、<code>MC命令</code> 用指定的classloader将修改后类在内存中编译（MC：内存编译器）</p>\n<p>$ mc -c 20ad9418 &#x2F;data&#x2F;DemoApplication.java -d &#x2F;data  </p>\n<p>Memory compiler output: &#x2F;data&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplication.class</p>\n<p>4、<code>redefine命令</code> 将编译后的类加载到JVM</p>\n<p>$ redefine &#x2F;data&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplication.class   redefine success, size: 1</p>\n</blockquote>\n<ul>\n<li>一顿操作猛如虎后，Kylin的源码已经在不停机、运行时完成了更改，最后问题解决，用户完全无感。</li>\n</ul>\n"},{"title":"基于LockSupport进行线程间的同步","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":55687,"date":"2022-12-31T13:07:13.000Z","updated":"2022-12-31T13:07:13.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 描述\n\n看到一道有意思的Java多线程面试题：要求两个线程交替打印a和b，且都打印50次，且a必须先打印。\n\n这是一个关于线程同步的问题，显然有比较多的解法，比如利用synchronized、CyclicBarrier等来实现。下面是利用LockSupport的代码。\n\n```java\n/**\n * 要求两个线程交替打印a和b，且都打印50次，且a必须先打印。\n * 实现两个线程之间的同步\n */\npublic class LockSupportDemo {\n\n    public static void main(String[] args) throws InterruptedException {\n        Thread[] threads = new Thread[2];\n\n        threads[0] = new Thread(() -> {\n            int i = 51;\n            while (i-- > 1) {\n                System.out.printf(\"%s %d---> %s%n\", Thread.currentThread().getName(), i, 'a');\n                // 先释放b线程，然后阻塞a线程，否则a线程直接阻塞，无法向下执行\n                LockSupport.unpark(threads[1]);\n                LockSupport.park();\n            }\n\n        });\n\n        threads[1] = new Thread(() -> {\n\n            int i = 51;\n            while (i-- > 1) {\n                // 先阻塞次线程，防止此线程先打印出b\n                LockSupport.park();\n                System.out.printf(\"%s %d---> %s%n\", Thread.currentThread().getName(), i, 'b');\n                LockSupport.unpark(threads[0]);\n            }\n        });\n\n        Arrays.stream(threads).forEach(Thread::start);\n        Thread.currentThread().join(1_000L);\n    }\n}\n\n```\n\n","source":"_posts/java/LockSupport线程同步.md","raw":"---\ntitle: 基于LockSupport进行线程间的同步\ntags:\n  - Threads\ncategories:\n  - - Java\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 55687\ndate: 2022-12-31 21:07:13\nupdated: 2022-12-31 21:07:13\ncover:\ndescription:\nkeywords:\n---\n\n## 描述\n\n看到一道有意思的Java多线程面试题：要求两个线程交替打印a和b，且都打印50次，且a必须先打印。\n\n这是一个关于线程同步的问题，显然有比较多的解法，比如利用synchronized、CyclicBarrier等来实现。下面是利用LockSupport的代码。\n\n```java\n/**\n * 要求两个线程交替打印a和b，且都打印50次，且a必须先打印。\n * 实现两个线程之间的同步\n */\npublic class LockSupportDemo {\n\n    public static void main(String[] args) throws InterruptedException {\n        Thread[] threads = new Thread[2];\n\n        threads[0] = new Thread(() -> {\n            int i = 51;\n            while (i-- > 1) {\n                System.out.printf(\"%s %d---> %s%n\", Thread.currentThread().getName(), i, 'a');\n                // 先释放b线程，然后阻塞a线程，否则a线程直接阻塞，无法向下执行\n                LockSupport.unpark(threads[1]);\n                LockSupport.park();\n            }\n\n        });\n\n        threads[1] = new Thread(() -> {\n\n            int i = 51;\n            while (i-- > 1) {\n                // 先阻塞次线程，防止此线程先打印出b\n                LockSupport.park();\n                System.out.printf(\"%s %d---> %s%n\", Thread.currentThread().getName(), i, 'b');\n                LockSupport.unpark(threads[0]);\n            }\n        });\n\n        Arrays.stream(threads).forEach(Thread::start);\n        Thread.currentThread().join(1_000L);\n    }\n}\n\n```\n\n","slug":"java/LockSupport线程同步","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt100318j5m50y08otz","content":"<h2 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h2><p>看到一道有意思的Java多线程面试题：要求两个线程交替打印a和b，且都打印50次，且a必须先打印。</p>\n<p>这是一个关于线程同步的问题，显然有比较多的解法，比如利用synchronized、CyclicBarrier等来实现。下面是利用LockSupport的代码。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 要求两个线程交替打印a和b，且都打印50次，且a必须先打印。</span></span><br><span class=\"line\"><span class=\"comment\"> * 实现两个线程之间的同步</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">LockSupportDemo</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        Thread[] threads = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        threads[<span class=\"number\">0</span>] = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(() -&gt; &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">51</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (i-- &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                System.out.printf(<span class=\"string\">&quot;%s %d---&gt; %s%n&quot;</span>, Thread.currentThread().getName(), i, <span class=\"string\">&#x27;a&#x27;</span>);</span><br><span class=\"line\">                <span class=\"comment\">// 先释放b线程，然后阻塞a线程，否则a线程直接阻塞，无法向下执行</span></span><br><span class=\"line\">                LockSupport.unpark(threads[<span class=\"number\">1</span>]);</span><br><span class=\"line\">                LockSupport.park();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        threads[<span class=\"number\">1</span>] = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(() -&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">51</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (i-- &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 先阻塞次线程，防止此线程先打印出b</span></span><br><span class=\"line\">                LockSupport.park();</span><br><span class=\"line\">                System.out.printf(<span class=\"string\">&quot;%s %d---&gt; %s%n&quot;</span>, Thread.currentThread().getName(), i, <span class=\"string\">&#x27;b&#x27;</span>);</span><br><span class=\"line\">                LockSupport.unpark(threads[<span class=\"number\">0</span>]);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        Arrays.stream(threads).forEach(Thread::start);</span><br><span class=\"line\">        Thread.currentThread().join(<span class=\"number\">1_000L</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h2><p>看到一道有意思的Java多线程面试题：要求两个线程交替打印a和b，且都打印50次，且a必须先打印。</p>\n<p>这是一个关于线程同步的问题，显然有比较多的解法，比如利用synchronized、CyclicBarrier等来实现。下面是利用LockSupport的代码。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 要求两个线程交替打印a和b，且都打印50次，且a必须先打印。</span></span><br><span class=\"line\"><span class=\"comment\"> * 实现两个线程之间的同步</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">LockSupportDemo</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> InterruptedException &#123;</span><br><span class=\"line\">        Thread[] threads = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        threads[<span class=\"number\">0</span>] = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(() -&gt; &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">51</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (i-- &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                System.out.printf(<span class=\"string\">&quot;%s %d---&gt; %s%n&quot;</span>, Thread.currentThread().getName(), i, <span class=\"string\">&#x27;a&#x27;</span>);</span><br><span class=\"line\">                <span class=\"comment\">// 先释放b线程，然后阻塞a线程，否则a线程直接阻塞，无法向下执行</span></span><br><span class=\"line\">                LockSupport.unpark(threads[<span class=\"number\">1</span>]);</span><br><span class=\"line\">                LockSupport.park();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        threads[<span class=\"number\">1</span>] = <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(() -&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">51</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (i-- &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 先阻塞次线程，防止此线程先打印出b</span></span><br><span class=\"line\">                LockSupport.park();</span><br><span class=\"line\">                System.out.printf(<span class=\"string\">&quot;%s %d---&gt; %s%n&quot;</span>, Thread.currentThread().getName(), i, <span class=\"string\">&#x27;b&#x27;</span>);</span><br><span class=\"line\">                LockSupport.unpark(threads[<span class=\"number\">0</span>]);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        Arrays.stream(threads).forEach(Thread::start);</span><br><span class=\"line\">        Thread.currentThread().join(<span class=\"number\">1_000L</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n"},{"title":"Netty粘包、半包问题解决方案","abbrlink":41988,"date":"2022-11-29T11:35:27.000Z","updated":"2022-11-29T11:35:27.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 前言\n> TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。\n>\n> UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有 Nagle 算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP 头+IP 头等等发一 次数据封装一次）也就没有粘包一说了。\n>\n> 1、客户端要发送的数据小于TCP发送缓冲区的大小，TCP为了提升效率，将多个写入缓冲区的数据包一次发送出去，多个数据包粘在一起，造成粘包； \n>\n> 2、服务端的应用层没有及时处理接收缓冲区中的数据，再次进行读取时出现粘包问题；\n>\n>  3、数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去；\n>\n>  4、拆包一般由于一次发送的数据包太大，超过MSS的大小，那么这个数据包就会被拆成多个TCP报文分开进行传输。\n\n发生粘包与半包现象的本质是**因为 TCP 是流式协议，消息无边界**。\n\n\n\n## 解决方案\n\n通过上文我们知道，底层的的TCP协议负责数据传输，它是无法理解上层的业务数据的具体语义的，所以在底层我们没有办法进行解决。那么我们只能通过上层的协议设计来解决粘包、拆包问题，主要有以下几种方法：\n\n#### 1、消息定长\n\n客户端于服务器**约定一个最大长度，保证客户端每次发送的数据长度都不会大于该长度**。若发送数据长度不足则需要**补齐**至该长度。服务器接收数据时，**将接收到的数据按照约定的最大长度进行拆分**，即使发送过程中产生了粘包，也可以通过定长解码器将数据正确地进行拆分。**服务端需要用到`FixedLengthFrameDecoder`对数据进行定长解码**，具体使用方法如下\n\n#### 2、明确消息边界\n\n行解码器的是**通过分隔符对数据进行拆分**来解决粘包半包问题的。\n\n可以通过`LineBasedFrameDecoder(int maxLength)`来拆分以**换行符(\\n)**为分隔符的数据，也可以通过`DelimiterBasedFrameDecoder(int maxFrameLength, ByteBuf... delimiters)`来**指定通过什么分隔符来拆分数据（可以传入多个分隔符）**。\n\n两种解码器**都需要传入数据的最大长度**，若超出最大长度，会抛出`TooLongFrameException`异常。\n\n#### 3、长度字段解码器\n\n在传送数据时可以在数据中**添加一个用于表示有用数据长度的字段**，在解码时读取出这个用于表明长度的字段，同时读取其他相关参数，即可知道最终需要的数据是什么样子的。\n\n`LengthFieldBasedFrameDecoder`解码器可以提供更为丰富的拆分方法。","source":"_posts/netty/Netty粘包、半包问题.md","raw":"---\ntitle: Netty粘包、半包问题解决方案\ntags:\n  - Netty\ncategories:\n  - - Netty\nabbrlink: 41988\ndate: 2022-11-29 19:35:27\nupdated: 2022-11-29 19:35:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 前言\n> TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。\n>\n> UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有 Nagle 算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP 头+IP 头等等发一 次数据封装一次）也就没有粘包一说了。\n>\n> 1、客户端要发送的数据小于TCP发送缓冲区的大小，TCP为了提升效率，将多个写入缓冲区的数据包一次发送出去，多个数据包粘在一起，造成粘包； \n>\n> 2、服务端的应用层没有及时处理接收缓冲区中的数据，再次进行读取时出现粘包问题；\n>\n>  3、数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去；\n>\n>  4、拆包一般由于一次发送的数据包太大，超过MSS的大小，那么这个数据包就会被拆成多个TCP报文分开进行传输。\n\n发生粘包与半包现象的本质是**因为 TCP 是流式协议，消息无边界**。\n\n\n\n## 解决方案\n\n通过上文我们知道，底层的的TCP协议负责数据传输，它是无法理解上层的业务数据的具体语义的，所以在底层我们没有办法进行解决。那么我们只能通过上层的协议设计来解决粘包、拆包问题，主要有以下几种方法：\n\n#### 1、消息定长\n\n客户端于服务器**约定一个最大长度，保证客户端每次发送的数据长度都不会大于该长度**。若发送数据长度不足则需要**补齐**至该长度。服务器接收数据时，**将接收到的数据按照约定的最大长度进行拆分**，即使发送过程中产生了粘包，也可以通过定长解码器将数据正确地进行拆分。**服务端需要用到`FixedLengthFrameDecoder`对数据进行定长解码**，具体使用方法如下\n\n#### 2、明确消息边界\n\n行解码器的是**通过分隔符对数据进行拆分**来解决粘包半包问题的。\n\n可以通过`LineBasedFrameDecoder(int maxLength)`来拆分以**换行符(\\n)**为分隔符的数据，也可以通过`DelimiterBasedFrameDecoder(int maxFrameLength, ByteBuf... delimiters)`来**指定通过什么分隔符来拆分数据（可以传入多个分隔符）**。\n\n两种解码器**都需要传入数据的最大长度**，若超出最大长度，会抛出`TooLongFrameException`异常。\n\n#### 3、长度字段解码器\n\n在传送数据时可以在数据中**添加一个用于表示有用数据长度的字段**，在解码时读取出这个用于表明长度的字段，同时读取其他相关参数，即可知道最终需要的数据是什么样子的。\n\n`LengthFieldBasedFrameDecoder`解码器可以提供更为丰富的拆分方法。","slug":"netty/Netty粘包、半包问题","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt200348j5m11qb1z2e","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。</p>\n<p>UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有 Nagle 算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP 头+IP 头等等发一 次数据封装一次）也就没有粘包一说了。</p>\n<p>1、客户端要发送的数据小于TCP发送缓冲区的大小，TCP为了提升效率，将多个写入缓冲区的数据包一次发送出去，多个数据包粘在一起，造成粘包； </p>\n<p>2、服务端的应用层没有及时处理接收缓冲区中的数据，再次进行读取时出现粘包问题；</p>\n<p> 3、数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去；</p>\n<p> 4、拆包一般由于一次发送的数据包太大，超过MSS的大小，那么这个数据包就会被拆成多个TCP报文分开进行传输。</p>\n</blockquote>\n<p>发生粘包与半包现象的本质是<strong>因为 TCP 是流式协议，消息无边界</strong>。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>通过上文我们知道，底层的的TCP协议负责数据传输，它是无法理解上层的业务数据的具体语义的，所以在底层我们没有办法进行解决。那么我们只能通过上层的协议设计来解决粘包、拆包问题，主要有以下几种方法：</p>\n<h4 id=\"1、消息定长\"><a href=\"#1、消息定长\" class=\"headerlink\" title=\"1、消息定长\"></a>1、消息定长</h4><p>客户端于服务器<strong>约定一个最大长度，保证客户端每次发送的数据长度都不会大于该长度</strong>。若发送数据长度不足则需要<strong>补齐</strong>至该长度。服务器接收数据时，<strong>将接收到的数据按照约定的最大长度进行拆分</strong>，即使发送过程中产生了粘包，也可以通过定长解码器将数据正确地进行拆分。<strong>服务端需要用到<code>FixedLengthFrameDecoder</code>对数据进行定长解码</strong>，具体使用方法如下</p>\n<h4 id=\"2、明确消息边界\"><a href=\"#2、明确消息边界\" class=\"headerlink\" title=\"2、明确消息边界\"></a>2、明确消息边界</h4><p>行解码器的是<strong>通过分隔符对数据进行拆分</strong>来解决粘包半包问题的。</p>\n<p>可以通过<code>LineBasedFrameDecoder(int maxLength)</code>来拆分以<strong>换行符(\\n)<strong>为分隔符的数据，也可以通过<code>DelimiterBasedFrameDecoder(int maxFrameLength, ByteBuf... delimiters)</code>来</strong>指定通过什么分隔符来拆分数据（可以传入多个分隔符）</strong>。</p>\n<p>两种解码器<strong>都需要传入数据的最大长度</strong>，若超出最大长度，会抛出<code>TooLongFrameException</code>异常。</p>\n<h4 id=\"3、长度字段解码器\"><a href=\"#3、长度字段解码器\" class=\"headerlink\" title=\"3、长度字段解码器\"></a>3、长度字段解码器</h4><p>在传送数据时可以在数据中<strong>添加一个用于表示有用数据长度的字段</strong>，在解码时读取出这个用于表明长度的字段，同时读取其他相关参数，即可知道最终需要的数据是什么样子的。</p>\n<p><code>LengthFieldBasedFrameDecoder</code>解码器可以提供更为丰富的拆分方法。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。</p>\n<p>UDP：本身作为无连接的不可靠的传输协议（适合频繁发送较小的数据包），他不会对数据包进行合并发送（也就没有 Nagle 算法之说了），他直接是一端发送什么数据，直接就发出去了，既然他不会对数据合并，每一个数据包都是完整的（数据+UDP 头+IP 头等等发一 次数据封装一次）也就没有粘包一说了。</p>\n<p>1、客户端要发送的数据小于TCP发送缓冲区的大小，TCP为了提升效率，将多个写入缓冲区的数据包一次发送出去，多个数据包粘在一起，造成粘包； </p>\n<p>2、服务端的应用层没有及时处理接收缓冲区中的数据，再次进行读取时出现粘包问题；</p>\n<p> 3、数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去；</p>\n<p> 4、拆包一般由于一次发送的数据包太大，超过MSS的大小，那么这个数据包就会被拆成多个TCP报文分开进行传输。</p>\n</blockquote>\n<p>发生粘包与半包现象的本质是<strong>因为 TCP 是流式协议，消息无边界</strong>。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>通过上文我们知道，底层的的TCP协议负责数据传输，它是无法理解上层的业务数据的具体语义的，所以在底层我们没有办法进行解决。那么我们只能通过上层的协议设计来解决粘包、拆包问题，主要有以下几种方法：</p>\n<h4 id=\"1、消息定长\"><a href=\"#1、消息定长\" class=\"headerlink\" title=\"1、消息定长\"></a>1、消息定长</h4><p>客户端于服务器<strong>约定一个最大长度，保证客户端每次发送的数据长度都不会大于该长度</strong>。若发送数据长度不足则需要<strong>补齐</strong>至该长度。服务器接收数据时，<strong>将接收到的数据按照约定的最大长度进行拆分</strong>，即使发送过程中产生了粘包，也可以通过定长解码器将数据正确地进行拆分。<strong>服务端需要用到<code>FixedLengthFrameDecoder</code>对数据进行定长解码</strong>，具体使用方法如下</p>\n<h4 id=\"2、明确消息边界\"><a href=\"#2、明确消息边界\" class=\"headerlink\" title=\"2、明确消息边界\"></a>2、明确消息边界</h4><p>行解码器的是<strong>通过分隔符对数据进行拆分</strong>来解决粘包半包问题的。</p>\n<p>可以通过<code>LineBasedFrameDecoder(int maxLength)</code>来拆分以<strong>换行符(\\n)<strong>为分隔符的数据，也可以通过<code>DelimiterBasedFrameDecoder(int maxFrameLength, ByteBuf... delimiters)</code>来</strong>指定通过什么分隔符来拆分数据（可以传入多个分隔符）</strong>。</p>\n<p>两种解码器<strong>都需要传入数据的最大长度</strong>，若超出最大长度，会抛出<code>TooLongFrameException</code>异常。</p>\n<h4 id=\"3、长度字段解码器\"><a href=\"#3、长度字段解码器\" class=\"headerlink\" title=\"3、长度字段解码器\"></a>3、长度字段解码器</h4><p>在传送数据时可以在数据中<strong>添加一个用于表示有用数据长度的字段</strong>，在解码时读取出这个用于表明长度的字段，同时读取其他相关参数，即可知道最终需要的数据是什么样子的。</p>\n<p><code>LengthFieldBasedFrameDecoder</code>解码器可以提供更为丰富的拆分方法。</p>\n"},{"title":"Flink-Hudi日志超频繁打印问题","abbrlink":14619,"date":"2022-12-04T09:57:25.000Z","updated":"2022-11-30T09:57:25.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n\n\n## 问题描述\n\n将从Kafka读取CDC日志写入Hudi的Flink SQL作业部署到集群后，发现Flink Job Manager频繁打印以下日志，差不多1000次每秒，非常恐怖。Job Manager日志文件快速膨胀，占用大量磁盘空间，已经影响到集群稳定性。\n\n```shell\n2022-12-04 09:24:40,897 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,899 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,933 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,935 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n```\n\n\n\n## 问题排查\n\n- 显然这不是正常现象，然后查看hudi源码，发现是日志是`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个方法打印的。\n\n  ```java\n    private IOStreamPair checkTrustAndSend(\n        InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn,\n        DataEncryptionKeyFactory encryptionKeyFactory,\n        Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId,\n        SecretKey secretKey)\n        throws IOException {\n      boolean localTrusted = trustedChannelResolver.isTrusted();\n      boolean remoteTrusted = trustedChannelResolver.isTrusted(addr);\n      LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, \"\n          + \"remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n      if (!localTrusted || !remoteTrusted) {\n        // The encryption key factory only returns a key if encryption is enabled.\n        DataEncryptionKey encryptionKey =\n            encryptionKeyFactory.newDataEncryptionKey();\n        return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,\n            datanodeId, secretKey);\n      } else {\n        LOG.debug(\n            \"SASL client skipping handshake on trusted connection for addr = {}, \"\n                + \"datanodeId = {}\", addr, datanodeId);\n        return null;\n      }\n    }\n  ```\n\n- 源码显示打印的是debug日志，但是实际打印出来的日志显示是info级别，很是奇怪。看来是一个Bug。\n\n\n\n### 线上利用Arthas分析问题\n\n- 1、利用trace命令看看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个方法里面的执行情况：\n\n  ```shell\n  [arthas@1302]$ trace org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2\n  Press Q or Ctrl+C to abort.\n  Affect(class count: 1 , method count: 2) cost in 203 ms, listenerId: 6\n  `---ts=2022-12-04 10:25:01;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086\n      `---[1.121797ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()\n          `---[0.93434ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend() #227\n              `---[0.839963ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()\n                  +---[0.092959ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #237\n                  +---[0.012338ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #238\n                  +---[0.023832ms] org.slf4j.Logger:info() #239\n                  +---[0.021391ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory:newDataEncryptionKey() #244\n                  `---[0.045789ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send() #245\n  ```\n\n  发现调用的竟然是`org.slf4j.Logger:info()`方法和源码中的LOG.debug()方法根本不符，很是奇怪。\n\n- 2、利用stack命令看看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`方法的调用链路。\n\n  ```shell\n  [arthas@1302]$ stack org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2\n  ts=2022-12-04 09:48:33;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086\n      @org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend()\n          at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:170)\n          at org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:730)\n          at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2942)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)\n          at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)\n          at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)\n          at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)\n          at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)\n          at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:681)\n          at java.io.FilterInputStream.read(FilterInputStream.java:83)\n          at java.io.DataInputStream.readInt(DataInputStream.java:387)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.readVersion(HoodieLogFileReader.java:361)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.readBlock(HoodieLogFileReader.java:171)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:388)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:68)\n          at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.loadInstants(HoodieArchivedTimeline.java:255)\n          at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.<init>(HoodieArchivedTimeline.java:109)\n          at org.apache.hudi.common.table.HoodieTableMetaClient.getArchivedTimeline(HoodieTableMetaClient.java:392)\n          at org.apache.hudi.sync.common.HoodieSyncClient.getDroppedPartitionsSince(HoodieSyncClient.java:91)\n          at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:231)\n          at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158)\n          at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142)\n          at org.apache.hudi.sink.StreamWriteOperatorCoordinator.doSyncHive(StreamWriteOperatorCoordinator.java:335)\n          at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)\n          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n          at java.lang.Thread.run(Thread.java:748)\n  ```\n\n  发现是Flink Hive Sync模块在频繁的调用这个方法。\n\n- 3、利用arthas的logger info命令查看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个类的logger信息。\n\n  ```shell\n  logger -n org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n  \n  name                        org.apache.hadoop\n  class                       org.apache.logging.log4j.core.config.LoggerConfig\n  classLoader                 sun.misc.Launcher$AppClassLoader@5cad8086\n  classLoaderHash             5cad8086\n  level                       INFO\n  config                      org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@1448eed2\n  additivity                  true\n  codeSource                  file:/data/hadoop/nm-local-dir/usercache/hadoop/appcache/application_1666403512407_0148/filecache/13/flink-doris-connector-1.14_2.11-1.1.0.jar\n  ```\n\n- 4、暂时通过arthas将`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个类的logger级别由INFO提升到WARN。这样INFO级别的日志就不会再打印。问题暂时得到解决。\n\n  ```shell\n  logger --name org.apache.hadoop --level WARN\n  ```\n\n  \n\n### 解决问题\n\n> 线上集群跑的代码和Hudi的源码不一致，说明线上这个类加载自线上的Hadoop3.2.1版本，而hudi0.12.1依赖的是Hadoop2.10.1，因此出现源码不一致的情况。\n\n- 1、利用Arthas sc命令打印类信息\n\n  ```shell\n  [arthas@1302]$ sc -d org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   class-info        org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   code-source       /opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar\n   name              org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   isInterface       false\n   isAnnotation      false\n   isEnum            false\n   isAnonymousClass  false\n   isArray           false\n   isLocalClass      false\n   isMemberClass     false\n   isPrimitive       false\n   isSynthetic       false\n   simple-name       SaslDataTransferClient\n   modifier          public\n   annotation        org.apache.hadoop.classification.InterfaceAudience$Private\n   interfaces\n   super-class       +-java.lang.Object\n   class-loader      +-sun.misc.Launcher$AppClassLoader@5cad8086\n                       +-sun.misc.Launcher$ExtClassLoader@340f438e\n   classLoaderHash   5cad8086\n  \n  Affect(row-cnt:1) cost in 22 ms.\n  ```\n\n  可以发现该类的路径为：/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar。\n\n- 2、通过对比Hadoop各个版本的源码，发现是Hadoop3.2.1版本中存在的一个小Bug，而线上集群正是这个版本,随后在，晦气~。**这应该是开发人员粗心导致的一个小Bug，最终导致了日志的疯狂打印，影响服务稳定性。这提醒我们不要随意提升Logger Level。**\n\n  ```shell\n  (hudi12.1依赖的版本源码)hadoop2.10.1 version -> LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  (线上集群依赖的版本源码)hadoop3.2.1 version -> LOG.info(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  (该Bug修复的版本源码)hadoop3.2.2 version -> LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  ```\n\n  \n","source":"_posts/bigdata/TroubleShooting/Flink-hudi日志超频繁打印.md","raw":"---\ntitle: Flink-Hudi日志超频繁打印问题\ntags:\n  - Troubleshooting\n  - Flink\n  - Hudi\ncategories:\n  - - Troubleshooting\nabbrlink: 14619\ndate: 2022-12-04 17:57:25\nupdated: 2022-11-30 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n\n\n## 问题描述\n\n将从Kafka读取CDC日志写入Hudi的Flink SQL作业部署到集群后，发现Flink Job Manager频繁打印以下日志，差不多1000次每秒，非常恐怖。Job Manager日志文件快速膨胀，占用大量磁盘空间，已经影响到集群稳定性。\n\n```shell\n2022-12-04 09:24:40,897 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,899 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,933 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n2022-12-04 09:24:40,935 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n```\n\n\n\n## 问题排查\n\n- 显然这不是正常现象，然后查看hudi源码，发现是日志是`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个方法打印的。\n\n  ```java\n    private IOStreamPair checkTrustAndSend(\n        InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn,\n        DataEncryptionKeyFactory encryptionKeyFactory,\n        Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId,\n        SecretKey secretKey)\n        throws IOException {\n      boolean localTrusted = trustedChannelResolver.isTrusted();\n      boolean remoteTrusted = trustedChannelResolver.isTrusted(addr);\n      LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, \"\n          + \"remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n      if (!localTrusted || !remoteTrusted) {\n        // The encryption key factory only returns a key if encryption is enabled.\n        DataEncryptionKey encryptionKey =\n            encryptionKeyFactory.newDataEncryptionKey();\n        return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,\n            datanodeId, secretKey);\n      } else {\n        LOG.debug(\n            \"SASL client skipping handshake on trusted connection for addr = {}, \"\n                + \"datanodeId = {}\", addr, datanodeId);\n        return null;\n      }\n    }\n  ```\n\n- 源码显示打印的是debug日志，但是实际打印出来的日志显示是info级别，很是奇怪。看来是一个Bug。\n\n\n\n### 线上利用Arthas分析问题\n\n- 1、利用trace命令看看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个方法里面的执行情况：\n\n  ```shell\n  [arthas@1302]$ trace org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2\n  Press Q or Ctrl+C to abort.\n  Affect(class count: 1 , method count: 2) cost in 203 ms, listenerId: 6\n  `---ts=2022-12-04 10:25:01;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086\n      `---[1.121797ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()\n          `---[0.93434ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend() #227\n              `---[0.839963ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()\n                  +---[0.092959ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #237\n                  +---[0.012338ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #238\n                  +---[0.023832ms] org.slf4j.Logger:info() #239\n                  +---[0.021391ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory:newDataEncryptionKey() #244\n                  `---[0.045789ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send() #245\n  ```\n\n  发现调用的竟然是`org.slf4j.Logger:info()`方法和源码中的LOG.debug()方法根本不符，很是奇怪。\n\n- 2、利用stack命令看看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`方法的调用链路。\n\n  ```shell\n  [arthas@1302]$ stack org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2\n  ts=2022-12-04 09:48:33;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086\n      @org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend()\n          at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:170)\n          at org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:730)\n          at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2942)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)\n          at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)\n          at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)\n          at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)\n          at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)\n          at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)\n          at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:681)\n          at java.io.FilterInputStream.read(FilterInputStream.java:83)\n          at java.io.DataInputStream.readInt(DataInputStream.java:387)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.readVersion(HoodieLogFileReader.java:361)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.readBlock(HoodieLogFileReader.java:171)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:388)\n          at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:68)\n          at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.loadInstants(HoodieArchivedTimeline.java:255)\n          at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.<init>(HoodieArchivedTimeline.java:109)\n          at org.apache.hudi.common.table.HoodieTableMetaClient.getArchivedTimeline(HoodieTableMetaClient.java:392)\n          at org.apache.hudi.sync.common.HoodieSyncClient.getDroppedPartitionsSince(HoodieSyncClient.java:91)\n          at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:231)\n          at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158)\n          at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142)\n          at org.apache.hudi.sink.StreamWriteOperatorCoordinator.doSyncHive(StreamWriteOperatorCoordinator.java:335)\n          at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)\n          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n          at java.lang.Thread.run(Thread.java:748)\n  ```\n\n  发现是Flink Hive Sync模块在频繁的调用这个方法。\n\n- 3、利用arthas的logger info命令查看`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个类的logger信息。\n\n  ```shell\n  logger -n org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n  \n  name                        org.apache.hadoop\n  class                       org.apache.logging.log4j.core.config.LoggerConfig\n  classLoader                 sun.misc.Launcher$AppClassLoader@5cad8086\n  classLoaderHash             5cad8086\n  level                       INFO\n  config                      org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@1448eed2\n  additivity                  true\n  codeSource                  file:/data/hadoop/nm-local-dir/usercache/hadoop/appcache/application_1666403512407_0148/filecache/13/flink-doris-connector-1.14_2.11-1.1.0.jar\n  ```\n\n- 4、暂时通过arthas将`org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend`这个类的logger级别由INFO提升到WARN。这样INFO级别的日志就不会再打印。问题暂时得到解决。\n\n  ```shell\n  logger --name org.apache.hadoop --level WARN\n  ```\n\n  \n\n### 解决问题\n\n> 线上集群跑的代码和Hudi的源码不一致，说明线上这个类加载自线上的Hadoop3.2.1版本，而hudi0.12.1依赖的是Hadoop2.10.1，因此出现源码不一致的情况。\n\n- 1、利用Arthas sc命令打印类信息\n\n  ```shell\n  [arthas@1302]$ sc -d org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   class-info        org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   code-source       /opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar\n   name              org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient\n   isInterface       false\n   isAnnotation      false\n   isEnum            false\n   isAnonymousClass  false\n   isArray           false\n   isLocalClass      false\n   isMemberClass     false\n   isPrimitive       false\n   isSynthetic       false\n   simple-name       SaslDataTransferClient\n   modifier          public\n   annotation        org.apache.hadoop.classification.InterfaceAudience$Private\n   interfaces\n   super-class       +-java.lang.Object\n   class-loader      +-sun.misc.Launcher$AppClassLoader@5cad8086\n                       +-sun.misc.Launcher$ExtClassLoader@340f438e\n   classLoaderHash   5cad8086\n  \n  Affect(row-cnt:1) cost in 22 ms.\n  ```\n\n  可以发现该类的路径为：/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar。\n\n- 2、通过对比Hadoop各个版本的源码，发现是Hadoop3.2.1版本中存在的一个小Bug，而线上集群正是这个版本,随后在，晦气~。**这应该是开发人员粗心导致的一个小Bug，最终导致了日志的疯狂打印，影响服务稳定性。这提醒我们不要随意提升Logger Level。**\n\n  ```shell\n  (hudi12.1依赖的版本源码)hadoop2.10.1 version -> LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  (线上集群依赖的版本源码)hadoop3.2.1 version -> LOG.info(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  (该Bug修复的版本源码)hadoop3.2.2 version -> LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}\", localTrusted, remoteTrusted);\n  ```\n\n  \n","slug":"bigdata/TroubleShooting/Flink-hudi日志超频繁打印","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt200368j5maxbeh87t","content":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>将从Kafka读取CDC日志写入Hudi的Flink SQL作业部署到集群后，发现Flink Job Manager频繁打印以下日志，差不多1000次每秒，非常恐怖。Job Manager日志文件快速膨胀，占用大量磁盘空间，已经影响到集群稳定性。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-12-04 09:24:40,897 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,899 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,933 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,935 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"问题排查\"><a href=\"#问题排查\" class=\"headerlink\" title=\"问题排查\"></a>问题排查</h2><ul>\n<li><p>显然这不是正常现象，然后查看hudi源码，发现是日志是<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个方法打印的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> IOStreamPair <span class=\"title function_\">checkTrustAndSend</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">    InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn,</span></span><br><span class=\"line\"><span class=\"params\">    DataEncryptionKeyFactory encryptionKeyFactory,</span></span><br><span class=\"line\"><span class=\"params\">    Token&lt;BlockTokenIdentifier&gt; accessToken, DatanodeID datanodeId,</span></span><br><span class=\"line\"><span class=\"params\">    SecretKey secretKey)</span></span><br><span class=\"line\">    <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">localTrusted</span> <span class=\"operator\">=</span> trustedChannelResolver.isTrusted();</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">remoteTrusted</span> <span class=\"operator\">=</span> trustedChannelResolver.isTrusted(addr);</span><br><span class=\"line\">  LOG.debug(<span class=\"string\">&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, &quot;</span></span><br><span class=\"line\">      + <span class=\"string\">&quot;remoteHostTrusted = &#123;&#125;&quot;</span>, localTrusted, remoteTrusted);</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!localTrusted || !remoteTrusted) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// The encryption key factory only returns a key if encryption is enabled.</span></span><br><span class=\"line\">    <span class=\"type\">DataEncryptionKey</span> <span class=\"variable\">encryptionKey</span> <span class=\"operator\">=</span></span><br><span class=\"line\">        encryptionKeyFactory.newDataEncryptionKey();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,</span><br><span class=\"line\">        datanodeId, secretKey);</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    LOG.debug(</span><br><span class=\"line\">        <span class=\"string\">&quot;SASL client skipping handshake on trusted connection for addr = &#123;&#125;, &quot;</span></span><br><span class=\"line\">            + <span class=\"string\">&quot;datanodeId = &#123;&#125;&quot;</span>, addr, datanodeId);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>源码显示打印的是debug日志，但是实际打印出来的日志显示是info级别，很是奇怪。看来是一个Bug。</p>\n</li>\n</ul>\n<h3 id=\"线上利用Arthas分析问题\"><a href=\"#线上利用Arthas分析问题\" class=\"headerlink\" title=\"线上利用Arthas分析问题\"></a>线上利用Arthas分析问题</h3><ul>\n<li><p>1、利用trace命令看看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个方法里面的执行情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">trace org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2</span></span><br><span class=\"line\">Press Q or Ctrl+C to abort.</span><br><span class=\"line\">Affect(class count: 1 , method count: 2) cost in 203 ms, listenerId: 6</span><br><span class=\"line\">`---ts=2022-12-04 10:25:01;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">    `---[1.121797ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()</span><br><span class=\"line\">        `---[0.93434ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend() #227</span><br><span class=\"line\">            `---[0.839963ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()</span><br><span class=\"line\">                +---[0.092959ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #237</span><br><span class=\"line\">                +---[0.012338ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #238</span><br><span class=\"line\">                +---[0.023832ms] org.slf4j.Logger:info() #239</span><br><span class=\"line\">                +---[0.021391ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory:newDataEncryptionKey() #244</span><br><span class=\"line\">                `---[0.045789ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send() #245</span><br></pre></td></tr></table></figure>\n\n<p>发现调用的竟然是<code>org.slf4j.Logger:info()</code>方法和源码中的LOG.debug()方法根本不符，很是奇怪。</p>\n</li>\n<li><p>2、利用stack命令看看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>方法的调用链路。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">stack org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2</span></span><br><span class=\"line\">ts=2022-12-04 09:48:33;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">    @org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend()</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:170)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:730)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2942)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:681)</span><br><span class=\"line\">        at java.io.FilterInputStream.read(FilterInputStream.java:83)</span><br><span class=\"line\">        at java.io.DataInputStream.readInt(DataInputStream.java:387)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.readVersion(HoodieLogFileReader.java:361)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.readBlock(HoodieLogFileReader.java:171)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:388)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:68)</span><br><span class=\"line\">        at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.loadInstants(HoodieArchivedTimeline.java:255)</span><br><span class=\"line\">        at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.&lt;init&gt;(HoodieArchivedTimeline.java:109)</span><br><span class=\"line\">        at org.apache.hudi.common.table.HoodieTableMetaClient.getArchivedTimeline(HoodieTableMetaClient.java:392)</span><br><span class=\"line\">        at org.apache.hudi.sync.common.HoodieSyncClient.getDroppedPartitionsSince(HoodieSyncClient.java:91)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:231)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142)</span><br><span class=\"line\">        at org.apache.hudi.sink.StreamWriteOperatorCoordinator.doSyncHive(StreamWriteOperatorCoordinator.java:335)</span><br><span class=\"line\">        at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class=\"line\">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>\n\n<p>发现是Flink Hive Sync模块在频繁的调用这个方法。</p>\n</li>\n<li><p>3、利用arthas的logger info命令查看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个类的logger信息。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger -n org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"></span><br><span class=\"line\">name                        org.apache.hadoop</span><br><span class=\"line\">class                       org.apache.logging.log4j.core.config.LoggerConfig</span><br><span class=\"line\">classLoader                 sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">classLoaderHash             5cad8086</span><br><span class=\"line\">level                       INFO</span><br><span class=\"line\">config                      org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@1448eed2</span><br><span class=\"line\">additivity                  true</span><br><span class=\"line\">codeSource                  file:/data/hadoop/nm-local-dir/usercache/hadoop/appcache/application_1666403512407_0148/filecache/13/flink-doris-connector-1.14_2.11-1.1.0.jar</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4、暂时通过arthas将<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个类的logger级别由INFO提升到WARN。这样INFO级别的日志就不会再打印。问题暂时得到解决。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger --name org.apache.hadoop --level WARN</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"解决问题\"><a href=\"#解决问题\" class=\"headerlink\" title=\"解决问题\"></a>解决问题</h3><blockquote>\n<p>线上集群跑的代码和Hudi的源码不一致，说明线上这个类加载自线上的Hadoop3.2.1版本，而hudi0.12.1依赖的是Hadoop2.10.1，因此出现源码不一致的情况。</p>\n</blockquote>\n<ul>\n<li><p>1、利用Arthas sc命令打印类信息</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">sc -d org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span></span><br><span class=\"line\"> class-info        org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"> code-source       /opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar</span><br><span class=\"line\"> name              org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"> isInterface       false</span><br><span class=\"line\"> isAnnotation      false</span><br><span class=\"line\"> isEnum            false</span><br><span class=\"line\"> isAnonymousClass  false</span><br><span class=\"line\"> isArray           false</span><br><span class=\"line\"> isLocalClass      false</span><br><span class=\"line\"> isMemberClass     false</span><br><span class=\"line\"> isPrimitive       false</span><br><span class=\"line\"> isSynthetic       false</span><br><span class=\"line\"> simple-name       SaslDataTransferClient</span><br><span class=\"line\"> modifier          public</span><br><span class=\"line\"> annotation        org.apache.hadoop.classification.InterfaceAudience$Private</span><br><span class=\"line\"> interfaces</span><br><span class=\"line\"> super-class       +-java.lang.Object</span><br><span class=\"line\"> class-loader      +-sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">                     +-sun.misc.Launcher$ExtClassLoader@340f438e</span><br><span class=\"line\"> classLoaderHash   5cad8086</span><br><span class=\"line\"></span><br><span class=\"line\">Affect(row-cnt:1) cost in 22 ms.</span><br></pre></td></tr></table></figure>\n\n<p>可以发现该类的路径为：&#x2F;opt&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;hadoop-hdfs-client-3.2.1.jar。</p>\n</li>\n<li><p>2、通过对比Hadoop各个版本的源码，发现是Hadoop3.2.1版本中存在的一个小Bug，而线上集群正是这个版本,随后在，晦气~。<strong>这应该是开发人员粗心导致的一个小Bug，最终导致了日志的疯狂打印，影响服务稳定性。这提醒我们不要随意提升Logger Level。</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(hudi12.1依赖的版本源码)hadoop2.10.1 version -&gt; LOG.debug(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br><span class=\"line\">(线上集群依赖的版本源码)hadoop3.2.1 version -&gt; LOG.info(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br><span class=\"line\">(该Bug修复的版本源码)hadoop3.2.2 version -&gt; LOG.debug(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>将从Kafka读取CDC日志写入Hudi的Flink SQL作业部署到集群后，发现Flink Job Manager频繁打印以下日志，差不多1000次每秒，非常恐怖。Job Manager日志文件快速膨胀，占用大量磁盘空间，已经影响到集群稳定性。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-12-04 09:24:40,897 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,899 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,933 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class=\"line\">2022-12-04 09:24:40,935 INFO  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient [] - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"问题排查\"><a href=\"#问题排查\" class=\"headerlink\" title=\"问题排查\"></a>问题排查</h2><ul>\n<li><p>显然这不是正常现象，然后查看hudi源码，发现是日志是<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个方法打印的。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> IOStreamPair <span class=\"title function_\">checkTrustAndSend</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">    InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn,</span></span><br><span class=\"line\"><span class=\"params\">    DataEncryptionKeyFactory encryptionKeyFactory,</span></span><br><span class=\"line\"><span class=\"params\">    Token&lt;BlockTokenIdentifier&gt; accessToken, DatanodeID datanodeId,</span></span><br><span class=\"line\"><span class=\"params\">    SecretKey secretKey)</span></span><br><span class=\"line\">    <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">localTrusted</span> <span class=\"operator\">=</span> trustedChannelResolver.isTrusted();</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">remoteTrusted</span> <span class=\"operator\">=</span> trustedChannelResolver.isTrusted(addr);</span><br><span class=\"line\">  LOG.debug(<span class=\"string\">&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, &quot;</span></span><br><span class=\"line\">      + <span class=\"string\">&quot;remoteHostTrusted = &#123;&#125;&quot;</span>, localTrusted, remoteTrusted);</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!localTrusted || !remoteTrusted) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// The encryption key factory only returns a key if encryption is enabled.</span></span><br><span class=\"line\">    <span class=\"type\">DataEncryptionKey</span> <span class=\"variable\">encryptionKey</span> <span class=\"operator\">=</span></span><br><span class=\"line\">        encryptionKeyFactory.newDataEncryptionKey();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken,</span><br><span class=\"line\">        datanodeId, secretKey);</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    LOG.debug(</span><br><span class=\"line\">        <span class=\"string\">&quot;SASL client skipping handshake on trusted connection for addr = &#123;&#125;, &quot;</span></span><br><span class=\"line\">            + <span class=\"string\">&quot;datanodeId = &#123;&#125;&quot;</span>, addr, datanodeId);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>源码显示打印的是debug日志，但是实际打印出来的日志显示是info级别，很是奇怪。看来是一个Bug。</p>\n</li>\n</ul>\n<h3 id=\"线上利用Arthas分析问题\"><a href=\"#线上利用Arthas分析问题\" class=\"headerlink\" title=\"线上利用Arthas分析问题\"></a>线上利用Arthas分析问题</h3><ul>\n<li><p>1、利用trace命令看看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个方法里面的执行情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">trace org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2</span></span><br><span class=\"line\">Press Q or Ctrl+C to abort.</span><br><span class=\"line\">Affect(class count: 1 , method count: 2) cost in 203 ms, listenerId: 6</span><br><span class=\"line\">`---ts=2022-12-04 10:25:01;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">    `---[1.121797ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()</span><br><span class=\"line\">        `---[0.93434ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend() #227</span><br><span class=\"line\">            `---[0.839963ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend()</span><br><span class=\"line\">                +---[0.092959ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #237</span><br><span class=\"line\">                +---[0.012338ms] org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted() #238</span><br><span class=\"line\">                +---[0.023832ms] org.slf4j.Logger:info() #239</span><br><span class=\"line\">                +---[0.021391ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory:newDataEncryptionKey() #244</span><br><span class=\"line\">                `---[0.045789ms] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send() #245</span><br></pre></td></tr></table></figure>\n\n<p>发现调用的竟然是<code>org.slf4j.Logger:info()</code>方法和源码中的LOG.debug()方法根本不符，很是奇怪。</p>\n</li>\n<li><p>2、利用stack命令看看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>方法的调用链路。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">stack org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient checkTrustAndSend -n 2</span></span><br><span class=\"line\">ts=2022-12-04 09:48:33;thread_name=pool-20-thread-1;id=bf;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">    @org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend()</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:170)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:730)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2942)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:681)</span><br><span class=\"line\">        at java.io.FilterInputStream.read(FilterInputStream.java:83)</span><br><span class=\"line\">        at java.io.DataInputStream.readInt(DataInputStream.java:387)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.readVersion(HoodieLogFileReader.java:361)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.readBlock(HoodieLogFileReader.java:171)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:388)</span><br><span class=\"line\">        at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:68)</span><br><span class=\"line\">        at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.loadInstants(HoodieArchivedTimeline.java:255)</span><br><span class=\"line\">        at org.apache.hudi.common.table.timeline.HoodieArchivedTimeline.&lt;init&gt;(HoodieArchivedTimeline.java:109)</span><br><span class=\"line\">        at org.apache.hudi.common.table.HoodieTableMetaClient.getArchivedTimeline(HoodieTableMetaClient.java:392)</span><br><span class=\"line\">        at org.apache.hudi.sync.common.HoodieSyncClient.getDroppedPartitionsSince(HoodieSyncClient.java:91)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:231)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:158)</span><br><span class=\"line\">        at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:142)</span><br><span class=\"line\">        at org.apache.hudi.sink.StreamWriteOperatorCoordinator.doSyncHive(StreamWriteOperatorCoordinator.java:335)</span><br><span class=\"line\">        at org.apache.hudi.sink.utils.NonThrownExecutor.lambda$wrapAction$0(NonThrownExecutor.java:130)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class=\"line\">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>\n\n<p>发现是Flink Hive Sync模块在频繁的调用这个方法。</p>\n</li>\n<li><p>3、利用arthas的logger info命令查看<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个类的logger信息。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger -n org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"></span><br><span class=\"line\">name                        org.apache.hadoop</span><br><span class=\"line\">class                       org.apache.logging.log4j.core.config.LoggerConfig</span><br><span class=\"line\">classLoader                 sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">classLoaderHash             5cad8086</span><br><span class=\"line\">level                       INFO</span><br><span class=\"line\">config                      org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@1448eed2</span><br><span class=\"line\">additivity                  true</span><br><span class=\"line\">codeSource                  file:/data/hadoop/nm-local-dir/usercache/hadoop/appcache/application_1666403512407_0148/filecache/13/flink-doris-connector-1.14_2.11-1.1.0.jar</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4、暂时通过arthas将<code>org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient#checkTrustAndSend</code>这个类的logger级别由INFO提升到WARN。这样INFO级别的日志就不会再打印。问题暂时得到解决。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger --name org.apache.hadoop --level WARN</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"解决问题\"><a href=\"#解决问题\" class=\"headerlink\" title=\"解决问题\"></a>解决问题</h3><blockquote>\n<p>线上集群跑的代码和Hudi的源码不一致，说明线上这个类加载自线上的Hadoop3.2.1版本，而hudi0.12.1依赖的是Hadoop2.10.1，因此出现源码不一致的情况。</p>\n</blockquote>\n<ul>\n<li><p>1、利用Arthas sc命令打印类信息</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">[arthas@1302]$ </span><span class=\"language-bash\">sc -d org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span></span><br><span class=\"line\"> class-info        org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"> code-source       /opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar</span><br><span class=\"line\"> name              org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient</span><br><span class=\"line\"> isInterface       false</span><br><span class=\"line\"> isAnnotation      false</span><br><span class=\"line\"> isEnum            false</span><br><span class=\"line\"> isAnonymousClass  false</span><br><span class=\"line\"> isArray           false</span><br><span class=\"line\"> isLocalClass      false</span><br><span class=\"line\"> isMemberClass     false</span><br><span class=\"line\"> isPrimitive       false</span><br><span class=\"line\"> isSynthetic       false</span><br><span class=\"line\"> simple-name       SaslDataTransferClient</span><br><span class=\"line\"> modifier          public</span><br><span class=\"line\"> annotation        org.apache.hadoop.classification.InterfaceAudience$Private</span><br><span class=\"line\"> interfaces</span><br><span class=\"line\"> super-class       +-java.lang.Object</span><br><span class=\"line\"> class-loader      +-sun.misc.Launcher$AppClassLoader@5cad8086</span><br><span class=\"line\">                     +-sun.misc.Launcher$ExtClassLoader@340f438e</span><br><span class=\"line\"> classLoaderHash   5cad8086</span><br><span class=\"line\"></span><br><span class=\"line\">Affect(row-cnt:1) cost in 22 ms.</span><br></pre></td></tr></table></figure>\n\n<p>可以发现该类的路径为：&#x2F;opt&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;hadoop-hdfs-client-3.2.1.jar。</p>\n</li>\n<li><p>2、通过对比Hadoop各个版本的源码，发现是Hadoop3.2.1版本中存在的一个小Bug，而线上集群正是这个版本,随后在，晦气~。<strong>这应该是开发人员粗心导致的一个小Bug，最终导致了日志的疯狂打印，影响服务稳定性。这提醒我们不要随意提升Logger Level。</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(hudi12.1依赖的版本源码)hadoop2.10.1 version -&gt; LOG.debug(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br><span class=\"line\">(线上集群依赖的版本源码)hadoop3.2.1 version -&gt; LOG.info(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br><span class=\"line\">(该Bug修复的版本源码)hadoop3.2.2 version -&gt; LOG.debug(&quot;SASL encryption trust check: localHostTrusted = &#123;&#125;, remoteHostTrusted = &#123;&#125;&quot;, localTrusted, remoteTrusted);</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"修复Kylin4.0.x不正常的push-down query查询耗时","abbrlink":3347,"date":"2023-01-11T09:57:25.000Z","updated":"2023-01-12T09:57:25.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 问题描述\n\n> 发现kylin4.0.x中的push-down query对于简单的明细查询`select * from table limit 10`非常慢，本来应该秒级响应，却往往耗时几分钟，并且查询的数据集越大，耗时越长,这非常不正常。BI工具往往会执行明细查询，进行数据展示，不正常的查询时长，往往造成BI工具超时，返回错误信息，这对用户体验非常不友好\n>\n> 通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。\n>\n> 线上定位问题代码当然离不开Arthas了，然后仔细阅读Kylin源码，找到问题代码所在！！！\n\n## 修改源码\n\n- Kylin执行push-down query的主要逻辑集中在`org.apache.kylin.query.pushdown.SparkSqlClient`中，代码质量简直不忍直视，出现这个问题的主要原因就是代码质量太低。\n\n- 在`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`中，不必要的Spark DataFrame类型转换transform是这个问题的主要原因。\n\n- 修改后的代码如下`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`：\n\n  ```scala\n  private def dfToList(ss: SparkSession, sql: String, df: DataFrame): Pair[JList[JList[String]], JList[StructField]] = {\n  \tval jobGroup = Thread.currentThread.getName\n  \tss.sparkContext.setJobGroup(jobGroup,\n  \t\t\"Pushdown Query Id: \" + QueryContextFacade.current().getQueryId, interruptOnCancel = true)\n  \ttry {\n  \t\tval rowList = df.collect().map(_.toSeq.map(String.valueOf).asJava).toSeq.asJava\n  \t\tval fieldList = df.schema.map(field => SparkTypeUtil.convertSparkFieldToJavaField(field)).asJava\n  \t\tval (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = QueryMetricUtils.collectScanMetrics(df.queryExecution.executedPlan)\n  \t\tQueryContextFacade.current().addAndGetScannedRows(scanRows.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanFiles(scanFiles.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScannedBytes(scanBytes.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetMetadataTime(metadataTime.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanTime(scanTime.asScala.map(Long2long(_)).sum)\n  \t\tPair.newPair(rowList, fieldList)\n  \t} catch {\n  \t\tcase e: Throwable =>\n  \t\t\tif (e.isInstanceOf[InterruptedException]) {\n  \t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)\n  \t\t\t\tlogger.info(\"Query timeout \", e)\n  \t\t\t\tThread.currentThread.interrupt()\n  \t\t\t\tthrow new KylinTimeoutException(\"Query timeout after: \" + KylinConfig.getInstanceFromEnv.getQueryTimeoutSeconds + \"s\")\n  \t\t\t}\n  \t\t\telse {\n  \t\t\t\tthrow e\n  \t\t\t}\n  \t} finally {\n  \t\tHadoopUtil.setCurrentConfiguration(_)\n  \t}\n  }\n  ```\n\n- 修改前的代码`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`：\n\n  ```scala\n  private def DFToList(ss: SparkSession, sql: String, df: DataFrame): Pair[JList[JList[String]], JList[StructField]] = {\n  \tval jobGroup = Thread.currentThread.getName\n  \tss.sparkContext.setJobGroup(jobGroup,\n  \t\t\"Pushdown Query Id: \" + QueryContextFacade.current().getQueryId, interruptOnCancel = true)\n  \ttry {\n  \t\tval temporarySchema = df.schema.fields.zipWithIndex.map {\n  \t\t\tcase (_, index) => s\"temporary_$index\"\n  \t\t}\n  \t\tval tempDF = df.toDF(temporarySchema: _*)\n  \t\tval columns = tempDF.schema.map(tp => col(s\"`${tp.name}`\").cast(StringType))\n  \t\tval frame = tempDF.select(columns: _*)\n  \t\tval rowList = frame.collect().map(_.toSeq.map(_.asInstanceOf[String]).asJava).toSeq.asJava\n  \t\tval fieldList = df.schema.map(field => SparkTypeUtil.convertSparkFieldToJavaField(field)).asJava\n  \t\tval (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = QueryMetricUtils.collectScanMetrics(frame.queryExecution.executedPlan)\n  \t\tQueryContextFacade.current().addAndGetScannedRows(scanRows.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanFiles(scanFiles.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScannedBytes(scanBytes.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetMetadataTime(metadataTime.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanTime(scanTime.asScala.map(Long2long(_)).sum)\n  \t\tPair.newPair(rowList, fieldList)\n  \t} catch {\n  \t\tcase e: Throwable =>\n  \t\t\tif (e.isInstanceOf[InterruptedException]) {\n  \t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)\n  \t\t\t\tlogger.info(\"Query timeout \", e)\n  \t\t\t\tThread.currentThread.interrupt()\n  \t\t\t\tthrow new KylinTimeoutException(\"Query timeout after: \" + KylinConfig.getInstanceFromEnv.getQueryTimeoutSeconds + \"s\")\n  \t\t\t}\n  \t\t\telse throw e\n  \t} finally {\n  \t\tHadoopUtil.setCurrentConfiguration(null)\n  \t}\n  }\n  ```\n\n## 修复\n\n- 1、`mvn clean package -DskipTests -pl kylin-spark-project/kylin-spark-query`重新编译此模块。\n- 2、用重新编译生成的jar包对线上相应的jar包进行替换。\n- 3、重启Kylin，问题解决！","source":"_posts/bigdata/TroubleShooting/修复Kylin4.0.x不正常的push-down query.md","raw":"---\ntitle: 修复Kylin4.0.x不正常的push-down query查询耗时\ntags:\n  - Troubleshooting\n  - Kylin\ncategories:\n  - - Troubleshooting\n    - Kylin\nabbrlink: 3347\ndate: 2023-01-11 17:57:25\nupdated: 2023-01-12 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 问题描述\n\n> 发现kylin4.0.x中的push-down query对于简单的明细查询`select * from table limit 10`非常慢，本来应该秒级响应，却往往耗时几分钟，并且查询的数据集越大，耗时越长,这非常不正常。BI工具往往会执行明细查询，进行数据展示，不正常的查询时长，往往造成BI工具超时，返回错误信息，这对用户体验非常不友好\n>\n> 通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。\n>\n> 线上定位问题代码当然离不开Arthas了，然后仔细阅读Kylin源码，找到问题代码所在！！！\n\n## 修改源码\n\n- Kylin执行push-down query的主要逻辑集中在`org.apache.kylin.query.pushdown.SparkSqlClient`中，代码质量简直不忍直视，出现这个问题的主要原因就是代码质量太低。\n\n- 在`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`中，不必要的Spark DataFrame类型转换transform是这个问题的主要原因。\n\n- 修改后的代码如下`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`：\n\n  ```scala\n  private def dfToList(ss: SparkSession, sql: String, df: DataFrame): Pair[JList[JList[String]], JList[StructField]] = {\n  \tval jobGroup = Thread.currentThread.getName\n  \tss.sparkContext.setJobGroup(jobGroup,\n  \t\t\"Pushdown Query Id: \" + QueryContextFacade.current().getQueryId, interruptOnCancel = true)\n  \ttry {\n  \t\tval rowList = df.collect().map(_.toSeq.map(String.valueOf).asJava).toSeq.asJava\n  \t\tval fieldList = df.schema.map(field => SparkTypeUtil.convertSparkFieldToJavaField(field)).asJava\n  \t\tval (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = QueryMetricUtils.collectScanMetrics(df.queryExecution.executedPlan)\n  \t\tQueryContextFacade.current().addAndGetScannedRows(scanRows.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanFiles(scanFiles.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScannedBytes(scanBytes.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetMetadataTime(metadataTime.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanTime(scanTime.asScala.map(Long2long(_)).sum)\n  \t\tPair.newPair(rowList, fieldList)\n  \t} catch {\n  \t\tcase e: Throwable =>\n  \t\t\tif (e.isInstanceOf[InterruptedException]) {\n  \t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)\n  \t\t\t\tlogger.info(\"Query timeout \", e)\n  \t\t\t\tThread.currentThread.interrupt()\n  \t\t\t\tthrow new KylinTimeoutException(\"Query timeout after: \" + KylinConfig.getInstanceFromEnv.getQueryTimeoutSeconds + \"s\")\n  \t\t\t}\n  \t\t\telse {\n  \t\t\t\tthrow e\n  \t\t\t}\n  \t} finally {\n  \t\tHadoopUtil.setCurrentConfiguration(_)\n  \t}\n  }\n  ```\n\n- 修改前的代码`org.apache.kylin.query.pushdown.SparkSqlClient#DFToList`：\n\n  ```scala\n  private def DFToList(ss: SparkSession, sql: String, df: DataFrame): Pair[JList[JList[String]], JList[StructField]] = {\n  \tval jobGroup = Thread.currentThread.getName\n  \tss.sparkContext.setJobGroup(jobGroup,\n  \t\t\"Pushdown Query Id: \" + QueryContextFacade.current().getQueryId, interruptOnCancel = true)\n  \ttry {\n  \t\tval temporarySchema = df.schema.fields.zipWithIndex.map {\n  \t\t\tcase (_, index) => s\"temporary_$index\"\n  \t\t}\n  \t\tval tempDF = df.toDF(temporarySchema: _*)\n  \t\tval columns = tempDF.schema.map(tp => col(s\"`${tp.name}`\").cast(StringType))\n  \t\tval frame = tempDF.select(columns: _*)\n  \t\tval rowList = frame.collect().map(_.toSeq.map(_.asInstanceOf[String]).asJava).toSeq.asJava\n  \t\tval fieldList = df.schema.map(field => SparkTypeUtil.convertSparkFieldToJavaField(field)).asJava\n  \t\tval (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = QueryMetricUtils.collectScanMetrics(frame.queryExecution.executedPlan)\n  \t\tQueryContextFacade.current().addAndGetScannedRows(scanRows.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanFiles(scanFiles.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScannedBytes(scanBytes.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetMetadataTime(metadataTime.asScala.map(Long2long(_)).sum)\n  \t\tQueryContextFacade.current().addAndGetScanTime(scanTime.asScala.map(Long2long(_)).sum)\n  \t\tPair.newPair(rowList, fieldList)\n  \t} catch {\n  \t\tcase e: Throwable =>\n  \t\t\tif (e.isInstanceOf[InterruptedException]) {\n  \t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)\n  \t\t\t\tlogger.info(\"Query timeout \", e)\n  \t\t\t\tThread.currentThread.interrupt()\n  \t\t\t\tthrow new KylinTimeoutException(\"Query timeout after: \" + KylinConfig.getInstanceFromEnv.getQueryTimeoutSeconds + \"s\")\n  \t\t\t}\n  \t\t\telse throw e\n  \t} finally {\n  \t\tHadoopUtil.setCurrentConfiguration(null)\n  \t}\n  }\n  ```\n\n## 修复\n\n- 1、`mvn clean package -DskipTests -pl kylin-spark-project/kylin-spark-query`重新编译此模块。\n- 2、用重新编译生成的jar包对线上相应的jar包进行替换。\n- 3、重启Kylin，问题解决！","slug":"bigdata/TroubleShooting/修复Kylin4.0.x不正常的push-down query","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt2003b8j5m8o5khunh","content":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><blockquote>\n<p>发现kylin4.0.x中的push-down query对于简单的明细查询<code>select * from table limit 10</code>非常慢，本来应该秒级响应，却往往耗时几分钟，并且查询的数据集越大，耗时越长,这非常不正常。BI工具往往会执行明细查询，进行数据展示，不正常的查询时长，往往造成BI工具超时，返回错误信息，这对用户体验非常不友好</p>\n<p>通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。</p>\n<p>线上定位问题代码当然离不开Arthas了，然后仔细阅读Kylin源码，找到问题代码所在！！！</p>\n</blockquote>\n<h2 id=\"修改源码\"><a href=\"#修改源码\" class=\"headerlink\" title=\"修改源码\"></a>修改源码</h2><ul>\n<li><p>Kylin执行push-down query的主要逻辑集中在<code>org.apache.kylin.query.pushdown.SparkSqlClient</code>中，代码质量简直不忍直视，出现这个问题的主要原因就是代码质量太低。</p>\n</li>\n<li><p>在<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>中，不必要的Spark DataFrame类型转换transform是这个问题的主要原因。</p>\n</li>\n<li><p>修改后的代码如下<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfToList</span></span>(ss: <span class=\"type\">SparkSession</span>, sql: <span class=\"type\">String</span>, df: <span class=\"type\">DataFrame</span>): <span class=\"type\">Pair</span>[<span class=\"type\">JList</span>[<span class=\"type\">JList</span>[<span class=\"type\">String</span>]], <span class=\"type\">JList</span>[<span class=\"type\">StructField</span>]] = &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobGroup = <span class=\"type\">Thread</span>.currentThread.getName</span><br><span class=\"line\">\tss.sparkContext.setJobGroup(jobGroup,</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;Pushdown Query Id: &quot;</span> + <span class=\"type\">QueryContextFacade</span>.current().getQueryId, interruptOnCancel = <span class=\"literal\">true</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> rowList = df.collect().map(_.toSeq.map(<span class=\"type\">String</span>.valueOf).asJava).toSeq.asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> fieldList = df.schema.map(field =&gt; <span class=\"type\">SparkTypeUtil</span>.convertSparkFieldToJavaField(field)).asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = <span class=\"type\">QueryMetricUtils</span>.collectScanMetrics(df.queryExecution.executedPlan)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedRows(scanRows.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanFiles(scanFiles.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedBytes(scanBytes.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetMetadataTime(metadataTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanTime(scanTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">Pair</span>.newPair(rowList, fieldList)</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (e.isInstanceOf[<span class=\"type\">InterruptedException</span>]) &#123;</span><br><span class=\"line\">\t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)</span><br><span class=\"line\">\t\t\t\tlogger.info(<span class=\"string\">&quot;Query timeout &quot;</span>, e)</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Thread</span>.currentThread.interrupt()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">KylinTimeoutException</span>(<span class=\"string\">&quot;Query timeout after: &quot;</span> + <span class=\"type\">KylinConfig</span>.getInstanceFromEnv.getQueryTimeoutSeconds + <span class=\"string\">&quot;s&quot;</span>)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> e</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">HadoopUtil</span>.setCurrentConfiguration(_)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改前的代码<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">DFToList</span></span>(ss: <span class=\"type\">SparkSession</span>, sql: <span class=\"type\">String</span>, df: <span class=\"type\">DataFrame</span>): <span class=\"type\">Pair</span>[<span class=\"type\">JList</span>[<span class=\"type\">JList</span>[<span class=\"type\">String</span>]], <span class=\"type\">JList</span>[<span class=\"type\">StructField</span>]] = &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobGroup = <span class=\"type\">Thread</span>.currentThread.getName</span><br><span class=\"line\">\tss.sparkContext.setJobGroup(jobGroup,</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;Pushdown Query Id: &quot;</span> + <span class=\"type\">QueryContextFacade</span>.current().getQueryId, interruptOnCancel = <span class=\"literal\">true</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> temporarySchema = df.schema.fields.zipWithIndex.map &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> (_, index) =&gt; <span class=\"string\">s&quot;temporary_<span class=\"subst\">$index</span>&quot;</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> tempDF = df.toDF(temporarySchema: _*)</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> columns = tempDF.schema.map(tp =&gt; col(<span class=\"string\">s&quot;`<span class=\"subst\">$&#123;tp.name&#125;</span>`&quot;</span>).cast(<span class=\"type\">StringType</span>))</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> frame = tempDF.select(columns: _*)</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> rowList = frame.collect().map(_.toSeq.map(_.asInstanceOf[<span class=\"type\">String</span>]).asJava).toSeq.asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> fieldList = df.schema.map(field =&gt; <span class=\"type\">SparkTypeUtil</span>.convertSparkFieldToJavaField(field)).asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = <span class=\"type\">QueryMetricUtils</span>.collectScanMetrics(frame.queryExecution.executedPlan)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedRows(scanRows.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanFiles(scanFiles.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedBytes(scanBytes.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetMetadataTime(metadataTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanTime(scanTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">Pair</span>.newPair(rowList, fieldList)</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (e.isInstanceOf[<span class=\"type\">InterruptedException</span>]) &#123;</span><br><span class=\"line\">\t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)</span><br><span class=\"line\">\t\t\t\tlogger.info(<span class=\"string\">&quot;Query timeout &quot;</span>, e)</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Thread</span>.currentThread.interrupt()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">KylinTimeoutException</span>(<span class=\"string\">&quot;Query timeout after: &quot;</span> + <span class=\"type\">KylinConfig</span>.getInstanceFromEnv.getQueryTimeoutSeconds + <span class=\"string\">&quot;s&quot;</span>)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">HadoopUtil</span>.setCurrentConfiguration(<span class=\"literal\">null</span>)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"修复\"><a href=\"#修复\" class=\"headerlink\" title=\"修复\"></a>修复</h2><ul>\n<li>1、<code>mvn clean package -DskipTests -pl kylin-spark-project/kylin-spark-query</code>重新编译此模块。</li>\n<li>2、用重新编译生成的jar包对线上相应的jar包进行替换。</li>\n<li>3、重启Kylin，问题解决！</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><blockquote>\n<p>发现kylin4.0.x中的push-down query对于简单的明细查询<code>select * from table limit 10</code>非常慢，本来应该秒级响应，却往往耗时几分钟，并且查询的数据集越大，耗时越长,这非常不正常。BI工具往往会执行明细查询，进行数据展示，不正常的查询时长，往往造成BI工具超时，返回错误信息，这对用户体验非常不友好</p>\n<p>通过排查发现，在这类非常简单的明细查询的查询计划中，竟然有shuffle过程，简直离谱。</p>\n<p>线上定位问题代码当然离不开Arthas了，然后仔细阅读Kylin源码，找到问题代码所在！！！</p>\n</blockquote>\n<h2 id=\"修改源码\"><a href=\"#修改源码\" class=\"headerlink\" title=\"修改源码\"></a>修改源码</h2><ul>\n<li><p>Kylin执行push-down query的主要逻辑集中在<code>org.apache.kylin.query.pushdown.SparkSqlClient</code>中，代码质量简直不忍直视，出现这个问题的主要原因就是代码质量太低。</p>\n</li>\n<li><p>在<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>中，不必要的Spark DataFrame类型转换transform是这个问题的主要原因。</p>\n</li>\n<li><p>修改后的代码如下<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfToList</span></span>(ss: <span class=\"type\">SparkSession</span>, sql: <span class=\"type\">String</span>, df: <span class=\"type\">DataFrame</span>): <span class=\"type\">Pair</span>[<span class=\"type\">JList</span>[<span class=\"type\">JList</span>[<span class=\"type\">String</span>]], <span class=\"type\">JList</span>[<span class=\"type\">StructField</span>]] = &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobGroup = <span class=\"type\">Thread</span>.currentThread.getName</span><br><span class=\"line\">\tss.sparkContext.setJobGroup(jobGroup,</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;Pushdown Query Id: &quot;</span> + <span class=\"type\">QueryContextFacade</span>.current().getQueryId, interruptOnCancel = <span class=\"literal\">true</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> rowList = df.collect().map(_.toSeq.map(<span class=\"type\">String</span>.valueOf).asJava).toSeq.asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> fieldList = df.schema.map(field =&gt; <span class=\"type\">SparkTypeUtil</span>.convertSparkFieldToJavaField(field)).asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = <span class=\"type\">QueryMetricUtils</span>.collectScanMetrics(df.queryExecution.executedPlan)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedRows(scanRows.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanFiles(scanFiles.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedBytes(scanBytes.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetMetadataTime(metadataTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanTime(scanTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">Pair</span>.newPair(rowList, fieldList)</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (e.isInstanceOf[<span class=\"type\">InterruptedException</span>]) &#123;</span><br><span class=\"line\">\t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)</span><br><span class=\"line\">\t\t\t\tlogger.info(<span class=\"string\">&quot;Query timeout &quot;</span>, e)</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Thread</span>.currentThread.interrupt()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">KylinTimeoutException</span>(<span class=\"string\">&quot;Query timeout after: &quot;</span> + <span class=\"type\">KylinConfig</span>.getInstanceFromEnv.getQueryTimeoutSeconds + <span class=\"string\">&quot;s&quot;</span>)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> e</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">HadoopUtil</span>.setCurrentConfiguration(_)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改前的代码<code>org.apache.kylin.query.pushdown.SparkSqlClient#DFToList</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">DFToList</span></span>(ss: <span class=\"type\">SparkSession</span>, sql: <span class=\"type\">String</span>, df: <span class=\"type\">DataFrame</span>): <span class=\"type\">Pair</span>[<span class=\"type\">JList</span>[<span class=\"type\">JList</span>[<span class=\"type\">String</span>]], <span class=\"type\">JList</span>[<span class=\"type\">StructField</span>]] = &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> jobGroup = <span class=\"type\">Thread</span>.currentThread.getName</span><br><span class=\"line\">\tss.sparkContext.setJobGroup(jobGroup,</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;Pushdown Query Id: &quot;</span> + <span class=\"type\">QueryContextFacade</span>.current().getQueryId, interruptOnCancel = <span class=\"literal\">true</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> temporarySchema = df.schema.fields.zipWithIndex.map &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">case</span> (_, index) =&gt; <span class=\"string\">s&quot;temporary_<span class=\"subst\">$index</span>&quot;</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> tempDF = df.toDF(temporarySchema: _*)</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> columns = tempDF.schema.map(tp =&gt; col(<span class=\"string\">s&quot;`<span class=\"subst\">$&#123;tp.name&#125;</span>`&quot;</span>).cast(<span class=\"type\">StringType</span>))</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> frame = tempDF.select(columns: _*)</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> rowList = frame.collect().map(_.toSeq.map(_.asInstanceOf[<span class=\"type\">String</span>]).asJava).toSeq.asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> fieldList = df.schema.map(field =&gt; <span class=\"type\">SparkTypeUtil</span>.convertSparkFieldToJavaField(field)).asJava</span><br><span class=\"line\">\t\t<span class=\"keyword\">val</span> (scanRows, scanFiles, metadataTime, scanTime, scanBytes) = <span class=\"type\">QueryMetricUtils</span>.collectScanMetrics(frame.queryExecution.executedPlan)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedRows(scanRows.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanFiles(scanFiles.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScannedBytes(scanBytes.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetMetadataTime(metadataTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">QueryContextFacade</span>.current().addAndGetScanTime(scanTime.asScala.map(<span class=\"type\">Long2long</span>(_)).sum)</span><br><span class=\"line\">\t\t<span class=\"type\">Pair</span>.newPair(rowList, fieldList)</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (e.isInstanceOf[<span class=\"type\">InterruptedException</span>]) &#123;</span><br><span class=\"line\">\t\t\t\tss.sparkContext.cancelJobGroup(jobGroup)</span><br><span class=\"line\">\t\t\t\tlogger.info(<span class=\"string\">&quot;Query timeout &quot;</span>, e)</span><br><span class=\"line\">\t\t\t\t<span class=\"type\">Thread</span>.currentThread.interrupt()</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">KylinTimeoutException</span>(<span class=\"string\">&quot;Query timeout after: &quot;</span> + <span class=\"type\">KylinConfig</span>.getInstanceFromEnv.getQueryTimeoutSeconds + <span class=\"string\">&quot;s&quot;</span>)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">HadoopUtil</span>.setCurrentConfiguration(<span class=\"literal\">null</span>)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"修复\"><a href=\"#修复\" class=\"headerlink\" title=\"修复\"></a>修复</h2><ul>\n<li>1、<code>mvn clean package -DskipTests -pl kylin-spark-project/kylin-spark-query</code>重新编译此模块。</li>\n<li>2、用重新编译生成的jar包对线上相应的jar包进行替换。</li>\n<li>3、重启Kylin，问题解决！</li>\n</ul>\n"},{"title":"thrift-从入门到放弃","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":11082,"date":"2022-08-31T13:07:13.000Z","updated":"2022-08-31T13:07:13.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## Thrift-Java-Maven使用指北\n\n- 1、下载安装Thrift，配置Thrift环境变量\n\n- 2、Maven中引入libthrift依赖\n\n  ```xml\n  <dependency>\n     <groupId>org.apache.thrift</groupId>\n     <artifactId>libthrift</artifactId>\n     <version>0.14.1</version>\n  </dependency>\n  ```\n\n- 3、引入Maven插件maven-thrift-plugin\n\n  ```xml\n  <build>\n        <plugins>\n           <plugin>\n              <groupId>org.apache.thrift.tools</groupId>\n              <artifactId>maven-thrift-plugin</artifactId>\n              <version>0.1.11</version>\n              <configuration>\n                 <!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定-->\n                 <thriftExecutable>./thrift/thrift.exe</thriftExecutable>\n                 <!--指定待编译的  IDL文件目录，默认为src/main/thrift-->\n                 <thriftSourceRoot>src/main/resources/thrift</thriftSourceRoot>\n                 <!--在0.1.10版本后的plugin需要添加的参数-->\n                 <generator>java</generator> \n                 <!--指定编译输出目录-->\n                 <outputDirectory>src/main/java</outputDirectory>\n              </configuration>\n           </plugin>\n        </plugins>\n     </build>\n  ```\n\n  >  然后通过执行plugin 的compile指令即可将文件直接编译转化为java类，注意有些版本需要添加<generator>java</generator>，否则可能会报错：[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode。\n  >\n  > \n  >\n  > 同时，如果我们像上面一样指定了编译输出目录为项目目录，会覆盖原有目录下的文件，所以可以保持默认配置，输出至target目录下，然后复制到我们想要的package下。\n\n## FQA\n\n- 执行mvn clean install编译失败\n\n  > [ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode\n  >\n  > [ERROR] Failed to execute goal org.apache.thrift.tools:maven-thrift-plugin:0.1.11:compile (thrift-sources) on project HelloService: thrift did n\n  > ot exit cleanly. Review output for more information. -> [Help 1]\n\n  Maven插件maven-thrift-plugin配置中添加`<generator>java</generator>`\n\n  ```xml\n  \t\t<plugin>\n              <groupId>org.apache.thrift.tools</groupId>\n              <artifactId>maven-thrift-plugin</artifactId>\n              <version>0.1.11</version>\n              <configuration>\n                 <!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定-->\n                 <thriftExecutable>./thrift/thrift.exe</thriftExecutable>\n                 <!--指定待编译的  IDL文件目录，默认为src/main/thrift-->\n                 <thriftSourceRoot>src/main/resources/thrift</thriftSourceRoot>\n                 <!--在0.1.10版本后的plugin需要添加的参数-->\n                 <generator>java</generator> \n                 <!--指定编译输出目录-->\n                 <outputDirectory>src/main/java</outputDirectory>\n              </configuration>\n           </plugin>\n  ```\n\n  \n\n- Thrift生成的java文件，无法被import引用，原因是Thrift生成的java文件路径不对\n\n  >1、<outputDirectory></outputDirectory>配置为<outputDirectory>src/main/java</outputDirectory>\n  >\n  >2、将生成的目录在IDEA中指定为源文件目录：\n  >\n  >![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661953198633-ec36aeea-e0ef-4398-beaa-cbefdef85f3d.png)\n","source":"_posts/rpc/thrift-从入门到放弃.md","raw":"---\ntitle: thrift-从入门到放弃\ntags:\n  - thrift\ncategories:\n  - - RPC\n    - thrift\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 11082\ndate: 2022-08-31 21:07:13\nupdated: 2022-08-31 21:07:13\ncover:\ndescription:\nkeywords:\n---\n\n## Thrift-Java-Maven使用指北\n\n- 1、下载安装Thrift，配置Thrift环境变量\n\n- 2、Maven中引入libthrift依赖\n\n  ```xml\n  <dependency>\n     <groupId>org.apache.thrift</groupId>\n     <artifactId>libthrift</artifactId>\n     <version>0.14.1</version>\n  </dependency>\n  ```\n\n- 3、引入Maven插件maven-thrift-plugin\n\n  ```xml\n  <build>\n        <plugins>\n           <plugin>\n              <groupId>org.apache.thrift.tools</groupId>\n              <artifactId>maven-thrift-plugin</artifactId>\n              <version>0.1.11</version>\n              <configuration>\n                 <!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定-->\n                 <thriftExecutable>./thrift/thrift.exe</thriftExecutable>\n                 <!--指定待编译的  IDL文件目录，默认为src/main/thrift-->\n                 <thriftSourceRoot>src/main/resources/thrift</thriftSourceRoot>\n                 <!--在0.1.10版本后的plugin需要添加的参数-->\n                 <generator>java</generator> \n                 <!--指定编译输出目录-->\n                 <outputDirectory>src/main/java</outputDirectory>\n              </configuration>\n           </plugin>\n        </plugins>\n     </build>\n  ```\n\n  >  然后通过执行plugin 的compile指令即可将文件直接编译转化为java类，注意有些版本需要添加<generator>java</generator>，否则可能会报错：[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode。\n  >\n  > \n  >\n  > 同时，如果我们像上面一样指定了编译输出目录为项目目录，会覆盖原有目录下的文件，所以可以保持默认配置，输出至target目录下，然后复制到我们想要的package下。\n\n## FQA\n\n- 执行mvn clean install编译失败\n\n  > [ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode\n  >\n  > [ERROR] Failed to execute goal org.apache.thrift.tools:maven-thrift-plugin:0.1.11:compile (thrift-sources) on project HelloService: thrift did n\n  > ot exit cleanly. Review output for more information. -> [Help 1]\n\n  Maven插件maven-thrift-plugin配置中添加`<generator>java</generator>`\n\n  ```xml\n  \t\t<plugin>\n              <groupId>org.apache.thrift.tools</groupId>\n              <artifactId>maven-thrift-plugin</artifactId>\n              <version>0.1.11</version>\n              <configuration>\n                 <!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定-->\n                 <thriftExecutable>./thrift/thrift.exe</thriftExecutable>\n                 <!--指定待编译的  IDL文件目录，默认为src/main/thrift-->\n                 <thriftSourceRoot>src/main/resources/thrift</thriftSourceRoot>\n                 <!--在0.1.10版本后的plugin需要添加的参数-->\n                 <generator>java</generator> \n                 <!--指定编译输出目录-->\n                 <outputDirectory>src/main/java</outputDirectory>\n              </configuration>\n           </plugin>\n  ```\n\n  \n\n- Thrift生成的java文件，无法被import引用，原因是Thrift生成的java文件路径不对\n\n  >1、<outputDirectory></outputDirectory>配置为<outputDirectory>src/main/java</outputDirectory>\n  >\n  >2、将生成的目录在IDEA中指定为源文件目录：\n  >\n  >![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1661953198633-ec36aeea-e0ef-4398-beaa-cbefdef85f3d.png)\n","slug":"rpc/thrift-从入门到放弃","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt2003e8j5m4srz4x6k","content":"<h2 id=\"Thrift-Java-Maven使用指北\"><a href=\"#Thrift-Java-Maven使用指北\" class=\"headerlink\" title=\"Thrift-Java-Maven使用指北\"></a>Thrift-Java-Maven使用指北</h2><ul>\n<li><p>1、下载安装Thrift，配置Thrift环境变量</p>\n</li>\n<li><p>2、Maven中引入libthrift依赖</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>libthrift<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.14.1<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、引入Maven插件maven-thrift-plugin</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">build</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">plugins</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift.tools<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>maven-thrift-plugin<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.1.11<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">thriftExecutable</span>&gt;</span>./thrift/thrift.exe<span class=\"tag\">&lt;/<span class=\"name\">thriftExecutable</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定待编译的  IDL文件目录，默认为src/main/thrift--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">thriftSourceRoot</span>&gt;</span>src/main/resources/thrift<span class=\"tag\">&lt;/<span class=\"name\">thriftSourceRoot</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--在0.1.10版本后的plugin需要添加的参数--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">generator</span>&gt;</span>java<span class=\"tag\">&lt;/<span class=\"name\">generator</span>&gt;</span> </span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定编译输出目录--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">outputDirectory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">outputDirectory</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;/<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;/<span class=\"name\">plugins</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">build</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p> 然后通过执行plugin 的compile指令即可将文件直接编译转化为java类，注意有些版本需要添加<generator>java</generator>，否则可能会报错：[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode。</p>\n<p>同时，如果我们像上面一样指定了编译输出目录为项目目录，会覆盖原有目录下的文件，所以可以保持默认配置，输出至target目录下，然后复制到我们想要的package下。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"FQA\"><a href=\"#FQA\" class=\"headerlink\" title=\"FQA\"></a>FQA</h2><ul>\n<li><p>执行mvn clean install编译失败</p>\n<blockquote>\n<p>[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode</p>\n<p>[ERROR] Failed to execute goal org.apache.thrift.tools:maven-thrift-plugin:0.1.11:compile (thrift-sources) on project HelloService: thrift did n<br>ot exit cleanly. Review output for more information. -&gt; [Help 1]</p>\n</blockquote>\n<p>Maven插件maven-thrift-plugin配置中添加<code>&lt;generator&gt;java&lt;/generator&gt;</code></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift.tools<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>maven-thrift-plugin<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.1.11<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">thriftExecutable</span>&gt;</span>./thrift/thrift.exe<span class=\"tag\">&lt;/<span class=\"name\">thriftExecutable</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定待编译的  IDL文件目录，默认为src/main/thrift--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">thriftSourceRoot</span>&gt;</span>src/main/resources/thrift<span class=\"tag\">&lt;/<span class=\"name\">thriftSourceRoot</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--在0.1.10版本后的plugin需要添加的参数--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">generator</span>&gt;</span>java<span class=\"tag\">&lt;/<span class=\"name\">generator</span>&gt;</span> </span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定编译输出目录--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">outputDirectory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">outputDirectory</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;/<span class=\"name\">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>Thrift生成的java文件，无法被import引用，原因是Thrift生成的java文件路径不对</p>\n<blockquote>\n<p>1、<outputDirectory></outputDirectory>配置为<outputDirectory>src&#x2F;main&#x2F;java</outputDirectory></p>\n<p>2、将生成的目录在IDEA中指定为源文件目录：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661953198633-ec36aeea-e0ef-4398-beaa-cbefdef85f3d.png\" alt=\"img\"></p>\n</blockquote>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"Thrift-Java-Maven使用指北\"><a href=\"#Thrift-Java-Maven使用指北\" class=\"headerlink\" title=\"Thrift-Java-Maven使用指北\"></a>Thrift-Java-Maven使用指北</h2><ul>\n<li><p>1、下载安装Thrift，配置Thrift环境变量</p>\n</li>\n<li><p>2、Maven中引入libthrift依赖</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>libthrift<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.14.1<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、引入Maven插件maven-thrift-plugin</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">build</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">plugins</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift.tools<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>maven-thrift-plugin<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.1.11<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">thriftExecutable</span>&gt;</span>./thrift/thrift.exe<span class=\"tag\">&lt;/<span class=\"name\">thriftExecutable</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定待编译的  IDL文件目录，默认为src/main/thrift--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">thriftSourceRoot</span>&gt;</span>src/main/resources/thrift<span class=\"tag\">&lt;/<span class=\"name\">thriftSourceRoot</span>&gt;</span></span><br><span class=\"line\">               <span class=\"comment\">&lt;!--在0.1.10版本后的plugin需要添加的参数--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">generator</span>&gt;</span>java<span class=\"tag\">&lt;/<span class=\"name\">generator</span>&gt;</span> </span><br><span class=\"line\">               <span class=\"comment\">&lt;!--指定编译输出目录--&gt;</span></span><br><span class=\"line\">               <span class=\"tag\">&lt;<span class=\"name\">outputDirectory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">outputDirectory</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">         <span class=\"tag\">&lt;/<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;/<span class=\"name\">plugins</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">build</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p> 然后通过执行plugin 的compile指令即可将文件直接编译转化为java类，注意有些版本需要添加<generator>java</generator>，否则可能会报错：[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode。</p>\n<p>同时，如果我们像上面一样指定了编译输出目录为项目目录，会覆盖原有目录下的文件，所以可以保持默认配置，输出至target目录下，然后复制到我们想要的package下。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"FQA\"><a href=\"#FQA\" class=\"headerlink\" title=\"FQA\"></a>FQA</h2><ul>\n<li><p>执行mvn clean install编译失败</p>\n<blockquote>\n<p>[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown option java:hashcode</p>\n<p>[ERROR] Failed to execute goal org.apache.thrift.tools:maven-thrift-plugin:0.1.11:compile (thrift-sources) on project HelloService: thrift did n<br>ot exit cleanly. Review output for more information. -&gt; [Help 1]</p>\n</blockquote>\n<p>Maven插件maven-thrift-plugin配置中添加<code>&lt;generator&gt;java&lt;/generator&gt;</code></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">plugin</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.thrift.tools<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>maven-thrift-plugin<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.1.11<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定Thrift编译文件的目录和位置，设定环境变量便可不用指定--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">thriftExecutable</span>&gt;</span>./thrift/thrift.exe<span class=\"tag\">&lt;/<span class=\"name\">thriftExecutable</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定待编译的  IDL文件目录，默认为src/main/thrift--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">thriftSourceRoot</span>&gt;</span>src/main/resources/thrift<span class=\"tag\">&lt;/<span class=\"name\">thriftSourceRoot</span>&gt;</span></span><br><span class=\"line\">             <span class=\"comment\">&lt;!--在0.1.10版本后的plugin需要添加的参数--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">generator</span>&gt;</span>java<span class=\"tag\">&lt;/<span class=\"name\">generator</span>&gt;</span> </span><br><span class=\"line\">             <span class=\"comment\">&lt;!--指定编译输出目录--&gt;</span></span><br><span class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">outputDirectory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">outputDirectory</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;/<span class=\"name\">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>Thrift生成的java文件，无法被import引用，原因是Thrift生成的java文件路径不对</p>\n<blockquote>\n<p>1、<outputDirectory></outputDirectory>配置为<outputDirectory>src&#x2F;main&#x2F;java</outputDirectory></p>\n<p>2、将生成的目录在IDEA中指定为源文件目录：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1661953198633-ec36aeea-e0ef-4398-beaa-cbefdef85f3d.png\" alt=\"img\"></p>\n</blockquote>\n</li>\n</ul>\n"},{"title":"修复hudi metadata table与HDFS3.x不兼容的问题","abbrlink":4987,"date":"2022-12-28T09:57:25.000Z","updated":"2022-12-28T09:57:25.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n> From 0.11.0 release, we have upgraded the HBase version to 2.4.9, which is released based on Hadoop 2.x. Hudi's metadata table uses HFile as the base file format, relying on the HBase library. When enabling metadata table in a Hudi table on HDFS using Hadoop 3.x, NoSuchMethodError can be thrown due to compatibility issues between Hadoop 2.x and 3.x.\n\n\n\n## 简述\n\nhudi的metadata table使用HFile作为基础文件格式，HFile依赖于HBase库。在Hudi0.12.1中使用HBase2.4.9版本，HBase2.4.9默认构建在Hadoop2.X，因此在HDFS3.x上使用Hudi metadata table会出现兼容性问题。抛出NoSuchMethodException异常。\n\n## 修复方法\n\n- 1、Download HBase source code from `https://github.com/apache/hbase`，切换到git checkout rel/2.4.9分支。\n\n- 2、mvn install HBase2.4.9 with Hadoop3。构建基于Hadoop3的HBase2.4.9版本，mvn install安装到本地maven仓库，以便后续编译hudi的时候使用正确的版本。\n\n  ```shell\n  mvn clean install -Denforcer.skip -DskipTests -Dhadoop.profile=3.0 -Psite-install-step\n  ```\n\n- 3、分别重新编译hudi with Flink && Spark\n\n  - Spark3.2x\n\n    ```shell\n    mvn clean package -DskipTests -Dspark3.2 -Dscala-2.12\n    ```\n\n  - Flink1.14.x\n\n    ```shell\n    mvn clean package -DskipTests -Dflink1.14 -Dscala-2.11 -am -amd\n    ```\n\n## 后记\n\n- 这是一个由于第三方库（HBase HFile相关库）的依赖问题导致的Bug，因此需要先解决第三方的依赖问题，再解决Hudi的兼容性问题。","source":"_posts/bigdata/TroubleShooting/修复hudi metadata table与HDFS3.x不兼容的问题.md","raw":"---\ntitle: 修复hudi metadata table与HDFS3.x不兼容的问题\ntags:\n  - Troubleshooting\ncategories:\n  - - Troubleshooting\nabbrlink: 4987\ndate: 2022-12-28 17:57:25\nupdated: 2022-12-28 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n> From 0.11.0 release, we have upgraded the HBase version to 2.4.9, which is released based on Hadoop 2.x. Hudi's metadata table uses HFile as the base file format, relying on the HBase library. When enabling metadata table in a Hudi table on HDFS using Hadoop 3.x, NoSuchMethodError can be thrown due to compatibility issues between Hadoop 2.x and 3.x.\n\n\n\n## 简述\n\nhudi的metadata table使用HFile作为基础文件格式，HFile依赖于HBase库。在Hudi0.12.1中使用HBase2.4.9版本，HBase2.4.9默认构建在Hadoop2.X，因此在HDFS3.x上使用Hudi metadata table会出现兼容性问题。抛出NoSuchMethodException异常。\n\n## 修复方法\n\n- 1、Download HBase source code from `https://github.com/apache/hbase`，切换到git checkout rel/2.4.9分支。\n\n- 2、mvn install HBase2.4.9 with Hadoop3。构建基于Hadoop3的HBase2.4.9版本，mvn install安装到本地maven仓库，以便后续编译hudi的时候使用正确的版本。\n\n  ```shell\n  mvn clean install -Denforcer.skip -DskipTests -Dhadoop.profile=3.0 -Psite-install-step\n  ```\n\n- 3、分别重新编译hudi with Flink && Spark\n\n  - Spark3.2x\n\n    ```shell\n    mvn clean package -DskipTests -Dspark3.2 -Dscala-2.12\n    ```\n\n  - Flink1.14.x\n\n    ```shell\n    mvn clean package -DskipTests -Dflink1.14 -Dscala-2.11 -am -amd\n    ```\n\n## 后记\n\n- 这是一个由于第三方库（HBase HFile相关库）的依赖问题导致的Bug，因此需要先解决第三方的依赖问题，再解决Hudi的兼容性问题。","slug":"bigdata/TroubleShooting/修复hudi metadata table与HDFS3.x不兼容的问题","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt2003h8j5maqsk8ncg","content":"<blockquote>\n<p>From 0.11.0 release, we have upgraded the HBase version to 2.4.9, which is released based on Hadoop 2.x. Hudi’s metadata table uses HFile as the base file format, relying on the HBase library. When enabling metadata table in a Hudi table on HDFS using Hadoop 3.x, NoSuchMethodError can be thrown due to compatibility issues between Hadoop 2.x and 3.x.</p>\n</blockquote>\n<h2 id=\"简述\"><a href=\"#简述\" class=\"headerlink\" title=\"简述\"></a>简述</h2><p>hudi的metadata table使用HFile作为基础文件格式，HFile依赖于HBase库。在Hudi0.12.1中使用HBase2.4.9版本，HBase2.4.9默认构建在Hadoop2.X，因此在HDFS3.x上使用Hudi metadata table会出现兼容性问题。抛出NoSuchMethodException异常。</p>\n<h2 id=\"修复方法\"><a href=\"#修复方法\" class=\"headerlink\" title=\"修复方法\"></a>修复方法</h2><ul>\n<li><p>1、Download HBase source code from <code>https://github.com/apache/hbase</code>，切换到git checkout rel&#x2F;2.4.9分支。</p>\n</li>\n<li><p>2、mvn install HBase2.4.9 with Hadoop3。构建基于Hadoop3的HBase2.4.9版本，mvn install安装到本地maven仓库，以便后续编译hudi的时候使用正确的版本。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean install -Denforcer.skip -DskipTests -Dhadoop.profile=3.0 -Psite-install-step</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、分别重新编译hudi with Flink &amp;&amp; Spark</p>\n<ul>\n<li><p>Spark3.2x</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests -Dspark3.2 -Dscala-2.12</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Flink1.14.x</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests -Dflink1.14 -Dscala-2.11 -am -amd</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h2><ul>\n<li>这是一个由于第三方库（HBase HFile相关库）的依赖问题导致的Bug，因此需要先解决第三方的依赖问题，再解决Hudi的兼容性问题。</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>From 0.11.0 release, we have upgraded the HBase version to 2.4.9, which is released based on Hadoop 2.x. Hudi’s metadata table uses HFile as the base file format, relying on the HBase library. When enabling metadata table in a Hudi table on HDFS using Hadoop 3.x, NoSuchMethodError can be thrown due to compatibility issues between Hadoop 2.x and 3.x.</p>\n</blockquote>\n<h2 id=\"简述\"><a href=\"#简述\" class=\"headerlink\" title=\"简述\"></a>简述</h2><p>hudi的metadata table使用HFile作为基础文件格式，HFile依赖于HBase库。在Hudi0.12.1中使用HBase2.4.9版本，HBase2.4.9默认构建在Hadoop2.X，因此在HDFS3.x上使用Hudi metadata table会出现兼容性问题。抛出NoSuchMethodException异常。</p>\n<h2 id=\"修复方法\"><a href=\"#修复方法\" class=\"headerlink\" title=\"修复方法\"></a>修复方法</h2><ul>\n<li><p>1、Download HBase source code from <code>https://github.com/apache/hbase</code>，切换到git checkout rel&#x2F;2.4.9分支。</p>\n</li>\n<li><p>2、mvn install HBase2.4.9 with Hadoop3。构建基于Hadoop3的HBase2.4.9版本，mvn install安装到本地maven仓库，以便后续编译hudi的时候使用正确的版本。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean install -Denforcer.skip -DskipTests -Dhadoop.profile=3.0 -Psite-install-step</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、分别重新编译hudi with Flink &amp;&amp; Spark</p>\n<ul>\n<li><p>Spark3.2x</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests -Dspark3.2 -Dscala-2.12</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Flink1.14.x</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests -Dflink1.14 -Dscala-2.11 -am -amd</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h2><ul>\n<li>这是一个由于第三方库（HBase HFile相关库）的依赖问题导致的Bug，因此需要先解决第三方的依赖问题，再解决Hudi的兼容性问题。</li>\n</ul>\n"},{"title":"LSM基本概念与其经典实现Level-DB","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":61336,"date":"2023-04-30T09:57:25.000Z","updated":"2022-04-30T09:57:25.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## 前言\n\n> LSM 是以牺牲读取性能以及空间利用率为代价而换取顺序写入性能的。因此，对LSM结构的优化目标就是想办法提高读取性能和空间利用率。读取性能的瓶颈在于读写放大以及合并压缩过程的抖动。\n\n![](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/lsm-01.png)\n\n## LSM经典实现Level-DB：Level引入分层机制\n\n- 1、将最近最新写入的kv存储在内存数据结构中，如红黑树，跳表等。 那么问题是何时将此数据结构dump到磁盘?最简单的是根据其大小的区别，然而在dump之前我们不能继续向其中写入数据，因此在内存中应该存在一个活跃内存表和一个不变内存表，二者相互交替，周期性的将不变内存表dump到内存中形成一个分段文件。\n\n- 2、为了优化LSM的读取性能：\n\n  > LSM结构引入了**分层设计**的思想。将所有的kv文件分为c0-ck 共k+1层。c0层是直接从不变的内存表中dump下的结果。而c1-ck是发生过合并的文件。由于ci+1 是ci中具有重叠部分的文件合并的产物，因此可以说在同一层内是不存在重叠key的，因为重叠key已经在其上一层被合并了。那么只有c0层是可能存在重叠的文件的。所以当要读取磁盘上的数据时，最坏情况下只需要读取c0的所有文件以及c1-ck每一层中的一个文件即c0+k个文件即可找到key的位置，分层合并思想使得非就地更新索引在常数次的IO中读取数据。\n\n  > 通常c0文件定义为2M，每一级比上一级大一个数量级的文件大小。所以高层的文件难以被一次性的加载到内存，因此需要一定的磁盘**索引机制**。我们对每个磁盘文件进行布局设计，分为元数据块，索引块，数据块三大块。元数据块中存储布隆过滤器快速的判断这个文件中是否存在某个key，同时通过对排序索引(通常缓存在内存中)二分查找定位key所在磁盘的位置。进而加速读取的速度，我们叫这种数据文件为SSTABLE(字符串排序表)。\n\n  > 为了标记哪些SStable属于那一层因此要存在一个sstable的元数据管理文件，在levelDB中叫做MANIFEST文件。其中存储每一个sstable的文件名，所属的级别，最大与最小key的前缀。\n\n### 解读\n\n- 内存中的数据周期性的dump到磁盘中，一次dump就是在磁盘中写入一个sstable文件。\n- 每一次从内存中dump到磁盘中的文件自动成为第零层L0中的文件。**LSM每一层都可以有多个文件**。第零层也不例外，会存在多个从内存中dump下来的文件，因此每个文件中的KV数据都是排好序的，并且每个文件元数据中都包含这个文件的最大key和最小key。比如file1[k1 ->  k100]，file2[k50 -> k150]，这两个文件就存在重叠的key，需要进行压缩归并，从而保证同一层内所有文件不能存在重叠key。\n- 通过对比每个从内存中dump下来的文件的key区间，我们很容易的判断出两个文件中是否含有重叠的key区间，如果有，那么就要触发压缩，也就是归并排序，将这两个文件进行压缩合并。合并后会生成一个新的文件，同时L0层的两个文件就可以删掉了，由于这个文件是L0层的文件压缩合并而来的，因此这个文件会晋升到L1层，依次递归。\n\n## LSM Compaction机制\n\n> compaction在以LSM-Tree为架构的系统中是非常关键的模块，log append的方式带来了高吞吐的写，内存中的数据到达上限后不断刷盘，数据范围互相交叠的层越来越多，相同key的数据不断积累，引起读性能下降和空间膨胀。因此，compaction机制被引入，通过周期性的后台任务不断的回收旧版本数据和将多层合并方式来优化读性能和空间问题。而compaction的策略和任务调度成为新的难题，看似简单的功能，实则需要各方面的权衡，涉及空间、I/O、cpu资源和缓存等多个层面。这篇文章将从compaction策略、挑战、几个主流lsmtree系统的实现和学术上的研究几个方向来探讨这个机制。\n\n### compaction策略\n\ncompaction的主要作用是数据的gc和归并排序，是lsm-tree系统正常运转必须要做的操作，但是compaction任务运行期间会带来很大的资源开销，压缩/解压缩、数据拷贝和compare消耗大量cpu，读写数据引起disk I/O。compaction策略约束了lsm-tree的形状，决定哪些文件需要合并、任务的大小和触发的条件，不同的策略对读写放大、空间放大和临时空间的大小有不同的影响，一般系统会支持不同的策略并配有多个调整参数，可根据不同的应用场景选取更合适的方式。\n\n#### Size-tired compaction\n\n- size-tired适合write-intensive workload，有较低的写放大，缺点是读放大和空间放大较高。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/size-tired-01.png)\n\n#### leveled compaction\n\n- leveled策略可以减小空间放大和读放大。leveled策略的问题是写放大。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/level-comp-01.png)\t\n\n#### Hybrid\n\n- tiered和leveled混合的方式。很多系统使用两者混合的方式以取得读写放大、空间放大之间进一步的权衡。相比tiered可以获得更少的空间放大和读放大，相比leveled可以有更少的写放大。\n\n\n\n## Compaction in RocksDB\n\n由于Size-tired compaction和leveled compaction两种策略都有各自的优缺点，所以RocksDB在L1层及以上采用leveled compaction，而在L0层采用size-tiered compaction。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/rocksdb-compaction-01.png)\n\n### universal compaction\nuniversal compaction是RocksDB中size-tiered compaction的别名，专门用于L0层的compaction，**因为L0层的SST的key区间是几乎肯定有重合的。**\n\n前文已经说过，当L0层的文件数目达到level0_file_num_compaction_trigger阈值时，就会触发L0层SST合并到L1。universal compaction还会检查以下条件。\n\n- 空间放大比例\n  假设L0层现有的SST文件为(R1, R1, R2, ..., Rn)，其中R1是最新写入的SST，Rn是较旧的SST。所谓空间放大比例，就是指R1~Rn-1文件的总大小除以Rn的大小，如果这个比值比max_size_amplification_percent / 100要大，那么就会将L0层所有SST做compaction。\n\n- 相邻文件大小比例\n  有一个参数size_ratio用于控制相邻文件大小比例的阈值。如果size(R2) / size(R1)的比值小于1 + size_ratio / 100，就表示R1和R2两个SST可以做compaction。接下来继续检查size(R3) / size(R1 + R2)是否小于1 + size_ratio / 100，若仍满足，就将R3也加入待compaction的SST里来。如此往复，直到不再满足上述比例条件为止。\n\n当然，如果上述两个条件都没能触发compaction，该策略就会线性地从R1开始合并，直到L0层的文件数目小于level0_file_num_compaction_trigger阈值。\n\n\n\n## LSM in paimon\n\n### 写入\n\npaimon并没有在内存中维护一个排序数据结构，比如红黑树或者跳表，而是直接追加写入到内存中，当到达一定大小，需要dump到磁盘时，才会进行排序，并flush到文件系统。\n\npaimon为什么不在内存中维护一个排序数据结构，是因为相比其他LSM系统如HBase、Kudu、Doris等，不是一个在线的服务，更像一种表格式。不支持从内存中查找最新的数据，只有当内存中的数据dump到文件系统以后，才会保证数据的可见性，更准确的说是完成快照以后，数据才可见。因此paimon相比其他数据系统拥有更大的数据延迟，因为其不支持从内存中查找最新的数据。\n\npaimon会将到来的Record对象先序列化为二进制字节，以节省内存空间，再将序列化后的Record写入到LSM的内存缓存区中。一切的一切都是为了减少内存空间占用。\n\n### 压缩策略Compact Strategy\n\npaimon采用类似于Rocksdb的[universal compaction](https://github.com/facebook/rocksdb/wiki/Universal-Compaction)的压缩策略。","source":"_posts/bigdata/lsm/LSM基本概念.md","raw":"---\ntitle: LSM基本概念与其经典实现Level-DB\ntags:\n  - LSM\ncategories:\n  - - LSM\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 61336\ndate: 2023-04-30 17:57:25\nupdated: 2022-04-30 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n> LSM 是以牺牲读取性能以及空间利用率为代价而换取顺序写入性能的。因此，对LSM结构的优化目标就是想办法提高读取性能和空间利用率。读取性能的瓶颈在于读写放大以及合并压缩过程的抖动。\n\n![](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/lsm-01.png)\n\n## LSM经典实现Level-DB：Level引入分层机制\n\n- 1、将最近最新写入的kv存储在内存数据结构中，如红黑树，跳表等。 那么问题是何时将此数据结构dump到磁盘?最简单的是根据其大小的区别，然而在dump之前我们不能继续向其中写入数据，因此在内存中应该存在一个活跃内存表和一个不变内存表，二者相互交替，周期性的将不变内存表dump到内存中形成一个分段文件。\n\n- 2、为了优化LSM的读取性能：\n\n  > LSM结构引入了**分层设计**的思想。将所有的kv文件分为c0-ck 共k+1层。c0层是直接从不变的内存表中dump下的结果。而c1-ck是发生过合并的文件。由于ci+1 是ci中具有重叠部分的文件合并的产物，因此可以说在同一层内是不存在重叠key的，因为重叠key已经在其上一层被合并了。那么只有c0层是可能存在重叠的文件的。所以当要读取磁盘上的数据时，最坏情况下只需要读取c0的所有文件以及c1-ck每一层中的一个文件即c0+k个文件即可找到key的位置，分层合并思想使得非就地更新索引在常数次的IO中读取数据。\n\n  > 通常c0文件定义为2M，每一级比上一级大一个数量级的文件大小。所以高层的文件难以被一次性的加载到内存，因此需要一定的磁盘**索引机制**。我们对每个磁盘文件进行布局设计，分为元数据块，索引块，数据块三大块。元数据块中存储布隆过滤器快速的判断这个文件中是否存在某个key，同时通过对排序索引(通常缓存在内存中)二分查找定位key所在磁盘的位置。进而加速读取的速度，我们叫这种数据文件为SSTABLE(字符串排序表)。\n\n  > 为了标记哪些SStable属于那一层因此要存在一个sstable的元数据管理文件，在levelDB中叫做MANIFEST文件。其中存储每一个sstable的文件名，所属的级别，最大与最小key的前缀。\n\n### 解读\n\n- 内存中的数据周期性的dump到磁盘中，一次dump就是在磁盘中写入一个sstable文件。\n- 每一次从内存中dump到磁盘中的文件自动成为第零层L0中的文件。**LSM每一层都可以有多个文件**。第零层也不例外，会存在多个从内存中dump下来的文件，因此每个文件中的KV数据都是排好序的，并且每个文件元数据中都包含这个文件的最大key和最小key。比如file1[k1 ->  k100]，file2[k50 -> k150]，这两个文件就存在重叠的key，需要进行压缩归并，从而保证同一层内所有文件不能存在重叠key。\n- 通过对比每个从内存中dump下来的文件的key区间，我们很容易的判断出两个文件中是否含有重叠的key区间，如果有，那么就要触发压缩，也就是归并排序，将这两个文件进行压缩合并。合并后会生成一个新的文件，同时L0层的两个文件就可以删掉了，由于这个文件是L0层的文件压缩合并而来的，因此这个文件会晋升到L1层，依次递归。\n\n## LSM Compaction机制\n\n> compaction在以LSM-Tree为架构的系统中是非常关键的模块，log append的方式带来了高吞吐的写，内存中的数据到达上限后不断刷盘，数据范围互相交叠的层越来越多，相同key的数据不断积累，引起读性能下降和空间膨胀。因此，compaction机制被引入，通过周期性的后台任务不断的回收旧版本数据和将多层合并方式来优化读性能和空间问题。而compaction的策略和任务调度成为新的难题，看似简单的功能，实则需要各方面的权衡，涉及空间、I/O、cpu资源和缓存等多个层面。这篇文章将从compaction策略、挑战、几个主流lsmtree系统的实现和学术上的研究几个方向来探讨这个机制。\n\n### compaction策略\n\ncompaction的主要作用是数据的gc和归并排序，是lsm-tree系统正常运转必须要做的操作，但是compaction任务运行期间会带来很大的资源开销，压缩/解压缩、数据拷贝和compare消耗大量cpu，读写数据引起disk I/O。compaction策略约束了lsm-tree的形状，决定哪些文件需要合并、任务的大小和触发的条件，不同的策略对读写放大、空间放大和临时空间的大小有不同的影响，一般系统会支持不同的策略并配有多个调整参数，可根据不同的应用场景选取更合适的方式。\n\n#### Size-tired compaction\n\n- size-tired适合write-intensive workload，有较低的写放大，缺点是读放大和空间放大较高。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/size-tired-01.png)\n\n#### leveled compaction\n\n- leveled策略可以减小空间放大和读放大。leveled策略的问题是写放大。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/level-comp-01.png)\t\n\n#### Hybrid\n\n- tiered和leveled混合的方式。很多系统使用两者混合的方式以取得读写放大、空间放大之间进一步的权衡。相比tiered可以获得更少的空间放大和读放大，相比leveled可以有更少的写放大。\n\n\n\n## Compaction in RocksDB\n\n由于Size-tired compaction和leveled compaction两种策略都有各自的优缺点，所以RocksDB在L1层及以上采用leveled compaction，而在L0层采用size-tiered compaction。\n\n![](https://github.com/yuanoOo/learngit/raw/master/jpg/rocksdb-compaction-01.png)\n\n### universal compaction\nuniversal compaction是RocksDB中size-tiered compaction的别名，专门用于L0层的compaction，**因为L0层的SST的key区间是几乎肯定有重合的。**\n\n前文已经说过，当L0层的文件数目达到level0_file_num_compaction_trigger阈值时，就会触发L0层SST合并到L1。universal compaction还会检查以下条件。\n\n- 空间放大比例\n  假设L0层现有的SST文件为(R1, R1, R2, ..., Rn)，其中R1是最新写入的SST，Rn是较旧的SST。所谓空间放大比例，就是指R1~Rn-1文件的总大小除以Rn的大小，如果这个比值比max_size_amplification_percent / 100要大，那么就会将L0层所有SST做compaction。\n\n- 相邻文件大小比例\n  有一个参数size_ratio用于控制相邻文件大小比例的阈值。如果size(R2) / size(R1)的比值小于1 + size_ratio / 100，就表示R1和R2两个SST可以做compaction。接下来继续检查size(R3) / size(R1 + R2)是否小于1 + size_ratio / 100，若仍满足，就将R3也加入待compaction的SST里来。如此往复，直到不再满足上述比例条件为止。\n\n当然，如果上述两个条件都没能触发compaction，该策略就会线性地从R1开始合并，直到L0层的文件数目小于level0_file_num_compaction_trigger阈值。\n\n\n\n## LSM in paimon\n\n### 写入\n\npaimon并没有在内存中维护一个排序数据结构，比如红黑树或者跳表，而是直接追加写入到内存中，当到达一定大小，需要dump到磁盘时，才会进行排序，并flush到文件系统。\n\npaimon为什么不在内存中维护一个排序数据结构，是因为相比其他LSM系统如HBase、Kudu、Doris等，不是一个在线的服务，更像一种表格式。不支持从内存中查找最新的数据，只有当内存中的数据dump到文件系统以后，才会保证数据的可见性，更准确的说是完成快照以后，数据才可见。因此paimon相比其他数据系统拥有更大的数据延迟，因为其不支持从内存中查找最新的数据。\n\npaimon会将到来的Record对象先序列化为二进制字节，以节省内存空间，再将序列化后的Record写入到LSM的内存缓存区中。一切的一切都是为了减少内存空间占用。\n\n### 压缩策略Compact Strategy\n\npaimon采用类似于Rocksdb的[universal compaction](https://github.com/facebook/rocksdb/wiki/Universal-Compaction)的压缩策略。","slug":"bigdata/lsm/LSM基本概念","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt2003l8j5mdfxrevys","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>LSM 是以牺牲读取性能以及空间利用率为代价而换取顺序写入性能的。因此，对LSM结构的优化目标就是想办法提高读取性能和空间利用率。读取性能的瓶颈在于读写放大以及合并压缩过程的抖动。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/lsm-01.png\"></p>\n<h2 id=\"LSM经典实现Level-DB：Level引入分层机制\"><a href=\"#LSM经典实现Level-DB：Level引入分层机制\" class=\"headerlink\" title=\"LSM经典实现Level-DB：Level引入分层机制\"></a>LSM经典实现Level-DB：Level引入分层机制</h2><ul>\n<li><p>1、将最近最新写入的kv存储在内存数据结构中，如红黑树，跳表等。 那么问题是何时将此数据结构dump到磁盘?最简单的是根据其大小的区别，然而在dump之前我们不能继续向其中写入数据，因此在内存中应该存在一个活跃内存表和一个不变内存表，二者相互交替，周期性的将不变内存表dump到内存中形成一个分段文件。</p>\n</li>\n<li><p>2、为了优化LSM的读取性能：</p>\n<blockquote>\n<p>LSM结构引入了<strong>分层设计</strong>的思想。将所有的kv文件分为c0-ck 共k+1层。c0层是直接从不变的内存表中dump下的结果。而c1-ck是发生过合并的文件。由于ci+1 是ci中具有重叠部分的文件合并的产物，因此可以说在同一层内是不存在重叠key的，因为重叠key已经在其上一层被合并了。那么只有c0层是可能存在重叠的文件的。所以当要读取磁盘上的数据时，最坏情况下只需要读取c0的所有文件以及c1-ck每一层中的一个文件即c0+k个文件即可找到key的位置，分层合并思想使得非就地更新索引在常数次的IO中读取数据。</p>\n</blockquote>\n<blockquote>\n<p>通常c0文件定义为2M，每一级比上一级大一个数量级的文件大小。所以高层的文件难以被一次性的加载到内存，因此需要一定的磁盘<strong>索引机制</strong>。我们对每个磁盘文件进行布局设计，分为元数据块，索引块，数据块三大块。元数据块中存储布隆过滤器快速的判断这个文件中是否存在某个key，同时通过对排序索引(通常缓存在内存中)二分查找定位key所在磁盘的位置。进而加速读取的速度，我们叫这种数据文件为SSTABLE(字符串排序表)。</p>\n</blockquote>\n<blockquote>\n<p>为了标记哪些SStable属于那一层因此要存在一个sstable的元数据管理文件，在levelDB中叫做MANIFEST文件。其中存储每一个sstable的文件名，所属的级别，最大与最小key的前缀。</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"解读\"><a href=\"#解读\" class=\"headerlink\" title=\"解读\"></a>解读</h3><ul>\n<li>内存中的数据周期性的dump到磁盘中，一次dump就是在磁盘中写入一个sstable文件。</li>\n<li>每一次从内存中dump到磁盘中的文件自动成为第零层L0中的文件。<strong>LSM每一层都可以有多个文件</strong>。第零层也不例外，会存在多个从内存中dump下来的文件，因此每个文件中的KV数据都是排好序的，并且每个文件元数据中都包含这个文件的最大key和最小key。比如file1[k1 -&gt;  k100]，file2[k50 -&gt; k150]，这两个文件就存在重叠的key，需要进行压缩归并，从而保证同一层内所有文件不能存在重叠key。</li>\n<li>通过对比每个从内存中dump下来的文件的key区间，我们很容易的判断出两个文件中是否含有重叠的key区间，如果有，那么就要触发压缩，也就是归并排序，将这两个文件进行压缩合并。合并后会生成一个新的文件，同时L0层的两个文件就可以删掉了，由于这个文件是L0层的文件压缩合并而来的，因此这个文件会晋升到L1层，依次递归。</li>\n</ul>\n<h2 id=\"LSM-Compaction机制\"><a href=\"#LSM-Compaction机制\" class=\"headerlink\" title=\"LSM Compaction机制\"></a>LSM Compaction机制</h2><blockquote>\n<p>compaction在以LSM-Tree为架构的系统中是非常关键的模块，log append的方式带来了高吞吐的写，内存中的数据到达上限后不断刷盘，数据范围互相交叠的层越来越多，相同key的数据不断积累，引起读性能下降和空间膨胀。因此，compaction机制被引入，通过周期性的后台任务不断的回收旧版本数据和将多层合并方式来优化读性能和空间问题。而compaction的策略和任务调度成为新的难题，看似简单的功能，实则需要各方面的权衡，涉及空间、I&#x2F;O、cpu资源和缓存等多个层面。这篇文章将从compaction策略、挑战、几个主流lsmtree系统的实现和学术上的研究几个方向来探讨这个机制。</p>\n</blockquote>\n<h3 id=\"compaction策略\"><a href=\"#compaction策略\" class=\"headerlink\" title=\"compaction策略\"></a>compaction策略</h3><p>compaction的主要作用是数据的gc和归并排序，是lsm-tree系统正常运转必须要做的操作，但是compaction任务运行期间会带来很大的资源开销，压缩&#x2F;解压缩、数据拷贝和compare消耗大量cpu，读写数据引起disk I&#x2F;O。compaction策略约束了lsm-tree的形状，决定哪些文件需要合并、任务的大小和触发的条件，不同的策略对读写放大、空间放大和临时空间的大小有不同的影响，一般系统会支持不同的策略并配有多个调整参数，可根据不同的应用场景选取更合适的方式。</p>\n<h4 id=\"Size-tired-compaction\"><a href=\"#Size-tired-compaction\" class=\"headerlink\" title=\"Size-tired compaction\"></a>Size-tired compaction</h4><ul>\n<li>size-tired适合write-intensive workload，有较低的写放大，缺点是读放大和空间放大较高。</li>\n</ul>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/size-tired-01.png\"></p>\n<h4 id=\"leveled-compaction\"><a href=\"#leveled-compaction\" class=\"headerlink\" title=\"leveled compaction\"></a>leveled compaction</h4><ul>\n<li>leveled策略可以减小空间放大和读放大。leveled策略的问题是写放大。</li>\n</ul>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/level-comp-01.png\">\t</p>\n<h4 id=\"Hybrid\"><a href=\"#Hybrid\" class=\"headerlink\" title=\"Hybrid\"></a>Hybrid</h4><ul>\n<li>tiered和leveled混合的方式。很多系统使用两者混合的方式以取得读写放大、空间放大之间进一步的权衡。相比tiered可以获得更少的空间放大和读放大，相比leveled可以有更少的写放大。</li>\n</ul>\n<h2 id=\"Compaction-in-RocksDB\"><a href=\"#Compaction-in-RocksDB\" class=\"headerlink\" title=\"Compaction in RocksDB\"></a>Compaction in RocksDB</h2><p>由于Size-tired compaction和leveled compaction两种策略都有各自的优缺点，所以RocksDB在L1层及以上采用leveled compaction，而在L0层采用size-tiered compaction。</p>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/rocksdb-compaction-01.png\"></p>\n<h3 id=\"universal-compaction\"><a href=\"#universal-compaction\" class=\"headerlink\" title=\"universal compaction\"></a>universal compaction</h3><p>universal compaction是RocksDB中size-tiered compaction的别名，专门用于L0层的compaction，<strong>因为L0层的SST的key区间是几乎肯定有重合的。</strong></p>\n<p>前文已经说过，当L0层的文件数目达到level0_file_num_compaction_trigger阈值时，就会触发L0层SST合并到L1。universal compaction还会检查以下条件。</p>\n<ul>\n<li><p>空间放大比例<br>假设L0层现有的SST文件为(R1, R1, R2, …, Rn)，其中R1是最新写入的SST，Rn是较旧的SST。所谓空间放大比例，就是指R1~Rn-1文件的总大小除以Rn的大小，如果这个比值比max_size_amplification_percent &#x2F; 100要大，那么就会将L0层所有SST做compaction。</p>\n</li>\n<li><p>相邻文件大小比例<br>有一个参数size_ratio用于控制相邻文件大小比例的阈值。如果size(R2) &#x2F; size(R1)的比值小于1 + size_ratio &#x2F; 100，就表示R1和R2两个SST可以做compaction。接下来继续检查size(R3) &#x2F; size(R1 + R2)是否小于1 + size_ratio &#x2F; 100，若仍满足，就将R3也加入待compaction的SST里来。如此往复，直到不再满足上述比例条件为止。</p>\n</li>\n</ul>\n<p>当然，如果上述两个条件都没能触发compaction，该策略就会线性地从R1开始合并，直到L0层的文件数目小于level0_file_num_compaction_trigger阈值。</p>\n<h2 id=\"LSM-in-paimon\"><a href=\"#LSM-in-paimon\" class=\"headerlink\" title=\"LSM in paimon\"></a>LSM in paimon</h2><h3 id=\"写入\"><a href=\"#写入\" class=\"headerlink\" title=\"写入\"></a>写入</h3><p>paimon并没有在内存中维护一个排序数据结构，比如红黑树或者跳表，而是直接追加写入到内存中，当到达一定大小，需要dump到磁盘时，才会进行排序，并flush到文件系统。</p>\n<p>paimon为什么不在内存中维护一个排序数据结构，是因为相比其他LSM系统如HBase、Kudu、Doris等，不是一个在线的服务，更像一种表格式。不支持从内存中查找最新的数据，只有当内存中的数据dump到文件系统以后，才会保证数据的可见性，更准确的说是完成快照以后，数据才可见。因此paimon相比其他数据系统拥有更大的数据延迟，因为其不支持从内存中查找最新的数据。</p>\n<p>paimon会将到来的Record对象先序列化为二进制字节，以节省内存空间，再将序列化后的Record写入到LSM的内存缓存区中。一切的一切都是为了减少内存空间占用。</p>\n<h3 id=\"压缩策略Compact-Strategy\"><a href=\"#压缩策略Compact-Strategy\" class=\"headerlink\" title=\"压缩策略Compact Strategy\"></a>压缩策略Compact Strategy</h3><p>paimon采用类似于Rocksdb的<a href=\"https://github.com/facebook/rocksdb/wiki/Universal-Compaction\">universal compaction</a>的压缩策略。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>LSM 是以牺牲读取性能以及空间利用率为代价而换取顺序写入性能的。因此，对LSM结构的优化目标就是想办法提高读取性能和空间利用率。读取性能的瓶颈在于读写放大以及合并压缩过程的抖动。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/lsm-01.png\"></p>\n<h2 id=\"LSM经典实现Level-DB：Level引入分层机制\"><a href=\"#LSM经典实现Level-DB：Level引入分层机制\" class=\"headerlink\" title=\"LSM经典实现Level-DB：Level引入分层机制\"></a>LSM经典实现Level-DB：Level引入分层机制</h2><ul>\n<li><p>1、将最近最新写入的kv存储在内存数据结构中，如红黑树，跳表等。 那么问题是何时将此数据结构dump到磁盘?最简单的是根据其大小的区别，然而在dump之前我们不能继续向其中写入数据，因此在内存中应该存在一个活跃内存表和一个不变内存表，二者相互交替，周期性的将不变内存表dump到内存中形成一个分段文件。</p>\n</li>\n<li><p>2、为了优化LSM的读取性能：</p>\n<blockquote>\n<p>LSM结构引入了<strong>分层设计</strong>的思想。将所有的kv文件分为c0-ck 共k+1层。c0层是直接从不变的内存表中dump下的结果。而c1-ck是发生过合并的文件。由于ci+1 是ci中具有重叠部分的文件合并的产物，因此可以说在同一层内是不存在重叠key的，因为重叠key已经在其上一层被合并了。那么只有c0层是可能存在重叠的文件的。所以当要读取磁盘上的数据时，最坏情况下只需要读取c0的所有文件以及c1-ck每一层中的一个文件即c0+k个文件即可找到key的位置，分层合并思想使得非就地更新索引在常数次的IO中读取数据。</p>\n</blockquote>\n<blockquote>\n<p>通常c0文件定义为2M，每一级比上一级大一个数量级的文件大小。所以高层的文件难以被一次性的加载到内存，因此需要一定的磁盘<strong>索引机制</strong>。我们对每个磁盘文件进行布局设计，分为元数据块，索引块，数据块三大块。元数据块中存储布隆过滤器快速的判断这个文件中是否存在某个key，同时通过对排序索引(通常缓存在内存中)二分查找定位key所在磁盘的位置。进而加速读取的速度，我们叫这种数据文件为SSTABLE(字符串排序表)。</p>\n</blockquote>\n<blockquote>\n<p>为了标记哪些SStable属于那一层因此要存在一个sstable的元数据管理文件，在levelDB中叫做MANIFEST文件。其中存储每一个sstable的文件名，所属的级别，最大与最小key的前缀。</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"解读\"><a href=\"#解读\" class=\"headerlink\" title=\"解读\"></a>解读</h3><ul>\n<li>内存中的数据周期性的dump到磁盘中，一次dump就是在磁盘中写入一个sstable文件。</li>\n<li>每一次从内存中dump到磁盘中的文件自动成为第零层L0中的文件。<strong>LSM每一层都可以有多个文件</strong>。第零层也不例外，会存在多个从内存中dump下来的文件，因此每个文件中的KV数据都是排好序的，并且每个文件元数据中都包含这个文件的最大key和最小key。比如file1[k1 -&gt;  k100]，file2[k50 -&gt; k150]，这两个文件就存在重叠的key，需要进行压缩归并，从而保证同一层内所有文件不能存在重叠key。</li>\n<li>通过对比每个从内存中dump下来的文件的key区间，我们很容易的判断出两个文件中是否含有重叠的key区间，如果有，那么就要触发压缩，也就是归并排序，将这两个文件进行压缩合并。合并后会生成一个新的文件，同时L0层的两个文件就可以删掉了，由于这个文件是L0层的文件压缩合并而来的，因此这个文件会晋升到L1层，依次递归。</li>\n</ul>\n<h2 id=\"LSM-Compaction机制\"><a href=\"#LSM-Compaction机制\" class=\"headerlink\" title=\"LSM Compaction机制\"></a>LSM Compaction机制</h2><blockquote>\n<p>compaction在以LSM-Tree为架构的系统中是非常关键的模块，log append的方式带来了高吞吐的写，内存中的数据到达上限后不断刷盘，数据范围互相交叠的层越来越多，相同key的数据不断积累，引起读性能下降和空间膨胀。因此，compaction机制被引入，通过周期性的后台任务不断的回收旧版本数据和将多层合并方式来优化读性能和空间问题。而compaction的策略和任务调度成为新的难题，看似简单的功能，实则需要各方面的权衡，涉及空间、I&#x2F;O、cpu资源和缓存等多个层面。这篇文章将从compaction策略、挑战、几个主流lsmtree系统的实现和学术上的研究几个方向来探讨这个机制。</p>\n</blockquote>\n<h3 id=\"compaction策略\"><a href=\"#compaction策略\" class=\"headerlink\" title=\"compaction策略\"></a>compaction策略</h3><p>compaction的主要作用是数据的gc和归并排序，是lsm-tree系统正常运转必须要做的操作，但是compaction任务运行期间会带来很大的资源开销，压缩&#x2F;解压缩、数据拷贝和compare消耗大量cpu，读写数据引起disk I&#x2F;O。compaction策略约束了lsm-tree的形状，决定哪些文件需要合并、任务的大小和触发的条件，不同的策略对读写放大、空间放大和临时空间的大小有不同的影响，一般系统会支持不同的策略并配有多个调整参数，可根据不同的应用场景选取更合适的方式。</p>\n<h4 id=\"Size-tired-compaction\"><a href=\"#Size-tired-compaction\" class=\"headerlink\" title=\"Size-tired compaction\"></a>Size-tired compaction</h4><ul>\n<li>size-tired适合write-intensive workload，有较低的写放大，缺点是读放大和空间放大较高。</li>\n</ul>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/size-tired-01.png\"></p>\n<h4 id=\"leveled-compaction\"><a href=\"#leveled-compaction\" class=\"headerlink\" title=\"leveled compaction\"></a>leveled compaction</h4><ul>\n<li>leveled策略可以减小空间放大和读放大。leveled策略的问题是写放大。</li>\n</ul>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/level-comp-01.png\">\t</p>\n<h4 id=\"Hybrid\"><a href=\"#Hybrid\" class=\"headerlink\" title=\"Hybrid\"></a>Hybrid</h4><ul>\n<li>tiered和leveled混合的方式。很多系统使用两者混合的方式以取得读写放大、空间放大之间进一步的权衡。相比tiered可以获得更少的空间放大和读放大，相比leveled可以有更少的写放大。</li>\n</ul>\n<h2 id=\"Compaction-in-RocksDB\"><a href=\"#Compaction-in-RocksDB\" class=\"headerlink\" title=\"Compaction in RocksDB\"></a>Compaction in RocksDB</h2><p>由于Size-tired compaction和leveled compaction两种策略都有各自的优缺点，所以RocksDB在L1层及以上采用leveled compaction，而在L0层采用size-tiered compaction。</p>\n<p><img src=\"https://github.com/yuanoOo/learngit/raw/master/jpg/rocksdb-compaction-01.png\"></p>\n<h3 id=\"universal-compaction\"><a href=\"#universal-compaction\" class=\"headerlink\" title=\"universal compaction\"></a>universal compaction</h3><p>universal compaction是RocksDB中size-tiered compaction的别名，专门用于L0层的compaction，<strong>因为L0层的SST的key区间是几乎肯定有重合的。</strong></p>\n<p>前文已经说过，当L0层的文件数目达到level0_file_num_compaction_trigger阈值时，就会触发L0层SST合并到L1。universal compaction还会检查以下条件。</p>\n<ul>\n<li><p>空间放大比例<br>假设L0层现有的SST文件为(R1, R1, R2, …, Rn)，其中R1是最新写入的SST，Rn是较旧的SST。所谓空间放大比例，就是指R1~Rn-1文件的总大小除以Rn的大小，如果这个比值比max_size_amplification_percent &#x2F; 100要大，那么就会将L0层所有SST做compaction。</p>\n</li>\n<li><p>相邻文件大小比例<br>有一个参数size_ratio用于控制相邻文件大小比例的阈值。如果size(R2) &#x2F; size(R1)的比值小于1 + size_ratio &#x2F; 100，就表示R1和R2两个SST可以做compaction。接下来继续检查size(R3) &#x2F; size(R1 + R2)是否小于1 + size_ratio &#x2F; 100，若仍满足，就将R3也加入待compaction的SST里来。如此往复，直到不再满足上述比例条件为止。</p>\n</li>\n</ul>\n<p>当然，如果上述两个条件都没能触发compaction，该策略就会线性地从R1开始合并，直到L0层的文件数目小于level0_file_num_compaction_trigger阈值。</p>\n<h2 id=\"LSM-in-paimon\"><a href=\"#LSM-in-paimon\" class=\"headerlink\" title=\"LSM in paimon\"></a>LSM in paimon</h2><h3 id=\"写入\"><a href=\"#写入\" class=\"headerlink\" title=\"写入\"></a>写入</h3><p>paimon并没有在内存中维护一个排序数据结构，比如红黑树或者跳表，而是直接追加写入到内存中，当到达一定大小，需要dump到磁盘时，才会进行排序，并flush到文件系统。</p>\n<p>paimon为什么不在内存中维护一个排序数据结构，是因为相比其他LSM系统如HBase、Kudu、Doris等，不是一个在线的服务，更像一种表格式。不支持从内存中查找最新的数据，只有当内存中的数据dump到文件系统以后，才会保证数据的可见性，更准确的说是完成快照以后，数据才可见。因此paimon相比其他数据系统拥有更大的数据延迟，因为其不支持从内存中查找最新的数据。</p>\n<p>paimon会将到来的Record对象先序列化为二进制字节，以节省内存空间，再将序列化后的Record写入到LSM的内存缓存区中。一切的一切都是为了减少内存空间占用。</p>\n<h3 id=\"压缩策略Compact-Strategy\"><a href=\"#压缩策略Compact-Strategy\" class=\"headerlink\" title=\"压缩策略Compact Strategy\"></a>压缩策略Compact Strategy</h3><p>paimon采用类似于Rocksdb的<a href=\"https://github.com/facebook/rocksdb/wiki/Universal-Compaction\">universal compaction</a>的压缩策略。</p>\n"},{"title":"Paimon动态Bucket设计与实现","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":24799,"date":"2023-06-21T09:57:25.000Z","updated":"2023-06-21T09:57:25.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n## 前言\n\nPaimon Dynamic Bucket是Paimon-0.5引入的新特性，现在Paimon可以动态的创建Bucket进行扩容，旨在进一步简化了创建Paimon表的过程，用户无需关心需要创建多少个Bucket。通过`dynamic-bucket.target-row-num`配置指定每个桶存储多少条记录，默认是2_000_000L。\n\n为了实现这个特性，Paimon需要利用文件记录所有Record与其Bucket的映射关系。在paimon中，使用Record的主键的hashcode代表一个Record，而hashcode是Int类型，减少了内存占用。使用主键的hashcode代表一个Record还有一个好处就是，使用int就可以覆盖所有的Record，即使`dynamic-bucket.target-row-num`是Long类型，避免了空间无限膨胀的问题。这是因为即使hash冲突，并不影响正确性。\n\n通过不断检查映射文件中key的行数，当大于`dynamic-bucket.target-row-num`时，创建新的bucket进行扩容。\n\n## 设计与实现\n\n实现集中在paimon-core/src/main/java/org/apache/paimon/index包中。\n\n\n\n#### 入口类\n\n`org.apache.paimon.index.HashBucketAssigner#HashBucketAssigner`是实现的入口类，被\n`org.apache.paimon.flink.sink.HashBucketAssignerOperator#initializeState`方法调用，找到了入口类，接下来就是一步步阅读源码，理清逻辑了。\n\n```java\n    @Override\n    public void initializeState(StateInitializationContext context) throws Exception {\n        super.initializeState(context);\n\n        // Each job can only have one user name and this name must be consistent across restarts.\n        // We cannot use job id as commit user name here because user may change job id by creating\n        // a savepoint, stop the job and then resume from savepoint.\n        String commitUser =\n                StateUtils.getSingleValueFromState(\n                        context, \"commit_user_state\", String.class, initialCommitUser);\n        \n        // 初始化bucket分配器，因此HashBucketAssigner是入口类\n        this.assigner =\n                new HashBucketAssigner(\n                        table.snapshotManager(),\n                        commitUser,\n                        table.store().newIndexFileHandler(),\n                        getRuntimeContext().getNumberOfParallelSubtasks(),\n                        getRuntimeContext().getIndexOfThisSubtask(),\n                        table.coreOptions().dynamicBucketTargetRowNum());\n        this.extractor = extractorFunction.apply(table.schema());\n    }\n    \n        @Override\n    public void processElement(StreamRecord<T> streamRecord) throws Exception {\n        T value = streamRecord.getValue();\n        \n        // 通过调用assign方法，获取每一个record对应的bucket\n        int bucket =\n                assigner.assign(\n                        extractor.partition(value), extractor.trimmedPrimaryKey(value).hashCode());\n        output.collect(new StreamRecord<>(new Tuple2<>(value, bucket)));\n    }\n```\n\n```java\n    public HashBucketAssigner(\n            SnapshotManager snapshotManager,\n            String commitUser,\n            IndexFileHandler indexFileHandler,\n            int numAssigners,\n            int assignId,\n            long targetBucketRowNumber) {\n        this.snapshotManager = snapshotManager;\n        this.commitUser = commitUser;\n        this.indexFileHandler = indexFileHandler;\n        this.numAssigners = numAssigners;\n        this.assignId = assignId; // getRuntimeContext().getIndexOfThisSubtask()\n        this.targetBucketRowNumber = targetBucketRowNumber;\n        this.partitionIndex = new HashMap<>();\n    }\n\n    /** Assign a bucket for key hash of a record. */\n    public int assign(BinaryRow partition, int hash) {\n        // hash: Record主键的hashcode，唯一确认一个Record\n        int recordAssignId = computeAssignId(hash);\n        // 可能是因为，Flink DAG前面已经通过主键的hashcode % channels了，所以一定相等\n        checkArgument(\n                recordAssignId == assignId,\n                \"This is a bug, record assign id %s should equal to assign id %s.\",\n                recordAssignId,\n                assignId);\n        \n        // PartitionIndex: Bucket Index Per Partition.\n        // 为每一个partition计算对应的Bucket Index\n        PartitionIndex index = partitionIndex.computeIfAbsent(partition, this::loadIndex);\n        return index.assign(hash, (bucket) -> computeAssignId(bucket) == assignId);\n    }\n\n    private int computeAssignId(int hash) {\n        // numAssigners: getRuntimeContext().getNumberOfParallelSubtasks()\n        return Math.abs(hash % numAssigners);\n    }\n```\n\n```java\n    org.apache.paimon.index.PartitionIndex#assign\n\tpublic int assign(int hash, IntPredicate bucketFilterFunc) {\n        accessed = true;\n\n        // 1. is it a key that has appeared before\n        // 注意：当发生Hash冲突的时候，两个不同的parimary key，会有相同的hashcode\n        // 但是我们无法知道是否发生了冲突，本来需要bucketInformation.put(bucket, number + 1)，加1\n        // 因此会导致设置的dynamic-bucket.target-row-num bucket中的条数不准确。\n        // 只要hash冲突不严重，无伤大雅\n        if (hash2Bucket.containsKey(hash)) {\n            return hash2Bucket.get(hash);\n        }\n\n        // 2. find bucket from existing buckets\n        for (Integer bucket : bucketInformation.keySet()) {\n            if (bucketFilterFunc.test(bucket)) {\n                // it is my bucket\n                Long number = bucketInformation.get(bucket);\n                if (number < targetBucketRowNumber) {\n                    bucketInformation.put(bucket, number + 1);\n                    hash2Bucket.put(hash, bucket.shortValue());\n                    return bucket;\n                }\n            }\n        }\n\n        // 3. create a new bucket\n        for (int i = 0; i < Short.MAX_VALUE; i++) {\n            if (bucketFilterFunc.test(i) && !bucketInformation.containsKey(i)) {\n                hash2Bucket.put(hash, (short) i);\n                bucketInformation.put(i, 1L);\n                return i;\n            }\n        }\n\n        @SuppressWarnings(\"OptionalGetWithoutIsPresent\")\n        int maxBucket =\n                bucketInformation.keySet().stream().mapToInt(Integer::intValue).max().getAsInt();\n        throw new RuntimeException(\n                String.format(\n                        \"To more bucket %s, you should increase target bucket row number %s.\",\n                        maxBucket, targetBucketRowNumber));\n    }\n```\n\n","source":"_posts/bigdata/paimon/Paimon动态Bucket设计与实现.md","raw":"---\ntitle: Paimon动态Bucket设计与实现\ntags:\n  - paimon\ncategories:\n  - - bigdata\n    - paimon\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 24799\ndate: 2023-06-21 17:57:25\nupdated: 2023-06-21 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\nPaimon Dynamic Bucket是Paimon-0.5引入的新特性，现在Paimon可以动态的创建Bucket进行扩容，旨在进一步简化了创建Paimon表的过程，用户无需关心需要创建多少个Bucket。通过`dynamic-bucket.target-row-num`配置指定每个桶存储多少条记录，默认是2_000_000L。\n\n为了实现这个特性，Paimon需要利用文件记录所有Record与其Bucket的映射关系。在paimon中，使用Record的主键的hashcode代表一个Record，而hashcode是Int类型，减少了内存占用。使用主键的hashcode代表一个Record还有一个好处就是，使用int就可以覆盖所有的Record，即使`dynamic-bucket.target-row-num`是Long类型，避免了空间无限膨胀的问题。这是因为即使hash冲突，并不影响正确性。\n\n通过不断检查映射文件中key的行数，当大于`dynamic-bucket.target-row-num`时，创建新的bucket进行扩容。\n\n## 设计与实现\n\n实现集中在paimon-core/src/main/java/org/apache/paimon/index包中。\n\n\n\n#### 入口类\n\n`org.apache.paimon.index.HashBucketAssigner#HashBucketAssigner`是实现的入口类，被\n`org.apache.paimon.flink.sink.HashBucketAssignerOperator#initializeState`方法调用，找到了入口类，接下来就是一步步阅读源码，理清逻辑了。\n\n```java\n    @Override\n    public void initializeState(StateInitializationContext context) throws Exception {\n        super.initializeState(context);\n\n        // Each job can only have one user name and this name must be consistent across restarts.\n        // We cannot use job id as commit user name here because user may change job id by creating\n        // a savepoint, stop the job and then resume from savepoint.\n        String commitUser =\n                StateUtils.getSingleValueFromState(\n                        context, \"commit_user_state\", String.class, initialCommitUser);\n        \n        // 初始化bucket分配器，因此HashBucketAssigner是入口类\n        this.assigner =\n                new HashBucketAssigner(\n                        table.snapshotManager(),\n                        commitUser,\n                        table.store().newIndexFileHandler(),\n                        getRuntimeContext().getNumberOfParallelSubtasks(),\n                        getRuntimeContext().getIndexOfThisSubtask(),\n                        table.coreOptions().dynamicBucketTargetRowNum());\n        this.extractor = extractorFunction.apply(table.schema());\n    }\n    \n        @Override\n    public void processElement(StreamRecord<T> streamRecord) throws Exception {\n        T value = streamRecord.getValue();\n        \n        // 通过调用assign方法，获取每一个record对应的bucket\n        int bucket =\n                assigner.assign(\n                        extractor.partition(value), extractor.trimmedPrimaryKey(value).hashCode());\n        output.collect(new StreamRecord<>(new Tuple2<>(value, bucket)));\n    }\n```\n\n```java\n    public HashBucketAssigner(\n            SnapshotManager snapshotManager,\n            String commitUser,\n            IndexFileHandler indexFileHandler,\n            int numAssigners,\n            int assignId,\n            long targetBucketRowNumber) {\n        this.snapshotManager = snapshotManager;\n        this.commitUser = commitUser;\n        this.indexFileHandler = indexFileHandler;\n        this.numAssigners = numAssigners;\n        this.assignId = assignId; // getRuntimeContext().getIndexOfThisSubtask()\n        this.targetBucketRowNumber = targetBucketRowNumber;\n        this.partitionIndex = new HashMap<>();\n    }\n\n    /** Assign a bucket for key hash of a record. */\n    public int assign(BinaryRow partition, int hash) {\n        // hash: Record主键的hashcode，唯一确认一个Record\n        int recordAssignId = computeAssignId(hash);\n        // 可能是因为，Flink DAG前面已经通过主键的hashcode % channels了，所以一定相等\n        checkArgument(\n                recordAssignId == assignId,\n                \"This is a bug, record assign id %s should equal to assign id %s.\",\n                recordAssignId,\n                assignId);\n        \n        // PartitionIndex: Bucket Index Per Partition.\n        // 为每一个partition计算对应的Bucket Index\n        PartitionIndex index = partitionIndex.computeIfAbsent(partition, this::loadIndex);\n        return index.assign(hash, (bucket) -> computeAssignId(bucket) == assignId);\n    }\n\n    private int computeAssignId(int hash) {\n        // numAssigners: getRuntimeContext().getNumberOfParallelSubtasks()\n        return Math.abs(hash % numAssigners);\n    }\n```\n\n```java\n    org.apache.paimon.index.PartitionIndex#assign\n\tpublic int assign(int hash, IntPredicate bucketFilterFunc) {\n        accessed = true;\n\n        // 1. is it a key that has appeared before\n        // 注意：当发生Hash冲突的时候，两个不同的parimary key，会有相同的hashcode\n        // 但是我们无法知道是否发生了冲突，本来需要bucketInformation.put(bucket, number + 1)，加1\n        // 因此会导致设置的dynamic-bucket.target-row-num bucket中的条数不准确。\n        // 只要hash冲突不严重，无伤大雅\n        if (hash2Bucket.containsKey(hash)) {\n            return hash2Bucket.get(hash);\n        }\n\n        // 2. find bucket from existing buckets\n        for (Integer bucket : bucketInformation.keySet()) {\n            if (bucketFilterFunc.test(bucket)) {\n                // it is my bucket\n                Long number = bucketInformation.get(bucket);\n                if (number < targetBucketRowNumber) {\n                    bucketInformation.put(bucket, number + 1);\n                    hash2Bucket.put(hash, bucket.shortValue());\n                    return bucket;\n                }\n            }\n        }\n\n        // 3. create a new bucket\n        for (int i = 0; i < Short.MAX_VALUE; i++) {\n            if (bucketFilterFunc.test(i) && !bucketInformation.containsKey(i)) {\n                hash2Bucket.put(hash, (short) i);\n                bucketInformation.put(i, 1L);\n                return i;\n            }\n        }\n\n        @SuppressWarnings(\"OptionalGetWithoutIsPresent\")\n        int maxBucket =\n                bucketInformation.keySet().stream().mapToInt(Integer::intValue).max().getAsInt();\n        throw new RuntimeException(\n                String.format(\n                        \"To more bucket %s, you should increase target bucket row number %s.\",\n                        maxBucket, targetBucketRowNumber));\n    }\n```\n\n","slug":"bigdata/paimon/Paimon动态Bucket设计与实现","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt3003n8j5m7qhbhgdj","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Paimon Dynamic Bucket是Paimon-0.5引入的新特性，现在Paimon可以动态的创建Bucket进行扩容，旨在进一步简化了创建Paimon表的过程，用户无需关心需要创建多少个Bucket。通过<code>dynamic-bucket.target-row-num</code>配置指定每个桶存储多少条记录，默认是2_000_000L。</p>\n<p>为了实现这个特性，Paimon需要利用文件记录所有Record与其Bucket的映射关系。在paimon中，使用Record的主键的hashcode代表一个Record，而hashcode是Int类型，减少了内存占用。使用主键的hashcode代表一个Record还有一个好处就是，使用int就可以覆盖所有的Record，即使<code>dynamic-bucket.target-row-num</code>是Long类型，避免了空间无限膨胀的问题。这是因为即使hash冲突，并不影响正确性。</p>\n<p>通过不断检查映射文件中key的行数，当大于<code>dynamic-bucket.target-row-num</code>时，创建新的bucket进行扩容。</p>\n<h2 id=\"设计与实现\"><a href=\"#设计与实现\" class=\"headerlink\" title=\"设计与实现\"></a>设计与实现</h2><p>实现集中在paimon-core&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;paimon&#x2F;index包中。</p>\n<h4 id=\"入口类\"><a href=\"#入口类\" class=\"headerlink\" title=\"入口类\"></a>入口类</h4><p><code>org.apache.paimon.index.HashBucketAssigner#HashBucketAssigner</code>是实现的入口类，被<br><code>org.apache.paimon.flink.sink.HashBucketAssignerOperator#initializeState</code>方法调用，找到了入口类，接下来就是一步步阅读源码，理清逻辑了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initializeState</span><span class=\"params\">(StateInitializationContext context)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"built_in\">super</span>.initializeState(context);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Each job can only have one user name and this name must be consistent across restarts.</span></span><br><span class=\"line\">    <span class=\"comment\">// We cannot use job id as commit user name here because user may change job id by creating</span></span><br><span class=\"line\">    <span class=\"comment\">// a savepoint, stop the job and then resume from savepoint.</span></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">commitUser</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            StateUtils.getSingleValueFromState(</span><br><span class=\"line\">                    context, <span class=\"string\">&quot;commit_user_state&quot;</span>, String.class, initialCommitUser);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 初始化bucket分配器，因此HashBucketAssigner是入口类</span></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.assigner =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">HashBucketAssigner</span>(</span><br><span class=\"line\">                    table.snapshotManager(),</span><br><span class=\"line\">                    commitUser,</span><br><span class=\"line\">                    table.store().newIndexFileHandler(),</span><br><span class=\"line\">                    getRuntimeContext().getNumberOfParallelSubtasks(),</span><br><span class=\"line\">                    getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class=\"line\">                    table.coreOptions().dynamicBucketTargetRowNum());</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.extractor = extractorFunction.apply(table.schema());</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(StreamRecord&lt;T&gt; streamRecord)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"type\">T</span> <span class=\"variable\">value</span> <span class=\"operator\">=</span> streamRecord.getValue();</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 通过调用assign方法，获取每一个record对应的bucket</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">bucket</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            assigner.assign(</span><br><span class=\"line\">                    extractor.partition(value), extractor.trimmedPrimaryKey(value).hashCode());</span><br><span class=\"line\">    output.collect(<span class=\"keyword\">new</span> <span class=\"title class_\">StreamRecord</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(value, bucket)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"title function_\">HashBucketAssigner</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">        SnapshotManager snapshotManager,</span></span><br><span class=\"line\"><span class=\"params\">        String commitUser,</span></span><br><span class=\"line\"><span class=\"params\">        IndexFileHandler indexFileHandler,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">int</span> numAssigners,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">int</span> assignId,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">long</span> targetBucketRowNumber)</span> &#123;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.snapshotManager = snapshotManager;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.commitUser = commitUser;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.indexFileHandler = indexFileHandler;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.numAssigners = numAssigners;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.assignId = assignId; <span class=\"comment\">// getRuntimeContext().getIndexOfThisSubtask()</span></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.targetBucketRowNumber = targetBucketRowNumber;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.partitionIndex = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/** Assign a bucket for key hash of a record. */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">assign</span><span class=\"params\">(BinaryRow partition, <span class=\"type\">int</span> hash)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// hash: Record主键的hashcode，唯一确认一个Record</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">recordAssignId</span> <span class=\"operator\">=</span> computeAssignId(hash);</span><br><span class=\"line\">    <span class=\"comment\">// 可能是因为，Flink DAG前面已经通过主键的hashcode % channels了，所以一定相等</span></span><br><span class=\"line\">    checkArgument(</span><br><span class=\"line\">            recordAssignId == assignId,</span><br><span class=\"line\">            <span class=\"string\">&quot;This is a bug, record assign id %s should equal to assign id %s.&quot;</span>,</span><br><span class=\"line\">            recordAssignId,</span><br><span class=\"line\">            assignId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// PartitionIndex: Bucket Index Per Partition.</span></span><br><span class=\"line\">    <span class=\"comment\">// 为每一个partition计算对应的Bucket Index</span></span><br><span class=\"line\">    <span class=\"type\">PartitionIndex</span> <span class=\"variable\">index</span> <span class=\"operator\">=</span> partitionIndex.computeIfAbsent(partition, <span class=\"built_in\">this</span>::loadIndex);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> index.assign(hash, (bucket) -&gt; computeAssignId(bucket) == assignId);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">int</span> <span class=\"title function_\">computeAssignId</span><span class=\"params\">(<span class=\"type\">int</span> hash)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// numAssigners: getRuntimeContext().getNumberOfParallelSubtasks()</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.abs(hash % numAssigners);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   org.apache.paimon.index.PartitionIndex#assign</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">assign</span><span class=\"params\">(<span class=\"type\">int</span> hash, IntPredicate bucketFilterFunc)</span> &#123;</span><br><span class=\"line\">       accessed = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 1. is it a key that has appeared before</span></span><br><span class=\"line\">       <span class=\"comment\">// 注意：当发生Hash冲突的时候，两个不同的parimary key，会有相同的hashcode</span></span><br><span class=\"line\">       <span class=\"comment\">// 但是我们无法知道是否发生了冲突，本来需要bucketInformation.put(bucket, number + 1)，加1</span></span><br><span class=\"line\">       <span class=\"comment\">// 因此会导致设置的dynamic-bucket.target-row-num bucket中的条数不准确。</span></span><br><span class=\"line\">       <span class=\"comment\">// 只要hash冲突不严重，无伤大雅</span></span><br><span class=\"line\">       <span class=\"keyword\">if</span> (hash2Bucket.containsKey(hash)) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> hash2Bucket.get(hash);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 2. find bucket from existing buckets</span></span><br><span class=\"line\">       <span class=\"keyword\">for</span> (Integer bucket : bucketInformation.keySet()) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (bucketFilterFunc.test(bucket)) &#123;</span><br><span class=\"line\">               <span class=\"comment\">// it is my bucket</span></span><br><span class=\"line\">               <span class=\"type\">Long</span> <span class=\"variable\">number</span> <span class=\"operator\">=</span> bucketInformation.get(bucket);</span><br><span class=\"line\">               <span class=\"keyword\">if</span> (number &lt; targetBucketRowNumber) &#123;</span><br><span class=\"line\">                   bucketInformation.put(bucket, number + <span class=\"number\">1</span>);</span><br><span class=\"line\">                   hash2Bucket.put(hash, bucket.shortValue());</span><br><span class=\"line\">                   <span class=\"keyword\">return</span> bucket;</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 3. create a new bucket</span></span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>; i &lt; Short.MAX_VALUE; i++) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (bucketFilterFunc.test(i) &amp;&amp; !bucketInformation.containsKey(i)) &#123;</span><br><span class=\"line\">               hash2Bucket.put(hash, (<span class=\"type\">short</span>) i);</span><br><span class=\"line\">               bucketInformation.put(i, <span class=\"number\">1L</span>);</span><br><span class=\"line\">               <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"meta\">@SuppressWarnings(&quot;OptionalGetWithoutIsPresent&quot;)</span></span><br><span class=\"line\">       <span class=\"type\">int</span> <span class=\"variable\">maxBucket</span> <span class=\"operator\">=</span></span><br><span class=\"line\">               bucketInformation.keySet().stream().mapToInt(Integer::intValue).max().getAsInt();</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(</span><br><span class=\"line\">               String.format(</span><br><span class=\"line\">                       <span class=\"string\">&quot;To more bucket %s, you should increase target bucket row number %s.&quot;</span>,</span><br><span class=\"line\">                       maxBucket, targetBucketRowNumber));</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Paimon Dynamic Bucket是Paimon-0.5引入的新特性，现在Paimon可以动态的创建Bucket进行扩容，旨在进一步简化了创建Paimon表的过程，用户无需关心需要创建多少个Bucket。通过<code>dynamic-bucket.target-row-num</code>配置指定每个桶存储多少条记录，默认是2_000_000L。</p>\n<p>为了实现这个特性，Paimon需要利用文件记录所有Record与其Bucket的映射关系。在paimon中，使用Record的主键的hashcode代表一个Record，而hashcode是Int类型，减少了内存占用。使用主键的hashcode代表一个Record还有一个好处就是，使用int就可以覆盖所有的Record，即使<code>dynamic-bucket.target-row-num</code>是Long类型，避免了空间无限膨胀的问题。这是因为即使hash冲突，并不影响正确性。</p>\n<p>通过不断检查映射文件中key的行数，当大于<code>dynamic-bucket.target-row-num</code>时，创建新的bucket进行扩容。</p>\n<h2 id=\"设计与实现\"><a href=\"#设计与实现\" class=\"headerlink\" title=\"设计与实现\"></a>设计与实现</h2><p>实现集中在paimon-core&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;paimon&#x2F;index包中。</p>\n<h4 id=\"入口类\"><a href=\"#入口类\" class=\"headerlink\" title=\"入口类\"></a>入口类</h4><p><code>org.apache.paimon.index.HashBucketAssigner#HashBucketAssigner</code>是实现的入口类，被<br><code>org.apache.paimon.flink.sink.HashBucketAssignerOperator#initializeState</code>方法调用，找到了入口类，接下来就是一步步阅读源码，理清逻辑了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initializeState</span><span class=\"params\">(StateInitializationContext context)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"built_in\">super</span>.initializeState(context);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Each job can only have one user name and this name must be consistent across restarts.</span></span><br><span class=\"line\">    <span class=\"comment\">// We cannot use job id as commit user name here because user may change job id by creating</span></span><br><span class=\"line\">    <span class=\"comment\">// a savepoint, stop the job and then resume from savepoint.</span></span><br><span class=\"line\">    <span class=\"type\">String</span> <span class=\"variable\">commitUser</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            StateUtils.getSingleValueFromState(</span><br><span class=\"line\">                    context, <span class=\"string\">&quot;commit_user_state&quot;</span>, String.class, initialCommitUser);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 初始化bucket分配器，因此HashBucketAssigner是入口类</span></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.assigner =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">HashBucketAssigner</span>(</span><br><span class=\"line\">                    table.snapshotManager(),</span><br><span class=\"line\">                    commitUser,</span><br><span class=\"line\">                    table.store().newIndexFileHandler(),</span><br><span class=\"line\">                    getRuntimeContext().getNumberOfParallelSubtasks(),</span><br><span class=\"line\">                    getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class=\"line\">                    table.coreOptions().dynamicBucketTargetRowNum());</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.extractor = extractorFunction.apply(table.schema());</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">processElement</span><span class=\"params\">(StreamRecord&lt;T&gt; streamRecord)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"type\">T</span> <span class=\"variable\">value</span> <span class=\"operator\">=</span> streamRecord.getValue();</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 通过调用assign方法，获取每一个record对应的bucket</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">bucket</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            assigner.assign(</span><br><span class=\"line\">                    extractor.partition(value), extractor.trimmedPrimaryKey(value).hashCode());</span><br><span class=\"line\">    output.collect(<span class=\"keyword\">new</span> <span class=\"title class_\">StreamRecord</span>&lt;&gt;(<span class=\"keyword\">new</span> <span class=\"title class_\">Tuple2</span>&lt;&gt;(value, bucket)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"title function_\">HashBucketAssigner</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">        SnapshotManager snapshotManager,</span></span><br><span class=\"line\"><span class=\"params\">        String commitUser,</span></span><br><span class=\"line\"><span class=\"params\">        IndexFileHandler indexFileHandler,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">int</span> numAssigners,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">int</span> assignId,</span></span><br><span class=\"line\"><span class=\"params\">        <span class=\"type\">long</span> targetBucketRowNumber)</span> &#123;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.snapshotManager = snapshotManager;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.commitUser = commitUser;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.indexFileHandler = indexFileHandler;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.numAssigners = numAssigners;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.assignId = assignId; <span class=\"comment\">// getRuntimeContext().getIndexOfThisSubtask()</span></span><br><span class=\"line\">    <span class=\"built_in\">this</span>.targetBucketRowNumber = targetBucketRowNumber;</span><br><span class=\"line\">    <span class=\"built_in\">this</span>.partitionIndex = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/** Assign a bucket for key hash of a record. */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">assign</span><span class=\"params\">(BinaryRow partition, <span class=\"type\">int</span> hash)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// hash: Record主键的hashcode，唯一确认一个Record</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">recordAssignId</span> <span class=\"operator\">=</span> computeAssignId(hash);</span><br><span class=\"line\">    <span class=\"comment\">// 可能是因为，Flink DAG前面已经通过主键的hashcode % channels了，所以一定相等</span></span><br><span class=\"line\">    checkArgument(</span><br><span class=\"line\">            recordAssignId == assignId,</span><br><span class=\"line\">            <span class=\"string\">&quot;This is a bug, record assign id %s should equal to assign id %s.&quot;</span>,</span><br><span class=\"line\">            recordAssignId,</span><br><span class=\"line\">            assignId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// PartitionIndex: Bucket Index Per Partition.</span></span><br><span class=\"line\">    <span class=\"comment\">// 为每一个partition计算对应的Bucket Index</span></span><br><span class=\"line\">    <span class=\"type\">PartitionIndex</span> <span class=\"variable\">index</span> <span class=\"operator\">=</span> partitionIndex.computeIfAbsent(partition, <span class=\"built_in\">this</span>::loadIndex);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> index.assign(hash, (bucket) -&gt; computeAssignId(bucket) == assignId);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">int</span> <span class=\"title function_\">computeAssignId</span><span class=\"params\">(<span class=\"type\">int</span> hash)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// numAssigners: getRuntimeContext().getNumberOfParallelSubtasks()</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.abs(hash % numAssigners);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   org.apache.paimon.index.PartitionIndex#assign</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">assign</span><span class=\"params\">(<span class=\"type\">int</span> hash, IntPredicate bucketFilterFunc)</span> &#123;</span><br><span class=\"line\">       accessed = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 1. is it a key that has appeared before</span></span><br><span class=\"line\">       <span class=\"comment\">// 注意：当发生Hash冲突的时候，两个不同的parimary key，会有相同的hashcode</span></span><br><span class=\"line\">       <span class=\"comment\">// 但是我们无法知道是否发生了冲突，本来需要bucketInformation.put(bucket, number + 1)，加1</span></span><br><span class=\"line\">       <span class=\"comment\">// 因此会导致设置的dynamic-bucket.target-row-num bucket中的条数不准确。</span></span><br><span class=\"line\">       <span class=\"comment\">// 只要hash冲突不严重，无伤大雅</span></span><br><span class=\"line\">       <span class=\"keyword\">if</span> (hash2Bucket.containsKey(hash)) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> hash2Bucket.get(hash);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 2. find bucket from existing buckets</span></span><br><span class=\"line\">       <span class=\"keyword\">for</span> (Integer bucket : bucketInformation.keySet()) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (bucketFilterFunc.test(bucket)) &#123;</span><br><span class=\"line\">               <span class=\"comment\">// it is my bucket</span></span><br><span class=\"line\">               <span class=\"type\">Long</span> <span class=\"variable\">number</span> <span class=\"operator\">=</span> bucketInformation.get(bucket);</span><br><span class=\"line\">               <span class=\"keyword\">if</span> (number &lt; targetBucketRowNumber) &#123;</span><br><span class=\"line\">                   bucketInformation.put(bucket, number + <span class=\"number\">1</span>);</span><br><span class=\"line\">                   hash2Bucket.put(hash, bucket.shortValue());</span><br><span class=\"line\">                   <span class=\"keyword\">return</span> bucket;</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">// 3. create a new bucket</span></span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>; i &lt; Short.MAX_VALUE; i++) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (bucketFilterFunc.test(i) &amp;&amp; !bucketInformation.containsKey(i)) &#123;</span><br><span class=\"line\">               hash2Bucket.put(hash, (<span class=\"type\">short</span>) i);</span><br><span class=\"line\">               bucketInformation.put(i, <span class=\"number\">1L</span>);</span><br><span class=\"line\">               <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"meta\">@SuppressWarnings(&quot;OptionalGetWithoutIsPresent&quot;)</span></span><br><span class=\"line\">       <span class=\"type\">int</span> <span class=\"variable\">maxBucket</span> <span class=\"operator\">=</span></span><br><span class=\"line\">               bucketInformation.keySet().stream().mapToInt(Integer::intValue).max().getAsInt();</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">RuntimeException</span>(</span><br><span class=\"line\">               String.format(</span><br><span class=\"line\">                       <span class=\"string\">&quot;To more bucket %s, you should increase target bucket row number %s.&quot;</span>,</span><br><span class=\"line\">                       maxBucket, targetBucketRowNumber));</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Spark Batch Read Paimon源码分析","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":26343,"date":"2023-06-21T09:57:25.000Z","updated":"2023-06-21T09:57:25.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n> A bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.\n>\n> | num-sorted-run.compaction-trigger | 5    | Integer | The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run). |\n> | --------------------------------- | ---- | ------- | ------------------------------------------------------------ |\n\n## Paimon Bucket && Sorted Run\n\n对于Paimon Primary Key主键表，一个Bucket对应一个LSM树，一个LSM树由多个Sorted Run构成。\n\n一个Sorted Run可能包含一个或多个文件，但是每个文件只能属于一个Sorted Run。\n\n- 对于Level-0层来说：一个SST文件对应一个Sorted Run，Level-0的文件是每次刷盘形成的，而Flink流写Paimon刷盘的时机是CheckPoint的时候或memory buffer is full。所以Level-0层会不断产生新文件，而每个文件就是一个Sorted Run，为了防止Level-0层小文件过多，Paimon会按照合并策略进行小文件合并。Paimon采用类似RocksDB中的UniversalCompaction合并策略，进行合并。\n- 对于其他Level层来说：一层对应一个Sorted Run。\n\n在排序运行中，数据文件的主键范围永远不会重叠。不同的排序运行可能具有重叠的主键范围，甚至可能包含相同的主键。查询LSM树时，必须合并所有排序的运行，并且必须根据用户指定的[合并引擎](https://paimon.apache.org/docs/master/concepts/primary-key-table/#merge-engines)和每条记录的时间戳来合并具有相同主键的所有记录。\n\n\n\n## Spark批读Paimon\n\nSpark批读Paimon的核心实现类为`org.apache.paimon.spark.SparkScan`。将Paimon表中的文件分成一个个Split交给Spark，Spark会一个Task读取一个Split，然后Spark就可以同时启动多个Task并行读取Paimon了。\n\n那么Paimon是如何将Paimon表中的文件划分成一个个Split呢？对于主键表，其核心实现类为`org.apache.paimon.table.source.MergeTreeSplitGenerator`。注释写的非常清楚，这个类就是为了将每个Bucket下面的文件划分为一个个Split，以达到并行执行的目的，大致流程如下：\n\n- 1、将文件划分为一个个section，该算法保证，每个section中的数据文件的主键范围永远不会重叠。这是为了保证Split和Split之间不存在主键范围重叠，这样每个Spark Task在读取Split的时候根据MergeEngine进行合并去重，就能保证全局范围上的主键唯一性。当然这个全局范围是指每个桶内主键不会重复，同时Paimon要求bucket-key必须为主键的一部分，这样就保证了Paimon在分区内主键的唯一性。\n- 2、根据算法将一个个section合并为一个个split，该算法主要是为了将一些小section合并为一个split，减少小文件过多对查询性能的影响。\n\n```java\n    @Override\n    public List<List<DataFileMeta>> split(List<DataFileMeta> files) {\n        /*\n         * The generator aims to parallel the scan execution by slicing the files of each bucket\n         * into multiple splits. The generation has one constraint: files with intersected key\n         * ranges (within one section) must go to the same split. Therefore, the files are first to go\n         * through the interval partition algorithm to generate sections and then through the\n         * OrderedPack algorithm. Note that the item to be packed here is each section, the capacity\n         * is denoted as the targetSplitSize, and the final number of the bins is the number of\n         * splits generated.\n         *\n         * For instance, there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]\n         * with targetSplitSize 128M. After interval partition, there are four sections:\n         * - section1: [1, 2]\n         * - section2: [3, 4]\n         * - section3: [5, 180], [5, 190]\n         * - section4: [200, 600], [210, 700]\n         *\n         * After OrderedPack, section1 and section2 will be put into one bin (split), so the final result will be:\n         * - split1: [1, 2] [3, 4]\n         * - split2: [5, 180] [5,190]\n         * - split3: [200, 600] [210, 700]\n         */\n        List<List<DataFileMeta>> sections =\n                new IntervalPartition(files, keyComparator)\n                        .partition().stream().map(this::flatRun).collect(Collectors.toList());\n\n        return packSplits(sections);\n    }\n```\n\n\n\n## Paimon主键表分区数据量最佳大小\n\n那么Paimon每个分区的数据量，这里指data size in each bucket，多少是最合适呢？文档中推荐为1GB。\n\n根据上文可知，存在overlap的文件必须划分进一个Split，也就是一个并行度。如果是几个大文件存在overlap，这几个文件就只能划分进一个Split，就会造成只能一个Task读取合并这几个大文件，这个Task会处理的很慢，进而拖慢整个作业，甚至会在合并去重的时候，因为内存不足造成Task失败，进而造成整个Job失败，这显然不能接受。因此data size in each bucket一定不能太大。\n\nParquet最佳文件大小为每个文件数百 Mb（最高可达 1 GB）。那么data size in each bucket的最佳文件大小也应该为数百 Mb（最高可达 1 GB）。\n\n","source":"_posts/bigdata/paimon/Spark Batch Read on Paimon.md","raw":"---\ntitle: Spark Batch Read Paimon源码分析\ntags:\n  - paimon\ncategories:\n  - - bigdata\n    - paimon\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 26343\ndate: 2023-06-21 17:57:25\nupdated: 2023-06-21 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n> A bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.\n>\n> | num-sorted-run.compaction-trigger | 5    | Integer | The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run). |\n> | --------------------------------- | ---- | ------- | ------------------------------------------------------------ |\n\n## Paimon Bucket && Sorted Run\n\n对于Paimon Primary Key主键表，一个Bucket对应一个LSM树，一个LSM树由多个Sorted Run构成。\n\n一个Sorted Run可能包含一个或多个文件，但是每个文件只能属于一个Sorted Run。\n\n- 对于Level-0层来说：一个SST文件对应一个Sorted Run，Level-0的文件是每次刷盘形成的，而Flink流写Paimon刷盘的时机是CheckPoint的时候或memory buffer is full。所以Level-0层会不断产生新文件，而每个文件就是一个Sorted Run，为了防止Level-0层小文件过多，Paimon会按照合并策略进行小文件合并。Paimon采用类似RocksDB中的UniversalCompaction合并策略，进行合并。\n- 对于其他Level层来说：一层对应一个Sorted Run。\n\n在排序运行中，数据文件的主键范围永远不会重叠。不同的排序运行可能具有重叠的主键范围，甚至可能包含相同的主键。查询LSM树时，必须合并所有排序的运行，并且必须根据用户指定的[合并引擎](https://paimon.apache.org/docs/master/concepts/primary-key-table/#merge-engines)和每条记录的时间戳来合并具有相同主键的所有记录。\n\n\n\n## Spark批读Paimon\n\nSpark批读Paimon的核心实现类为`org.apache.paimon.spark.SparkScan`。将Paimon表中的文件分成一个个Split交给Spark，Spark会一个Task读取一个Split，然后Spark就可以同时启动多个Task并行读取Paimon了。\n\n那么Paimon是如何将Paimon表中的文件划分成一个个Split呢？对于主键表，其核心实现类为`org.apache.paimon.table.source.MergeTreeSplitGenerator`。注释写的非常清楚，这个类就是为了将每个Bucket下面的文件划分为一个个Split，以达到并行执行的目的，大致流程如下：\n\n- 1、将文件划分为一个个section，该算法保证，每个section中的数据文件的主键范围永远不会重叠。这是为了保证Split和Split之间不存在主键范围重叠，这样每个Spark Task在读取Split的时候根据MergeEngine进行合并去重，就能保证全局范围上的主键唯一性。当然这个全局范围是指每个桶内主键不会重复，同时Paimon要求bucket-key必须为主键的一部分，这样就保证了Paimon在分区内主键的唯一性。\n- 2、根据算法将一个个section合并为一个个split，该算法主要是为了将一些小section合并为一个split，减少小文件过多对查询性能的影响。\n\n```java\n    @Override\n    public List<List<DataFileMeta>> split(List<DataFileMeta> files) {\n        /*\n         * The generator aims to parallel the scan execution by slicing the files of each bucket\n         * into multiple splits. The generation has one constraint: files with intersected key\n         * ranges (within one section) must go to the same split. Therefore, the files are first to go\n         * through the interval partition algorithm to generate sections and then through the\n         * OrderedPack algorithm. Note that the item to be packed here is each section, the capacity\n         * is denoted as the targetSplitSize, and the final number of the bins is the number of\n         * splits generated.\n         *\n         * For instance, there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]\n         * with targetSplitSize 128M. After interval partition, there are four sections:\n         * - section1: [1, 2]\n         * - section2: [3, 4]\n         * - section3: [5, 180], [5, 190]\n         * - section4: [200, 600], [210, 700]\n         *\n         * After OrderedPack, section1 and section2 will be put into one bin (split), so the final result will be:\n         * - split1: [1, 2] [3, 4]\n         * - split2: [5, 180] [5,190]\n         * - split3: [200, 600] [210, 700]\n         */\n        List<List<DataFileMeta>> sections =\n                new IntervalPartition(files, keyComparator)\n                        .partition().stream().map(this::flatRun).collect(Collectors.toList());\n\n        return packSplits(sections);\n    }\n```\n\n\n\n## Paimon主键表分区数据量最佳大小\n\n那么Paimon每个分区的数据量，这里指data size in each bucket，多少是最合适呢？文档中推荐为1GB。\n\n根据上文可知，存在overlap的文件必须划分进一个Split，也就是一个并行度。如果是几个大文件存在overlap，这几个文件就只能划分进一个Split，就会造成只能一个Task读取合并这几个大文件，这个Task会处理的很慢，进而拖慢整个作业，甚至会在合并去重的时候，因为内存不足造成Task失败，进而造成整个Job失败，这显然不能接受。因此data size in each bucket一定不能太大。\n\nParquet最佳文件大小为每个文件数百 Mb（最高可达 1 GB）。那么data size in each bucket的最佳文件大小也应该为数百 Mb（最高可达 1 GB）。\n\n","slug":"bigdata/paimon/Spark Batch Read on Paimon","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt3003r8j5mfp1v6n5a","content":"<blockquote>\n<p>A bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.</p>\n<table>\n<thead>\n<tr>\n<th>num-sorted-run.compaction-trigger</th>\n<th>5</th>\n<th>Integer</th>\n<th>The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).</th>\n</tr>\n</thead>\n</table>\n</blockquote>\n<h2 id=\"Paimon-Bucket-amp-amp-Sorted-Run\"><a href=\"#Paimon-Bucket-amp-amp-Sorted-Run\" class=\"headerlink\" title=\"Paimon Bucket &amp;&amp; Sorted Run\"></a>Paimon Bucket &amp;&amp; Sorted Run</h2><p>对于Paimon Primary Key主键表，一个Bucket对应一个LSM树，一个LSM树由多个Sorted Run构成。</p>\n<p>一个Sorted Run可能包含一个或多个文件，但是每个文件只能属于一个Sorted Run。</p>\n<ul>\n<li>对于Level-0层来说：一个SST文件对应一个Sorted Run，Level-0的文件是每次刷盘形成的，而Flink流写Paimon刷盘的时机是CheckPoint的时候或memory buffer is full。所以Level-0层会不断产生新文件，而每个文件就是一个Sorted Run，为了防止Level-0层小文件过多，Paimon会按照合并策略进行小文件合并。Paimon采用类似RocksDB中的UniversalCompaction合并策略，进行合并。</li>\n<li>对于其他Level层来说：一层对应一个Sorted Run。</li>\n</ul>\n<p>在排序运行中，数据文件的主键范围永远不会重叠。不同的排序运行可能具有重叠的主键范围，甚至可能包含相同的主键。查询LSM树时，必须合并所有排序的运行，并且必须根据用户指定的<a href=\"https://paimon.apache.org/docs/master/concepts/primary-key-table/#merge-engines\">合并引擎</a>和每条记录的时间戳来合并具有相同主键的所有记录。</p>\n<h2 id=\"Spark批读Paimon\"><a href=\"#Spark批读Paimon\" class=\"headerlink\" title=\"Spark批读Paimon\"></a>Spark批读Paimon</h2><p>Spark批读Paimon的核心实现类为<code>org.apache.paimon.spark.SparkScan</code>。将Paimon表中的文件分成一个个Split交给Spark，Spark会一个Task读取一个Split，然后Spark就可以同时启动多个Task并行读取Paimon了。</p>\n<p>那么Paimon是如何将Paimon表中的文件划分成一个个Split呢？对于主键表，其核心实现类为<code>org.apache.paimon.table.source.MergeTreeSplitGenerator</code>。注释写的非常清楚，这个类就是为了将每个Bucket下面的文件划分为一个个Split，以达到并行执行的目的，大致流程如下：</p>\n<ul>\n<li>1、将文件划分为一个个section，该算法保证，每个section中的数据文件的主键范围永远不会重叠。这是为了保证Split和Split之间不存在主键范围重叠，这样每个Spark Task在读取Split的时候根据MergeEngine进行合并去重，就能保证全局范围上的主键唯一性。当然这个全局范围是指每个桶内主键不会重复，同时Paimon要求bucket-key必须为主键的一部分，这样就保证了Paimon在分区内主键的唯一性。</li>\n<li>2、根据算法将一个个section合并为一个个split，该算法主要是为了将一些小section合并为一个split，减少小文件过多对查询性能的影响。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> List&lt;List&lt;DataFileMeta&gt;&gt; <span class=\"title function_\">split</span><span class=\"params\">(List&lt;DataFileMeta&gt; files)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">     * The generator aims to parallel the scan execution by slicing the files of each bucket</span></span><br><span class=\"line\"><span class=\"comment\">     * into multiple splits. The generation has one constraint: files with intersected key</span></span><br><span class=\"line\"><span class=\"comment\">     * ranges (within one section) must go to the same split. Therefore, the files are first to go</span></span><br><span class=\"line\"><span class=\"comment\">     * through the interval partition algorithm to generate sections and then through the</span></span><br><span class=\"line\"><span class=\"comment\">     * OrderedPack algorithm. Note that the item to be packed here is each section, the capacity</span></span><br><span class=\"line\"><span class=\"comment\">     * is denoted as the targetSplitSize, and the final number of the bins is the number of</span></span><br><span class=\"line\"><span class=\"comment\">     * splits generated.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * For instance, there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     * with targetSplitSize 128M. After interval partition, there are four sections:</span></span><br><span class=\"line\"><span class=\"comment\">     * - section1: [1, 2]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section2: [3, 4]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section3: [5, 180], [5, 190]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section4: [200, 600], [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * After OrderedPack, section1 and section2 will be put into one bin (split), so the final result will be:</span></span><br><span class=\"line\"><span class=\"comment\">     * - split1: [1, 2] [3, 4]</span></span><br><span class=\"line\"><span class=\"comment\">     * - split2: [5, 180] [5,190]</span></span><br><span class=\"line\"><span class=\"comment\">     * - split3: [200, 600] [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;List&lt;DataFileMeta&gt;&gt; sections =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">IntervalPartition</span>(files, keyComparator)</span><br><span class=\"line\">                    .partition().stream().map(<span class=\"built_in\">this</span>::flatRun).collect(Collectors.toList());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> packSplits(sections);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"Paimon主键表分区数据量最佳大小\"><a href=\"#Paimon主键表分区数据量最佳大小\" class=\"headerlink\" title=\"Paimon主键表分区数据量最佳大小\"></a>Paimon主键表分区数据量最佳大小</h2><p>那么Paimon每个分区的数据量，这里指data size in each bucket，多少是最合适呢？文档中推荐为1GB。</p>\n<p>根据上文可知，存在overlap的文件必须划分进一个Split，也就是一个并行度。如果是几个大文件存在overlap，这几个文件就只能划分进一个Split，就会造成只能一个Task读取合并这几个大文件，这个Task会处理的很慢，进而拖慢整个作业，甚至会在合并去重的时候，因为内存不足造成Task失败，进而造成整个Job失败，这显然不能接受。因此data size in each bucket一定不能太大。</p>\n<p>Parquet最佳文件大小为每个文件数百 Mb（最高可达 1 GB）。那么data size in each bucket的最佳文件大小也应该为数百 Mb（最高可达 1 GB）。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>A bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.</p>\n<table>\n<thead>\n<tr>\n<th>num-sorted-run.compaction-trigger</th>\n<th>5</th>\n<th>Integer</th>\n<th>The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).</th>\n</tr>\n</thead>\n</table>\n</blockquote>\n<h2 id=\"Paimon-Bucket-amp-amp-Sorted-Run\"><a href=\"#Paimon-Bucket-amp-amp-Sorted-Run\" class=\"headerlink\" title=\"Paimon Bucket &amp;&amp; Sorted Run\"></a>Paimon Bucket &amp;&amp; Sorted Run</h2><p>对于Paimon Primary Key主键表，一个Bucket对应一个LSM树，一个LSM树由多个Sorted Run构成。</p>\n<p>一个Sorted Run可能包含一个或多个文件，但是每个文件只能属于一个Sorted Run。</p>\n<ul>\n<li>对于Level-0层来说：一个SST文件对应一个Sorted Run，Level-0的文件是每次刷盘形成的，而Flink流写Paimon刷盘的时机是CheckPoint的时候或memory buffer is full。所以Level-0层会不断产生新文件，而每个文件就是一个Sorted Run，为了防止Level-0层小文件过多，Paimon会按照合并策略进行小文件合并。Paimon采用类似RocksDB中的UniversalCompaction合并策略，进行合并。</li>\n<li>对于其他Level层来说：一层对应一个Sorted Run。</li>\n</ul>\n<p>在排序运行中，数据文件的主键范围永远不会重叠。不同的排序运行可能具有重叠的主键范围，甚至可能包含相同的主键。查询LSM树时，必须合并所有排序的运行，并且必须根据用户指定的<a href=\"https://paimon.apache.org/docs/master/concepts/primary-key-table/#merge-engines\">合并引擎</a>和每条记录的时间戳来合并具有相同主键的所有记录。</p>\n<h2 id=\"Spark批读Paimon\"><a href=\"#Spark批读Paimon\" class=\"headerlink\" title=\"Spark批读Paimon\"></a>Spark批读Paimon</h2><p>Spark批读Paimon的核心实现类为<code>org.apache.paimon.spark.SparkScan</code>。将Paimon表中的文件分成一个个Split交给Spark，Spark会一个Task读取一个Split，然后Spark就可以同时启动多个Task并行读取Paimon了。</p>\n<p>那么Paimon是如何将Paimon表中的文件划分成一个个Split呢？对于主键表，其核心实现类为<code>org.apache.paimon.table.source.MergeTreeSplitGenerator</code>。注释写的非常清楚，这个类就是为了将每个Bucket下面的文件划分为一个个Split，以达到并行执行的目的，大致流程如下：</p>\n<ul>\n<li>1、将文件划分为一个个section，该算法保证，每个section中的数据文件的主键范围永远不会重叠。这是为了保证Split和Split之间不存在主键范围重叠，这样每个Spark Task在读取Split的时候根据MergeEngine进行合并去重，就能保证全局范围上的主键唯一性。当然这个全局范围是指每个桶内主键不会重复，同时Paimon要求bucket-key必须为主键的一部分，这样就保证了Paimon在分区内主键的唯一性。</li>\n<li>2、根据算法将一个个section合并为一个个split，该算法主要是为了将一些小section合并为一个split，减少小文件过多对查询性能的影响。</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> List&lt;List&lt;DataFileMeta&gt;&gt; <span class=\"title function_\">split</span><span class=\"params\">(List&lt;DataFileMeta&gt; files)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">     * The generator aims to parallel the scan execution by slicing the files of each bucket</span></span><br><span class=\"line\"><span class=\"comment\">     * into multiple splits. The generation has one constraint: files with intersected key</span></span><br><span class=\"line\"><span class=\"comment\">     * ranges (within one section) must go to the same split. Therefore, the files are first to go</span></span><br><span class=\"line\"><span class=\"comment\">     * through the interval partition algorithm to generate sections and then through the</span></span><br><span class=\"line\"><span class=\"comment\">     * OrderedPack algorithm. Note that the item to be packed here is each section, the capacity</span></span><br><span class=\"line\"><span class=\"comment\">     * is denoted as the targetSplitSize, and the final number of the bins is the number of</span></span><br><span class=\"line\"><span class=\"comment\">     * splits generated.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * For instance, there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     * with targetSplitSize 128M. After interval partition, there are four sections:</span></span><br><span class=\"line\"><span class=\"comment\">     * - section1: [1, 2]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section2: [3, 4]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section3: [5, 180], [5, 190]</span></span><br><span class=\"line\"><span class=\"comment\">     * - section4: [200, 600], [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * After OrderedPack, section1 and section2 will be put into one bin (split), so the final result will be:</span></span><br><span class=\"line\"><span class=\"comment\">     * - split1: [1, 2] [3, 4]</span></span><br><span class=\"line\"><span class=\"comment\">     * - split2: [5, 180] [5,190]</span></span><br><span class=\"line\"><span class=\"comment\">     * - split3: [200, 600] [210, 700]</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;List&lt;DataFileMeta&gt;&gt; sections =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">IntervalPartition</span>(files, keyComparator)</span><br><span class=\"line\">                    .partition().stream().map(<span class=\"built_in\">this</span>::flatRun).collect(Collectors.toList());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> packSplits(sections);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"Paimon主键表分区数据量最佳大小\"><a href=\"#Paimon主键表分区数据量最佳大小\" class=\"headerlink\" title=\"Paimon主键表分区数据量最佳大小\"></a>Paimon主键表分区数据量最佳大小</h2><p>那么Paimon每个分区的数据量，这里指data size in each bucket，多少是最合适呢？文档中推荐为1GB。</p>\n<p>根据上文可知，存在overlap的文件必须划分进一个Split，也就是一个并行度。如果是几个大文件存在overlap，这几个文件就只能划分进一个Split，就会造成只能一个Task读取合并这几个大文件，这个Task会处理的很慢，进而拖慢整个作业，甚至会在合并去重的时候，因为内存不足造成Task失败，进而造成整个Job失败，这显然不能接受。因此data size in each bucket一定不能太大。</p>\n<p>Parquet最佳文件大小为每个文件数百 Mb（最高可达 1 GB）。那么data size in each bucket的最佳文件大小也应该为数百 Mb（最高可达 1 GB）。</p>\n"},{"title":"Paimon-Flink-Sink源码分析","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":27532,"date":"2023-05-24T09:57:25.000Z","updated":"2022-05-24T09:57:25.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## 前言\n\nPaimon在Flink下面的层次结构，大概为：Catalog -> Database -> Table -> Record。因此看Paimon如何在Flink中实现Read、Write等操作，先从Catalog开始。\n\n\n\n## Catalog\n\nPaimon在Flink中实现Catalog的源码位于`org.apache.paimon.flink.FlinkCatalog`，该类实现了`org.apache.flink.table.catalog.Catalog`接口。该接口中定义了一系列方法，包括listTables、listViews等方法，而paimon一一实现了这些接口。\n\n在Flink中自定义Catalog，还需要实现`org.apache.flink.table.factories.CatalogFactory`工厂接口，paimon中对应的为`org.apache.paimon.flink.FlinkCatalogFactory`类。该工厂类当然是用来创建Catalog了。\n\n最后还需要将Catalog工厂实现类，添加到将此实现类添加到 `META_INF/services/org.apache.flink.table.factories.Factory` 中。用于SPI。\n\n\n\n## Table\n\n在paimon实现的FlinkCatalog类中，`org.apache.paimon.flink.FlinkCatalog#getFactory`方法，用于提供写入和读取Paimon的具体实现。\n\n```java\n    @Override\n    public Optional<Factory> getFactory() {\n        return Optional.of(new FlinkTableFactory(catalog.lockFactory().orElse(null)));\n    }\n```\n\n在Paimon中，这个方法返回的是`org.apache.paimon.flink.FlinkTableFactory`。该工厂类实现了`org.apache.flink.table.factories.DynamicTableSourceFactory`，`org.apache.flink.table.factories.DynamicTableSinkFactory`接口。分别用来实现读取和写入Paimon表的逻辑。而写入Paimon表的实现类为`org.apache.paimon.flink.sink.FlinkTableSink`。该类主要实现了`org.apache.flink.table.connector.sink.DynamicTableSink`接口。\n\n```java\n/**\n * Sink of a dynamic table to an external storage system.\n *\n * <p>Dynamic tables are the core concept of Flink's Table & SQL API for processing both bounded and\n * unbounded data in a unified fashion. By definition, a dynamic table can change over time.\n *\n * <p>When writing a dynamic table, the content can always be considered as a changelog (finite or\n * infinite) for which all changes are written out continuously until the changelog is exhausted.\n * The given {@link ChangelogMode} indicates the set of changes that the sink accepts during\n * runtime.\n *\n * <p>For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded\n * streams.\n *\n * <p>For regular streaming scenarios, the sink can solely accept insert-only rows and can write out\n * unbounded streams.\n *\n * <p>For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams\n * with insert, update, and delete rows. See also {@link RowKind}.\n *\n * <p>Instances of {@link DynamicTableSink} can be seen as factories that eventually produce\n * concrete runtime implementation for writing the actual data.\n *\n * <p>Depending on the optionally declared abilities, the planner might apply changes to an instance\n * and thus mutate the produced runtime implementation.\n *\n * <p>A {@link DynamicTableSink} can implement the following abilities:\n *\n * <ul>\n *   <li>{@link SupportsPartitioning}\n *   <li>{@link SupportsOverwrite}\n *   <li>{@link SupportsWritingMetadata}\n * </ul>\n *\n * <p>In the last step, the planner will call {@link #getSinkRuntimeProvider(Context)} for obtaining\n * a provider of runtime implementation.\n */\n@PublicEvolving\npublic interface DynamicTableSink {\n\n    /**\n     * Returns the set of changes that the sink accepts during runtime.\n     *\n     * <p>The planner can make suggestions but the sink has the final decision what it requires. If\n     * the planner does not support this mode, it will throw an error. For example, the sink can\n     * return that it only supports {@link ChangelogMode#insertOnly()}.\n     *\n     * @param requestedMode expected set of changes by the current plan\n     */\n    ChangelogMode getChangelogMode(ChangelogMode requestedMode);\n\n    /**\n     * Returns a provider of runtime implementation for writing the data.\n     *\n     * <p>There might exist different interfaces for runtime implementation which is why {@link\n     * SinkRuntimeProvider} serves as the base interface. Concrete {@link SinkRuntimeProvider}\n     * interfaces might be located in other Flink modules.\n     *\n     * <p>Independent of the provider interface, the table runtime expects that a sink\n     * implementation accepts internal data structures (see {@link RowData} for more information).\n     *\n     * <p>The given {@link Context} offers utilities by the planner for creating runtime\n     * implementation with minimal dependencies to internal data structures.\n     *\n     * <p>{@link SinkProvider} is the recommended core interface. {@code SinkFunctionProvider} in\n     * {@code flink-table-api-java-bridge} and {@link OutputFormatProvider} are available for\n     * backwards compatibility.\n     *\n     * @see SinkProvider\n     */\n    SinkRuntimeProvider getSinkRuntimeProvider(Context context);\n\n    /**\n     * Creates a copy of this instance during planning. The copy should be a deep copy of all\n     * mutable members.\n     */\n    DynamicTableSink copy();\n\n    /** Returns a string that summarizes this sink for printing to a console or log. */\n    String asSummaryString();\n\n}\n\n```\n\n> 将动态表汇入外部存储系统。\n> 动态表是 Flink 的 Table & SQL API 的核心概念，用于以统一的方式处理有界和无界数据。根据定义，动态表可以随时间变化。\n> 在编写动态表时，内容始终可以被视为一个 changelog（有限或无限），所有更改都被连续写出，直到 changelog 耗尽。给定的ChangelogMode指示接收器在运行时接受的更改集。\n> 对于常规批处理方案，接收器只能接受仅插入行并写出有界流。\n> 对于常规流场景，接收器只能接受仅插入行，并且可以写出无界流。\n> 对于变更数据捕获 (CDC) 场景，接收器可以写出带有插入、更新和删除行的有界或无界流。另请参阅RowKind 。\n> DynamicTableSink的实例可以被视为最终生成用于写入实际数据的具体运行时实现的工厂。\n> 根据可选声明的能力，规划器可能会将更改应用于实例，从而改变生成的运行时实现。\n> DynamicTableSink可以实现以下功能：\n> SupportsPartitioning\n> SupportsOverwrite\n> SupportsWritingMetadata\n> 在最后一步中，规划器将调用getSinkRuntimeProvider(DynamicTableSink.Context)来获取运行时实现的提供者。\n\n\n\n## Write\n\n### 构建Paimon Sink Flink DAG源码流程\n\n入口类以及对应的入口方法为：org.apache.paimon.flink.sink.FlinkSinkBuilder#build，\n进入这个方法中看看，发现会先对DataStream进行，按照分桶进行分区转换\n\n```java\norg.apache.paimon.flink.sink.FlinkSinkBuilder#build\n\npublic DataStreamSink<?> build() {\n    BucketingStreamPartitioner<RowData> partitioner =\n            new BucketingStreamPartitioner<>(\n                    new RowDataChannelComputer(table.schema(), logSinkFunction != null));\n    PartitionTransformation<RowData> partitioned =\n            new PartitionTransformation<>(input.getTransformation(), partitioner);\n    if (parallelism != null) {\n        partitioned.setParallelism(parallelism);\n    }\n\n    StreamExecutionEnvironment env = input.getExecutionEnvironment();\n    // 构建Flink paimon sink DAG类\n    FileStoreSink sink =\n            new FileStoreSink(table, lockFactory, overwritePartition, logSinkFunction);\n    return commitUser != null && sinkProvider != null\n            ? sink.sinkFrom(new DataStream<>(env, partitioned), commitUser, sinkProvider)\n            : sink.sinkFrom(new DataStream<>(env, partitioned));\n}\n```\n\n- 1、createWriteOperator：实际进行写入Record的算子, org.apache.paimon.flink.sink.RowDataStoreWriteOperator\n  org.apache.paimon.flink.AbstractFlinkTableFactory#buildPaimonTable\n\n- 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。\n```java\norg.apache.paimon.flink.sink.FlinkSink#sinkFrom(org.apache.flink.streaming.api.datastream.DataStream<T>, java.lang.String, org.apache.paimon.flink.sink.StoreSinkWrite.Provider);\n\npublic DataStreamSink<?> sinkFrom(\n  DataStream<T> input, String commitUser, StoreSinkWrite.Provider sinkProvider) {\n  StreamExecutionEnvironment env = input.getExecutionEnvironment();\n  ReadableConfig conf = StreamExecutionEnvironmentUtils.getConfiguration(env);\n  CheckpointConfig checkpointConfig = env.getCheckpointConfig();\n\n  boolean isStreaming =\n    conf.get(ExecutionOptions.RUNTIME_MODE) == RuntimeExecutionMode.STREAMING;\n  boolean streamingCheckpointEnabled =\n    isStreaming && checkpointConfig.isCheckpointingEnabled();\n  if (streamingCheckpointEnabled) {\n    assertCheckpointConfiguration(env);\n  }\n\n  CommittableTypeInfo typeInfo = new CommittableTypeInfo();\n  SingleOutputStreamOperator<Committable> written =\n    input.transform(\n      WRITER_NAME + \" -> \" + table.name(),\n      typeInfo,\n      // 1、createWriteOperator：实际进行写入Record的算子\n      createWriteOperator(sinkProvider, isStreaming, commitUser))\n      .setParallelism(input.getParallelism());\n\n  SingleOutputStreamOperator<?> committed =\n    written.transform(\n      GLOBAL_COMMITTER_NAME + \" -> \" + table.name(),\n      typeInfo,\n      // 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。\n      new CommitterOperator(\n        streamingCheckpointEnabled,\n        commitUser,\n        createCommitterFactory(streamingCheckpointEnabled),\n        createCommittableStateManager()))\n      .setParallelism(1)\n      .setMaxParallelism(1);\n  return committed.addSink(new DiscardingSink<>()).name(\"end\").setParallelism(1);\n}\n```\n\n\n-  1、createWriteOperator\n```scala\nprivate StoreSinkWrite.Provider createWriteProvider(CheckpointConfig checkpointConfig) {\n    boolean waitCompaction;\n\n    if (table.coreOptions().writeOnly()) {\n        // 如果配置为writeOnly()，则不进行在线压缩\n        waitCompaction = false;\n    } else {\n        Options options = table.coreOptions().toConfiguration();\n        ChangelogProducer changelogProducer = table.coreOptions().changelogProducer();\n        // 当ChangelogProducer为LOOKUP时，则等待压缩\n        waitCompaction =\n                changelogProducer == ChangelogProducer.LOOKUP\n                        && options.get(CHANGELOG_PRODUCER_LOOKUP_WAIT);\n        \n        // 决定FULL_COMPACTION的压缩间隔\n        int deltaCommits = -1;\n        if (options.contains(FULL_COMPACTION_DELTA_COMMITS)) {\n            deltaCommits = options.get(FULL_COMPACTION_DELTA_COMMITS);\n        } else if (options.contains(CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL)) {\n            long fullCompactionThresholdMs =\n                    options.get(CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL).toMillis();\n            deltaCommits =\n                    (int)\n                            (fullCompactionThresholdMs\n                                    / checkpointConfig.getCheckpointInterval());\n        }\n        \n        // Generate changelog files with each full compaction\n        // 当进行FULL_COMPACTION的时候，需要生成changelog files\n        if (changelogProducer == ChangelogProducer.FULL_COMPACTION || deltaCommits >= 0) {\n            int finalDeltaCommits = Math.max(deltaCommits, 1);\n            return (table, commitUser, state, ioManager) ->\n                    new GlobalFullCompactionSinkWrite(\n                            table,\n                            commitUser,\n                            state,\n                            ioManager,\n                            isOverwrite,\n                            waitCompaction,\n                            finalDeltaCommits);\n        }\n    }\n\n    return (table, commitUser, state, ioManager) ->\n            new StoreSinkWriteImpl(\n                    table, commitUser, state, ioManager, isOverwrite, waitCompaction);\n}\n```\n\n### Paimon Flink CK流程\n我们知道Flink paimon写入主要涉及两个算子：\n\n1、org.apache.paimon.flink.sink.RowDataStoreWriteOperator\n这个算子实现了`org.apache.flink.streaming.api.operators.StreamOperator#prepareSnapshotPreBarrier`方法，这个方法会在算子接受到driver的checkpoint请求后被调用。在prepareSnapshotPreBarrier方法中会调用`org.apache.paimon.flink.sink.PrepareCommitOperator#emitCommittables`方法，emitCommittables方法的作用是向后面的commit算子发送committable信息。\n\n然而这个emitCommittables方法，又会调用prepareCommit方法，最终会调用`org.apache.paimon.mergetree.MergeTreeWriter#prepareCommit`\n**因此Flink每次进行checkpoint的时候，Paimon都会强制进行Memory Flush，完成数据的落盘，保证数据写入到文件系统，完成写入事务，保证一致性。**\n\n```java\norg.apache.paimon.flink.sink.PrepareCommitOperator#prepareSnapshotPreBarrier\n\n@Override\npublic void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n  if (!endOfInput) {\n    emitCommittables(false, checkpointId);\n  }\n  // no records are expected to emit after endOfInput\n}\n\n@Override\npublic void endInput() throws Exception {\n  endOfInput = true;\n  emitCommittables(true, Long.MAX_VALUE);\n}\n\nprivate void emitCommittables(boolean doCompaction, long checkpointId) throws IOException {\n  prepareCommit(doCompaction, checkpointId)\n    .forEach(committable -> output.collect(new StreamRecord<>(committable)));\n}\n\nprotected abstract List<Committable> prepareCommit(boolean doCompaction, long checkpointId) throws IOException;\n```\n2、CommitterOperator\n这个算子实现了`org.apache.flink.api.common.state.CheckpointListener#notifyCheckpointComplete`方法，该方法会在FLink CK完成后被调用，paimon-flink-sink在这个方法中会进行snapshot快照的提交，主要就是将本次快照生成的snapshot、manifest文件写入到文件系统。\n\n","source":"_posts/bigdata/paimon/Paimon-Flink-Sink源码分析.md","raw":"---\ntitle: Paimon-Flink-Sink源码分析\ntags:\n  - paimon\ncategories:\n  - - bigdata\n    - paimon\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 27532\ndate: 2023-05-24 17:57:25\nupdated: 2022-05-24 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\nPaimon在Flink下面的层次结构，大概为：Catalog -> Database -> Table -> Record。因此看Paimon如何在Flink中实现Read、Write等操作，先从Catalog开始。\n\n\n\n## Catalog\n\nPaimon在Flink中实现Catalog的源码位于`org.apache.paimon.flink.FlinkCatalog`，该类实现了`org.apache.flink.table.catalog.Catalog`接口。该接口中定义了一系列方法，包括listTables、listViews等方法，而paimon一一实现了这些接口。\n\n在Flink中自定义Catalog，还需要实现`org.apache.flink.table.factories.CatalogFactory`工厂接口，paimon中对应的为`org.apache.paimon.flink.FlinkCatalogFactory`类。该工厂类当然是用来创建Catalog了。\n\n最后还需要将Catalog工厂实现类，添加到将此实现类添加到 `META_INF/services/org.apache.flink.table.factories.Factory` 中。用于SPI。\n\n\n\n## Table\n\n在paimon实现的FlinkCatalog类中，`org.apache.paimon.flink.FlinkCatalog#getFactory`方法，用于提供写入和读取Paimon的具体实现。\n\n```java\n    @Override\n    public Optional<Factory> getFactory() {\n        return Optional.of(new FlinkTableFactory(catalog.lockFactory().orElse(null)));\n    }\n```\n\n在Paimon中，这个方法返回的是`org.apache.paimon.flink.FlinkTableFactory`。该工厂类实现了`org.apache.flink.table.factories.DynamicTableSourceFactory`，`org.apache.flink.table.factories.DynamicTableSinkFactory`接口。分别用来实现读取和写入Paimon表的逻辑。而写入Paimon表的实现类为`org.apache.paimon.flink.sink.FlinkTableSink`。该类主要实现了`org.apache.flink.table.connector.sink.DynamicTableSink`接口。\n\n```java\n/**\n * Sink of a dynamic table to an external storage system.\n *\n * <p>Dynamic tables are the core concept of Flink's Table & SQL API for processing both bounded and\n * unbounded data in a unified fashion. By definition, a dynamic table can change over time.\n *\n * <p>When writing a dynamic table, the content can always be considered as a changelog (finite or\n * infinite) for which all changes are written out continuously until the changelog is exhausted.\n * The given {@link ChangelogMode} indicates the set of changes that the sink accepts during\n * runtime.\n *\n * <p>For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded\n * streams.\n *\n * <p>For regular streaming scenarios, the sink can solely accept insert-only rows and can write out\n * unbounded streams.\n *\n * <p>For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams\n * with insert, update, and delete rows. See also {@link RowKind}.\n *\n * <p>Instances of {@link DynamicTableSink} can be seen as factories that eventually produce\n * concrete runtime implementation for writing the actual data.\n *\n * <p>Depending on the optionally declared abilities, the planner might apply changes to an instance\n * and thus mutate the produced runtime implementation.\n *\n * <p>A {@link DynamicTableSink} can implement the following abilities:\n *\n * <ul>\n *   <li>{@link SupportsPartitioning}\n *   <li>{@link SupportsOverwrite}\n *   <li>{@link SupportsWritingMetadata}\n * </ul>\n *\n * <p>In the last step, the planner will call {@link #getSinkRuntimeProvider(Context)} for obtaining\n * a provider of runtime implementation.\n */\n@PublicEvolving\npublic interface DynamicTableSink {\n\n    /**\n     * Returns the set of changes that the sink accepts during runtime.\n     *\n     * <p>The planner can make suggestions but the sink has the final decision what it requires. If\n     * the planner does not support this mode, it will throw an error. For example, the sink can\n     * return that it only supports {@link ChangelogMode#insertOnly()}.\n     *\n     * @param requestedMode expected set of changes by the current plan\n     */\n    ChangelogMode getChangelogMode(ChangelogMode requestedMode);\n\n    /**\n     * Returns a provider of runtime implementation for writing the data.\n     *\n     * <p>There might exist different interfaces for runtime implementation which is why {@link\n     * SinkRuntimeProvider} serves as the base interface. Concrete {@link SinkRuntimeProvider}\n     * interfaces might be located in other Flink modules.\n     *\n     * <p>Independent of the provider interface, the table runtime expects that a sink\n     * implementation accepts internal data structures (see {@link RowData} for more information).\n     *\n     * <p>The given {@link Context} offers utilities by the planner for creating runtime\n     * implementation with minimal dependencies to internal data structures.\n     *\n     * <p>{@link SinkProvider} is the recommended core interface. {@code SinkFunctionProvider} in\n     * {@code flink-table-api-java-bridge} and {@link OutputFormatProvider} are available for\n     * backwards compatibility.\n     *\n     * @see SinkProvider\n     */\n    SinkRuntimeProvider getSinkRuntimeProvider(Context context);\n\n    /**\n     * Creates a copy of this instance during planning. The copy should be a deep copy of all\n     * mutable members.\n     */\n    DynamicTableSink copy();\n\n    /** Returns a string that summarizes this sink for printing to a console or log. */\n    String asSummaryString();\n\n}\n\n```\n\n> 将动态表汇入外部存储系统。\n> 动态表是 Flink 的 Table & SQL API 的核心概念，用于以统一的方式处理有界和无界数据。根据定义，动态表可以随时间变化。\n> 在编写动态表时，内容始终可以被视为一个 changelog（有限或无限），所有更改都被连续写出，直到 changelog 耗尽。给定的ChangelogMode指示接收器在运行时接受的更改集。\n> 对于常规批处理方案，接收器只能接受仅插入行并写出有界流。\n> 对于常规流场景，接收器只能接受仅插入行，并且可以写出无界流。\n> 对于变更数据捕获 (CDC) 场景，接收器可以写出带有插入、更新和删除行的有界或无界流。另请参阅RowKind 。\n> DynamicTableSink的实例可以被视为最终生成用于写入实际数据的具体运行时实现的工厂。\n> 根据可选声明的能力，规划器可能会将更改应用于实例，从而改变生成的运行时实现。\n> DynamicTableSink可以实现以下功能：\n> SupportsPartitioning\n> SupportsOverwrite\n> SupportsWritingMetadata\n> 在最后一步中，规划器将调用getSinkRuntimeProvider(DynamicTableSink.Context)来获取运行时实现的提供者。\n\n\n\n## Write\n\n### 构建Paimon Sink Flink DAG源码流程\n\n入口类以及对应的入口方法为：org.apache.paimon.flink.sink.FlinkSinkBuilder#build，\n进入这个方法中看看，发现会先对DataStream进行，按照分桶进行分区转换\n\n```java\norg.apache.paimon.flink.sink.FlinkSinkBuilder#build\n\npublic DataStreamSink<?> build() {\n    BucketingStreamPartitioner<RowData> partitioner =\n            new BucketingStreamPartitioner<>(\n                    new RowDataChannelComputer(table.schema(), logSinkFunction != null));\n    PartitionTransformation<RowData> partitioned =\n            new PartitionTransformation<>(input.getTransformation(), partitioner);\n    if (parallelism != null) {\n        partitioned.setParallelism(parallelism);\n    }\n\n    StreamExecutionEnvironment env = input.getExecutionEnvironment();\n    // 构建Flink paimon sink DAG类\n    FileStoreSink sink =\n            new FileStoreSink(table, lockFactory, overwritePartition, logSinkFunction);\n    return commitUser != null && sinkProvider != null\n            ? sink.sinkFrom(new DataStream<>(env, partitioned), commitUser, sinkProvider)\n            : sink.sinkFrom(new DataStream<>(env, partitioned));\n}\n```\n\n- 1、createWriteOperator：实际进行写入Record的算子, org.apache.paimon.flink.sink.RowDataStoreWriteOperator\n  org.apache.paimon.flink.AbstractFlinkTableFactory#buildPaimonTable\n\n- 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。\n```java\norg.apache.paimon.flink.sink.FlinkSink#sinkFrom(org.apache.flink.streaming.api.datastream.DataStream<T>, java.lang.String, org.apache.paimon.flink.sink.StoreSinkWrite.Provider);\n\npublic DataStreamSink<?> sinkFrom(\n  DataStream<T> input, String commitUser, StoreSinkWrite.Provider sinkProvider) {\n  StreamExecutionEnvironment env = input.getExecutionEnvironment();\n  ReadableConfig conf = StreamExecutionEnvironmentUtils.getConfiguration(env);\n  CheckpointConfig checkpointConfig = env.getCheckpointConfig();\n\n  boolean isStreaming =\n    conf.get(ExecutionOptions.RUNTIME_MODE) == RuntimeExecutionMode.STREAMING;\n  boolean streamingCheckpointEnabled =\n    isStreaming && checkpointConfig.isCheckpointingEnabled();\n  if (streamingCheckpointEnabled) {\n    assertCheckpointConfiguration(env);\n  }\n\n  CommittableTypeInfo typeInfo = new CommittableTypeInfo();\n  SingleOutputStreamOperator<Committable> written =\n    input.transform(\n      WRITER_NAME + \" -> \" + table.name(),\n      typeInfo,\n      // 1、createWriteOperator：实际进行写入Record的算子\n      createWriteOperator(sinkProvider, isStreaming, commitUser))\n      .setParallelism(input.getParallelism());\n\n  SingleOutputStreamOperator<?> committed =\n    written.transform(\n      GLOBAL_COMMITTER_NAME + \" -> \" + table.name(),\n      typeInfo,\n      // 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。\n      new CommitterOperator(\n        streamingCheckpointEnabled,\n        commitUser,\n        createCommitterFactory(streamingCheckpointEnabled),\n        createCommittableStateManager()))\n      .setParallelism(1)\n      .setMaxParallelism(1);\n  return committed.addSink(new DiscardingSink<>()).name(\"end\").setParallelism(1);\n}\n```\n\n\n-  1、createWriteOperator\n```scala\nprivate StoreSinkWrite.Provider createWriteProvider(CheckpointConfig checkpointConfig) {\n    boolean waitCompaction;\n\n    if (table.coreOptions().writeOnly()) {\n        // 如果配置为writeOnly()，则不进行在线压缩\n        waitCompaction = false;\n    } else {\n        Options options = table.coreOptions().toConfiguration();\n        ChangelogProducer changelogProducer = table.coreOptions().changelogProducer();\n        // 当ChangelogProducer为LOOKUP时，则等待压缩\n        waitCompaction =\n                changelogProducer == ChangelogProducer.LOOKUP\n                        && options.get(CHANGELOG_PRODUCER_LOOKUP_WAIT);\n        \n        // 决定FULL_COMPACTION的压缩间隔\n        int deltaCommits = -1;\n        if (options.contains(FULL_COMPACTION_DELTA_COMMITS)) {\n            deltaCommits = options.get(FULL_COMPACTION_DELTA_COMMITS);\n        } else if (options.contains(CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL)) {\n            long fullCompactionThresholdMs =\n                    options.get(CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL).toMillis();\n            deltaCommits =\n                    (int)\n                            (fullCompactionThresholdMs\n                                    / checkpointConfig.getCheckpointInterval());\n        }\n        \n        // Generate changelog files with each full compaction\n        // 当进行FULL_COMPACTION的时候，需要生成changelog files\n        if (changelogProducer == ChangelogProducer.FULL_COMPACTION || deltaCommits >= 0) {\n            int finalDeltaCommits = Math.max(deltaCommits, 1);\n            return (table, commitUser, state, ioManager) ->\n                    new GlobalFullCompactionSinkWrite(\n                            table,\n                            commitUser,\n                            state,\n                            ioManager,\n                            isOverwrite,\n                            waitCompaction,\n                            finalDeltaCommits);\n        }\n    }\n\n    return (table, commitUser, state, ioManager) ->\n            new StoreSinkWriteImpl(\n                    table, commitUser, state, ioManager, isOverwrite, waitCompaction);\n}\n```\n\n### Paimon Flink CK流程\n我们知道Flink paimon写入主要涉及两个算子：\n\n1、org.apache.paimon.flink.sink.RowDataStoreWriteOperator\n这个算子实现了`org.apache.flink.streaming.api.operators.StreamOperator#prepareSnapshotPreBarrier`方法，这个方法会在算子接受到driver的checkpoint请求后被调用。在prepareSnapshotPreBarrier方法中会调用`org.apache.paimon.flink.sink.PrepareCommitOperator#emitCommittables`方法，emitCommittables方法的作用是向后面的commit算子发送committable信息。\n\n然而这个emitCommittables方法，又会调用prepareCommit方法，最终会调用`org.apache.paimon.mergetree.MergeTreeWriter#prepareCommit`\n**因此Flink每次进行checkpoint的时候，Paimon都会强制进行Memory Flush，完成数据的落盘，保证数据写入到文件系统，完成写入事务，保证一致性。**\n\n```java\norg.apache.paimon.flink.sink.PrepareCommitOperator#prepareSnapshotPreBarrier\n\n@Override\npublic void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n  if (!endOfInput) {\n    emitCommittables(false, checkpointId);\n  }\n  // no records are expected to emit after endOfInput\n}\n\n@Override\npublic void endInput() throws Exception {\n  endOfInput = true;\n  emitCommittables(true, Long.MAX_VALUE);\n}\n\nprivate void emitCommittables(boolean doCompaction, long checkpointId) throws IOException {\n  prepareCommit(doCompaction, checkpointId)\n    .forEach(committable -> output.collect(new StreamRecord<>(committable)));\n}\n\nprotected abstract List<Committable> prepareCommit(boolean doCompaction, long checkpointId) throws IOException;\n```\n2、CommitterOperator\n这个算子实现了`org.apache.flink.api.common.state.CheckpointListener#notifyCheckpointComplete`方法，该方法会在FLink CK完成后被调用，paimon-flink-sink在这个方法中会进行snapshot快照的提交，主要就是将本次快照生成的snapshot、manifest文件写入到文件系统。\n\n","slug":"bigdata/paimon/Paimon-Flink-Sink源码分析","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt3003u8j5mgtsxcm0k","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Paimon在Flink下面的层次结构，大概为：Catalog -&gt; Database -&gt; Table -&gt; Record。因此看Paimon如何在Flink中实现Read、Write等操作，先从Catalog开始。</p>\n<h2 id=\"Catalog\"><a href=\"#Catalog\" class=\"headerlink\" title=\"Catalog\"></a>Catalog</h2><p>Paimon在Flink中实现Catalog的源码位于<code>org.apache.paimon.flink.FlinkCatalog</code>，该类实现了<code>org.apache.flink.table.catalog.Catalog</code>接口。该接口中定义了一系列方法，包括listTables、listViews等方法，而paimon一一实现了这些接口。</p>\n<p>在Flink中自定义Catalog，还需要实现<code>org.apache.flink.table.factories.CatalogFactory</code>工厂接口，paimon中对应的为<code>org.apache.paimon.flink.FlinkCatalogFactory</code>类。该工厂类当然是用来创建Catalog了。</p>\n<p>最后还需要将Catalog工厂实现类，添加到将此实现类添加到 <code>META_INF/services/org.apache.flink.table.factories.Factory</code> 中。用于SPI。</p>\n<h2 id=\"Table\"><a href=\"#Table\" class=\"headerlink\" title=\"Table\"></a>Table</h2><p>在paimon实现的FlinkCatalog类中，<code>org.apache.paimon.flink.FlinkCatalog#getFactory</code>方法，用于提供写入和读取Paimon的具体实现。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> Optional&lt;Factory&gt; <span class=\"title function_\">getFactory</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Optional.of(<span class=\"keyword\">new</span> <span class=\"title class_\">FlinkTableFactory</span>(catalog.lockFactory().orElse(<span class=\"literal\">null</span>)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在Paimon中，这个方法返回的是<code>org.apache.paimon.flink.FlinkTableFactory</code>。该工厂类实现了<code>org.apache.flink.table.factories.DynamicTableSourceFactory</code>，<code>org.apache.flink.table.factories.DynamicTableSinkFactory</code>接口。分别用来实现读取和写入Paimon表的逻辑。而写入Paimon表的实现类为<code>org.apache.paimon.flink.sink.FlinkTableSink</code>。该类主要实现了<code>org.apache.flink.table.connector.sink.DynamicTableSink</code>接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Sink of a dynamic table to an external storage system.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Dynamic tables are the core concept of Flink&#x27;s Table &amp; SQL API for processing both bounded and</span></span><br><span class=\"line\"><span class=\"comment\"> * unbounded data in a unified fashion. By definition, a dynamic table can change over time.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;When writing a dynamic table, the content can always be considered as a changelog (finite or</span></span><br><span class=\"line\"><span class=\"comment\"> * infinite) for which all changes are written out continuously until the changelog is exhausted.</span></span><br><span class=\"line\"><span class=\"comment\"> * The given &#123;<span class=\"doctag\">@link</span> ChangelogMode&#125; indicates the set of changes that the sink accepts during</span></span><br><span class=\"line\"><span class=\"comment\"> * runtime.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded</span></span><br><span class=\"line\"><span class=\"comment\"> * streams.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For regular streaming scenarios, the sink can solely accept insert-only rows and can write out</span></span><br><span class=\"line\"><span class=\"comment\"> * unbounded streams.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams</span></span><br><span class=\"line\"><span class=\"comment\"> * with insert, update, and delete rows. See also &#123;<span class=\"doctag\">@link</span> RowKind&#125;.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Instances of &#123;<span class=\"doctag\">@link</span> DynamicTableSink&#125; can be seen as factories that eventually produce</span></span><br><span class=\"line\"><span class=\"comment\"> * concrete runtime implementation for writing the actual data.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Depending on the optionally declared abilities, the planner might apply changes to an instance</span></span><br><span class=\"line\"><span class=\"comment\"> * and thus mutate the produced runtime implementation.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;A &#123;<span class=\"doctag\">@link</span> DynamicTableSink&#125; can implement the following abilities:</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;ul&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsPartitioning&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsOverwrite&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsWritingMetadata&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;/ul&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;In the last step, the planner will call &#123;<span class=\"doctag\">@link</span> #getSinkRuntimeProvider(Context)&#125; for obtaining</span></span><br><span class=\"line\"><span class=\"comment\"> * a provider of runtime implementation.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@PublicEvolving</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">DynamicTableSink</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the set of changes that the sink accepts during runtime.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;The planner can make suggestions but the sink has the final decision what it requires. If</span></span><br><span class=\"line\"><span class=\"comment\">     * the planner does not support this mode, it will throw an error. For example, the sink can</span></span><br><span class=\"line\"><span class=\"comment\">     * return that it only supports &#123;<span class=\"doctag\">@link</span> ChangelogMode#insertOnly()&#125;.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> requestedMode expected set of changes by the current plan</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    ChangelogMode <span class=\"title function_\">getChangelogMode</span><span class=\"params\">(ChangelogMode requestedMode)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns a provider of runtime implementation for writing the data.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;There might exist different interfaces for runtime implementation which is why &#123;<span class=\"doctag\">@link</span></span></span><br><span class=\"line\"><span class=\"comment\">     * SinkRuntimeProvider&#125; serves as the base interface. Concrete &#123;<span class=\"doctag\">@link</span> SinkRuntimeProvider&#125;</span></span><br><span class=\"line\"><span class=\"comment\">     * interfaces might be located in other Flink modules.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;Independent of the provider interface, the table runtime expects that a sink</span></span><br><span class=\"line\"><span class=\"comment\">     * implementation accepts internal data structures (see &#123;<span class=\"doctag\">@link</span> RowData&#125; for more information).</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;The given &#123;<span class=\"doctag\">@link</span> Context&#125; offers utilities by the planner for creating runtime</span></span><br><span class=\"line\"><span class=\"comment\">     * implementation with minimal dependencies to internal data structures.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;&#123;<span class=\"doctag\">@link</span> SinkProvider&#125; is the recommended core interface. &#123;<span class=\"doctag\">@code</span> SinkFunctionProvider&#125; in</span></span><br><span class=\"line\"><span class=\"comment\">     * &#123;<span class=\"doctag\">@code</span> flink-table-api-java-bridge&#125; and &#123;<span class=\"doctag\">@link</span> OutputFormatProvider&#125; are available for</span></span><br><span class=\"line\"><span class=\"comment\">     * backwards compatibility.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@see</span> SinkProvider</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    SinkRuntimeProvider <span class=\"title function_\">getSinkRuntimeProvider</span><span class=\"params\">(Context context)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Creates a copy of this instance during planning. The copy should be a deep copy of all</span></span><br><span class=\"line\"><span class=\"comment\">     * mutable members.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    DynamicTableSink <span class=\"title function_\">copy</span><span class=\"params\">()</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns a string that summarizes this sink for printing to a console or log. */</span></span><br><span class=\"line\">    String <span class=\"title function_\">asSummaryString</span><span class=\"params\">()</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>将动态表汇入外部存储系统。<br>动态表是 Flink 的 Table &amp; SQL API 的核心概念，用于以统一的方式处理有界和无界数据。根据定义，动态表可以随时间变化。<br>在编写动态表时，内容始终可以被视为一个 changelog（有限或无限），所有更改都被连续写出，直到 changelog 耗尽。给定的ChangelogMode指示接收器在运行时接受的更改集。<br>对于常规批处理方案，接收器只能接受仅插入行并写出有界流。<br>对于常规流场景，接收器只能接受仅插入行，并且可以写出无界流。<br>对于变更数据捕获 (CDC) 场景，接收器可以写出带有插入、更新和删除行的有界或无界流。另请参阅RowKind 。<br>DynamicTableSink的实例可以被视为最终生成用于写入实际数据的具体运行时实现的工厂。<br>根据可选声明的能力，规划器可能会将更改应用于实例，从而改变生成的运行时实现。<br>DynamicTableSink可以实现以下功能：<br>SupportsPartitioning<br>SupportsOverwrite<br>SupportsWritingMetadata<br>在最后一步中，规划器将调用getSinkRuntimeProvider(DynamicTableSink.Context)来获取运行时实现的提供者。</p>\n</blockquote>\n<h2 id=\"Write\"><a href=\"#Write\" class=\"headerlink\" title=\"Write\"></a>Write</h2><h3 id=\"构建Paimon-Sink-Flink-DAG源码流程\"><a href=\"#构建Paimon-Sink-Flink-DAG源码流程\" class=\"headerlink\" title=\"构建Paimon Sink Flink DAG源码流程\"></a>构建Paimon Sink Flink DAG源码流程</h3><p>入口类以及对应的入口方法为：org.apache.paimon.flink.sink.FlinkSinkBuilder#build，<br>进入这个方法中看看，发现会先对DataStream进行，按照分桶进行分区转换</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.FlinkSinkBuilder#build</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataStreamSink&lt;?&gt; build() &#123;</span><br><span class=\"line\">    BucketingStreamPartitioner&lt;RowData&gt; partitioner =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">BucketingStreamPartitioner</span>&lt;&gt;(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"title class_\">RowDataChannelComputer</span>(table.schema(), logSinkFunction != <span class=\"literal\">null</span>));</span><br><span class=\"line\">    PartitionTransformation&lt;RowData&gt; partitioned =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">PartitionTransformation</span>&lt;&gt;(input.getTransformation(), partitioner);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (parallelism != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        partitioned.setParallelism(parallelism);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> input.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 构建Flink paimon sink DAG类</span></span><br><span class=\"line\">    <span class=\"type\">FileStoreSink</span> <span class=\"variable\">sink</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">FileStoreSink</span>(table, lockFactory, overwritePartition, logSinkFunction);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> commitUser != <span class=\"literal\">null</span> &amp;&amp; sinkProvider != <span class=\"literal\">null</span></span><br><span class=\"line\">            ? sink.sinkFrom(<span class=\"keyword\">new</span> <span class=\"title class_\">DataStream</span>&lt;&gt;(env, partitioned), commitUser, sinkProvider)</span><br><span class=\"line\">            : sink.sinkFrom(<span class=\"keyword\">new</span> <span class=\"title class_\">DataStream</span>&lt;&gt;(env, partitioned));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>1、createWriteOperator：实际进行写入Record的算子, org.apache.paimon.flink.sink.RowDataStoreWriteOperator<br>org.apache.paimon.flink.AbstractFlinkTableFactory#buildPaimonTable</p>\n</li>\n<li><p>2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.FlinkSink#sinkFrom(org.apache.flink.streaming.api.datastream.DataStream&lt;T&gt;, java.lang.String, org.apache.paimon.flink.sink.StoreSinkWrite.Provider);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataStreamSink&lt;?&gt; sinkFrom(</span><br><span class=\"line\">  DataStream&lt;T&gt; input, String commitUser, StoreSinkWrite.Provider sinkProvider) &#123;</span><br><span class=\"line\">  <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> input.getExecutionEnvironment();</span><br><span class=\"line\">  <span class=\"type\">ReadableConfig</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> StreamExecutionEnvironmentUtils.getConfiguration(env);</span><br><span class=\"line\">  <span class=\"type\">CheckpointConfig</span> <span class=\"variable\">checkpointConfig</span> <span class=\"operator\">=</span> env.getCheckpointConfig();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">isStreaming</span> <span class=\"operator\">=</span></span><br><span class=\"line\">    conf.get(ExecutionOptions.RUNTIME_MODE) == RuntimeExecutionMode.STREAMING;</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">streamingCheckpointEnabled</span> <span class=\"operator\">=</span></span><br><span class=\"line\">    isStreaming &amp;&amp; checkpointConfig.isCheckpointingEnabled();</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (streamingCheckpointEnabled) &#123;</span><br><span class=\"line\">    assertCheckpointConfiguration(env);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"type\">CommittableTypeInfo</span> <span class=\"variable\">typeInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">CommittableTypeInfo</span>();</span><br><span class=\"line\">  SingleOutputStreamOperator&lt;Committable&gt; written =</span><br><span class=\"line\">    input.transform(</span><br><span class=\"line\">      WRITER_NAME + <span class=\"string\">&quot; -&gt; &quot;</span> + table.name(),</span><br><span class=\"line\">      typeInfo,</span><br><span class=\"line\">      <span class=\"comment\">// 1、createWriteOperator：实际进行写入Record的算子</span></span><br><span class=\"line\">      createWriteOperator(sinkProvider, isStreaming, commitUser))</span><br><span class=\"line\">      .setParallelism(input.getParallelism());</span><br><span class=\"line\"></span><br><span class=\"line\">  SingleOutputStreamOperator&lt;?&gt; committed =</span><br><span class=\"line\">    written.transform(</span><br><span class=\"line\">      GLOBAL_COMMITTER_NAME + <span class=\"string\">&quot; -&gt; &quot;</span> + table.name(),</span><br><span class=\"line\">      typeInfo,</span><br><span class=\"line\">      <span class=\"comment\">// 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。</span></span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"title class_\">CommitterOperator</span>(</span><br><span class=\"line\">        streamingCheckpointEnabled,</span><br><span class=\"line\">        commitUser,</span><br><span class=\"line\">        createCommitterFactory(streamingCheckpointEnabled),</span><br><span class=\"line\">        createCommittableStateManager()))</span><br><span class=\"line\">      .setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">      .setMaxParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> committed.addSink(<span class=\"keyword\">new</span> <span class=\"title class_\">DiscardingSink</span>&lt;&gt;()).name(<span class=\"string\">&quot;end&quot;</span>).setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n</li>\n<li><p>1、createWriteOperator</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">StoreSinkWrite</span>.<span class=\"type\">Provider</span> createWriteProvider(<span class=\"type\">CheckpointConfig</span> checkpointConfig) &#123;</span><br><span class=\"line\">    boolean waitCompaction;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (table.coreOptions().writeOnly()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果配置为writeOnly()，则不进行在线压缩</span></span><br><span class=\"line\">        waitCompaction = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Options</span> options = table.coreOptions().toConfiguration();</span><br><span class=\"line\">        <span class=\"type\">ChangelogProducer</span> changelogProducer = table.coreOptions().changelogProducer();</span><br><span class=\"line\">        <span class=\"comment\">// 当ChangelogProducer为LOOKUP时，则等待压缩</span></span><br><span class=\"line\">        waitCompaction =</span><br><span class=\"line\">                changelogProducer == <span class=\"type\">ChangelogProducer</span>.<span class=\"type\">LOOKUP</span></span><br><span class=\"line\">                        &amp;&amp; options.get(<span class=\"type\">CHANGELOG_PRODUCER_LOOKUP_WAIT</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 决定FULL_COMPACTION的压缩间隔</span></span><br><span class=\"line\">        int deltaCommits = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (options.contains(<span class=\"type\">FULL_COMPACTION_DELTA_COMMITS</span>)) &#123;</span><br><span class=\"line\">            deltaCommits = options.get(<span class=\"type\">FULL_COMPACTION_DELTA_COMMITS</span>);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (options.contains(<span class=\"type\">CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL</span>)) &#123;</span><br><span class=\"line\">            long fullCompactionThresholdMs =</span><br><span class=\"line\">                    options.get(<span class=\"type\">CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL</span>).toMillis();</span><br><span class=\"line\">            deltaCommits =</span><br><span class=\"line\">                    (int)</span><br><span class=\"line\">                            (fullCompactionThresholdMs</span><br><span class=\"line\">                                    / checkpointConfig.getCheckpointInterval());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// Generate changelog files with each full compaction</span></span><br><span class=\"line\">        <span class=\"comment\">// 当进行FULL_COMPACTION的时候，需要生成changelog files</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (changelogProducer == <span class=\"type\">ChangelogProducer</span>.<span class=\"type\">FULL_COMPACTION</span> || deltaCommits &gt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            int finalDeltaCommits = <span class=\"type\">Math</span>.max(deltaCommits, <span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (table, commitUser, state, ioManager) -&gt;</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"type\">GlobalFullCompactionSinkWrite</span>(</span><br><span class=\"line\">                            table,</span><br><span class=\"line\">                            commitUser,</span><br><span class=\"line\">                            state,</span><br><span class=\"line\">                            ioManager,</span><br><span class=\"line\">                            isOverwrite,</span><br><span class=\"line\">                            waitCompaction,</span><br><span class=\"line\">                            finalDeltaCommits);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (table, commitUser, state, ioManager) -&gt;</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"type\">StoreSinkWriteImpl</span>(</span><br><span class=\"line\">                    table, commitUser, state, ioManager, isOverwrite, waitCompaction);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Paimon-Flink-CK流程\"><a href=\"#Paimon-Flink-CK流程\" class=\"headerlink\" title=\"Paimon Flink CK流程\"></a>Paimon Flink CK流程</h3><p>我们知道Flink paimon写入主要涉及两个算子：</p>\n<p>1、org.apache.paimon.flink.sink.RowDataStoreWriteOperator<br>这个算子实现了<code>org.apache.flink.streaming.api.operators.StreamOperator#prepareSnapshotPreBarrier</code>方法，这个方法会在算子接受到driver的checkpoint请求后被调用。在prepareSnapshotPreBarrier方法中会调用<code>org.apache.paimon.flink.sink.PrepareCommitOperator#emitCommittables</code>方法，emitCommittables方法的作用是向后面的commit算子发送committable信息。</p>\n<p>然而这个emitCommittables方法，又会调用prepareCommit方法，最终会调用<code>org.apache.paimon.mergetree.MergeTreeWriter#prepareCommit</code><br><strong>因此Flink每次进行checkpoint的时候，Paimon都会强制进行Memory Flush，完成数据的落盘，保证数据写入到文件系统，完成写入事务，保证一致性。</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.PrepareCommitOperator#prepareSnapshotPreBarrier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">prepareSnapshotPreBarrier</span><span class=\"params\">(<span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!endOfInput) &#123;</span><br><span class=\"line\">    emitCommittables(<span class=\"literal\">false</span>, checkpointId);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// no records are expected to emit after endOfInput</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">endInput</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">  endOfInput = <span class=\"literal\">true</span>;</span><br><span class=\"line\">  emitCommittables(<span class=\"literal\">true</span>, Long.MAX_VALUE);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">emitCommittables</span><span class=\"params\">(<span class=\"type\">boolean</span> doCompaction, <span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">  prepareCommit(doCompaction, checkpointId)</span><br><span class=\"line\">    .forEach(committable -&gt; output.collect(<span class=\"keyword\">new</span> <span class=\"title class_\">StreamRecord</span>&lt;&gt;(committable)));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> List&lt;Committable&gt; <span class=\"title function_\">prepareCommit</span><span class=\"params\">(<span class=\"type\">boolean</span> doCompaction, <span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> IOException;</span><br></pre></td></tr></table></figure>\n<p>2、CommitterOperator<br>这个算子实现了<code>org.apache.flink.api.common.state.CheckpointListener#notifyCheckpointComplete</code>方法，该方法会在FLink CK完成后被调用，paimon-flink-sink在这个方法中会进行snapshot快照的提交，主要就是将本次快照生成的snapshot、manifest文件写入到文件系统。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Paimon在Flink下面的层次结构，大概为：Catalog -&gt; Database -&gt; Table -&gt; Record。因此看Paimon如何在Flink中实现Read、Write等操作，先从Catalog开始。</p>\n<h2 id=\"Catalog\"><a href=\"#Catalog\" class=\"headerlink\" title=\"Catalog\"></a>Catalog</h2><p>Paimon在Flink中实现Catalog的源码位于<code>org.apache.paimon.flink.FlinkCatalog</code>，该类实现了<code>org.apache.flink.table.catalog.Catalog</code>接口。该接口中定义了一系列方法，包括listTables、listViews等方法，而paimon一一实现了这些接口。</p>\n<p>在Flink中自定义Catalog，还需要实现<code>org.apache.flink.table.factories.CatalogFactory</code>工厂接口，paimon中对应的为<code>org.apache.paimon.flink.FlinkCatalogFactory</code>类。该工厂类当然是用来创建Catalog了。</p>\n<p>最后还需要将Catalog工厂实现类，添加到将此实现类添加到 <code>META_INF/services/org.apache.flink.table.factories.Factory</code> 中。用于SPI。</p>\n<h2 id=\"Table\"><a href=\"#Table\" class=\"headerlink\" title=\"Table\"></a>Table</h2><p>在paimon实现的FlinkCatalog类中，<code>org.apache.paimon.flink.FlinkCatalog#getFactory</code>方法，用于提供写入和读取Paimon的具体实现。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> Optional&lt;Factory&gt; <span class=\"title function_\">getFactory</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Optional.of(<span class=\"keyword\">new</span> <span class=\"title class_\">FlinkTableFactory</span>(catalog.lockFactory().orElse(<span class=\"literal\">null</span>)));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在Paimon中，这个方法返回的是<code>org.apache.paimon.flink.FlinkTableFactory</code>。该工厂类实现了<code>org.apache.flink.table.factories.DynamicTableSourceFactory</code>，<code>org.apache.flink.table.factories.DynamicTableSinkFactory</code>接口。分别用来实现读取和写入Paimon表的逻辑。而写入Paimon表的实现类为<code>org.apache.paimon.flink.sink.FlinkTableSink</code>。该类主要实现了<code>org.apache.flink.table.connector.sink.DynamicTableSink</code>接口。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Sink of a dynamic table to an external storage system.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Dynamic tables are the core concept of Flink&#x27;s Table &amp; SQL API for processing both bounded and</span></span><br><span class=\"line\"><span class=\"comment\"> * unbounded data in a unified fashion. By definition, a dynamic table can change over time.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;When writing a dynamic table, the content can always be considered as a changelog (finite or</span></span><br><span class=\"line\"><span class=\"comment\"> * infinite) for which all changes are written out continuously until the changelog is exhausted.</span></span><br><span class=\"line\"><span class=\"comment\"> * The given &#123;<span class=\"doctag\">@link</span> ChangelogMode&#125; indicates the set of changes that the sink accepts during</span></span><br><span class=\"line\"><span class=\"comment\"> * runtime.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded</span></span><br><span class=\"line\"><span class=\"comment\"> * streams.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For regular streaming scenarios, the sink can solely accept insert-only rows and can write out</span></span><br><span class=\"line\"><span class=\"comment\"> * unbounded streams.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams</span></span><br><span class=\"line\"><span class=\"comment\"> * with insert, update, and delete rows. See also &#123;<span class=\"doctag\">@link</span> RowKind&#125;.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Instances of &#123;<span class=\"doctag\">@link</span> DynamicTableSink&#125; can be seen as factories that eventually produce</span></span><br><span class=\"line\"><span class=\"comment\"> * concrete runtime implementation for writing the actual data.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;Depending on the optionally declared abilities, the planner might apply changes to an instance</span></span><br><span class=\"line\"><span class=\"comment\"> * and thus mutate the produced runtime implementation.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;A &#123;<span class=\"doctag\">@link</span> DynamicTableSink&#125; can implement the following abilities:</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;ul&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsPartitioning&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsOverwrite&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> *   &lt;li&gt;&#123;<span class=\"doctag\">@link</span> SupportsWritingMetadata&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;/ul&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;In the last step, the planner will call &#123;<span class=\"doctag\">@link</span> #getSinkRuntimeProvider(Context)&#125; for obtaining</span></span><br><span class=\"line\"><span class=\"comment\"> * a provider of runtime implementation.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@PublicEvolving</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">DynamicTableSink</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the set of changes that the sink accepts during runtime.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;The planner can make suggestions but the sink has the final decision what it requires. If</span></span><br><span class=\"line\"><span class=\"comment\">     * the planner does not support this mode, it will throw an error. For example, the sink can</span></span><br><span class=\"line\"><span class=\"comment\">     * return that it only supports &#123;<span class=\"doctag\">@link</span> ChangelogMode#insertOnly()&#125;.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> requestedMode expected set of changes by the current plan</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    ChangelogMode <span class=\"title function_\">getChangelogMode</span><span class=\"params\">(ChangelogMode requestedMode)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns a provider of runtime implementation for writing the data.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;There might exist different interfaces for runtime implementation which is why &#123;<span class=\"doctag\">@link</span></span></span><br><span class=\"line\"><span class=\"comment\">     * SinkRuntimeProvider&#125; serves as the base interface. Concrete &#123;<span class=\"doctag\">@link</span> SinkRuntimeProvider&#125;</span></span><br><span class=\"line\"><span class=\"comment\">     * interfaces might be located in other Flink modules.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;Independent of the provider interface, the table runtime expects that a sink</span></span><br><span class=\"line\"><span class=\"comment\">     * implementation accepts internal data structures (see &#123;<span class=\"doctag\">@link</span> RowData&#125; for more information).</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;The given &#123;<span class=\"doctag\">@link</span> Context&#125; offers utilities by the planner for creating runtime</span></span><br><span class=\"line\"><span class=\"comment\">     * implementation with minimal dependencies to internal data structures.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * &lt;p&gt;&#123;<span class=\"doctag\">@link</span> SinkProvider&#125; is the recommended core interface. &#123;<span class=\"doctag\">@code</span> SinkFunctionProvider&#125; in</span></span><br><span class=\"line\"><span class=\"comment\">     * &#123;<span class=\"doctag\">@code</span> flink-table-api-java-bridge&#125; and &#123;<span class=\"doctag\">@link</span> OutputFormatProvider&#125; are available for</span></span><br><span class=\"line\"><span class=\"comment\">     * backwards compatibility.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@see</span> SinkProvider</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    SinkRuntimeProvider <span class=\"title function_\">getSinkRuntimeProvider</span><span class=\"params\">(Context context)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Creates a copy of this instance during planning. The copy should be a deep copy of all</span></span><br><span class=\"line\"><span class=\"comment\">     * mutable members.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    DynamicTableSink <span class=\"title function_\">copy</span><span class=\"params\">()</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns a string that summarizes this sink for printing to a console or log. */</span></span><br><span class=\"line\">    String <span class=\"title function_\">asSummaryString</span><span class=\"params\">()</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>将动态表汇入外部存储系统。<br>动态表是 Flink 的 Table &amp; SQL API 的核心概念，用于以统一的方式处理有界和无界数据。根据定义，动态表可以随时间变化。<br>在编写动态表时，内容始终可以被视为一个 changelog（有限或无限），所有更改都被连续写出，直到 changelog 耗尽。给定的ChangelogMode指示接收器在运行时接受的更改集。<br>对于常规批处理方案，接收器只能接受仅插入行并写出有界流。<br>对于常规流场景，接收器只能接受仅插入行，并且可以写出无界流。<br>对于变更数据捕获 (CDC) 场景，接收器可以写出带有插入、更新和删除行的有界或无界流。另请参阅RowKind 。<br>DynamicTableSink的实例可以被视为最终生成用于写入实际数据的具体运行时实现的工厂。<br>根据可选声明的能力，规划器可能会将更改应用于实例，从而改变生成的运行时实现。<br>DynamicTableSink可以实现以下功能：<br>SupportsPartitioning<br>SupportsOverwrite<br>SupportsWritingMetadata<br>在最后一步中，规划器将调用getSinkRuntimeProvider(DynamicTableSink.Context)来获取运行时实现的提供者。</p>\n</blockquote>\n<h2 id=\"Write\"><a href=\"#Write\" class=\"headerlink\" title=\"Write\"></a>Write</h2><h3 id=\"构建Paimon-Sink-Flink-DAG源码流程\"><a href=\"#构建Paimon-Sink-Flink-DAG源码流程\" class=\"headerlink\" title=\"构建Paimon Sink Flink DAG源码流程\"></a>构建Paimon Sink Flink DAG源码流程</h3><p>入口类以及对应的入口方法为：org.apache.paimon.flink.sink.FlinkSinkBuilder#build，<br>进入这个方法中看看，发现会先对DataStream进行，按照分桶进行分区转换</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.FlinkSinkBuilder#build</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataStreamSink&lt;?&gt; build() &#123;</span><br><span class=\"line\">    BucketingStreamPartitioner&lt;RowData&gt; partitioner =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">BucketingStreamPartitioner</span>&lt;&gt;(</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"title class_\">RowDataChannelComputer</span>(table.schema(), logSinkFunction != <span class=\"literal\">null</span>));</span><br><span class=\"line\">    PartitionTransformation&lt;RowData&gt; partitioned =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">PartitionTransformation</span>&lt;&gt;(input.getTransformation(), partitioner);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (parallelism != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        partitioned.setParallelism(parallelism);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> input.getExecutionEnvironment();</span><br><span class=\"line\">    <span class=\"comment\">// 构建Flink paimon sink DAG类</span></span><br><span class=\"line\">    <span class=\"type\">FileStoreSink</span> <span class=\"variable\">sink</span> <span class=\"operator\">=</span></span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"title class_\">FileStoreSink</span>(table, lockFactory, overwritePartition, logSinkFunction);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> commitUser != <span class=\"literal\">null</span> &amp;&amp; sinkProvider != <span class=\"literal\">null</span></span><br><span class=\"line\">            ? sink.sinkFrom(<span class=\"keyword\">new</span> <span class=\"title class_\">DataStream</span>&lt;&gt;(env, partitioned), commitUser, sinkProvider)</span><br><span class=\"line\">            : sink.sinkFrom(<span class=\"keyword\">new</span> <span class=\"title class_\">DataStream</span>&lt;&gt;(env, partitioned));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>1、createWriteOperator：实际进行写入Record的算子, org.apache.paimon.flink.sink.RowDataStoreWriteOperator<br>org.apache.paimon.flink.AbstractFlinkTableFactory#buildPaimonTable</p>\n</li>\n<li><p>2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.FlinkSink#sinkFrom(org.apache.flink.streaming.api.datastream.DataStream&lt;T&gt;, java.lang.String, org.apache.paimon.flink.sink.StoreSinkWrite.Provider);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataStreamSink&lt;?&gt; sinkFrom(</span><br><span class=\"line\">  DataStream&lt;T&gt; input, String commitUser, StoreSinkWrite.Provider sinkProvider) &#123;</span><br><span class=\"line\">  <span class=\"type\">StreamExecutionEnvironment</span> <span class=\"variable\">env</span> <span class=\"operator\">=</span> input.getExecutionEnvironment();</span><br><span class=\"line\">  <span class=\"type\">ReadableConfig</span> <span class=\"variable\">conf</span> <span class=\"operator\">=</span> StreamExecutionEnvironmentUtils.getConfiguration(env);</span><br><span class=\"line\">  <span class=\"type\">CheckpointConfig</span> <span class=\"variable\">checkpointConfig</span> <span class=\"operator\">=</span> env.getCheckpointConfig();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">isStreaming</span> <span class=\"operator\">=</span></span><br><span class=\"line\">    conf.get(ExecutionOptions.RUNTIME_MODE) == RuntimeExecutionMode.STREAMING;</span><br><span class=\"line\">  <span class=\"type\">boolean</span> <span class=\"variable\">streamingCheckpointEnabled</span> <span class=\"operator\">=</span></span><br><span class=\"line\">    isStreaming &amp;&amp; checkpointConfig.isCheckpointingEnabled();</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (streamingCheckpointEnabled) &#123;</span><br><span class=\"line\">    assertCheckpointConfiguration(env);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"type\">CommittableTypeInfo</span> <span class=\"variable\">typeInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">CommittableTypeInfo</span>();</span><br><span class=\"line\">  SingleOutputStreamOperator&lt;Committable&gt; written =</span><br><span class=\"line\">    input.transform(</span><br><span class=\"line\">      WRITER_NAME + <span class=\"string\">&quot; -&gt; &quot;</span> + table.name(),</span><br><span class=\"line\">      typeInfo,</span><br><span class=\"line\">      <span class=\"comment\">// 1、createWriteOperator：实际进行写入Record的算子</span></span><br><span class=\"line\">      createWriteOperator(sinkProvider, isStreaming, commitUser))</span><br><span class=\"line\">      .setParallelism(input.getParallelism());</span><br><span class=\"line\"></span><br><span class=\"line\">  SingleOutputStreamOperator&lt;?&gt; committed =</span><br><span class=\"line\">    written.transform(</span><br><span class=\"line\">      GLOBAL_COMMITTER_NAME + <span class=\"string\">&quot; -&gt; &quot;</span> + table.name(),</span><br><span class=\"line\">      typeInfo,</span><br><span class=\"line\">      <span class=\"comment\">// 2、CommitterOperator：CK时进行snapshot commit的地方，保证数据可见性。</span></span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"title class_\">CommitterOperator</span>(</span><br><span class=\"line\">        streamingCheckpointEnabled,</span><br><span class=\"line\">        commitUser,</span><br><span class=\"line\">        createCommitterFactory(streamingCheckpointEnabled),</span><br><span class=\"line\">        createCommittableStateManager()))</span><br><span class=\"line\">      .setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">      .setMaxParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> committed.addSink(<span class=\"keyword\">new</span> <span class=\"title class_\">DiscardingSink</span>&lt;&gt;()).name(<span class=\"string\">&quot;end&quot;</span>).setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n</li>\n<li><p>1、createWriteOperator</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">StoreSinkWrite</span>.<span class=\"type\">Provider</span> createWriteProvider(<span class=\"type\">CheckpointConfig</span> checkpointConfig) &#123;</span><br><span class=\"line\">    boolean waitCompaction;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (table.coreOptions().writeOnly()) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 如果配置为writeOnly()，则不进行在线压缩</span></span><br><span class=\"line\">        waitCompaction = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Options</span> options = table.coreOptions().toConfiguration();</span><br><span class=\"line\">        <span class=\"type\">ChangelogProducer</span> changelogProducer = table.coreOptions().changelogProducer();</span><br><span class=\"line\">        <span class=\"comment\">// 当ChangelogProducer为LOOKUP时，则等待压缩</span></span><br><span class=\"line\">        waitCompaction =</span><br><span class=\"line\">                changelogProducer == <span class=\"type\">ChangelogProducer</span>.<span class=\"type\">LOOKUP</span></span><br><span class=\"line\">                        &amp;&amp; options.get(<span class=\"type\">CHANGELOG_PRODUCER_LOOKUP_WAIT</span>);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 决定FULL_COMPACTION的压缩间隔</span></span><br><span class=\"line\">        int deltaCommits = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (options.contains(<span class=\"type\">FULL_COMPACTION_DELTA_COMMITS</span>)) &#123;</span><br><span class=\"line\">            deltaCommits = options.get(<span class=\"type\">FULL_COMPACTION_DELTA_COMMITS</span>);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (options.contains(<span class=\"type\">CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL</span>)) &#123;</span><br><span class=\"line\">            long fullCompactionThresholdMs =</span><br><span class=\"line\">                    options.get(<span class=\"type\">CHANGELOG_PRODUCER_FULL_COMPACTION_TRIGGER_INTERVAL</span>).toMillis();</span><br><span class=\"line\">            deltaCommits =</span><br><span class=\"line\">                    (int)</span><br><span class=\"line\">                            (fullCompactionThresholdMs</span><br><span class=\"line\">                                    / checkpointConfig.getCheckpointInterval());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// Generate changelog files with each full compaction</span></span><br><span class=\"line\">        <span class=\"comment\">// 当进行FULL_COMPACTION的时候，需要生成changelog files</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (changelogProducer == <span class=\"type\">ChangelogProducer</span>.<span class=\"type\">FULL_COMPACTION</span> || deltaCommits &gt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            int finalDeltaCommits = <span class=\"type\">Math</span>.max(deltaCommits, <span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> (table, commitUser, state, ioManager) -&gt;</span><br><span class=\"line\">                    <span class=\"keyword\">new</span> <span class=\"type\">GlobalFullCompactionSinkWrite</span>(</span><br><span class=\"line\">                            table,</span><br><span class=\"line\">                            commitUser,</span><br><span class=\"line\">                            state,</span><br><span class=\"line\">                            ioManager,</span><br><span class=\"line\">                            isOverwrite,</span><br><span class=\"line\">                            waitCompaction,</span><br><span class=\"line\">                            finalDeltaCommits);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (table, commitUser, state, ioManager) -&gt;</span><br><span class=\"line\">            <span class=\"keyword\">new</span> <span class=\"type\">StoreSinkWriteImpl</span>(</span><br><span class=\"line\">                    table, commitUser, state, ioManager, isOverwrite, waitCompaction);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Paimon-Flink-CK流程\"><a href=\"#Paimon-Flink-CK流程\" class=\"headerlink\" title=\"Paimon Flink CK流程\"></a>Paimon Flink CK流程</h3><p>我们知道Flink paimon写入主要涉及两个算子：</p>\n<p>1、org.apache.paimon.flink.sink.RowDataStoreWriteOperator<br>这个算子实现了<code>org.apache.flink.streaming.api.operators.StreamOperator#prepareSnapshotPreBarrier</code>方法，这个方法会在算子接受到driver的checkpoint请求后被调用。在prepareSnapshotPreBarrier方法中会调用<code>org.apache.paimon.flink.sink.PrepareCommitOperator#emitCommittables</code>方法，emitCommittables方法的作用是向后面的commit算子发送committable信息。</p>\n<p>然而这个emitCommittables方法，又会调用prepareCommit方法，最终会调用<code>org.apache.paimon.mergetree.MergeTreeWriter#prepareCommit</code><br><strong>因此Flink每次进行checkpoint的时候，Paimon都会强制进行Memory Flush，完成数据的落盘，保证数据写入到文件系统，完成写入事务，保证一致性。</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.paimon.flink.sink.PrepareCommitOperator#prepareSnapshotPreBarrier</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">prepareSnapshotPreBarrier</span><span class=\"params\">(<span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!endOfInput) &#123;</span><br><span class=\"line\">    emitCommittables(<span class=\"literal\">false</span>, checkpointId);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// no records are expected to emit after endOfInput</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">endInput</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">  endOfInput = <span class=\"literal\">true</span>;</span><br><span class=\"line\">  emitCommittables(<span class=\"literal\">true</span>, Long.MAX_VALUE);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">emitCommittables</span><span class=\"params\">(<span class=\"type\">boolean</span> doCompaction, <span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">  prepareCommit(doCompaction, checkpointId)</span><br><span class=\"line\">    .forEach(committable -&gt; output.collect(<span class=\"keyword\">new</span> <span class=\"title class_\">StreamRecord</span>&lt;&gt;(committable)));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"keyword\">abstract</span> List&lt;Committable&gt; <span class=\"title function_\">prepareCommit</span><span class=\"params\">(<span class=\"type\">boolean</span> doCompaction, <span class=\"type\">long</span> checkpointId)</span> <span class=\"keyword\">throws</span> IOException;</span><br></pre></td></tr></table></figure>\n<p>2、CommitterOperator<br>这个算子实现了<code>org.apache.flink.api.common.state.CheckpointListener#notifyCheckpointComplete</code>方法，该方法会在FLink CK完成后被调用，paimon-flink-sink在这个方法中会进行snapshot快照的提交，主要就是将本次快照生成的snapshot、manifest文件写入到文件系统。</p>\n"},{"title":"初识paimon && Spark Catalog","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":9390,"date":"2023-04-08T09:57:25.000Z","updated":"2022-04-08T09:57:25.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 前言\n\n由于公司的业务场景涉及海量的数据更新和删除，因此一直对擅长处理海量数据更新的数据湖格式Apache paimon感兴趣。虽然Hudi对数据更新支持的也不错，但是经过测试，无论是吞吐量还是资源消耗都不能令人满意。究其根本像hudi、iceberg等数据湖格式在处理数据更新上都是通过简单粗暴的合并文件实现的，存在较大的写放大问题。\n\n在了解到Apache paimon是通过LSM实现海量数据更新后，可以预见的到海量数据更新对paimon不会存在问题，因为像使用LSM技术的kudu、doris、hbase等存储引擎都是非常成熟且久经考验的。经过测试Apache paimon的吞吐量是hudi MOR表的3-5倍，同时资源占用(IO和CPU和内存)也大幅下降。\n\n## PR on Paimon\n\n- 修复Date类型作为分区值的格式化问题，由于可能会造成与老版本(Flink Table Store)的兼容性问题，暂时无法进行合并。但是对于我们来说没有兼容性问题，因此在我们的内部版本中使用。https://github.com/apache/incubator-paimon/pull/853\n\n## Spark on Paimon\n\n由于Apache Paimon的前身是Flink Table Store，显然Paimon和Flink一起使用是最佳方案，但批处理主要还是依靠Spark来实现，因此测试Spark on Paimon将是重点工作。\n\n\n\n### Spark SQL join hive表和paimon表\n\n其中paimon表只用作ods层，实时写入cdc数据，dwd层还是用hive表，并且统一格式为parquet，因为spark对parquet格式支持的更好。paimon表当前的默认存储格式为orc，因此创建paimon表的时候，需要指定format='parquet'。\n\n```sql\nselect \n    * \nfrom  paimon.default.my_table paimon join spark_catalog.default.user_orc hive\non\n    paimon.user_id = hive.tid;\n```\n\n### spark paimon catalog\n\npaimon和hudi在实现spark catalog上有所不同，如下所示：\n\n```shell\n# hudi\nspark-sql --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'\n\n# paimon\nspark-sql ... \\\n    --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\\n    --conf spark.sql.catalog.paimon.warehouse=/tmp \\\n    --conf spark.sql.catalog.paimon.metastore=hive \\\n    --conf spark.sql.catalog.paimon.uri=thrift://localhost:9083\n```\n\n可以发现hudi的catalog名称就是spark默认的spark_catalog，其默认元数据为hive metastore。而paimon实现的catalog名称为paimon，因此需要先执行`use paimon.default;`切换到paimon catalog下，才能访问paimon表。之后执行`use spark_catalog;`访问hive表。或加上catalog前缀，`paimon.default.my_table`和`spark_catalog.default.user_orc`,以同时跨catalog访问表。\n\n\n\n## Flink on Paimon最佳实践\n\n- 使用Flink `STATEMENT SET`,重用数据源，减少资源消耗。\n\n  ```sql\n  set 'table.optimizer.source.report-statistics-enabled' = 'true';\n  set 'table.optimizer.reuse-source-enabled' = 'true';\n  \n  EXECUTE STATEMENT SET\n  BEGIN\n      insert into paimon_table_1 select name,age,city from kafka_source_1;\n      insert into paimon_table_2 select name,age,city from kafka_source_1;\n  END;\n  ```\n\n- Batch-Read的延迟和checkpoint间隔时间强相关，默认配置下，批读的延迟等于CK的间隔时间。\n\n  > 1、当配置`scan.mode = compacted-full`时，只会读取压缩完成的快照，可以提高读性能，但是延迟也增大了。同时配置`full-compaction.delta-commits = 5`后，假如CK间隔为3min，则延迟为5 * 3 + 5 * ck的持续时间，平均延迟差不多就是15分钟。\n  >\n  > 2、默认情况下scan.mode读取最新的快照，批读的延迟等于CK的间隔时间。\n  >\n  > 3、full compaction非常消耗资源，影响写入性能，同时造成CK持续时间过长，影响了作业稳定性，也增加了数据延迟。我们可以在流作业中不配置，而是用Dedicated Compaction Job进行压缩，或者将`full-compaction.delta-commits = 120`尽量调大，减少性能影响。\n\n- 单独设置bucket-key，而不是主键，可以增加除了主键外的一些索引列，提高性能。\n\n- 虽然Paimon默认`file-format`为ORC格式，但是实践好像Parquet格式更稳定。\n\n  ","source":"_posts/bigdata/paimon/初识paimon && Spark Catalog.md","raw":"---\ntitle: 初识paimon && Spark Catalog\ntags:\n  - paimon\ncategories:\n  - - bigdata\n    - paimon\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 9390\ndate: 2023-04-08 17:57:25\nupdated: 2022-04-08 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n由于公司的业务场景涉及海量的数据更新和删除，因此一直对擅长处理海量数据更新的数据湖格式Apache paimon感兴趣。虽然Hudi对数据更新支持的也不错，但是经过测试，无论是吞吐量还是资源消耗都不能令人满意。究其根本像hudi、iceberg等数据湖格式在处理数据更新上都是通过简单粗暴的合并文件实现的，存在较大的写放大问题。\n\n在了解到Apache paimon是通过LSM实现海量数据更新后，可以预见的到海量数据更新对paimon不会存在问题，因为像使用LSM技术的kudu、doris、hbase等存储引擎都是非常成熟且久经考验的。经过测试Apache paimon的吞吐量是hudi MOR表的3-5倍，同时资源占用(IO和CPU和内存)也大幅下降。\n\n## PR on Paimon\n\n- 修复Date类型作为分区值的格式化问题，由于可能会造成与老版本(Flink Table Store)的兼容性问题，暂时无法进行合并。但是对于我们来说没有兼容性问题，因此在我们的内部版本中使用。https://github.com/apache/incubator-paimon/pull/853\n\n## Spark on Paimon\n\n由于Apache Paimon的前身是Flink Table Store，显然Paimon和Flink一起使用是最佳方案，但批处理主要还是依靠Spark来实现，因此测试Spark on Paimon将是重点工作。\n\n\n\n### Spark SQL join hive表和paimon表\n\n其中paimon表只用作ods层，实时写入cdc数据，dwd层还是用hive表，并且统一格式为parquet，因为spark对parquet格式支持的更好。paimon表当前的默认存储格式为orc，因此创建paimon表的时候，需要指定format='parquet'。\n\n```sql\nselect \n    * \nfrom  paimon.default.my_table paimon join spark_catalog.default.user_orc hive\non\n    paimon.user_id = hive.tid;\n```\n\n### spark paimon catalog\n\npaimon和hudi在实现spark catalog上有所不同，如下所示：\n\n```shell\n# hudi\nspark-sql --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'\n\n# paimon\nspark-sql ... \\\n    --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\\n    --conf spark.sql.catalog.paimon.warehouse=/tmp \\\n    --conf spark.sql.catalog.paimon.metastore=hive \\\n    --conf spark.sql.catalog.paimon.uri=thrift://localhost:9083\n```\n\n可以发现hudi的catalog名称就是spark默认的spark_catalog，其默认元数据为hive metastore。而paimon实现的catalog名称为paimon，因此需要先执行`use paimon.default;`切换到paimon catalog下，才能访问paimon表。之后执行`use spark_catalog;`访问hive表。或加上catalog前缀，`paimon.default.my_table`和`spark_catalog.default.user_orc`,以同时跨catalog访问表。\n\n\n\n## Flink on Paimon最佳实践\n\n- 使用Flink `STATEMENT SET`,重用数据源，减少资源消耗。\n\n  ```sql\n  set 'table.optimizer.source.report-statistics-enabled' = 'true';\n  set 'table.optimizer.reuse-source-enabled' = 'true';\n  \n  EXECUTE STATEMENT SET\n  BEGIN\n      insert into paimon_table_1 select name,age,city from kafka_source_1;\n      insert into paimon_table_2 select name,age,city from kafka_source_1;\n  END;\n  ```\n\n- Batch-Read的延迟和checkpoint间隔时间强相关，默认配置下，批读的延迟等于CK的间隔时间。\n\n  > 1、当配置`scan.mode = compacted-full`时，只会读取压缩完成的快照，可以提高读性能，但是延迟也增大了。同时配置`full-compaction.delta-commits = 5`后，假如CK间隔为3min，则延迟为5 * 3 + 5 * ck的持续时间，平均延迟差不多就是15分钟。\n  >\n  > 2、默认情况下scan.mode读取最新的快照，批读的延迟等于CK的间隔时间。\n  >\n  > 3、full compaction非常消耗资源，影响写入性能，同时造成CK持续时间过长，影响了作业稳定性，也增加了数据延迟。我们可以在流作业中不配置，而是用Dedicated Compaction Job进行压缩，或者将`full-compaction.delta-commits = 120`尽量调大，减少性能影响。\n\n- 单独设置bucket-key，而不是主键，可以增加除了主键外的一些索引列，提高性能。\n\n- 虽然Paimon默认`file-format`为ORC格式，但是实践好像Parquet格式更稳定。\n\n  ","slug":"bigdata/paimon/初识paimon && Spark Catalog","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt3003y8j5mevlhe0il","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>由于公司的业务场景涉及海量的数据更新和删除，因此一直对擅长处理海量数据更新的数据湖格式Apache paimon感兴趣。虽然Hudi对数据更新支持的也不错，但是经过测试，无论是吞吐量还是资源消耗都不能令人满意。究其根本像hudi、iceberg等数据湖格式在处理数据更新上都是通过简单粗暴的合并文件实现的，存在较大的写放大问题。</p>\n<p>在了解到Apache paimon是通过LSM实现海量数据更新后，可以预见的到海量数据更新对paimon不会存在问题，因为像使用LSM技术的kudu、doris、hbase等存储引擎都是非常成熟且久经考验的。经过测试Apache paimon的吞吐量是hudi MOR表的3-5倍，同时资源占用(IO和CPU和内存)也大幅下降。</p>\n<h2 id=\"PR-on-Paimon\"><a href=\"#PR-on-Paimon\" class=\"headerlink\" title=\"PR on Paimon\"></a>PR on Paimon</h2><ul>\n<li>修复Date类型作为分区值的格式化问题，由于可能会造成与老版本(Flink Table Store)的兼容性问题，暂时无法进行合并。但是对于我们来说没有兼容性问题，因此在我们的内部版本中使用。<a href=\"https://github.com/apache/incubator-paimon/pull/853\">https://github.com/apache/incubator-paimon/pull/853</a></li>\n</ul>\n<h2 id=\"Spark-on-Paimon\"><a href=\"#Spark-on-Paimon\" class=\"headerlink\" title=\"Spark on Paimon\"></a>Spark on Paimon</h2><p>由于Apache Paimon的前身是Flink Table Store，显然Paimon和Flink一起使用是最佳方案，但批处理主要还是依靠Spark来实现，因此测试Spark on Paimon将是重点工作。</p>\n<h3 id=\"Spark-SQL-join-hive表和paimon表\"><a href=\"#Spark-SQL-join-hive表和paimon表\" class=\"headerlink\" title=\"Spark SQL join hive表和paimon表\"></a>Spark SQL join hive表和paimon表</h3><p>其中paimon表只用作ods层，实时写入cdc数据，dwd层还是用hive表，并且统一格式为parquet，因为spark对parquet格式支持的更好。paimon表当前的默认存储格式为orc，因此创建paimon表的时候，需要指定format&#x3D;’parquet’。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    <span class=\"operator\">*</span> </span><br><span class=\"line\"><span class=\"keyword\">from</span>  paimon.default.my_table paimon <span class=\"keyword\">join</span> spark_catalog.default.user_orc hive</span><br><span class=\"line\"><span class=\"keyword\">on</span></span><br><span class=\"line\">    paimon.user_id <span class=\"operator\">=</span> hive.tid;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"spark-paimon-catalog\"><a href=\"#spark-paimon-catalog\" class=\"headerlink\" title=\"spark paimon catalog\"></a>spark paimon catalog</h3><p>paimon和hudi在实现spark catalog上有所不同，如下所示：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">hudi</span></span><br><span class=\"line\">spark-sql --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\</span><br><span class=\"line\">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \\</span><br><span class=\"line\">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27; \\</span><br><span class=\"line\">--conf &#x27;spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog&#x27;</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">paimon</span></span><br><span class=\"line\">spark-sql ... \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.warehouse=/tmp \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.metastore=hive \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.uri=thrift://localhost:9083</span><br></pre></td></tr></table></figure>\n\n<p>可以发现hudi的catalog名称就是spark默认的spark_catalog，其默认元数据为hive metastore。而paimon实现的catalog名称为paimon，因此需要先执行<code>use paimon.default;</code>切换到paimon catalog下，才能访问paimon表。之后执行<code>use spark_catalog;</code>访问hive表。或加上catalog前缀，<code>paimon.default.my_table</code>和<code>spark_catalog.default.user_orc</code>,以同时跨catalog访问表。</p>\n<h2 id=\"Flink-on-Paimon最佳实践\"><a href=\"#Flink-on-Paimon最佳实践\" class=\"headerlink\" title=\"Flink on Paimon最佳实践\"></a>Flink on Paimon最佳实践</h2><ul>\n<li><p>使用Flink <code>STATEMENT SET</code>,重用数据源，减少资源消耗。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"string\">&#x27;table.optimizer.source.report-statistics-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"string\">&#x27;table.optimizer.reuse-source-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">EXECUTE</span> STATEMENT <span class=\"keyword\">SET</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span></span><br><span class=\"line\">    <span class=\"keyword\">insert</span> <span class=\"keyword\">into</span> paimon_table_1 <span class=\"keyword\">select</span> name,age,city <span class=\"keyword\">from</span> kafka_source_1;</span><br><span class=\"line\">    <span class=\"keyword\">insert</span> <span class=\"keyword\">into</span> paimon_table_2 <span class=\"keyword\">select</span> name,age,city <span class=\"keyword\">from</span> kafka_source_1;</span><br><span class=\"line\"><span class=\"keyword\">END</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Batch-Read的延迟和checkpoint间隔时间强相关，默认配置下，批读的延迟等于CK的间隔时间。</p>\n<blockquote>\n<p>1、当配置<code>scan.mode = compacted-full</code>时，只会读取压缩完成的快照，可以提高读性能，但是延迟也增大了。同时配置<code>full-compaction.delta-commits = 5</code>后，假如CK间隔为3min，则延迟为5 * 3 + 5 * ck的持续时间，平均延迟差不多就是15分钟。</p>\n<p>2、默认情况下scan.mode读取最新的快照，批读的延迟等于CK的间隔时间。</p>\n<p>3、full compaction非常消耗资源，影响写入性能，同时造成CK持续时间过长，影响了作业稳定性，也增加了数据延迟。我们可以在流作业中不配置，而是用Dedicated Compaction Job进行压缩，或者将<code>full-compaction.delta-commits = 120</code>尽量调大，减少性能影响。</p>\n</blockquote>\n</li>\n<li><p>单独设置bucket-key，而不是主键，可以增加除了主键外的一些索引列，提高性能。</p>\n</li>\n<li><p>虽然Paimon默认<code>file-format</code>为ORC格式，但是实践好像Parquet格式更稳定。</p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>由于公司的业务场景涉及海量的数据更新和删除，因此一直对擅长处理海量数据更新的数据湖格式Apache paimon感兴趣。虽然Hudi对数据更新支持的也不错，但是经过测试，无论是吞吐量还是资源消耗都不能令人满意。究其根本像hudi、iceberg等数据湖格式在处理数据更新上都是通过简单粗暴的合并文件实现的，存在较大的写放大问题。</p>\n<p>在了解到Apache paimon是通过LSM实现海量数据更新后，可以预见的到海量数据更新对paimon不会存在问题，因为像使用LSM技术的kudu、doris、hbase等存储引擎都是非常成熟且久经考验的。经过测试Apache paimon的吞吐量是hudi MOR表的3-5倍，同时资源占用(IO和CPU和内存)也大幅下降。</p>\n<h2 id=\"PR-on-Paimon\"><a href=\"#PR-on-Paimon\" class=\"headerlink\" title=\"PR on Paimon\"></a>PR on Paimon</h2><ul>\n<li>修复Date类型作为分区值的格式化问题，由于可能会造成与老版本(Flink Table Store)的兼容性问题，暂时无法进行合并。但是对于我们来说没有兼容性问题，因此在我们的内部版本中使用。<a href=\"https://github.com/apache/incubator-paimon/pull/853\">https://github.com/apache/incubator-paimon/pull/853</a></li>\n</ul>\n<h2 id=\"Spark-on-Paimon\"><a href=\"#Spark-on-Paimon\" class=\"headerlink\" title=\"Spark on Paimon\"></a>Spark on Paimon</h2><p>由于Apache Paimon的前身是Flink Table Store，显然Paimon和Flink一起使用是最佳方案，但批处理主要还是依靠Spark来实现，因此测试Spark on Paimon将是重点工作。</p>\n<h3 id=\"Spark-SQL-join-hive表和paimon表\"><a href=\"#Spark-SQL-join-hive表和paimon表\" class=\"headerlink\" title=\"Spark SQL join hive表和paimon表\"></a>Spark SQL join hive表和paimon表</h3><p>其中paimon表只用作ods层，实时写入cdc数据，dwd层还是用hive表，并且统一格式为parquet，因为spark对parquet格式支持的更好。paimon表当前的默认存储格式为orc，因此创建paimon表的时候，需要指定format&#x3D;’parquet’。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    <span class=\"operator\">*</span> </span><br><span class=\"line\"><span class=\"keyword\">from</span>  paimon.default.my_table paimon <span class=\"keyword\">join</span> spark_catalog.default.user_orc hive</span><br><span class=\"line\"><span class=\"keyword\">on</span></span><br><span class=\"line\">    paimon.user_id <span class=\"operator\">=</span> hive.tid;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"spark-paimon-catalog\"><a href=\"#spark-paimon-catalog\" class=\"headerlink\" title=\"spark paimon catalog\"></a>spark paimon catalog</h3><p>paimon和hudi在实现spark catalog上有所不同，如下所示：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">hudi</span></span><br><span class=\"line\">spark-sql --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\</span><br><span class=\"line\">--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \\</span><br><span class=\"line\">--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27; \\</span><br><span class=\"line\">--conf &#x27;spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog&#x27;</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">paimon</span></span><br><span class=\"line\">spark-sql ... \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.warehouse=/tmp \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.metastore=hive \\</span><br><span class=\"line\">    --conf spark.sql.catalog.paimon.uri=thrift://localhost:9083</span><br></pre></td></tr></table></figure>\n\n<p>可以发现hudi的catalog名称就是spark默认的spark_catalog，其默认元数据为hive metastore。而paimon实现的catalog名称为paimon，因此需要先执行<code>use paimon.default;</code>切换到paimon catalog下，才能访问paimon表。之后执行<code>use spark_catalog;</code>访问hive表。或加上catalog前缀，<code>paimon.default.my_table</code>和<code>spark_catalog.default.user_orc</code>,以同时跨catalog访问表。</p>\n<h2 id=\"Flink-on-Paimon最佳实践\"><a href=\"#Flink-on-Paimon最佳实践\" class=\"headerlink\" title=\"Flink on Paimon最佳实践\"></a>Flink on Paimon最佳实践</h2><ul>\n<li><p>使用Flink <code>STATEMENT SET</code>,重用数据源，减少资源消耗。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"string\">&#x27;table.optimizer.source.report-statistics-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"string\">&#x27;table.optimizer.reuse-source-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">EXECUTE</span> STATEMENT <span class=\"keyword\">SET</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span></span><br><span class=\"line\">    <span class=\"keyword\">insert</span> <span class=\"keyword\">into</span> paimon_table_1 <span class=\"keyword\">select</span> name,age,city <span class=\"keyword\">from</span> kafka_source_1;</span><br><span class=\"line\">    <span class=\"keyword\">insert</span> <span class=\"keyword\">into</span> paimon_table_2 <span class=\"keyword\">select</span> name,age,city <span class=\"keyword\">from</span> kafka_source_1;</span><br><span class=\"line\"><span class=\"keyword\">END</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Batch-Read的延迟和checkpoint间隔时间强相关，默认配置下，批读的延迟等于CK的间隔时间。</p>\n<blockquote>\n<p>1、当配置<code>scan.mode = compacted-full</code>时，只会读取压缩完成的快照，可以提高读性能，但是延迟也增大了。同时配置<code>full-compaction.delta-commits = 5</code>后，假如CK间隔为3min，则延迟为5 * 3 + 5 * ck的持续时间，平均延迟差不多就是15分钟。</p>\n<p>2、默认情况下scan.mode读取最新的快照，批读的延迟等于CK的间隔时间。</p>\n<p>3、full compaction非常消耗资源，影响写入性能，同时造成CK持续时间过长，影响了作业稳定性，也增加了数据延迟。我们可以在流作业中不配置，而是用Dedicated Compaction Job进行压缩，或者将<code>full-compaction.delta-commits = 120</code>尽量调大，减少性能影响。</p>\n</blockquote>\n</li>\n<li><p>单独设置bucket-key，而不是主键，可以增加除了主键外的一些索引列，提高性能。</p>\n</li>\n<li><p>虽然Paimon默认<code>file-format</code>为ORC格式，但是实践好像Parquet格式更稳定。</p>\n</li>\n</ul>\n"},{"title":"初识Apache Druid","abbrlink":17536,"date":"2023-06-11T09:57:25.000Z","updated":"2023-06-12T09:57:25.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## Why Druid\n\n> OLAP 是一种让用户可以用从不同视角方便快捷的分析数据的计算方法。主流的 OLAP 可以分为3类：多维 MOLAP ( Multi-dimensional OLAP )、关系型 ROLAP ( Relational OLAP ) 和混合 HOLAP ( Hybrid OLAP ) 三大类。\n\n在海量数据上进行亚秒级的多维分析，并且要求高并发，可以选择的OLAP系统并不多。而其中MOLAP最为知名的就是Apache Kylin和Apache Druid。\n\n先前我们一直采用Apache Kylin进行离线多维分析，在使用中发现了一系列问题，让我们不得不将目光放在Druid上面：\n\n- 1、由于Kylin构建cube的数量和维度的关系是2的n次方，指数级增长是非常可怕的，一般超过20个维度，在Kylin中就要小心了，因此使用Kylin需要时刻担心维度爆炸的问题。\n- 2、Kylin目前要求不超过63个Normal维度，这是因为cubeid是Long类型，而Long的最大值是**2^63 -1**，所以不能超过63个维度。而我们的业务场景最大会有二百多个维度，Kylin已经不满足我们的要求。\n- 3、当维度中存在大基数列\\维度的时候，还需要面临磁盘空间占用极度膨胀的问题。构建出来的cube占用的空间会远大于原数据占用的空间，这难以令人接受。\n- 4、预计算时间长度不稳定，会随着维度的增加，不可控的延长，难以保证数据的新鲜度，且不可控。\n\nDruid的优点：\n\n- 可以选择性的Rollup，Druid的Rollup预计算相当于Kylin中只进行Base Cube构建，因此无需担心维度数量的问题。\n- 基于LSM，Druid可以在进行海量数据实时导入的同时进行预计算，Druid可以实时导入。\n- Druid在每个维度列上面构建索引，来加速多维分析，而不是像Kylin那样完全的预计算。Druid中默认为每个维度列创建Bitmap索引, 都是先做字典在做bitmap。https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html\n- Druid经过Rollup的数据会比原始数据大量减少，占用的存储空间大大小于Kylin，节约了成本。","source":"_posts/bigdata/druid/初识Apache Druid.md","raw":"---\ntitle: 初识Apache Druid\ntags:\n  - Druid\ncategories:\n  - - Druid\nabbrlink: 17536\ndate: 2023-06-11 17:57:25\nupdated: 2023-06-12 17:57:25\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## Why Druid\n\n> OLAP 是一种让用户可以用从不同视角方便快捷的分析数据的计算方法。主流的 OLAP 可以分为3类：多维 MOLAP ( Multi-dimensional OLAP )、关系型 ROLAP ( Relational OLAP ) 和混合 HOLAP ( Hybrid OLAP ) 三大类。\n\n在海量数据上进行亚秒级的多维分析，并且要求高并发，可以选择的OLAP系统并不多。而其中MOLAP最为知名的就是Apache Kylin和Apache Druid。\n\n先前我们一直采用Apache Kylin进行离线多维分析，在使用中发现了一系列问题，让我们不得不将目光放在Druid上面：\n\n- 1、由于Kylin构建cube的数量和维度的关系是2的n次方，指数级增长是非常可怕的，一般超过20个维度，在Kylin中就要小心了，因此使用Kylin需要时刻担心维度爆炸的问题。\n- 2、Kylin目前要求不超过63个Normal维度，这是因为cubeid是Long类型，而Long的最大值是**2^63 -1**，所以不能超过63个维度。而我们的业务场景最大会有二百多个维度，Kylin已经不满足我们的要求。\n- 3、当维度中存在大基数列\\维度的时候，还需要面临磁盘空间占用极度膨胀的问题。构建出来的cube占用的空间会远大于原数据占用的空间，这难以令人接受。\n- 4、预计算时间长度不稳定，会随着维度的增加，不可控的延长，难以保证数据的新鲜度，且不可控。\n\nDruid的优点：\n\n- 可以选择性的Rollup，Druid的Rollup预计算相当于Kylin中只进行Base Cube构建，因此无需担心维度数量的问题。\n- 基于LSM，Druid可以在进行海量数据实时导入的同时进行预计算，Druid可以实时导入。\n- Druid在每个维度列上面构建索引，来加速多维分析，而不是像Kylin那样完全的预计算。Druid中默认为每个维度列创建Bitmap索引, 都是先做字典在做bitmap。https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html\n- Druid经过Rollup的数据会比原始数据大量减少，占用的存储空间大大小于Kylin，节约了成本。","slug":"bigdata/druid/初识Apache Druid","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt300418j5m3o0k6ocs","content":"<h2 id=\"Why-Druid\"><a href=\"#Why-Druid\" class=\"headerlink\" title=\"Why Druid\"></a>Why Druid</h2><blockquote>\n<p>OLAP 是一种让用户可以用从不同视角方便快捷的分析数据的计算方法。主流的 OLAP 可以分为3类：多维 MOLAP ( Multi-dimensional OLAP )、关系型 ROLAP ( Relational OLAP ) 和混合 HOLAP ( Hybrid OLAP ) 三大类。</p>\n</blockquote>\n<p>在海量数据上进行亚秒级的多维分析，并且要求高并发，可以选择的OLAP系统并不多。而其中MOLAP最为知名的就是Apache Kylin和Apache Druid。</p>\n<p>先前我们一直采用Apache Kylin进行离线多维分析，在使用中发现了一系列问题，让我们不得不将目光放在Druid上面：</p>\n<ul>\n<li>1、由于Kylin构建cube的数量和维度的关系是2的n次方，指数级增长是非常可怕的，一般超过20个维度，在Kylin中就要小心了，因此使用Kylin需要时刻担心维度爆炸的问题。</li>\n<li>2、Kylin目前要求不超过63个Normal维度，这是因为cubeid是Long类型，而Long的最大值是<strong>2^63 -1</strong>，所以不能超过63个维度。而我们的业务场景最大会有二百多个维度，Kylin已经不满足我们的要求。</li>\n<li>3、当维度中存在大基数列\\维度的时候，还需要面临磁盘空间占用极度膨胀的问题。构建出来的cube占用的空间会远大于原数据占用的空间，这难以令人接受。</li>\n<li>4、预计算时间长度不稳定，会随着维度的增加，不可控的延长，难以保证数据的新鲜度，且不可控。</li>\n</ul>\n<p>Druid的优点：</p>\n<ul>\n<li>可以选择性的Rollup，Druid的Rollup预计算相当于Kylin中只进行Base Cube构建，因此无需担心维度数量的问题。</li>\n<li>基于LSM，Druid可以在进行海量数据实时导入的同时进行预计算，Druid可以实时导入。</li>\n<li>Druid在每个维度列上面构建索引，来加速多维分析，而不是像Kylin那样完全的预计算。Druid中默认为每个维度列创建Bitmap索引, 都是先做字典在做bitmap。<a href=\"https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html\">https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html</a></li>\n<li>Druid经过Rollup的数据会比原始数据大量减少，占用的存储空间大大小于Kylin，节约了成本。</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"Why-Druid\"><a href=\"#Why-Druid\" class=\"headerlink\" title=\"Why Druid\"></a>Why Druid</h2><blockquote>\n<p>OLAP 是一种让用户可以用从不同视角方便快捷的分析数据的计算方法。主流的 OLAP 可以分为3类：多维 MOLAP ( Multi-dimensional OLAP )、关系型 ROLAP ( Relational OLAP ) 和混合 HOLAP ( Hybrid OLAP ) 三大类。</p>\n</blockquote>\n<p>在海量数据上进行亚秒级的多维分析，并且要求高并发，可以选择的OLAP系统并不多。而其中MOLAP最为知名的就是Apache Kylin和Apache Druid。</p>\n<p>先前我们一直采用Apache Kylin进行离线多维分析，在使用中发现了一系列问题，让我们不得不将目光放在Druid上面：</p>\n<ul>\n<li>1、由于Kylin构建cube的数量和维度的关系是2的n次方，指数级增长是非常可怕的，一般超过20个维度，在Kylin中就要小心了，因此使用Kylin需要时刻担心维度爆炸的问题。</li>\n<li>2、Kylin目前要求不超过63个Normal维度，这是因为cubeid是Long类型，而Long的最大值是<strong>2^63 -1</strong>，所以不能超过63个维度。而我们的业务场景最大会有二百多个维度，Kylin已经不满足我们的要求。</li>\n<li>3、当维度中存在大基数列\\维度的时候，还需要面临磁盘空间占用极度膨胀的问题。构建出来的cube占用的空间会远大于原数据占用的空间，这难以令人接受。</li>\n<li>4、预计算时间长度不稳定，会随着维度的增加，不可控的延长，难以保证数据的新鲜度，且不可控。</li>\n</ul>\n<p>Druid的优点：</p>\n<ul>\n<li>可以选择性的Rollup，Druid的Rollup预计算相当于Kylin中只进行Base Cube构建，因此无需担心维度数量的问题。</li>\n<li>基于LSM，Druid可以在进行海量数据实时导入的同时进行预计算，Druid可以实时导入。</li>\n<li>Druid在每个维度列上面构建索引，来加速多维分析，而不是像Kylin那样完全的预计算。Druid中默认为每个维度列创建Bitmap索引, 都是先做字典在做bitmap。<a href=\"https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html\">https://tianzhipeng-git.github.io/2020/09/07/bitmap-index.html</a></li>\n<li>Druid经过Rollup的数据会比原始数据大量减少，占用的存储空间大大小于Kylin，节约了成本。</li>\n</ul>\n"},{"title":"Spark SQL合并小文件","abbrlink":39051,"date":"2023-03-08T11:54:07.000Z","updated":"2022-03-08T11:54:07.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 前言\n\nHive 表中太多的小文件会影响数据的查询性能和效率，同时加大了 HDFS NameNode 的压力。\n\n\n\n## 方法1：Base Spark SQL Partitioning Hints\n\n```sql\nSELECT /*+ COALESCE(3) */ * FROM t;\n\nSELECT /*+ REPARTITION(3) */ * FROM t;\n\nSELECT /*+ REPARTITION(c) */ * FROM t;\n\nSELECT /*+ REPARTITION(3, c) */ * FROM t;\n\nSELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;\n\nSELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;\n\nSELECT /*+ REBALANCE */ * FROM t;\n\nSELECT /*+ REBALANCE(3) */ * FROM t;\n\nSELECT /*+ REBALANCE(c) */ * FROM t;\n\nSELECT /*+ REBALANCE(3, c) */ * FROM t;\n\n-- multiple partitioning hints\nEXPLAIN EXTENDED SELECT /*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */ * FROM t;\n== Parsed Logical Plan ==\n'UnresolvedHint REPARTITION, [100]\n+- 'UnresolvedHint COALESCE, [500]\n   +- 'UnresolvedHint REPARTITION_BY_RANGE, [3, 'c]\n      +- 'Project [*]\n         +- 'UnresolvedRelation [t]\n\n== Analyzed Logical Plan ==\nname: string, c: int\nRepartition 100, true\n+- Repartition 500, false\n   +- RepartitionByExpression [c#30 ASC NULLS FIRST], 3\n      +- Project [name#29, c#30]\n         +- SubqueryAlias spark_catalog.default.t\n            +- Relation[name#29,c#30] parquet\n\n== Optimized Logical Plan ==\nRepartition 100, true\n+- Relation[name#29,c#30] parquet\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(100), false, [id=#121]\n+- *(1) ColumnarToRow\n   +- FileScan parquet default.t[name#29,c#30] Batched: true, DataFilters: [], Format: Parquet,\n      Location: CatalogFileIndex[file:/spark/spark-warehouse/t], PartitionFilters: [],\n      PushedFilters: [], ReadSchema: struct<name:string>\n```\n\n\n\n### `/*+ COALESCE() */`  VS  `/*+ REPARTITION() */`\n\n翻阅代码可以得到，RDD 的 repartition 就是调用的 coalesce 函数,只是shuffle 参数设置为了true，我们的目的是减少小文件，所以这块可以使用 `coalesce`。\n\n> The repartition algorithm does a **full shuffle** of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a **full shuffle**.\n\n### `/*+ REPARTITION() */` VS `/*+ REBALANCE() */`\n\nSpark 3.2+ 引入了 Rebalance 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好地处理小文件问题。 Rebalance的目的是为了在AQE阶段,根据spark.sql.adaptive.advisoryPartitionSizeInBytes进行分区的重新分区，防止数据倾斜。再加上SPARK-35786,就可以根据hint进行重分区。\n\n一般在reparition用到的地方都可以Rebalance来替换，而且Rebalance有更好的文件大小的控制能力，更多的信息可以查看对应的 [spark-jira](https://issues.apache.org/jira/browse/SPARK-35725?spm=a2c6h.12873639.article-detail.7.1e1e6422yF136F)。\n\n### 总结\n\n对于合并小文件的场景，REBALANCE > COALESCE > REPARTITION.\n\n\n\n## 方法2：Base Kyuubi Spark SQL Extensions\n\nKyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 spark.sql.optimizer.insertRepartitionNum 和spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum 配置。与使用`/*+ REBALANCE() */` hint等效。\n\n```yaml\n# 配置Kyuubi Spark SQL Extensions\nspark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension\n\n# 开启RepartitionBeforeWrite优化（默认开启）\nspark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true\n\n# 配置AQE \nspark.sql.adaptive.enabled=true \nspark.sql.adaptive.advisoryPartitionSizeInBytes=512m \nspark.sql.adaptive.coalescePartitions.minPartitionmum=1 \n```\n\n","source":"_posts/bigdata/spark/Spark SQL合并小文件.md","raw":"---\ntitle: Spark SQL合并小文件\ntags:\n  - spark\ncategories:\n  - - bigdata\n    - spark\nabbrlink: 39051\ndate: 2023-03-08 19:54:07\nupdated: 2022-03-08 19:54:07\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\nHive 表中太多的小文件会影响数据的查询性能和效率，同时加大了 HDFS NameNode 的压力。\n\n\n\n## 方法1：Base Spark SQL Partitioning Hints\n\n```sql\nSELECT /*+ COALESCE(3) */ * FROM t;\n\nSELECT /*+ REPARTITION(3) */ * FROM t;\n\nSELECT /*+ REPARTITION(c) */ * FROM t;\n\nSELECT /*+ REPARTITION(3, c) */ * FROM t;\n\nSELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;\n\nSELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;\n\nSELECT /*+ REBALANCE */ * FROM t;\n\nSELECT /*+ REBALANCE(3) */ * FROM t;\n\nSELECT /*+ REBALANCE(c) */ * FROM t;\n\nSELECT /*+ REBALANCE(3, c) */ * FROM t;\n\n-- multiple partitioning hints\nEXPLAIN EXTENDED SELECT /*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */ * FROM t;\n== Parsed Logical Plan ==\n'UnresolvedHint REPARTITION, [100]\n+- 'UnresolvedHint COALESCE, [500]\n   +- 'UnresolvedHint REPARTITION_BY_RANGE, [3, 'c]\n      +- 'Project [*]\n         +- 'UnresolvedRelation [t]\n\n== Analyzed Logical Plan ==\nname: string, c: int\nRepartition 100, true\n+- Repartition 500, false\n   +- RepartitionByExpression [c#30 ASC NULLS FIRST], 3\n      +- Project [name#29, c#30]\n         +- SubqueryAlias spark_catalog.default.t\n            +- Relation[name#29,c#30] parquet\n\n== Optimized Logical Plan ==\nRepartition 100, true\n+- Relation[name#29,c#30] parquet\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(100), false, [id=#121]\n+- *(1) ColumnarToRow\n   +- FileScan parquet default.t[name#29,c#30] Batched: true, DataFilters: [], Format: Parquet,\n      Location: CatalogFileIndex[file:/spark/spark-warehouse/t], PartitionFilters: [],\n      PushedFilters: [], ReadSchema: struct<name:string>\n```\n\n\n\n### `/*+ COALESCE() */`  VS  `/*+ REPARTITION() */`\n\n翻阅代码可以得到，RDD 的 repartition 就是调用的 coalesce 函数,只是shuffle 参数设置为了true，我们的目的是减少小文件，所以这块可以使用 `coalesce`。\n\n> The repartition algorithm does a **full shuffle** of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a **full shuffle**.\n\n### `/*+ REPARTITION() */` VS `/*+ REBALANCE() */`\n\nSpark 3.2+ 引入了 Rebalance 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好地处理小文件问题。 Rebalance的目的是为了在AQE阶段,根据spark.sql.adaptive.advisoryPartitionSizeInBytes进行分区的重新分区，防止数据倾斜。再加上SPARK-35786,就可以根据hint进行重分区。\n\n一般在reparition用到的地方都可以Rebalance来替换，而且Rebalance有更好的文件大小的控制能力，更多的信息可以查看对应的 [spark-jira](https://issues.apache.org/jira/browse/SPARK-35725?spm=a2c6h.12873639.article-detail.7.1e1e6422yF136F)。\n\n### 总结\n\n对于合并小文件的场景，REBALANCE > COALESCE > REPARTITION.\n\n\n\n## 方法2：Base Kyuubi Spark SQL Extensions\n\nKyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 spark.sql.optimizer.insertRepartitionNum 和spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum 配置。与使用`/*+ REBALANCE() */` hint等效。\n\n```yaml\n# 配置Kyuubi Spark SQL Extensions\nspark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension\n\n# 开启RepartitionBeforeWrite优化（默认开启）\nspark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true\n\n# 配置AQE \nspark.sql.adaptive.enabled=true \nspark.sql.adaptive.advisoryPartitionSizeInBytes=512m \nspark.sql.adaptive.coalescePartitions.minPartitionmum=1 \n```\n\n","slug":"bigdata/spark/Spark SQL合并小文件","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt400448j5mdrjzb912","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了 HDFS NameNode 的压力。</p>\n<h2 id=\"方法1：Base-Spark-SQL-Partitioning-Hints\"><a href=\"#方法1：Base-Spark-SQL-Partitioning-Hints\" class=\"headerlink\" title=\"方法1：Base Spark SQL Partitioning Hints\"></a>方法1：Base Spark SQL Partitioning Hints</h2><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ COALESCE(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION_BY_RANGE(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION_BY_RANGE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- multiple partitioning hints</span></span><br><span class=\"line\">EXPLAIN EXTENDED <span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Parsed Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\"><span class=\"string\">&#x27;UnresolvedHint REPARTITION, [100]</span></span><br><span class=\"line\"><span class=\"string\">+- &#x27;</span>UnresolvedHint COALESCE, [<span class=\"number\">500</span>]</span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"string\">&#x27;UnresolvedHint REPARTITION_BY_RANGE, [3, &#x27;</span>c]</span><br><span class=\"line\">      <span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"string\">&#x27;Project [*]</span></span><br><span class=\"line\"><span class=\"string\">         +- &#x27;</span>UnresolvedRelation [t]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Analyzed Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">name: string, c: <span class=\"type\">int</span></span><br><span class=\"line\">Repartition <span class=\"number\">100</span>, <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> Repartition <span class=\"number\">500</span>, <span class=\"literal\">false</span></span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> RepartitionByExpression [c#<span class=\"number\">30</span> <span class=\"keyword\">ASC</span> NULLS <span class=\"keyword\">FIRST</span>], <span class=\"number\">3</span></span><br><span class=\"line\">      <span class=\"operator\">+</span><span class=\"operator\">-</span> Project [name#<span class=\"number\">29</span>, c#<span class=\"number\">30</span>]</span><br><span class=\"line\">         <span class=\"operator\">+</span><span class=\"operator\">-</span> SubqueryAlias spark_catalog.default.t</span><br><span class=\"line\">            <span class=\"operator\">+</span><span class=\"operator\">-</span> Relation[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] parquet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Optimized Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">Repartition <span class=\"number\">100</span>, <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> Relation[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] parquet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Physical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">Exchange RoundRobinPartitioning(<span class=\"number\">100</span>), <span class=\"literal\">false</span>, [id<span class=\"operator\">=</span>#<span class=\"number\">121</span>]</span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"operator\">*</span>(<span class=\"number\">1</span>) ColumnarToRow</span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> FileScan parquet default.t[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] Batched: <span class=\"literal\">true</span>, DataFilters: [], Format: Parquet,</span><br><span class=\"line\">      Location: CatalogFileIndex[file:<span class=\"operator\">/</span>spark<span class=\"operator\">/</span>spark<span class=\"operator\">-</span>warehouse<span class=\"operator\">/</span>t], PartitionFilters: [],</span><br><span class=\"line\">      PushedFilters: [], ReadSchema: struct<span class=\"operator\">&lt;</span>name:string<span class=\"operator\">&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"COALESCE-VS-REPARTITION\"><a href=\"#COALESCE-VS-REPARTITION\" class=\"headerlink\" title=\"/*+ COALESCE() */  VS  /*+ REPARTITION() */\"></a><code>/*+ COALESCE() */</code>  VS  <code>/*+ REPARTITION() */</code></h3><p>翻阅代码可以得到，RDD 的 repartition 就是调用的 coalesce 函数,只是shuffle 参数设置为了true，我们的目的是减少小文件，所以这块可以使用 <code>coalesce</code>。</p>\n<blockquote>\n<p>The repartition algorithm does a <strong>full shuffle</strong> of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a <strong>full shuffle</strong>.</p>\n</blockquote>\n<h3 id=\"REPARTITION-VS-REBALANCE\"><a href=\"#REPARTITION-VS-REBALANCE\" class=\"headerlink\" title=\"/*+ REPARTITION() */ VS /*+ REBALANCE() */\"></a><code>/*+ REPARTITION() */</code> VS <code>/*+ REBALANCE() */</code></h3><p>Spark 3.2+ 引入了 Rebalance 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好地处理小文件问题。 Rebalance的目的是为了在AQE阶段,根据spark.sql.adaptive.advisoryPartitionSizeInBytes进行分区的重新分区，防止数据倾斜。再加上SPARK-35786,就可以根据hint进行重分区。</p>\n<p>一般在reparition用到的地方都可以Rebalance来替换，而且Rebalance有更好的文件大小的控制能力，更多的信息可以查看对应的 <a href=\"https://issues.apache.org/jira/browse/SPARK-35725?spm=a2c6h.12873639.article-detail.7.1e1e6422yF136F\">spark-jira</a>。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>对于合并小文件的场景，REBALANCE &gt; COALESCE &gt; REPARTITION.</p>\n<h2 id=\"方法2：Base-Kyuubi-Spark-SQL-Extensions\"><a href=\"#方法2：Base-Kyuubi-Spark-SQL-Extensions\" class=\"headerlink\" title=\"方法2：Base Kyuubi Spark SQL Extensions\"></a>方法2：Base Kyuubi Spark SQL Extensions</h2><p>Kyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 spark.sql.optimizer.insertRepartitionNum 和spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum 配置。与使用<code>/*+ REBALANCE() */</code> hint等效。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 配置Kyuubi Spark SQL Extensions</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开启RepartitionBeforeWrite优化（默认开启）</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置AQE </span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.enabled=true</span> </span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.advisoryPartitionSizeInBytes=512m</span> </span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.minPartitionmum=1</span> </span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了 HDFS NameNode 的压力。</p>\n<h2 id=\"方法1：Base-Spark-SQL-Partitioning-Hints\"><a href=\"#方法1：Base-Spark-SQL-Partitioning-Hints\" class=\"headerlink\" title=\"方法1：Base Spark SQL Partitioning Hints\"></a>方法1：Base Spark SQL Partitioning Hints</h2><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ COALESCE(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION_BY_RANGE(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION_BY_RANGE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(3) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REBALANCE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- multiple partitioning hints</span></span><br><span class=\"line\">EXPLAIN EXTENDED <span class=\"keyword\">SELECT</span> <span class=\"comment\">/*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> t;</span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Parsed Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\"><span class=\"string\">&#x27;UnresolvedHint REPARTITION, [100]</span></span><br><span class=\"line\"><span class=\"string\">+- &#x27;</span>UnresolvedHint COALESCE, [<span class=\"number\">500</span>]</span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"string\">&#x27;UnresolvedHint REPARTITION_BY_RANGE, [3, &#x27;</span>c]</span><br><span class=\"line\">      <span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"string\">&#x27;Project [*]</span></span><br><span class=\"line\"><span class=\"string\">         +- &#x27;</span>UnresolvedRelation [t]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Analyzed Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">name: string, c: <span class=\"type\">int</span></span><br><span class=\"line\">Repartition <span class=\"number\">100</span>, <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> Repartition <span class=\"number\">500</span>, <span class=\"literal\">false</span></span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> RepartitionByExpression [c#<span class=\"number\">30</span> <span class=\"keyword\">ASC</span> NULLS <span class=\"keyword\">FIRST</span>], <span class=\"number\">3</span></span><br><span class=\"line\">      <span class=\"operator\">+</span><span class=\"operator\">-</span> Project [name#<span class=\"number\">29</span>, c#<span class=\"number\">30</span>]</span><br><span class=\"line\">         <span class=\"operator\">+</span><span class=\"operator\">-</span> SubqueryAlias spark_catalog.default.t</span><br><span class=\"line\">            <span class=\"operator\">+</span><span class=\"operator\">-</span> Relation[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] parquet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Optimized Logical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">Repartition <span class=\"number\">100</span>, <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> Relation[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] parquet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">=</span><span class=\"operator\">=</span> Physical Plan <span class=\"operator\">=</span><span class=\"operator\">=</span></span><br><span class=\"line\">Exchange RoundRobinPartitioning(<span class=\"number\">100</span>), <span class=\"literal\">false</span>, [id<span class=\"operator\">=</span>#<span class=\"number\">121</span>]</span><br><span class=\"line\"><span class=\"operator\">+</span><span class=\"operator\">-</span> <span class=\"operator\">*</span>(<span class=\"number\">1</span>) ColumnarToRow</span><br><span class=\"line\">   <span class=\"operator\">+</span><span class=\"operator\">-</span> FileScan parquet default.t[name#<span class=\"number\">29</span>,c#<span class=\"number\">30</span>] Batched: <span class=\"literal\">true</span>, DataFilters: [], Format: Parquet,</span><br><span class=\"line\">      Location: CatalogFileIndex[file:<span class=\"operator\">/</span>spark<span class=\"operator\">/</span>spark<span class=\"operator\">-</span>warehouse<span class=\"operator\">/</span>t], PartitionFilters: [],</span><br><span class=\"line\">      PushedFilters: [], ReadSchema: struct<span class=\"operator\">&lt;</span>name:string<span class=\"operator\">&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"COALESCE-VS-REPARTITION\"><a href=\"#COALESCE-VS-REPARTITION\" class=\"headerlink\" title=\"/*+ COALESCE() */  VS  /*+ REPARTITION() */\"></a><code>/*+ COALESCE() */</code>  VS  <code>/*+ REPARTITION() */</code></h3><p>翻阅代码可以得到，RDD 的 repartition 就是调用的 coalesce 函数,只是shuffle 参数设置为了true，我们的目的是减少小文件，所以这块可以使用 <code>coalesce</code>。</p>\n<blockquote>\n<p>The repartition algorithm does a <strong>full shuffle</strong> of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a <strong>full shuffle</strong>.</p>\n</blockquote>\n<h3 id=\"REPARTITION-VS-REBALANCE\"><a href=\"#REPARTITION-VS-REBALANCE\" class=\"headerlink\" title=\"/*+ REPARTITION() */ VS /*+ REBALANCE() */\"></a><code>/*+ REPARTITION() */</code> VS <code>/*+ REBALANCE() */</code></h3><p>Spark 3.2+ 引入了 Rebalance 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好地处理小文件问题。 Rebalance的目的是为了在AQE阶段,根据spark.sql.adaptive.advisoryPartitionSizeInBytes进行分区的重新分区，防止数据倾斜。再加上SPARK-35786,就可以根据hint进行重分区。</p>\n<p>一般在reparition用到的地方都可以Rebalance来替换，而且Rebalance有更好的文件大小的控制能力，更多的信息可以查看对应的 <a href=\"https://issues.apache.org/jira/browse/SPARK-35725?spm=a2c6h.12873639.article-detail.7.1e1e6422yF136F\">spark-jira</a>。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>对于合并小文件的场景，REBALANCE &gt; COALESCE &gt; REPARTITION.</p>\n<h2 id=\"方法2：Base-Kyuubi-Spark-SQL-Extensions\"><a href=\"#方法2：Base-Kyuubi-Spark-SQL-Extensions\" class=\"headerlink\" title=\"方法2：Base Kyuubi Spark SQL Extensions\"></a>方法2：Base Kyuubi Spark SQL Extensions</h2><p>Kyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 spark.sql.optimizer.insertRepartitionNum 和spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum 配置。与使用<code>/*+ REBALANCE() */</code> hint等效。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 配置Kyuubi Spark SQL Extensions</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开启RepartitionBeforeWrite优化（默认开启）</span></span><br><span class=\"line\"><span class=\"string\">spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置AQE </span></span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.enabled=true</span> </span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.advisoryPartitionSizeInBytes=512m</span> </span><br><span class=\"line\"><span class=\"string\">spark.sql.adaptive.coalescePartitions.minPartitionmum=1</span> </span><br></pre></td></tr></table></figure>\n\n"},{"title":"理解Spark ESS","abbrlink":54682,"date":"2023-02-04T07:55:27.000Z","updated":"2023-02-04T07:55:27.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","top_img":null,"description":null,"keywords":null,"_content":"\n## spark shuffle大概过程\n\n> spark shuffle分为两部分shuffle write和shuffle read。\n>\n> 在map write端，对每个task的数据，不管是按key hash还是在数据结构里先聚合再排序，最终都会将数据写到一个partitionFile里面，在partitionFile里面的数据是partitionId有序的，外加会生成一个索引文件，索引包含每个partition对应偏移量和长度。\n>\n> 而reduce read 端就是从这些partitionFile里面拉取相应partitionId的数据, 然后再进行聚合排序。\n>\n> 现在我们在来看下**external shuffle service（ESS）**，其乍从其名字上看，ESS是spark分布式集群为存储shuffle data而设计的分布式组件。但其实它只是Spark通过Executor获取Shuffle data块的代理。\n>\n> 我们可以理解为ESS负责管理shuffle write端生成的shuffle数据，ESS是和yarn一起使用的， 在yarn集群上的每一个nodemanager上面都运行一个ESS，是一个常驻进程。一个ESS管理每个nodemanager上所有的executor生成的shuffle数据。总而言之，ESS并不是分布式的组件，它的生命周期也不依赖于Executor。\n\n\n\n## 为什么需要ESS ?\n\n> 在Spark中，Executor进程除了运行task，还要负责写shuffle 数据，以及给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。同时，ESS的存在也使得，即使executor挂掉或者回收，都不影响其shuffle数据，因此只有在ESS开启情况下才能开启动态调整executor数目。\n>\n> 因此，spark提供了external shuffle service这个接口，常见的就是spark on yarn中的，YarnShuffleService。这样，在yarn的nodemanager中会常驻一个externalShuffleService服务进程来为所有的executor服务，默认为7337端口。\n>\n> 其实在spark中shuffleClient有两种，一种是blockTransferService，另一种是externalShuffleClient。如果在ESS开启，那么externalShuffleClient用来fetch shuffle数据，而blockTransferService用于获取broadCast等其他BlockManager保存的数据。\n>\n> 如果ESS没有开启，那么spark就只能使用自己的blockTransferService来拉取所有数据，包括shuffle数据以及broadcast数据。\n\n## ESS的架构与优势\n\n> 在启用ESS后，ESS服务会在node节点上创建，并且每次存在时，新创建的Executor都会向其注册。\n>\n> 在注册过程中，使用appId, execId和ExecutorShuffleInfo(localDirs, shuffleManager类型)作为参数，从参数信息可以看出Executor会通知ESS服务它创建在磁盘上文件的存储位置。由于这些信息，ESS服务守护进程能够在检索过程中将shuffle中间的临时文件返回给其他执行程序。\n>\n> ESS服务的存在也会影响文件删除。在正常情况下（没有外部 shuffle 服务），当Executor停止时，它会自动删除生成的文件。但是启用ESS服务后，Executor关闭后文件不会被清理。以下架构图说明了启用外部 shuffle 服务时工作程序节点上发生的情况：\n>\n> ESS服务的一大优势是提高了可靠性。即使其中一个 executor 出现故障，它的 shuffle 文件也不会丢失。另一个优点是可扩展性，因为在 Spark 中运行动态资源分配需要ESS服务，这块我们后续在进行介绍。\n>\n> ![img](https://pic1.zhimg.com/v2-cd760897dadf45fdbdcdd278032d5bbc_r.jpg)\n>\n> 总之使用Spark ESS 为 Spark Shuffle 操作带来了以下好处：\n>\n> 1. 即使 Spark Executor 正在经历 GC 停顿，Spark ESS 也可以为 Shuffle 块提供服务。\n> 2. 即使产生它们的 Spark Executor 挂了，Shuffle 块也能提供服务。\n> 3. 可以释放闲置的 Spark Executor 来节省集群的计算资源。\n\n## Spark 3.2新特性Push-based Shuffle\n\n> Spark 3.2为spark shuffle带来了重大的改变，其中新增了push-based shuffle机制。但其实在push-based shuffle 之前，业界也有人提出了remote shuffle service的实践，不过由于它们是依赖于外部组件实现的所以一直不被社区所接收。\n>\n> 在上一讲我们先来了解push-based shuffle机制的实现原理，这里我们来通过源码分析下其实现的过程。\n>\n> 首先，Push-based shuffle机制是不依赖于外部组件的方案，但使用升级版的ESS进行shuffle data的合并，所以PBS(Push-based shuffle)只支持Yarn方式的实现。\n>\n> 其次，引入PBS新特性的主要原因是为了解决大shuffle的场景存在的问题：\n>\n> - 第一个挑战是可靠性问题。由于计算节点数据量大和 shuffle 工作负载的规模，可能会导致 shuffle fetch 失败，从而导致昂贵的 stage 重试。\n> - 第二个挑战是效率问题。由于 reducer 的 shuffle fetch 请求是随机到达的，因此 shuffle 服务也会随机访问 shuffle 文件中的数据。如果单个 shuffle 块大小较小，则 shuffle 服务产生的小随机读取会严重影响磁盘吞吐量，从而延长 shuffle fetch 等待时间。\n> - 第三个挑战是扩展问题。由于 external shuffle service 是我们基础架构中的共享服务，因此一些对 shuffle services 错误调优的作业也会影响其他作业。当一个作业错误地配置导致产生许多小的 shuffle blocks 将会给 shuffle 服务带来压力时，它不仅会给自身带来性能下降，还会使共享相同 shuffle 服务的所有相邻作业的性能下降。这可能会导致原本正常运行的作业出现不可预测的运行时延迟，尤其是在集群高峰时段。\n>\n> 此外，PBS不仅适用于大shuffle的场景，对于大量小shuffle文件，这种严重影响磁盘IO性能的情况下, 也有很好的性能提升。push-based shuffle并不是来替换sort-based shuffle, 它是通过补充的方式来优化shuffle。\n>\n> Push-based Shuffle主要分为以下：shuffle service 准备、Map端push shuffle数据、shuffle service merge数据、更新MergeStatues和reducer拉取merge shuffle 数据五部分。\n\n \n\n## 多版本ESS和push-based shuffle\n\n- yarn-site.xml\n\n  ```xml\n      <property>\n          <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle,spark_shuffle,spark3_shuffle</value>\n      </property>\n  \n      <property>\n          <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n          <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n      </property>\n      <property>\n          <name>yarn.nodemanager.aux-services.spark3_shuffle.class</name>\n          <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n      </property>\n  \n      <property>\n          <name>yarn.nodemanager.aux-services.spark_shuffle.classpath</name>\n          <value>/opt/spark/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-2-config</value>\n      </property>\n      <property>\n          <name>yarn.nodemanager.aux-services.spark3_shuffle.classpath</name>\n          <value>/opt/spark3/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-3-config</value>\n      </property>\n  \n      <property>\n          <name>spark.shuffle.push.server.mergedShuffleFileManagerImpl</name>\n          <value>org.apache.spark.network.shuffle.RemoteBlockPushResolver</value>\n      </property>\n  ```\n\n  \n\n- spark-defaults.conf\n\n  ```yaml\n  # Spark ESS: with push-based shuffle service\n  # spark.shuffle.useOldFetchProtocol=true\n  spark.shuffle.service.name=spark3_shuffle\n  spark.shuffle.service.port=7773\n  spark.shuffle.push.enabled=true\n  spark.shuffle.push.mergersMinStaticThreshold=5\n  ```\n\n  \n","source":"_posts/bigdata/spark/insight into Spark ESS.md","raw":"---\ntitle: 理解Spark ESS\ntags:\n  - spark\ncategories:\n  - - bigdata\n    - spark\nabbrlink: 54682\ndate: 2023-02-04 15:55:27\nupdated: 2023-02-04 15:55:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## spark shuffle大概过程\n\n> spark shuffle分为两部分shuffle write和shuffle read。\n>\n> 在map write端，对每个task的数据，不管是按key hash还是在数据结构里先聚合再排序，最终都会将数据写到一个partitionFile里面，在partitionFile里面的数据是partitionId有序的，外加会生成一个索引文件，索引包含每个partition对应偏移量和长度。\n>\n> 而reduce read 端就是从这些partitionFile里面拉取相应partitionId的数据, 然后再进行聚合排序。\n>\n> 现在我们在来看下**external shuffle service（ESS）**，其乍从其名字上看，ESS是spark分布式集群为存储shuffle data而设计的分布式组件。但其实它只是Spark通过Executor获取Shuffle data块的代理。\n>\n> 我们可以理解为ESS负责管理shuffle write端生成的shuffle数据，ESS是和yarn一起使用的， 在yarn集群上的每一个nodemanager上面都运行一个ESS，是一个常驻进程。一个ESS管理每个nodemanager上所有的executor生成的shuffle数据。总而言之，ESS并不是分布式的组件，它的生命周期也不依赖于Executor。\n\n\n\n## 为什么需要ESS ?\n\n> 在Spark中，Executor进程除了运行task，还要负责写shuffle 数据，以及给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。同时，ESS的存在也使得，即使executor挂掉或者回收，都不影响其shuffle数据，因此只有在ESS开启情况下才能开启动态调整executor数目。\n>\n> 因此，spark提供了external shuffle service这个接口，常见的就是spark on yarn中的，YarnShuffleService。这样，在yarn的nodemanager中会常驻一个externalShuffleService服务进程来为所有的executor服务，默认为7337端口。\n>\n> 其实在spark中shuffleClient有两种，一种是blockTransferService，另一种是externalShuffleClient。如果在ESS开启，那么externalShuffleClient用来fetch shuffle数据，而blockTransferService用于获取broadCast等其他BlockManager保存的数据。\n>\n> 如果ESS没有开启，那么spark就只能使用自己的blockTransferService来拉取所有数据，包括shuffle数据以及broadcast数据。\n\n## ESS的架构与优势\n\n> 在启用ESS后，ESS服务会在node节点上创建，并且每次存在时，新创建的Executor都会向其注册。\n>\n> 在注册过程中，使用appId, execId和ExecutorShuffleInfo(localDirs, shuffleManager类型)作为参数，从参数信息可以看出Executor会通知ESS服务它创建在磁盘上文件的存储位置。由于这些信息，ESS服务守护进程能够在检索过程中将shuffle中间的临时文件返回给其他执行程序。\n>\n> ESS服务的存在也会影响文件删除。在正常情况下（没有外部 shuffle 服务），当Executor停止时，它会自动删除生成的文件。但是启用ESS服务后，Executor关闭后文件不会被清理。以下架构图说明了启用外部 shuffle 服务时工作程序节点上发生的情况：\n>\n> ESS服务的一大优势是提高了可靠性。即使其中一个 executor 出现故障，它的 shuffle 文件也不会丢失。另一个优点是可扩展性，因为在 Spark 中运行动态资源分配需要ESS服务，这块我们后续在进行介绍。\n>\n> ![img](https://pic1.zhimg.com/v2-cd760897dadf45fdbdcdd278032d5bbc_r.jpg)\n>\n> 总之使用Spark ESS 为 Spark Shuffle 操作带来了以下好处：\n>\n> 1. 即使 Spark Executor 正在经历 GC 停顿，Spark ESS 也可以为 Shuffle 块提供服务。\n> 2. 即使产生它们的 Spark Executor 挂了，Shuffle 块也能提供服务。\n> 3. 可以释放闲置的 Spark Executor 来节省集群的计算资源。\n\n## Spark 3.2新特性Push-based Shuffle\n\n> Spark 3.2为spark shuffle带来了重大的改变，其中新增了push-based shuffle机制。但其实在push-based shuffle 之前，业界也有人提出了remote shuffle service的实践，不过由于它们是依赖于外部组件实现的所以一直不被社区所接收。\n>\n> 在上一讲我们先来了解push-based shuffle机制的实现原理，这里我们来通过源码分析下其实现的过程。\n>\n> 首先，Push-based shuffle机制是不依赖于外部组件的方案，但使用升级版的ESS进行shuffle data的合并，所以PBS(Push-based shuffle)只支持Yarn方式的实现。\n>\n> 其次，引入PBS新特性的主要原因是为了解决大shuffle的场景存在的问题：\n>\n> - 第一个挑战是可靠性问题。由于计算节点数据量大和 shuffle 工作负载的规模，可能会导致 shuffle fetch 失败，从而导致昂贵的 stage 重试。\n> - 第二个挑战是效率问题。由于 reducer 的 shuffle fetch 请求是随机到达的，因此 shuffle 服务也会随机访问 shuffle 文件中的数据。如果单个 shuffle 块大小较小，则 shuffle 服务产生的小随机读取会严重影响磁盘吞吐量，从而延长 shuffle fetch 等待时间。\n> - 第三个挑战是扩展问题。由于 external shuffle service 是我们基础架构中的共享服务，因此一些对 shuffle services 错误调优的作业也会影响其他作业。当一个作业错误地配置导致产生许多小的 shuffle blocks 将会给 shuffle 服务带来压力时，它不仅会给自身带来性能下降，还会使共享相同 shuffle 服务的所有相邻作业的性能下降。这可能会导致原本正常运行的作业出现不可预测的运行时延迟，尤其是在集群高峰时段。\n>\n> 此外，PBS不仅适用于大shuffle的场景，对于大量小shuffle文件，这种严重影响磁盘IO性能的情况下, 也有很好的性能提升。push-based shuffle并不是来替换sort-based shuffle, 它是通过补充的方式来优化shuffle。\n>\n> Push-based Shuffle主要分为以下：shuffle service 准备、Map端push shuffle数据、shuffle service merge数据、更新MergeStatues和reducer拉取merge shuffle 数据五部分。\n\n \n\n## 多版本ESS和push-based shuffle\n\n- yarn-site.xml\n\n  ```xml\n      <property>\n          <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle,spark_shuffle,spark3_shuffle</value>\n      </property>\n  \n      <property>\n          <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n          <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n      </property>\n      <property>\n          <name>yarn.nodemanager.aux-services.spark3_shuffle.class</name>\n          <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n      </property>\n  \n      <property>\n          <name>yarn.nodemanager.aux-services.spark_shuffle.classpath</name>\n          <value>/opt/spark/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-2-config</value>\n      </property>\n      <property>\n          <name>yarn.nodemanager.aux-services.spark3_shuffle.classpath</name>\n          <value>/opt/spark3/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-3-config</value>\n      </property>\n  \n      <property>\n          <name>spark.shuffle.push.server.mergedShuffleFileManagerImpl</name>\n          <value>org.apache.spark.network.shuffle.RemoteBlockPushResolver</value>\n      </property>\n  ```\n\n  \n\n- spark-defaults.conf\n\n  ```yaml\n  # Spark ESS: with push-based shuffle service\n  # spark.shuffle.useOldFetchProtocol=true\n  spark.shuffle.service.name=spark3_shuffle\n  spark.shuffle.service.port=7773\n  spark.shuffle.push.enabled=true\n  spark.shuffle.push.mergersMinStaticThreshold=5\n  ```\n\n  \n","slug":"bigdata/spark/insight into Spark ESS","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt400478j5m8ul20f57","content":"<h2 id=\"spark-shuffle大概过程\"><a href=\"#spark-shuffle大概过程\" class=\"headerlink\" title=\"spark shuffle大概过程\"></a>spark shuffle大概过程</h2><blockquote>\n<p>spark shuffle分为两部分shuffle write和shuffle read。</p>\n<p>在map write端，对每个task的数据，不管是按key hash还是在数据结构里先聚合再排序，最终都会将数据写到一个partitionFile里面，在partitionFile里面的数据是partitionId有序的，外加会生成一个索引文件，索引包含每个partition对应偏移量和长度。</p>\n<p>而reduce read 端就是从这些partitionFile里面拉取相应partitionId的数据, 然后再进行聚合排序。</p>\n<p>现在我们在来看下<strong>external shuffle service（ESS）</strong>，其乍从其名字上看，ESS是spark分布式集群为存储shuffle data而设计的分布式组件。但其实它只是Spark通过Executor获取Shuffle data块的代理。</p>\n<p>我们可以理解为ESS负责管理shuffle write端生成的shuffle数据，ESS是和yarn一起使用的， 在yarn集群上的每一个nodemanager上面都运行一个ESS，是一个常驻进程。一个ESS管理每个nodemanager上所有的executor生成的shuffle数据。总而言之，ESS并不是分布式的组件，它的生命周期也不依赖于Executor。</p>\n</blockquote>\n<h2 id=\"为什么需要ESS\"><a href=\"#为什么需要ESS\" class=\"headerlink\" title=\"为什么需要ESS ?\"></a>为什么需要ESS ?</h2><blockquote>\n<p>在Spark中，Executor进程除了运行task，还要负责写shuffle 数据，以及给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。同时，ESS的存在也使得，即使executor挂掉或者回收，都不影响其shuffle数据，因此只有在ESS开启情况下才能开启动态调整executor数目。</p>\n<p>因此，spark提供了external shuffle service这个接口，常见的就是spark on yarn中的，YarnShuffleService。这样，在yarn的nodemanager中会常驻一个externalShuffleService服务进程来为所有的executor服务，默认为7337端口。</p>\n<p>其实在spark中shuffleClient有两种，一种是blockTransferService，另一种是externalShuffleClient。如果在ESS开启，那么externalShuffleClient用来fetch shuffle数据，而blockTransferService用于获取broadCast等其他BlockManager保存的数据。</p>\n<p>如果ESS没有开启，那么spark就只能使用自己的blockTransferService来拉取所有数据，包括shuffle数据以及broadcast数据。</p>\n</blockquote>\n<h2 id=\"ESS的架构与优势\"><a href=\"#ESS的架构与优势\" class=\"headerlink\" title=\"ESS的架构与优势\"></a>ESS的架构与优势</h2><blockquote>\n<p>在启用ESS后，ESS服务会在node节点上创建，并且每次存在时，新创建的Executor都会向其注册。</p>\n<p>在注册过程中，使用appId, execId和ExecutorShuffleInfo(localDirs, shuffleManager类型)作为参数，从参数信息可以看出Executor会通知ESS服务它创建在磁盘上文件的存储位置。由于这些信息，ESS服务守护进程能够在检索过程中将shuffle中间的临时文件返回给其他执行程序。</p>\n<p>ESS服务的存在也会影响文件删除。在正常情况下（没有外部 shuffle 服务），当Executor停止时，它会自动删除生成的文件。但是启用ESS服务后，Executor关闭后文件不会被清理。以下架构图说明了启用外部 shuffle 服务时工作程序节点上发生的情况：</p>\n<p>ESS服务的一大优势是提高了可靠性。即使其中一个 executor 出现故障，它的 shuffle 文件也不会丢失。另一个优点是可扩展性，因为在 Spark 中运行动态资源分配需要ESS服务，这块我们后续在进行介绍。</p>\n<p><img src=\"https://pic1.zhimg.com/v2-cd760897dadf45fdbdcdd278032d5bbc_r.jpg\" alt=\"img\"></p>\n<p>总之使用Spark ESS 为 Spark Shuffle 操作带来了以下好处：</p>\n<ol>\n<li>即使 Spark Executor 正在经历 GC 停顿，Spark ESS 也可以为 Shuffle 块提供服务。</li>\n<li>即使产生它们的 Spark Executor 挂了，Shuffle 块也能提供服务。</li>\n<li>可以释放闲置的 Spark Executor 来节省集群的计算资源。</li>\n</ol>\n</blockquote>\n<h2 id=\"Spark-3-2新特性Push-based-Shuffle\"><a href=\"#Spark-3-2新特性Push-based-Shuffle\" class=\"headerlink\" title=\"Spark 3.2新特性Push-based Shuffle\"></a>Spark 3.2新特性Push-based Shuffle</h2><blockquote>\n<p>Spark 3.2为spark shuffle带来了重大的改变，其中新增了push-based shuffle机制。但其实在push-based shuffle 之前，业界也有人提出了remote shuffle service的实践，不过由于它们是依赖于外部组件实现的所以一直不被社区所接收。</p>\n<p>在上一讲我们先来了解push-based shuffle机制的实现原理，这里我们来通过源码分析下其实现的过程。</p>\n<p>首先，Push-based shuffle机制是不依赖于外部组件的方案，但使用升级版的ESS进行shuffle data的合并，所以PBS(Push-based shuffle)只支持Yarn方式的实现。</p>\n<p>其次，引入PBS新特性的主要原因是为了解决大shuffle的场景存在的问题：</p>\n<ul>\n<li>第一个挑战是可靠性问题。由于计算节点数据量大和 shuffle 工作负载的规模，可能会导致 shuffle fetch 失败，从而导致昂贵的 stage 重试。</li>\n<li>第二个挑战是效率问题。由于 reducer 的 shuffle fetch 请求是随机到达的，因此 shuffle 服务也会随机访问 shuffle 文件中的数据。如果单个 shuffle 块大小较小，则 shuffle 服务产生的小随机读取会严重影响磁盘吞吐量，从而延长 shuffle fetch 等待时间。</li>\n<li>第三个挑战是扩展问题。由于 external shuffle service 是我们基础架构中的共享服务，因此一些对 shuffle services 错误调优的作业也会影响其他作业。当一个作业错误地配置导致产生许多小的 shuffle blocks 将会给 shuffle 服务带来压力时，它不仅会给自身带来性能下降，还会使共享相同 shuffle 服务的所有相邻作业的性能下降。这可能会导致原本正常运行的作业出现不可预测的运行时延迟，尤其是在集群高峰时段。</li>\n</ul>\n<p>此外，PBS不仅适用于大shuffle的场景，对于大量小shuffle文件，这种严重影响磁盘IO性能的情况下, 也有很好的性能提升。push-based shuffle并不是来替换sort-based shuffle, 它是通过补充的方式来优化shuffle。</p>\n<p>Push-based Shuffle主要分为以下：shuffle service 准备、Map端push shuffle数据、shuffle service merge数据、更新MergeStatues和reducer拉取merge shuffle 数据五部分。</p>\n</blockquote>\n<h2 id=\"多版本ESS和push-based-shuffle\"><a href=\"#多版本ESS和push-based-shuffle\" class=\"headerlink\" title=\"多版本ESS和push-based shuffle\"></a>多版本ESS和push-based shuffle</h2><ul>\n<li><p>yarn-site.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle,spark3_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark3_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/opt/spark/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-2-config<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark3_shuffle.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/opt/spark3/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-3-config<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>spark.shuffle.push.server.mergedShuffleFileManagerImpl<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.shuffle.RemoteBlockPushResolver<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>spark-defaults.conf</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Spark ESS: with push-based shuffle service</span></span><br><span class=\"line\"><span class=\"comment\"># spark.shuffle.useOldFetchProtocol=true</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.name=spark3_shuffle</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.port=7773</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.push.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.push.mergersMinStaticThreshold=5</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"spark-shuffle大概过程\"><a href=\"#spark-shuffle大概过程\" class=\"headerlink\" title=\"spark shuffle大概过程\"></a>spark shuffle大概过程</h2><blockquote>\n<p>spark shuffle分为两部分shuffle write和shuffle read。</p>\n<p>在map write端，对每个task的数据，不管是按key hash还是在数据结构里先聚合再排序，最终都会将数据写到一个partitionFile里面，在partitionFile里面的数据是partitionId有序的，外加会生成一个索引文件，索引包含每个partition对应偏移量和长度。</p>\n<p>而reduce read 端就是从这些partitionFile里面拉取相应partitionId的数据, 然后再进行聚合排序。</p>\n<p>现在我们在来看下<strong>external shuffle service（ESS）</strong>，其乍从其名字上看，ESS是spark分布式集群为存储shuffle data而设计的分布式组件。但其实它只是Spark通过Executor获取Shuffle data块的代理。</p>\n<p>我们可以理解为ESS负责管理shuffle write端生成的shuffle数据，ESS是和yarn一起使用的， 在yarn集群上的每一个nodemanager上面都运行一个ESS，是一个常驻进程。一个ESS管理每个nodemanager上所有的executor生成的shuffle数据。总而言之，ESS并不是分布式的组件，它的生命周期也不依赖于Executor。</p>\n</blockquote>\n<h2 id=\"为什么需要ESS\"><a href=\"#为什么需要ESS\" class=\"headerlink\" title=\"为什么需要ESS ?\"></a>为什么需要ESS ?</h2><blockquote>\n<p>在Spark中，Executor进程除了运行task，还要负责写shuffle 数据，以及给其他Executor提供shuffle数据。当Executor进程任务过重，导致GC而不能为其他Executor提供shuffle数据时，会影响任务运行。同时，ESS的存在也使得，即使executor挂掉或者回收，都不影响其shuffle数据，因此只有在ESS开启情况下才能开启动态调整executor数目。</p>\n<p>因此，spark提供了external shuffle service这个接口，常见的就是spark on yarn中的，YarnShuffleService。这样，在yarn的nodemanager中会常驻一个externalShuffleService服务进程来为所有的executor服务，默认为7337端口。</p>\n<p>其实在spark中shuffleClient有两种，一种是blockTransferService，另一种是externalShuffleClient。如果在ESS开启，那么externalShuffleClient用来fetch shuffle数据，而blockTransferService用于获取broadCast等其他BlockManager保存的数据。</p>\n<p>如果ESS没有开启，那么spark就只能使用自己的blockTransferService来拉取所有数据，包括shuffle数据以及broadcast数据。</p>\n</blockquote>\n<h2 id=\"ESS的架构与优势\"><a href=\"#ESS的架构与优势\" class=\"headerlink\" title=\"ESS的架构与优势\"></a>ESS的架构与优势</h2><blockquote>\n<p>在启用ESS后，ESS服务会在node节点上创建，并且每次存在时，新创建的Executor都会向其注册。</p>\n<p>在注册过程中，使用appId, execId和ExecutorShuffleInfo(localDirs, shuffleManager类型)作为参数，从参数信息可以看出Executor会通知ESS服务它创建在磁盘上文件的存储位置。由于这些信息，ESS服务守护进程能够在检索过程中将shuffle中间的临时文件返回给其他执行程序。</p>\n<p>ESS服务的存在也会影响文件删除。在正常情况下（没有外部 shuffle 服务），当Executor停止时，它会自动删除生成的文件。但是启用ESS服务后，Executor关闭后文件不会被清理。以下架构图说明了启用外部 shuffle 服务时工作程序节点上发生的情况：</p>\n<p>ESS服务的一大优势是提高了可靠性。即使其中一个 executor 出现故障，它的 shuffle 文件也不会丢失。另一个优点是可扩展性，因为在 Spark 中运行动态资源分配需要ESS服务，这块我们后续在进行介绍。</p>\n<p><img src=\"https://pic1.zhimg.com/v2-cd760897dadf45fdbdcdd278032d5bbc_r.jpg\" alt=\"img\"></p>\n<p>总之使用Spark ESS 为 Spark Shuffle 操作带来了以下好处：</p>\n<ol>\n<li>即使 Spark Executor 正在经历 GC 停顿，Spark ESS 也可以为 Shuffle 块提供服务。</li>\n<li>即使产生它们的 Spark Executor 挂了，Shuffle 块也能提供服务。</li>\n<li>可以释放闲置的 Spark Executor 来节省集群的计算资源。</li>\n</ol>\n</blockquote>\n<h2 id=\"Spark-3-2新特性Push-based-Shuffle\"><a href=\"#Spark-3-2新特性Push-based-Shuffle\" class=\"headerlink\" title=\"Spark 3.2新特性Push-based Shuffle\"></a>Spark 3.2新特性Push-based Shuffle</h2><blockquote>\n<p>Spark 3.2为spark shuffle带来了重大的改变，其中新增了push-based shuffle机制。但其实在push-based shuffle 之前，业界也有人提出了remote shuffle service的实践，不过由于它们是依赖于外部组件实现的所以一直不被社区所接收。</p>\n<p>在上一讲我们先来了解push-based shuffle机制的实现原理，这里我们来通过源码分析下其实现的过程。</p>\n<p>首先，Push-based shuffle机制是不依赖于外部组件的方案，但使用升级版的ESS进行shuffle data的合并，所以PBS(Push-based shuffle)只支持Yarn方式的实现。</p>\n<p>其次，引入PBS新特性的主要原因是为了解决大shuffle的场景存在的问题：</p>\n<ul>\n<li>第一个挑战是可靠性问题。由于计算节点数据量大和 shuffle 工作负载的规模，可能会导致 shuffle fetch 失败，从而导致昂贵的 stage 重试。</li>\n<li>第二个挑战是效率问题。由于 reducer 的 shuffle fetch 请求是随机到达的，因此 shuffle 服务也会随机访问 shuffle 文件中的数据。如果单个 shuffle 块大小较小，则 shuffle 服务产生的小随机读取会严重影响磁盘吞吐量，从而延长 shuffle fetch 等待时间。</li>\n<li>第三个挑战是扩展问题。由于 external shuffle service 是我们基础架构中的共享服务，因此一些对 shuffle services 错误调优的作业也会影响其他作业。当一个作业错误地配置导致产生许多小的 shuffle blocks 将会给 shuffle 服务带来压力时，它不仅会给自身带来性能下降，还会使共享相同 shuffle 服务的所有相邻作业的性能下降。这可能会导致原本正常运行的作业出现不可预测的运行时延迟，尤其是在集群高峰时段。</li>\n</ul>\n<p>此外，PBS不仅适用于大shuffle的场景，对于大量小shuffle文件，这种严重影响磁盘IO性能的情况下, 也有很好的性能提升。push-based shuffle并不是来替换sort-based shuffle, 它是通过补充的方式来优化shuffle。</p>\n<p>Push-based Shuffle主要分为以下：shuffle service 准备、Map端push shuffle数据、shuffle service merge数据、更新MergeStatues和reducer拉取merge shuffle 数据五部分。</p>\n</blockquote>\n<h2 id=\"多版本ESS和push-based-shuffle\"><a href=\"#多版本ESS和push-based-shuffle\" class=\"headerlink\" title=\"多版本ESS和push-based shuffle\"></a>多版本ESS和push-based shuffle</h2><ul>\n<li><p>yarn-site.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle,spark3_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark3_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/opt/spark/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-2-config<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark3_shuffle.classpath<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/opt/spark3/yarn/*:/opt/hadoop/etc/hadoop/ess/spark-3-config<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>spark.shuffle.push.server.mergedShuffleFileManagerImpl<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.shuffle.RemoteBlockPushResolver<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>spark-defaults.conf</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Spark ESS: with push-based shuffle service</span></span><br><span class=\"line\"><span class=\"comment\"># spark.shuffle.useOldFetchProtocol=true</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.name=spark3_shuffle</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.service.port=7773</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.push.enabled=true</span></span><br><span class=\"line\"><span class=\"string\">spark.shuffle.push.mergersMinStaticThreshold=5</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"Spark事件总线源码分析","abbrlink":32133,"date":"2023-03-08T11:54:07.000Z","updated":"2022-03-08T11:54:07.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## 前言\n\nSpark中很多组件之间是靠事件消息实现通信的，之前分析了一下Spark中RPC机制，RPC和事件消息机制目的都是实现组件之间的通信，前者解决远程通信问题，而后者则是在本地较为高效的方式。Spark中大量采用事件监听这种方式，实现driver端的组件之间的通信。\n\n![img](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/spark-listenbus.png)\n\n## ListenerBus\n\n```scala\n/**\n * An event bus which posts events to its listeners.\n */\nprivate[spark] trait ListenerBus[L <: AnyRef, E] extends Logging {\n\n  private[this] val listenersPlusTimers = new CopyOnWriteArrayList[(L, Option[Timer])]\n\n  // Marked `private[spark]` for access in tests.\n  private[spark] def listeners = listenersPlusTimers.asScala.map(_._1).asJava\n  ...\n}\n```\n\nListenerBus trait是Spark内所有事件总线实现的基类，有两个泛型参数L和E。L代表监听器的类型，并且它可以是任意类型的。E则代表事件的类型。**接受事件并且将事件提交到对应事件的监听器**。\n\n主要属性如下\n\n- `listeners`, `listenersPlusTimers`：**维护了所有的监听器和对应的定时器**，数据结构为线程安全的`CopyOnWriteArrayList`适用于读多写少的业务场景，满足数据的最终一致性\n\n主要方法如下\n\n- `addListener()`, `removeListener()`：**从`listenersPlusTimers`中增加或者删除监听器和计时器**\n- `postToAll()`：**遍历`listenersPlusTimers`并调用未实现的`doPostEvent()`方法发送事件**\n\n每个实现类实现了`doPostEvent`方法，利用模式匹配将特定的事件投递到对应的监视器类型。\n\n\n\n### SparkListenerBus\n\nSparkListenerBus特征是Spark Core内部事件总线的基类，其代码如下。\n\n```scala\n// 监听器\nprivate[spark] trait SparkListenerInterface {\n\n  /**\n   * Called when a stage completes successfully or fails, with information on the completed stage.\n   */\n  def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit\n  \n  ...\n}\n\n// 事件\n@DeveloperApi\n@JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, include = JsonTypeInfo.As.PROPERTY, property = \"Event\")\ntrait SparkListenerEvent {\n  /* Whether output this event to the event log */\n  protected[spark] def logEvent: Boolean = true\n}\n...\n@DeveloperApi\ncase class SparkListenerStageCompleted(stageInfo: StageInfo) extends SparkListenerEvent\n...\n\n// 事件总线\nprivate[spark] trait SparkListenerBus\nextends ListenerBus[SparkListenerInterface, SparkListenerEvent] {\n\n  protected override def doPostEvent(\n    listener: SparkListenerInterface,\n    event: SparkListenerEvent): Unit = {\n    event match {\n      case stageSubmitted: SparkListenerStageSubmitted =>\n      listener.onStageSubmitted(stageSubmitted)\n      ...\n    }\n  }\n}\n```\n\nSparkListenerBus继承了ListenerBus，实现了doPostEvent()方法，对事件进行匹配，并调用监听器的处理方法。如果无法匹配到事件，则调用onOtherEvent()方法。\n\nSparkListenerBus支持的监听器都是SparkListenerInterface的子类，事件则是SparkListenerEvent的子类。下面来了解一下。\n\n### SparkListenerInterface与SparkListenerEvent特征\n\n在SparkListenerInterface特征中，分别定义了处理每一个事件的处理方法，统一命名为“on+事件名称”，代码很简单，就不再贴出来了。\n\nSparkListenerEvent是一个没有抽象方法的特征，类似于Java中的标记接口（marker interface），它唯一的用途就是标记具体的事件类。事件类统一命名为“SparkListener+事件名称”，并且都是Scala样例类。\n\n\n\n## AsyncEventQueue\n\n在SparkListenerBus的实现类AsyncEventQueue中，提供了异步事件队列机制，它也是SparkContext中的事件总线LiveListenerBus的基础。\n\n实现原理是基于消息队列的异步通信，因此有以下优点：1、将Event发送者和Event listerner解耦。2、异步：Event发送者发送Event给消息队列后直接返回，无需等待listener处理后才返回，减少了Event发送者的阻塞，提高了性能。\n\n```scala\n/**\n * An asynchronous queue for events. All events posted to this queue will be delivered to the child\n * listeners in a separate thread.\n *\n * Delivery will only begin when the `start()` method is called. The `stop()` method should be\n * called when no more events need to be delivered.\n */\nprivate class AsyncEventQueue(\n    val name: String,\n    conf: SparkConf,\n    metrics: LiveListenerBusMetrics,\n    bus: LiveListenerBus)\n  extends SparkListenerBus\n  with Logging {\n  import AsyncEventQueue._\n\n  private val eventQueue = new LinkedBlockingQueue[SparkListenerEvent](\n    conf.get(LISTENER_BUS_EVENT_QUEUE_CAPACITY))\n\n  private val eventCount = new AtomicLong()\n\n  private val droppedEventsCounter = new AtomicLong(0L)\n\n  @volatile private var lastReportTimestamp = 0L\n\n  private val logDroppedEvent = new AtomicBoolean(false)\n\n  private var sc: SparkContext = null\n\n  private val started = new AtomicBoolean(false)\n  private val stopped = new AtomicBoolean(false)\n\n  private val droppedEvents = metrics.metricRegistry.counter(s\"queue.$name.numDroppedEvents\")\n  private val processingTime = metrics.metricRegistry.timer(s\"queue.$name.listenerProcessingTime\")\n\n  private val dispatchThread = new Thread(s\"spark-listener-group-$name\") {\n    setDaemon(true)\n    override def run(): Unit = Utils.tryOrStopSparkContext(sc) {\n      dispatch()\n    }\n  }\n\n  // ...\n}\n```\n\n该类的构造参数有四个，分别是队列名、Spark配置项、LiveListenerBus的监控度量，以及LiveListenerBus本身。下面来看一下它的主要属性。\n\n#### **eventQueue、eventCount属性**\n\neventQueue是一个存储SparkListenerEvent事件的阻塞队列LinkedBlockingQueue。它的大小是通过配置参数spark.scheduler.listenerbus.eventqueue.capacity来设置的，默认值10000。如果不设置阻塞队列的大小，那么默认值会是Integer.MAX_VALUE，有OOM的风险。\n\neventCount则是当前待处理事件的计数。因为事件从队列中弹出不代表已经处理完成，所以不能直接用队列的实际大小来表示。它是AtomicLong类型的，以保证修改的原子性。\n\n#### **droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性**\n\ndroppedEventsCounter是被丢弃事件的计数。当阻塞队列已满后，新产生的事件无法入队，就会被丢弃。日志中定期输出该计数器的值，用lastReportTimestamp记录下每次输出的时间戳，并且输出后都会将计数器重新置为0。\n\nlogDroppedEvent用于指示是否发生过了事件丢弃的情况。它与droppedEventsCounter一样也都是原子类型的。\n\n#### **started、stopped属性**\n\n这两个属性分别用来标记队列的启动与停止状态。\n\n#### **dispatchThread属性**\n\ndispatchThread是将队列中的事件分发到各监听器的守护线程，实际上调用了dispatch()方法。而Utils.tryOrStopSparkContext()方法的作用在于执行代码块时如果抛出异常，就另外起一个线程关闭SparkContext。\n\n下面就来看看dispatch()方法的源码。\n\n```scala\n  private def dispatch(): Unit = LiveListenerBus.withinListenerThread.withValue(true) {\n    var next: SparkListenerEvent = eventQueue.take()\n    while (next != POISON_PILL) {\n      val ctx = processingTime.time()\n      try {\n        super.postToAll(next)\n      } finally {\n        ctx.stop()\n      }\n      eventCount.decrementAndGet()\n      next = eventQueue.take()\n    }\n    eventCount.decrementAndGet()\n  }\n```\n\n可见，该方法循环地从事件队列中取出事件，并调用父类ListenerBus特征的postToAll()方法（文章#5已经讲过）将其投递给所有已注册的监听器，并减少计数器的值。“毒药丸”POISON_PILL是伴生对象中定义的一个特殊的空事件，在队列停止（即调用stop()方法）时会被放入，dispatcherThread取得它之后就会“中毒”退出循环。\n\n有了处理事件的方法，还得有将事件放入队列的方法才完整。下面是入队的方法post()。\n\n```scala\n  def post(event: SparkListenerEvent): Unit = {\n    if (stopped.get()) {\n      return\n    }\n\n    eventCount.incrementAndGet()\n    if (eventQueue.offer(event)) {\n      return\n    }\n\n    eventCount.decrementAndGet()\n    droppedEvents.inc()\n    droppedEventsCounter.incrementAndGet()\n    if (logDroppedEvent.compareAndSet(false, true)) {\n      // Only log the following message once to avoid duplicated annoying logs.\n      logError(s\"Dropping event from queue $name. \" +\n        \"This likely means one of the listeners is too slow and cannot keep up with \" +\n        \"the rate at which tasks are being started by the scheduler.\")\n    }\n    logTrace(s\"Dropping event $event\")\n\n    val droppedEventsCount = droppedEventsCounter.get\n    val droppedCountIncreased = droppedEventsCount - lastDroppedEventsCounter\n    val lastReportTime = lastReportTimestamp.get\n    val curTime = System.currentTimeMillis()\n    // Don't log too frequently\n    if (droppedCountIncreased > 0 && curTime - lastReportTime >= LOGGING_INTERVAL) {\n      // There may be multiple threads trying to logging dropped events,\n      // Use 'compareAndSet' to make sure only one thread can win.\n      if (lastReportTimestamp.compareAndSet(lastReportTime, curTime)) {\n        val previous = new java.util.Date(lastReportTime)\n        lastDroppedEventsCounter = droppedEventsCount\n        logWarning(s\"Dropped $droppedCountIncreased events from $name since \" +\n          s\"${if (lastReportTime == 0) \"the application started\" else s\"$previous\"}.\")\n      }\n    }\n  }\n```\n\n该方法首先检查队列是否已经停止。如果是运行状态，就试图将事件event入队。若offer()方法返回false，表示队列已满，将丢弃事件的计数器自增，并标记有事件被丢弃。最后，若当前的时间戳与上一次输出droppedEventsCounter值的间隔大于1分钟，就在日志里输出它的值。\n\n理解了AsyncEventQueue的细节之后，我们就可以进一步来看LiveListenerBus的实现了。\n\n## 异步事件总线LiveListenerBus\n\nAsyncEventQueue已经继承了SparkListenerBus特征，LiveListenerBus内部用到了AsyncEventQueue作为核心。来看它的声明以及属性的定义。\n\n```scala\nprivate[spark] class LiveListenerBus(conf: SparkConf) {\n  import LiveListenerBus._\n\n  private var sparkContext: SparkContext = _\n\n  private[spark] val metrics = new LiveListenerBusMetrics(conf)\n\n  private val started = new AtomicBoolean(false)\n  private val stopped = new AtomicBoolean(false)\n\n  private val droppedEventsCounter = new AtomicLong(0L)\n\n  @volatile private var lastReportTimestamp = 0L\n\n  private val queues = new CopyOnWriteArrayList[AsyncEventQueue]()\n\n  @volatile private[scheduler] var queuedEvents = new mutable.ListBuffer[SparkListenerEvent]()\n\n  // ...\n}\n```\n\n这里的属性与AsyncEventQueue大同小异，多出来的主要是queues与queuedEvents两个。\n\n#### **queues属性**\n\nqueues维护一个AsyncEventQueue的列表，也就是说LiveListenerBus中会有多个事件队列。它采用CopyOnWriteArrayList来保证线程安全性。\n\n#### **queuedEvents属性**\n\nqueuedEvents维护一个SparkListenerEvent的列表，它的用途是在LiveListenerBus启动成功之前，缓存可能已经收到的事件。在启动之后，这些缓存的事件会首先投递出去。\n\n**LiveListenerBus作为一个事件总线，也必须提供监听器注册、事件投递等功能，这些都是在AsyncEventQueue基础之上实现的，下面来看一看。**\n\n#### **addToQueue()方法**\n\n```scala\n  private[spark] def addToQueue(\n      listener: SparkListenerInterface,\n      queue: String): Unit = synchronized {\n    if (stopped.get()) {\n      throw new IllegalStateException(\"LiveListenerBus is stopped.\")\n    }\n\n    queues.asScala.find(_.name == queue) match {\n      case Some(queue) =>\n        queue.addListener(listener)\n\n      case None =>\n        val newQueue = new AsyncEventQueue(queue, conf, metrics, this)\n        newQueue.addListener(listener)\n        if (started.get()) {\n          newQueue.start(sparkContext)\n        }\n        queues.add(newQueue)\n    }\n  }\n```\n\n该方法将监听器listener注册到名为queue的队列中。它会在queues列表中寻找符合条件的队列，如果该队列已经存在，就调用父类ListenerBus的addListener()方法直接注册监听器。反之，就先创建一个AsyncEventQueue，注册监听器到新的队列中。\n\n#### post()、postToQueues()方法\n\n```scala\n  def post(event: SparkListenerEvent): Unit = {\n    if (stopped.get()) {\n      return\n    }\n    metrics.numEventsPosted.inc()\n\n    if (queuedEvents == null) {\n      postToQueues(event)\n      return\n    }\n\n    synchronized {\n      if (!started.get()) {\n        queuedEvents += event\n        return\n      }\n    }\n\n    postToQueues(event)\n  }\n\n  private def postToQueues(event: SparkListenerEvent): Unit = {\n    val it = queues.iterator()\n    while (it.hasNext()) {\n      it.next().post(event)\n    }\n  }\n```\n\npost()方法会检查queuedEvents中有无缓存的事件，以及事件总线是否还没有启动。投递时会调用postToQueues()方法，将事件发送给所有队列，由AsyncEventQueue来完成投递到监听器的工作。\n\n![img](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/gfvstvfzym.jpeg)\n\n","source":"_posts/bigdata/spark/spark事件总线.md","raw":"---\ntitle: Spark事件总线源码分析\ntags:\n  - spark\ncategories:\n  - - bigdata\n    - spark\nabbrlink: 32133\ndate: 2023-03-08 19:54:07\nupdated: 2022-03-08 19:54:07\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\nSpark中很多组件之间是靠事件消息实现通信的，之前分析了一下Spark中RPC机制，RPC和事件消息机制目的都是实现组件之间的通信，前者解决远程通信问题，而后者则是在本地较为高效的方式。Spark中大量采用事件监听这种方式，实现driver端的组件之间的通信。\n\n![img](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/spark-listenbus.png)\n\n## ListenerBus\n\n```scala\n/**\n * An event bus which posts events to its listeners.\n */\nprivate[spark] trait ListenerBus[L <: AnyRef, E] extends Logging {\n\n  private[this] val listenersPlusTimers = new CopyOnWriteArrayList[(L, Option[Timer])]\n\n  // Marked `private[spark]` for access in tests.\n  private[spark] def listeners = listenersPlusTimers.asScala.map(_._1).asJava\n  ...\n}\n```\n\nListenerBus trait是Spark内所有事件总线实现的基类，有两个泛型参数L和E。L代表监听器的类型，并且它可以是任意类型的。E则代表事件的类型。**接受事件并且将事件提交到对应事件的监听器**。\n\n主要属性如下\n\n- `listeners`, `listenersPlusTimers`：**维护了所有的监听器和对应的定时器**，数据结构为线程安全的`CopyOnWriteArrayList`适用于读多写少的业务场景，满足数据的最终一致性\n\n主要方法如下\n\n- `addListener()`, `removeListener()`：**从`listenersPlusTimers`中增加或者删除监听器和计时器**\n- `postToAll()`：**遍历`listenersPlusTimers`并调用未实现的`doPostEvent()`方法发送事件**\n\n每个实现类实现了`doPostEvent`方法，利用模式匹配将特定的事件投递到对应的监视器类型。\n\n\n\n### SparkListenerBus\n\nSparkListenerBus特征是Spark Core内部事件总线的基类，其代码如下。\n\n```scala\n// 监听器\nprivate[spark] trait SparkListenerInterface {\n\n  /**\n   * Called when a stage completes successfully or fails, with information on the completed stage.\n   */\n  def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit\n  \n  ...\n}\n\n// 事件\n@DeveloperApi\n@JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, include = JsonTypeInfo.As.PROPERTY, property = \"Event\")\ntrait SparkListenerEvent {\n  /* Whether output this event to the event log */\n  protected[spark] def logEvent: Boolean = true\n}\n...\n@DeveloperApi\ncase class SparkListenerStageCompleted(stageInfo: StageInfo) extends SparkListenerEvent\n...\n\n// 事件总线\nprivate[spark] trait SparkListenerBus\nextends ListenerBus[SparkListenerInterface, SparkListenerEvent] {\n\n  protected override def doPostEvent(\n    listener: SparkListenerInterface,\n    event: SparkListenerEvent): Unit = {\n    event match {\n      case stageSubmitted: SparkListenerStageSubmitted =>\n      listener.onStageSubmitted(stageSubmitted)\n      ...\n    }\n  }\n}\n```\n\nSparkListenerBus继承了ListenerBus，实现了doPostEvent()方法，对事件进行匹配，并调用监听器的处理方法。如果无法匹配到事件，则调用onOtherEvent()方法。\n\nSparkListenerBus支持的监听器都是SparkListenerInterface的子类，事件则是SparkListenerEvent的子类。下面来了解一下。\n\n### SparkListenerInterface与SparkListenerEvent特征\n\n在SparkListenerInterface特征中，分别定义了处理每一个事件的处理方法，统一命名为“on+事件名称”，代码很简单，就不再贴出来了。\n\nSparkListenerEvent是一个没有抽象方法的特征，类似于Java中的标记接口（marker interface），它唯一的用途就是标记具体的事件类。事件类统一命名为“SparkListener+事件名称”，并且都是Scala样例类。\n\n\n\n## AsyncEventQueue\n\n在SparkListenerBus的实现类AsyncEventQueue中，提供了异步事件队列机制，它也是SparkContext中的事件总线LiveListenerBus的基础。\n\n实现原理是基于消息队列的异步通信，因此有以下优点：1、将Event发送者和Event listerner解耦。2、异步：Event发送者发送Event给消息队列后直接返回，无需等待listener处理后才返回，减少了Event发送者的阻塞，提高了性能。\n\n```scala\n/**\n * An asynchronous queue for events. All events posted to this queue will be delivered to the child\n * listeners in a separate thread.\n *\n * Delivery will only begin when the `start()` method is called. The `stop()` method should be\n * called when no more events need to be delivered.\n */\nprivate class AsyncEventQueue(\n    val name: String,\n    conf: SparkConf,\n    metrics: LiveListenerBusMetrics,\n    bus: LiveListenerBus)\n  extends SparkListenerBus\n  with Logging {\n  import AsyncEventQueue._\n\n  private val eventQueue = new LinkedBlockingQueue[SparkListenerEvent](\n    conf.get(LISTENER_BUS_EVENT_QUEUE_CAPACITY))\n\n  private val eventCount = new AtomicLong()\n\n  private val droppedEventsCounter = new AtomicLong(0L)\n\n  @volatile private var lastReportTimestamp = 0L\n\n  private val logDroppedEvent = new AtomicBoolean(false)\n\n  private var sc: SparkContext = null\n\n  private val started = new AtomicBoolean(false)\n  private val stopped = new AtomicBoolean(false)\n\n  private val droppedEvents = metrics.metricRegistry.counter(s\"queue.$name.numDroppedEvents\")\n  private val processingTime = metrics.metricRegistry.timer(s\"queue.$name.listenerProcessingTime\")\n\n  private val dispatchThread = new Thread(s\"spark-listener-group-$name\") {\n    setDaemon(true)\n    override def run(): Unit = Utils.tryOrStopSparkContext(sc) {\n      dispatch()\n    }\n  }\n\n  // ...\n}\n```\n\n该类的构造参数有四个，分别是队列名、Spark配置项、LiveListenerBus的监控度量，以及LiveListenerBus本身。下面来看一下它的主要属性。\n\n#### **eventQueue、eventCount属性**\n\neventQueue是一个存储SparkListenerEvent事件的阻塞队列LinkedBlockingQueue。它的大小是通过配置参数spark.scheduler.listenerbus.eventqueue.capacity来设置的，默认值10000。如果不设置阻塞队列的大小，那么默认值会是Integer.MAX_VALUE，有OOM的风险。\n\neventCount则是当前待处理事件的计数。因为事件从队列中弹出不代表已经处理完成，所以不能直接用队列的实际大小来表示。它是AtomicLong类型的，以保证修改的原子性。\n\n#### **droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性**\n\ndroppedEventsCounter是被丢弃事件的计数。当阻塞队列已满后，新产生的事件无法入队，就会被丢弃。日志中定期输出该计数器的值，用lastReportTimestamp记录下每次输出的时间戳，并且输出后都会将计数器重新置为0。\n\nlogDroppedEvent用于指示是否发生过了事件丢弃的情况。它与droppedEventsCounter一样也都是原子类型的。\n\n#### **started、stopped属性**\n\n这两个属性分别用来标记队列的启动与停止状态。\n\n#### **dispatchThread属性**\n\ndispatchThread是将队列中的事件分发到各监听器的守护线程，实际上调用了dispatch()方法。而Utils.tryOrStopSparkContext()方法的作用在于执行代码块时如果抛出异常，就另外起一个线程关闭SparkContext。\n\n下面就来看看dispatch()方法的源码。\n\n```scala\n  private def dispatch(): Unit = LiveListenerBus.withinListenerThread.withValue(true) {\n    var next: SparkListenerEvent = eventQueue.take()\n    while (next != POISON_PILL) {\n      val ctx = processingTime.time()\n      try {\n        super.postToAll(next)\n      } finally {\n        ctx.stop()\n      }\n      eventCount.decrementAndGet()\n      next = eventQueue.take()\n    }\n    eventCount.decrementAndGet()\n  }\n```\n\n可见，该方法循环地从事件队列中取出事件，并调用父类ListenerBus特征的postToAll()方法（文章#5已经讲过）将其投递给所有已注册的监听器，并减少计数器的值。“毒药丸”POISON_PILL是伴生对象中定义的一个特殊的空事件，在队列停止（即调用stop()方法）时会被放入，dispatcherThread取得它之后就会“中毒”退出循环。\n\n有了处理事件的方法，还得有将事件放入队列的方法才完整。下面是入队的方法post()。\n\n```scala\n  def post(event: SparkListenerEvent): Unit = {\n    if (stopped.get()) {\n      return\n    }\n\n    eventCount.incrementAndGet()\n    if (eventQueue.offer(event)) {\n      return\n    }\n\n    eventCount.decrementAndGet()\n    droppedEvents.inc()\n    droppedEventsCounter.incrementAndGet()\n    if (logDroppedEvent.compareAndSet(false, true)) {\n      // Only log the following message once to avoid duplicated annoying logs.\n      logError(s\"Dropping event from queue $name. \" +\n        \"This likely means one of the listeners is too slow and cannot keep up with \" +\n        \"the rate at which tasks are being started by the scheduler.\")\n    }\n    logTrace(s\"Dropping event $event\")\n\n    val droppedEventsCount = droppedEventsCounter.get\n    val droppedCountIncreased = droppedEventsCount - lastDroppedEventsCounter\n    val lastReportTime = lastReportTimestamp.get\n    val curTime = System.currentTimeMillis()\n    // Don't log too frequently\n    if (droppedCountIncreased > 0 && curTime - lastReportTime >= LOGGING_INTERVAL) {\n      // There may be multiple threads trying to logging dropped events,\n      // Use 'compareAndSet' to make sure only one thread can win.\n      if (lastReportTimestamp.compareAndSet(lastReportTime, curTime)) {\n        val previous = new java.util.Date(lastReportTime)\n        lastDroppedEventsCounter = droppedEventsCount\n        logWarning(s\"Dropped $droppedCountIncreased events from $name since \" +\n          s\"${if (lastReportTime == 0) \"the application started\" else s\"$previous\"}.\")\n      }\n    }\n  }\n```\n\n该方法首先检查队列是否已经停止。如果是运行状态，就试图将事件event入队。若offer()方法返回false，表示队列已满，将丢弃事件的计数器自增，并标记有事件被丢弃。最后，若当前的时间戳与上一次输出droppedEventsCounter值的间隔大于1分钟，就在日志里输出它的值。\n\n理解了AsyncEventQueue的细节之后，我们就可以进一步来看LiveListenerBus的实现了。\n\n## 异步事件总线LiveListenerBus\n\nAsyncEventQueue已经继承了SparkListenerBus特征，LiveListenerBus内部用到了AsyncEventQueue作为核心。来看它的声明以及属性的定义。\n\n```scala\nprivate[spark] class LiveListenerBus(conf: SparkConf) {\n  import LiveListenerBus._\n\n  private var sparkContext: SparkContext = _\n\n  private[spark] val metrics = new LiveListenerBusMetrics(conf)\n\n  private val started = new AtomicBoolean(false)\n  private val stopped = new AtomicBoolean(false)\n\n  private val droppedEventsCounter = new AtomicLong(0L)\n\n  @volatile private var lastReportTimestamp = 0L\n\n  private val queues = new CopyOnWriteArrayList[AsyncEventQueue]()\n\n  @volatile private[scheduler] var queuedEvents = new mutable.ListBuffer[SparkListenerEvent]()\n\n  // ...\n}\n```\n\n这里的属性与AsyncEventQueue大同小异，多出来的主要是queues与queuedEvents两个。\n\n#### **queues属性**\n\nqueues维护一个AsyncEventQueue的列表，也就是说LiveListenerBus中会有多个事件队列。它采用CopyOnWriteArrayList来保证线程安全性。\n\n#### **queuedEvents属性**\n\nqueuedEvents维护一个SparkListenerEvent的列表，它的用途是在LiveListenerBus启动成功之前，缓存可能已经收到的事件。在启动之后，这些缓存的事件会首先投递出去。\n\n**LiveListenerBus作为一个事件总线，也必须提供监听器注册、事件投递等功能，这些都是在AsyncEventQueue基础之上实现的，下面来看一看。**\n\n#### **addToQueue()方法**\n\n```scala\n  private[spark] def addToQueue(\n      listener: SparkListenerInterface,\n      queue: String): Unit = synchronized {\n    if (stopped.get()) {\n      throw new IllegalStateException(\"LiveListenerBus is stopped.\")\n    }\n\n    queues.asScala.find(_.name == queue) match {\n      case Some(queue) =>\n        queue.addListener(listener)\n\n      case None =>\n        val newQueue = new AsyncEventQueue(queue, conf, metrics, this)\n        newQueue.addListener(listener)\n        if (started.get()) {\n          newQueue.start(sparkContext)\n        }\n        queues.add(newQueue)\n    }\n  }\n```\n\n该方法将监听器listener注册到名为queue的队列中。它会在queues列表中寻找符合条件的队列，如果该队列已经存在，就调用父类ListenerBus的addListener()方法直接注册监听器。反之，就先创建一个AsyncEventQueue，注册监听器到新的队列中。\n\n#### post()、postToQueues()方法\n\n```scala\n  def post(event: SparkListenerEvent): Unit = {\n    if (stopped.get()) {\n      return\n    }\n    metrics.numEventsPosted.inc()\n\n    if (queuedEvents == null) {\n      postToQueues(event)\n      return\n    }\n\n    synchronized {\n      if (!started.get()) {\n        queuedEvents += event\n        return\n      }\n    }\n\n    postToQueues(event)\n  }\n\n  private def postToQueues(event: SparkListenerEvent): Unit = {\n    val it = queues.iterator()\n    while (it.hasNext()) {\n      it.next().post(event)\n    }\n  }\n```\n\npost()方法会检查queuedEvents中有无缓存的事件，以及事件总线是否还没有启动。投递时会调用postToQueues()方法，将事件发送给所有队列，由AsyncEventQueue来完成投递到监听器的工作。\n\n![img](https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/gfvstvfzym.jpeg)\n\n","slug":"bigdata/spark/spark事件总线","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt4004a8j5mgay9ht0n","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Spark中很多组件之间是靠事件消息实现通信的，之前分析了一下Spark中RPC机制，RPC和事件消息机制目的都是实现组件之间的通信，前者解决远程通信问题，而后者则是在本地较为高效的方式。Spark中大量采用事件监听这种方式，实现driver端的组件之间的通信。</p>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/spark-listenbus.png\" alt=\"img\"></p>\n<h2 id=\"ListenerBus\"><a href=\"#ListenerBus\" class=\"headerlink\" title=\"ListenerBus\"></a>ListenerBus</h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An event bus which posts events to its listeners.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">ListenerBus</span>[<span class=\"type\">L</span> &lt;: <span class=\"type\">AnyRef</span>, <span class=\"type\">E</span>] <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> listenersPlusTimers = <span class=\"keyword\">new</span> <span class=\"type\">CopyOnWriteArrayList</span>[(<span class=\"type\">L</span>, <span class=\"type\">Option</span>[<span class=\"type\">Timer</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Marked `private[spark]` for access in tests.</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">listeners</span> </span>= listenersPlusTimers.asScala.map(_._1).asJava</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ListenerBus trait是Spark内所有事件总线实现的基类，有两个泛型参数L和E。L代表监听器的类型，并且它可以是任意类型的。E则代表事件的类型。<strong>接受事件并且将事件提交到对应事件的监听器</strong>。</p>\n<p>主要属性如下</p>\n<ul>\n<li><code>listeners</code>, <code>listenersPlusTimers</code>：<strong>维护了所有的监听器和对应的定时器</strong>，数据结构为线程安全的<code>CopyOnWriteArrayList</code>适用于读多写少的业务场景，满足数据的最终一致性</li>\n</ul>\n<p>主要方法如下</p>\n<ul>\n<li><code>addListener()</code>, <code>removeListener()</code>：<strong>从<code>listenersPlusTimers</code>中增加或者删除监听器和计时器</strong></li>\n<li><code>postToAll()</code>：<strong>遍历<code>listenersPlusTimers</code>并调用未实现的<code>doPostEvent()</code>方法发送事件</strong></li>\n</ul>\n<p>每个实现类实现了<code>doPostEvent</code>方法，利用模式匹配将特定的事件投递到对应的监视器类型。</p>\n<h3 id=\"SparkListenerBus\"><a href=\"#SparkListenerBus\" class=\"headerlink\" title=\"SparkListenerBus\"></a>SparkListenerBus</h3><p>SparkListenerBus特征是Spark Core内部事件总线的基类，其代码如下。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 监听器</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerInterface</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">   * Called when a stage completes successfully or fails, with information on the completed stage.</span></span><br><span class=\"line\"><span class=\"comment\">   */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStageCompleted</span></span>(stageCompleted: <span class=\"type\">SparkListenerStageCompleted</span>): <span class=\"type\">Unit</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 事件</span></span><br><span class=\"line\"><span class=\"meta\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"meta\">@JsonTypeInfo</span>(use = <span class=\"type\">JsonTypeInfo</span>.<span class=\"type\">Id</span>.<span class=\"type\">CLASS</span>, include = <span class=\"type\">JsonTypeInfo</span>.<span class=\"type\">As</span>.<span class=\"type\">PROPERTY</span>, property = <span class=\"string\">&quot;Event&quot;</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerEvent</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* Whether output this event to the event log */</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logEvent</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SparkListenerStageCompleted</span>(<span class=\"params\">stageInfo: <span class=\"type\">StageInfo</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">SparkListenerEvent</span></span></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 事件总线</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerBus</span></span></span><br><span class=\"line\"><span class=\"keyword\">extends</span> <span class=\"type\">ListenerBus</span>[<span class=\"type\">SparkListenerInterface</span>, <span class=\"type\">SparkListenerEvent</span>] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doPostEvent</span></span>(</span><br><span class=\"line\">    listener: <span class=\"type\">SparkListenerInterface</span>,</span><br><span class=\"line\">    event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    event <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> stageSubmitted: <span class=\"type\">SparkListenerStageSubmitted</span> =&gt;</span><br><span class=\"line\">      listener.onStageSubmitted(stageSubmitted)</span><br><span class=\"line\">      ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>SparkListenerBus继承了ListenerBus，实现了doPostEvent()方法，对事件进行匹配，并调用监听器的处理方法。如果无法匹配到事件，则调用onOtherEvent()方法。</p>\n<p>SparkListenerBus支持的监听器都是SparkListenerInterface的子类，事件则是SparkListenerEvent的子类。下面来了解一下。</p>\n<h3 id=\"SparkListenerInterface与SparkListenerEvent特征\"><a href=\"#SparkListenerInterface与SparkListenerEvent特征\" class=\"headerlink\" title=\"SparkListenerInterface与SparkListenerEvent特征\"></a>SparkListenerInterface与SparkListenerEvent特征</h3><p>在SparkListenerInterface特征中，分别定义了处理每一个事件的处理方法，统一命名为“on+事件名称”，代码很简单，就不再贴出来了。</p>\n<p>SparkListenerEvent是一个没有抽象方法的特征，类似于Java中的标记接口（marker interface），它唯一的用途就是标记具体的事件类。事件类统一命名为“SparkListener+事件名称”，并且都是Scala样例类。</p>\n<h2 id=\"AsyncEventQueue\"><a href=\"#AsyncEventQueue\" class=\"headerlink\" title=\"AsyncEventQueue\"></a>AsyncEventQueue</h2><p>在SparkListenerBus的实现类AsyncEventQueue中，提供了异步事件队列机制，它也是SparkContext中的事件总线LiveListenerBus的基础。</p>\n<p>实现原理是基于消息队列的异步通信，因此有以下优点：1、将Event发送者和Event listerner解耦。2、异步：Event发送者发送Event给消息队列后直接返回，无需等待listener处理后才返回，减少了Event发送者的阻塞，提高了性能。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An asynchronous queue for events. All events posted to this queue will be delivered to the child</span></span><br><span class=\"line\"><span class=\"comment\"> * listeners in a separate thread.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Delivery will only begin when the `start()` method is called. The `stop()` method should be</span></span><br><span class=\"line\"><span class=\"comment\"> * called when no more events need to be delivered.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AsyncEventQueue</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    val name: <span class=\"type\">String</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    conf: <span class=\"type\">SparkConf</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    metrics: <span class=\"type\">LiveListenerBusMetrics</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    bus: <span class=\"type\">LiveListenerBus</span></span>)</span></span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">SparkListenerBus</span></span><br><span class=\"line\">  <span class=\"keyword\">with</span> <span class=\"type\">Logging</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">import</span> <span class=\"type\">AsyncEventQueue</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> eventQueue = <span class=\"keyword\">new</span> <span class=\"type\">LinkedBlockingQueue</span>[<span class=\"type\">SparkListenerEvent</span>](</span><br><span class=\"line\">    conf.get(<span class=\"type\">LISTENER_BUS_EVENT_QUEUE_CAPACITY</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> eventCount = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEventsCounter = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>(<span class=\"number\">0</span>L)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastReportTimestamp = <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> logDroppedEvent = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> sc: <span class=\"type\">SparkContext</span> = <span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> started = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> stopped = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEvents = metrics.metricRegistry.counter(<span class=\"string\">s&quot;queue.<span class=\"subst\">$name</span>.numDroppedEvents&quot;</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> processingTime = metrics.metricRegistry.timer(<span class=\"string\">s&quot;queue.<span class=\"subst\">$name</span>.listenerProcessingTime&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> dispatchThread = <span class=\"keyword\">new</span> <span class=\"type\">Thread</span>(<span class=\"string\">s&quot;spark-listener-group-<span class=\"subst\">$name</span>&quot;</span>) &#123;</span><br><span class=\"line\">    setDaemon(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class=\"line\">      dispatch()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该类的构造参数有四个，分别是队列名、Spark配置项、LiveListenerBus的监控度量，以及LiveListenerBus本身。下面来看一下它的主要属性。</p>\n<h4 id=\"eventQueue、eventCount属性\"><a href=\"#eventQueue、eventCount属性\" class=\"headerlink\" title=\"eventQueue、eventCount属性\"></a><strong>eventQueue、eventCount属性</strong></h4><p>eventQueue是一个存储SparkListenerEvent事件的阻塞队列LinkedBlockingQueue。它的大小是通过配置参数spark.scheduler.listenerbus.eventqueue.capacity来设置的，默认值10000。如果不设置阻塞队列的大小，那么默认值会是Integer.MAX_VALUE，有OOM的风险。</p>\n<p>eventCount则是当前待处理事件的计数。因为事件从队列中弹出不代表已经处理完成，所以不能直接用队列的实际大小来表示。它是AtomicLong类型的，以保证修改的原子性。</p>\n<h4 id=\"droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\"><a href=\"#droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\" class=\"headerlink\" title=\"droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\"></a><strong>droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性</strong></h4><p>droppedEventsCounter是被丢弃事件的计数。当阻塞队列已满后，新产生的事件无法入队，就会被丢弃。日志中定期输出该计数器的值，用lastReportTimestamp记录下每次输出的时间戳，并且输出后都会将计数器重新置为0。</p>\n<p>logDroppedEvent用于指示是否发生过了事件丢弃的情况。它与droppedEventsCounter一样也都是原子类型的。</p>\n<h4 id=\"started、stopped属性\"><a href=\"#started、stopped属性\" class=\"headerlink\" title=\"started、stopped属性\"></a><strong>started、stopped属性</strong></h4><p>这两个属性分别用来标记队列的启动与停止状态。</p>\n<h4 id=\"dispatchThread属性\"><a href=\"#dispatchThread属性\" class=\"headerlink\" title=\"dispatchThread属性\"></a><strong>dispatchThread属性</strong></h4><p>dispatchThread是将队列中的事件分发到各监听器的守护线程，实际上调用了dispatch()方法。而Utils.tryOrStopSparkContext()方法的作用在于执行代码块时如果抛出异常，就另外起一个线程关闭SparkContext。</p>\n<p>下面就来看看dispatch()方法的源码。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dispatch</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">LiveListenerBus</span>.withinListenerThread.withValue(<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> next: <span class=\"type\">SparkListenerEvent</span> = eventQueue.take()</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (next != <span class=\"type\">POISON_PILL</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ctx = processingTime.time()</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">super</span>.postToAll(next)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      ctx.stop()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    eventCount.decrementAndGet()</span><br><span class=\"line\">    next = eventQueue.take()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  eventCount.decrementAndGet()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见，该方法循环地从事件队列中取出事件，并调用父类ListenerBus特征的postToAll()方法（文章#5已经讲过）将其投递给所有已注册的监听器，并减少计数器的值。“毒药丸”POISON_PILL是伴生对象中定义的一个特殊的空事件，在队列停止（即调用stop()方法）时会被放入，dispatcherThread取得它之后就会“中毒”退出循环。</p>\n<p>有了处理事件的方法，还得有将事件放入队列的方法才完整。下面是入队的方法post()。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  eventCount.incrementAndGet()</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (eventQueue.offer(event)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  eventCount.decrementAndGet()</span><br><span class=\"line\">  droppedEvents.inc()</span><br><span class=\"line\">  droppedEventsCounter.incrementAndGet()</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (logDroppedEvent.compareAndSet(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>)) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Only log the following message once to avoid duplicated annoying logs.</span></span><br><span class=\"line\">    logError(<span class=\"string\">s&quot;Dropping event from queue <span class=\"subst\">$name</span>. &quot;</span> +</span><br><span class=\"line\">      <span class=\"string\">&quot;This likely means one of the listeners is too slow and cannot keep up with &quot;</span> +</span><br><span class=\"line\">      <span class=\"string\">&quot;the rate at which tasks are being started by the scheduler.&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  logTrace(<span class=\"string\">s&quot;Dropping event <span class=\"subst\">$event</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> droppedEventsCount = droppedEventsCounter.get</span><br><span class=\"line\">  <span class=\"keyword\">val</span> droppedCountIncreased = droppedEventsCount - lastDroppedEventsCounter</span><br><span class=\"line\">  <span class=\"keyword\">val</span> lastReportTime = lastReportTimestamp.get</span><br><span class=\"line\">  <span class=\"keyword\">val</span> curTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">  <span class=\"comment\">// Don&#x27;t log too frequently</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (droppedCountIncreased &gt; <span class=\"number\">0</span> &amp;&amp; curTime - lastReportTime &gt;= <span class=\"type\">LOGGING_INTERVAL</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// There may be multiple threads trying to logging dropped events,</span></span><br><span class=\"line\">    <span class=\"comment\">// Use &#x27;compareAndSet&#x27; to make sure only one thread can win.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (lastReportTimestamp.compareAndSet(lastReportTime, curTime)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> previous = <span class=\"keyword\">new</span> java.util.<span class=\"type\">Date</span>(lastReportTime)</span><br><span class=\"line\">      lastDroppedEventsCounter = droppedEventsCount</span><br><span class=\"line\">      logWarning(<span class=\"string\">s&quot;Dropped <span class=\"subst\">$droppedCountIncreased</span> events from <span class=\"subst\">$name</span> since &quot;</span> +</span><br><span class=\"line\">        <span class=\"string\">s&quot;<span class=\"subst\">$&#123;if (lastReportTime == 0) &quot;the application started&quot; else s&quot;$previous&quot;&#125;</span>.&quot;</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该方法首先检查队列是否已经停止。如果是运行状态，就试图将事件event入队。若offer()方法返回false，表示队列已满，将丢弃事件的计数器自增，并标记有事件被丢弃。最后，若当前的时间戳与上一次输出droppedEventsCounter值的间隔大于1分钟，就在日志里输出它的值。</p>\n<p>理解了AsyncEventQueue的细节之后，我们就可以进一步来看LiveListenerBus的实现了。</p>\n<h2 id=\"异步事件总线LiveListenerBus\"><a href=\"#异步事件总线LiveListenerBus\" class=\"headerlink\" title=\"异步事件总线LiveListenerBus\"></a>异步事件总线LiveListenerBus</h2><p>AsyncEventQueue已经继承了SparkListenerBus特征，LiveListenerBus内部用到了AsyncEventQueue作为核心。来看它的声明以及属性的定义。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LiveListenerBus</span>(<span class=\"params\">conf: <span class=\"type\">SparkConf</span></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">import</span> <span class=\"type\">LiveListenerBus</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> sparkContext: <span class=\"type\">SparkContext</span> = _</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[spark] <span class=\"keyword\">val</span> metrics = <span class=\"keyword\">new</span> <span class=\"type\">LiveListenerBusMetrics</span>(conf)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> started = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> stopped = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEventsCounter = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>(<span class=\"number\">0</span>L)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastReportTimestamp = <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> queues = <span class=\"keyword\">new</span> <span class=\"type\">CopyOnWriteArrayList</span>[<span class=\"type\">AsyncEventQueue</span>]()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">var</span> queuedEvents = <span class=\"keyword\">new</span> mutable.<span class=\"type\">ListBuffer</span>[<span class=\"type\">SparkListenerEvent</span>]()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这里的属性与AsyncEventQueue大同小异，多出来的主要是queues与queuedEvents两个。</p>\n<h4 id=\"queues属性\"><a href=\"#queues属性\" class=\"headerlink\" title=\"queues属性\"></a><strong>queues属性</strong></h4><p>queues维护一个AsyncEventQueue的列表，也就是说LiveListenerBus中会有多个事件队列。它采用CopyOnWriteArrayList来保证线程安全性。</p>\n<h4 id=\"queuedEvents属性\"><a href=\"#queuedEvents属性\" class=\"headerlink\" title=\"queuedEvents属性\"></a><strong>queuedEvents属性</strong></h4><p>queuedEvents维护一个SparkListenerEvent的列表，它的用途是在LiveListenerBus启动成功之前，缓存可能已经收到的事件。在启动之后，这些缓存的事件会首先投递出去。</p>\n<p><strong>LiveListenerBus作为一个事件总线，也必须提供监听器注册、事件投递等功能，这些都是在AsyncEventQueue基础之上实现的，下面来看一看。</strong></p>\n<h4 id=\"addToQueue-方法\"><a href=\"#addToQueue-方法\" class=\"headerlink\" title=\"addToQueue()方法\"></a><strong>addToQueue()方法</strong></h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addToQueue</span></span>(</span><br><span class=\"line\">    listener: <span class=\"type\">SparkListenerInterface</span>,</span><br><span class=\"line\">    queue: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(<span class=\"string\">&quot;LiveListenerBus is stopped.&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  queues.asScala.find(_.name == queue) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(queue) =&gt;</span><br><span class=\"line\">      queue.addListener(listener)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newQueue = <span class=\"keyword\">new</span> <span class=\"type\">AsyncEventQueue</span>(queue, conf, metrics, <span class=\"keyword\">this</span>)</span><br><span class=\"line\">      newQueue.addListener(listener)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (started.get()) &#123;</span><br><span class=\"line\">        newQueue.start(sparkContext)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      queues.add(newQueue)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该方法将监听器listener注册到名为queue的队列中。它会在queues列表中寻找符合条件的队列，如果该队列已经存在，就调用父类ListenerBus的addListener()方法直接注册监听器。反之，就先创建一个AsyncEventQueue，注册监听器到新的队列中。</p>\n<h4 id=\"post-、postToQueues-方法\"><a href=\"#post-、postToQueues-方法\" class=\"headerlink\" title=\"post()、postToQueues()方法\"></a>post()、postToQueues()方法</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  metrics.numEventsPosted.inc()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (queuedEvents == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">    postToQueues(event)</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  synchronized &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!started.get()) &#123;</span><br><span class=\"line\">      queuedEvents += event</span><br><span class=\"line\">      <span class=\"keyword\">return</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  postToQueues(event)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">postToQueues</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> it = queues.iterator()</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (it.hasNext()) &#123;</span><br><span class=\"line\">    it.next().post(event)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>post()方法会检查queuedEvents中有无缓存的事件，以及事件总线是否还没有启动。投递时会调用postToQueues()方法，将事件发送给所有队列，由AsyncEventQueue来完成投递到监听器的工作。</p>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/gfvstvfzym.jpeg\" alt=\"img\"></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Spark中很多组件之间是靠事件消息实现通信的，之前分析了一下Spark中RPC机制，RPC和事件消息机制目的都是实现组件之间的通信，前者解决远程通信问题，而后者则是在本地较为高效的方式。Spark中大量采用事件监听这种方式，实现driver端的组件之间的通信。</p>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/spark-listenbus.png\" alt=\"img\"></p>\n<h2 id=\"ListenerBus\"><a href=\"#ListenerBus\" class=\"headerlink\" title=\"ListenerBus\"></a>ListenerBus</h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An event bus which posts events to its listeners.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">ListenerBus</span>[<span class=\"type\">L</span> &lt;: <span class=\"type\">AnyRef</span>, <span class=\"type\">E</span>] <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> listenersPlusTimers = <span class=\"keyword\">new</span> <span class=\"type\">CopyOnWriteArrayList</span>[(<span class=\"type\">L</span>, <span class=\"type\">Option</span>[<span class=\"type\">Timer</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Marked `private[spark]` for access in tests.</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">listeners</span> </span>= listenersPlusTimers.asScala.map(_._1).asJava</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ListenerBus trait是Spark内所有事件总线实现的基类，有两个泛型参数L和E。L代表监听器的类型，并且它可以是任意类型的。E则代表事件的类型。<strong>接受事件并且将事件提交到对应事件的监听器</strong>。</p>\n<p>主要属性如下</p>\n<ul>\n<li><code>listeners</code>, <code>listenersPlusTimers</code>：<strong>维护了所有的监听器和对应的定时器</strong>，数据结构为线程安全的<code>CopyOnWriteArrayList</code>适用于读多写少的业务场景，满足数据的最终一致性</li>\n</ul>\n<p>主要方法如下</p>\n<ul>\n<li><code>addListener()</code>, <code>removeListener()</code>：<strong>从<code>listenersPlusTimers</code>中增加或者删除监听器和计时器</strong></li>\n<li><code>postToAll()</code>：<strong>遍历<code>listenersPlusTimers</code>并调用未实现的<code>doPostEvent()</code>方法发送事件</strong></li>\n</ul>\n<p>每个实现类实现了<code>doPostEvent</code>方法，利用模式匹配将特定的事件投递到对应的监视器类型。</p>\n<h3 id=\"SparkListenerBus\"><a href=\"#SparkListenerBus\" class=\"headerlink\" title=\"SparkListenerBus\"></a>SparkListenerBus</h3><p>SparkListenerBus特征是Spark Core内部事件总线的基类，其代码如下。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 监听器</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerInterface</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">   * Called when a stage completes successfully or fails, with information on the completed stage.</span></span><br><span class=\"line\"><span class=\"comment\">   */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onStageCompleted</span></span>(stageCompleted: <span class=\"type\">SparkListenerStageCompleted</span>): <span class=\"type\">Unit</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 事件</span></span><br><span class=\"line\"><span class=\"meta\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"meta\">@JsonTypeInfo</span>(use = <span class=\"type\">JsonTypeInfo</span>.<span class=\"type\">Id</span>.<span class=\"type\">CLASS</span>, include = <span class=\"type\">JsonTypeInfo</span>.<span class=\"type\">As</span>.<span class=\"type\">PROPERTY</span>, property = <span class=\"string\">&quot;Event&quot;</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerEvent</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* Whether output this event to the event log */</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">logEvent</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SparkListenerStageCompleted</span>(<span class=\"params\">stageInfo: <span class=\"type\">StageInfo</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">SparkListenerEvent</span></span></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 事件总线</span></span><br><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">SparkListenerBus</span></span></span><br><span class=\"line\"><span class=\"keyword\">extends</span> <span class=\"type\">ListenerBus</span>[<span class=\"type\">SparkListenerInterface</span>, <span class=\"type\">SparkListenerEvent</span>] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doPostEvent</span></span>(</span><br><span class=\"line\">    listener: <span class=\"type\">SparkListenerInterface</span>,</span><br><span class=\"line\">    event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    event <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> stageSubmitted: <span class=\"type\">SparkListenerStageSubmitted</span> =&gt;</span><br><span class=\"line\">      listener.onStageSubmitted(stageSubmitted)</span><br><span class=\"line\">      ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>SparkListenerBus继承了ListenerBus，实现了doPostEvent()方法，对事件进行匹配，并调用监听器的处理方法。如果无法匹配到事件，则调用onOtherEvent()方法。</p>\n<p>SparkListenerBus支持的监听器都是SparkListenerInterface的子类，事件则是SparkListenerEvent的子类。下面来了解一下。</p>\n<h3 id=\"SparkListenerInterface与SparkListenerEvent特征\"><a href=\"#SparkListenerInterface与SparkListenerEvent特征\" class=\"headerlink\" title=\"SparkListenerInterface与SparkListenerEvent特征\"></a>SparkListenerInterface与SparkListenerEvent特征</h3><p>在SparkListenerInterface特征中，分别定义了处理每一个事件的处理方法，统一命名为“on+事件名称”，代码很简单，就不再贴出来了。</p>\n<p>SparkListenerEvent是一个没有抽象方法的特征，类似于Java中的标记接口（marker interface），它唯一的用途就是标记具体的事件类。事件类统一命名为“SparkListener+事件名称”，并且都是Scala样例类。</p>\n<h2 id=\"AsyncEventQueue\"><a href=\"#AsyncEventQueue\" class=\"headerlink\" title=\"AsyncEventQueue\"></a>AsyncEventQueue</h2><p>在SparkListenerBus的实现类AsyncEventQueue中，提供了异步事件队列机制，它也是SparkContext中的事件总线LiveListenerBus的基础。</p>\n<p>实现原理是基于消息队列的异步通信，因此有以下优点：1、将Event发送者和Event listerner解耦。2、异步：Event发送者发送Event给消息队列后直接返回，无需等待listener处理后才返回，减少了Event发送者的阻塞，提高了性能。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An asynchronous queue for events. All events posted to this queue will be delivered to the child</span></span><br><span class=\"line\"><span class=\"comment\"> * listeners in a separate thread.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * Delivery will only begin when the `start()` method is called. The `stop()` method should be</span></span><br><span class=\"line\"><span class=\"comment\"> * called when no more events need to be delivered.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AsyncEventQueue</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    val name: <span class=\"type\">String</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    conf: <span class=\"type\">SparkConf</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    metrics: <span class=\"type\">LiveListenerBusMetrics</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"class\">    bus: <span class=\"type\">LiveListenerBus</span></span>)</span></span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">SparkListenerBus</span></span><br><span class=\"line\">  <span class=\"keyword\">with</span> <span class=\"type\">Logging</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">import</span> <span class=\"type\">AsyncEventQueue</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> eventQueue = <span class=\"keyword\">new</span> <span class=\"type\">LinkedBlockingQueue</span>[<span class=\"type\">SparkListenerEvent</span>](</span><br><span class=\"line\">    conf.get(<span class=\"type\">LISTENER_BUS_EVENT_QUEUE_CAPACITY</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> eventCount = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEventsCounter = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>(<span class=\"number\">0</span>L)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastReportTimestamp = <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> logDroppedEvent = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> sc: <span class=\"type\">SparkContext</span> = <span class=\"literal\">null</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> started = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> stopped = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEvents = metrics.metricRegistry.counter(<span class=\"string\">s&quot;queue.<span class=\"subst\">$name</span>.numDroppedEvents&quot;</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> processingTime = metrics.metricRegistry.timer(<span class=\"string\">s&quot;queue.<span class=\"subst\">$name</span>.listenerProcessingTime&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> dispatchThread = <span class=\"keyword\">new</span> <span class=\"type\">Thread</span>(<span class=\"string\">s&quot;spark-listener-group-<span class=\"subst\">$name</span>&quot;</span>) &#123;</span><br><span class=\"line\">    setDaemon(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class=\"line\">      dispatch()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该类的构造参数有四个，分别是队列名、Spark配置项、LiveListenerBus的监控度量，以及LiveListenerBus本身。下面来看一下它的主要属性。</p>\n<h4 id=\"eventQueue、eventCount属性\"><a href=\"#eventQueue、eventCount属性\" class=\"headerlink\" title=\"eventQueue、eventCount属性\"></a><strong>eventQueue、eventCount属性</strong></h4><p>eventQueue是一个存储SparkListenerEvent事件的阻塞队列LinkedBlockingQueue。它的大小是通过配置参数spark.scheduler.listenerbus.eventqueue.capacity来设置的，默认值10000。如果不设置阻塞队列的大小，那么默认值会是Integer.MAX_VALUE，有OOM的风险。</p>\n<p>eventCount则是当前待处理事件的计数。因为事件从队列中弹出不代表已经处理完成，所以不能直接用队列的实际大小来表示。它是AtomicLong类型的，以保证修改的原子性。</p>\n<h4 id=\"droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\"><a href=\"#droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\" class=\"headerlink\" title=\"droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性\"></a><strong>droppedEventsCounter、lastReportTimestamp、logDroppedEvent属性</strong></h4><p>droppedEventsCounter是被丢弃事件的计数。当阻塞队列已满后，新产生的事件无法入队，就会被丢弃。日志中定期输出该计数器的值，用lastReportTimestamp记录下每次输出的时间戳，并且输出后都会将计数器重新置为0。</p>\n<p>logDroppedEvent用于指示是否发生过了事件丢弃的情况。它与droppedEventsCounter一样也都是原子类型的。</p>\n<h4 id=\"started、stopped属性\"><a href=\"#started、stopped属性\" class=\"headerlink\" title=\"started、stopped属性\"></a><strong>started、stopped属性</strong></h4><p>这两个属性分别用来标记队列的启动与停止状态。</p>\n<h4 id=\"dispatchThread属性\"><a href=\"#dispatchThread属性\" class=\"headerlink\" title=\"dispatchThread属性\"></a><strong>dispatchThread属性</strong></h4><p>dispatchThread是将队列中的事件分发到各监听器的守护线程，实际上调用了dispatch()方法。而Utils.tryOrStopSparkContext()方法的作用在于执行代码块时如果抛出异常，就另外起一个线程关闭SparkContext。</p>\n<p>下面就来看看dispatch()方法的源码。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dispatch</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">LiveListenerBus</span>.withinListenerThread.withValue(<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> next: <span class=\"type\">SparkListenerEvent</span> = eventQueue.take()</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (next != <span class=\"type\">POISON_PILL</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ctx = processingTime.time()</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">super</span>.postToAll(next)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      ctx.stop()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    eventCount.decrementAndGet()</span><br><span class=\"line\">    next = eventQueue.take()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  eventCount.decrementAndGet()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可见，该方法循环地从事件队列中取出事件，并调用父类ListenerBus特征的postToAll()方法（文章#5已经讲过）将其投递给所有已注册的监听器，并减少计数器的值。“毒药丸”POISON_PILL是伴生对象中定义的一个特殊的空事件，在队列停止（即调用stop()方法）时会被放入，dispatcherThread取得它之后就会“中毒”退出循环。</p>\n<p>有了处理事件的方法，还得有将事件放入队列的方法才完整。下面是入队的方法post()。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  eventCount.incrementAndGet()</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (eventQueue.offer(event)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  eventCount.decrementAndGet()</span><br><span class=\"line\">  droppedEvents.inc()</span><br><span class=\"line\">  droppedEventsCounter.incrementAndGet()</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (logDroppedEvent.compareAndSet(<span class=\"literal\">false</span>, <span class=\"literal\">true</span>)) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Only log the following message once to avoid duplicated annoying logs.</span></span><br><span class=\"line\">    logError(<span class=\"string\">s&quot;Dropping event from queue <span class=\"subst\">$name</span>. &quot;</span> +</span><br><span class=\"line\">      <span class=\"string\">&quot;This likely means one of the listeners is too slow and cannot keep up with &quot;</span> +</span><br><span class=\"line\">      <span class=\"string\">&quot;the rate at which tasks are being started by the scheduler.&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  logTrace(<span class=\"string\">s&quot;Dropping event <span class=\"subst\">$event</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> droppedEventsCount = droppedEventsCounter.get</span><br><span class=\"line\">  <span class=\"keyword\">val</span> droppedCountIncreased = droppedEventsCount - lastDroppedEventsCounter</span><br><span class=\"line\">  <span class=\"keyword\">val</span> lastReportTime = lastReportTimestamp.get</span><br><span class=\"line\">  <span class=\"keyword\">val</span> curTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">  <span class=\"comment\">// Don&#x27;t log too frequently</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (droppedCountIncreased &gt; <span class=\"number\">0</span> &amp;&amp; curTime - lastReportTime &gt;= <span class=\"type\">LOGGING_INTERVAL</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// There may be multiple threads trying to logging dropped events,</span></span><br><span class=\"line\">    <span class=\"comment\">// Use &#x27;compareAndSet&#x27; to make sure only one thread can win.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (lastReportTimestamp.compareAndSet(lastReportTime, curTime)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> previous = <span class=\"keyword\">new</span> java.util.<span class=\"type\">Date</span>(lastReportTime)</span><br><span class=\"line\">      lastDroppedEventsCounter = droppedEventsCount</span><br><span class=\"line\">      logWarning(<span class=\"string\">s&quot;Dropped <span class=\"subst\">$droppedCountIncreased</span> events from <span class=\"subst\">$name</span> since &quot;</span> +</span><br><span class=\"line\">        <span class=\"string\">s&quot;<span class=\"subst\">$&#123;if (lastReportTime == 0) &quot;the application started&quot; else s&quot;$previous&quot;&#125;</span>.&quot;</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该方法首先检查队列是否已经停止。如果是运行状态，就试图将事件event入队。若offer()方法返回false，表示队列已满，将丢弃事件的计数器自增，并标记有事件被丢弃。最后，若当前的时间戳与上一次输出droppedEventsCounter值的间隔大于1分钟，就在日志里输出它的值。</p>\n<p>理解了AsyncEventQueue的细节之后，我们就可以进一步来看LiveListenerBus的实现了。</p>\n<h2 id=\"异步事件总线LiveListenerBus\"><a href=\"#异步事件总线LiveListenerBus\" class=\"headerlink\" title=\"异步事件总线LiveListenerBus\"></a>异步事件总线LiveListenerBus</h2><p>AsyncEventQueue已经继承了SparkListenerBus特征，LiveListenerBus内部用到了AsyncEventQueue作为核心。来看它的声明以及属性的定义。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LiveListenerBus</span>(<span class=\"params\">conf: <span class=\"type\">SparkConf</span></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">import</span> <span class=\"type\">LiveListenerBus</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> sparkContext: <span class=\"type\">SparkContext</span> = _</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span>[spark] <span class=\"keyword\">val</span> metrics = <span class=\"keyword\">new</span> <span class=\"type\">LiveListenerBusMetrics</span>(conf)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> started = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> stopped = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> droppedEventsCounter = <span class=\"keyword\">new</span> <span class=\"type\">AtomicLong</span>(<span class=\"number\">0</span>L)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastReportTimestamp = <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> queues = <span class=\"keyword\">new</span> <span class=\"type\">CopyOnWriteArrayList</span>[<span class=\"type\">AsyncEventQueue</span>]()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@volatile</span> <span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">var</span> queuedEvents = <span class=\"keyword\">new</span> mutable.<span class=\"type\">ListBuffer</span>[<span class=\"type\">SparkListenerEvent</span>]()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// ...</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这里的属性与AsyncEventQueue大同小异，多出来的主要是queues与queuedEvents两个。</p>\n<h4 id=\"queues属性\"><a href=\"#queues属性\" class=\"headerlink\" title=\"queues属性\"></a><strong>queues属性</strong></h4><p>queues维护一个AsyncEventQueue的列表，也就是说LiveListenerBus中会有多个事件队列。它采用CopyOnWriteArrayList来保证线程安全性。</p>\n<h4 id=\"queuedEvents属性\"><a href=\"#queuedEvents属性\" class=\"headerlink\" title=\"queuedEvents属性\"></a><strong>queuedEvents属性</strong></h4><p>queuedEvents维护一个SparkListenerEvent的列表，它的用途是在LiveListenerBus启动成功之前，缓存可能已经收到的事件。在启动之后，这些缓存的事件会首先投递出去。</p>\n<p><strong>LiveListenerBus作为一个事件总线，也必须提供监听器注册、事件投递等功能，这些都是在AsyncEventQueue基础之上实现的，下面来看一看。</strong></p>\n<h4 id=\"addToQueue-方法\"><a href=\"#addToQueue-方法\" class=\"headerlink\" title=\"addToQueue()方法\"></a><strong>addToQueue()方法</strong></h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addToQueue</span></span>(</span><br><span class=\"line\">    listener: <span class=\"type\">SparkListenerInterface</span>,</span><br><span class=\"line\">    queue: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = synchronized &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(<span class=\"string\">&quot;LiveListenerBus is stopped.&quot;</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  queues.asScala.find(_.name == queue) <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(queue) =&gt;</span><br><span class=\"line\">      queue.addListener(listener)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newQueue = <span class=\"keyword\">new</span> <span class=\"type\">AsyncEventQueue</span>(queue, conf, metrics, <span class=\"keyword\">this</span>)</span><br><span class=\"line\">      newQueue.addListener(listener)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (started.get()) &#123;</span><br><span class=\"line\">        newQueue.start(sparkContext)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      queues.add(newQueue)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>该方法将监听器listener注册到名为queue的队列中。它会在queues列表中寻找符合条件的队列，如果该队列已经存在，就调用父类ListenerBus的addListener()方法直接注册监听器。反之，就先创建一个AsyncEventQueue，注册监听器到新的队列中。</p>\n<h4 id=\"post-、postToQueues-方法\"><a href=\"#post-、postToQueues-方法\" class=\"headerlink\" title=\"post()、postToQueues()方法\"></a>post()、postToQueues()方法</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (stopped.get()) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  metrics.numEventsPosted.inc()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (queuedEvents == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">    postToQueues(event)</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  synchronized &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!started.get()) &#123;</span><br><span class=\"line\">      queuedEvents += event</span><br><span class=\"line\">      <span class=\"keyword\">return</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  postToQueues(event)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">postToQueues</span></span>(event: <span class=\"type\">SparkListenerEvent</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> it = queues.iterator()</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (it.hasNext()) &#123;</span><br><span class=\"line\">    it.next().post(event)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>post()方法会检查queuedEvents中有无缓存的事件，以及事件总线是否还没有启动。投递时会调用postToQueues()方法，将事件发送给所有队列，由AsyncEventQueue来完成投递到监听器的工作。</p>\n<p><img src=\"https://raw.githubusercontent.com/yuanoOo/learngit/master/jpg/gfvstvfzym.jpeg\" alt=\"img\"></p>\n"},{"title":"利用RuntimeReplaceable实现Spark Native function","abbrlink":7631,"date":"2023-09-27T07:55:27.000Z","updated":"2023-09-27T07:55:27.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","top_img":null,"description":null,"keywords":null,"_content":"\n## 关于Spark Native Function\n\n在Spark中实现自定义函数，有多种方式：\n\n- 1、实现Hive UDF，Spark是兼容Hive UDF的，简单易用，支持纯SQL环境，因此这可能是使用最为广泛的。\n- 2、实现Spark-SQL UDF，需要嵌入到代码中，因此也主要用在代码中，目前还不支持纯SQL环境。\n- 3、通过拓展SparkSessionExtensions，基本等价于Spark Built-in内置函数，可以充分利用Spark catalyst优化器和Codegen，从而带来可观的性能提升，这里称之为Spark Native Function。但是这种方式也是实现最为困难的，需要对SQL解析器、优化器等有一定的理解。同时网上关于这种方式的资料几乎没有，Spark官方文档中也是根本没有提及这种方式，足以说明这种方式较高的门槛。\n\n## 应用场景：RuntimeReplaceable\n\nSpark已经内置足够多的UDF，已经可以满足绝大部分的应用场景。\n\n剩下的不能满足的应用场景中，其中很大一部分可以通过组合这些内置的函数，来满足。因此也就带来一个问题，就是有时候应用场景非常复杂，需要组合几十种函数，而Spark-SQL也不支持存储过程，最后导致SQL非常长，难以理解阅读，从而难以维护。\n\n而通过实现`RuntimeReplaceable`类型Spark Native Function，可以完美的解决我们的问题。`RuntimeReplaceable`是通过用我们自定义的函数Express替换掉抽象语法树中的函数Express，主要用于兼容不同数据库系统函数别名，也正好满足我们的应用场景。\n\n\n\n## 上代码\n\n在这个例子中，我们实现了一个`str_pivot` Spark Native Function，该函数解决的应用场景如下：\n\n> 有这样一个用逗号分隔的字符串`c1,c2,c3`包含三个元素c1、c2、c3，这三个元素通过排列组合，顺序不同也是一种组合，共有16中组合，例如：c1，c1c2，c2c1，c1c2c3等等。\n>\n> 给出另一个字符串`c2c1`，判断这个字符串是不是其中一个排列组合。这就是`str_pivot`函数要实现的。\n>\n> 我们可以通过下面这个算法实现：\n>\n> `size(array_union(array('1', '2', '3'), array('2','1'))) = size(array('1', '2', '3'))`\n\n### driver\n\n```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.execution.CodegenMode\n\n/**\n * org.apache.spark.sql.catalyst.analysis.FunctionRegistry\n *\n * org.apache.spark.sql.catalyst.expressions.Length\n *\n * -- CodeGen\n * org.apache.spark.sql.catalyst.expressions.UnaryMathExpression\n */\nobject StringPiovtFunctionDriver {\n  val sql = \"select str_pivot('1,2,3,4', '1,2')\"\n  val sql_udf = \"select str_pivot_udf('1,2,3,4', '1,2')\"\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder()\n      .master(\"local[1]\")\n      .appName(\"SparkNativeFunctionInject\")\n      .withExtensions(new FunctionSparkExtension)\n      .getOrCreate()\n\t\n    // UDF方式实现，对比执行计划等\n    spark.udf.register(\"str_pivot_udf\",\n      (left: String, right: String) => {\n        left.split(\",\").union(right.split(\",\")).toSet.size == left.split(\",\").length\n      }\n    )\n\n    spark.sql(sql).show()\n    spark.sql(sql).explain(true)\n    spark.sql(sql).explain(CodegenMode.name)\n\n    spark.sql(sql_udf).show()\n    spark.sql(sql_udf).explain(true)\n    spark.sql(sql_udf).explain(CodegenMode.name)\n\n  }\n}\n\n```\n\n### 拓展SparkSessionExtensions，injectFunction\n\n```scala\nimport org.apache.spark.sql.catalyst.FunctionIdentifier\nimport org.apache.spark.sql.catalyst.analysis.FunctionRegistry.FunctionBuilder\nimport org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionInfo}\nimport org.apache.spark.sql.{MLBStrPivot, SparkSessionExtensions, StringLength, StringPivot}\n\nclass FunctionSparkExtension extends (SparkSessionExtensions => Unit){\n  override def apply(extensions: SparkSessionExtensions): Unit = {\n    extensions.injectFunction(\n      (new FunctionIdentifier(\"str_pivot\"),\n        new ExpressionInfo(classOf[MLBStrPivot].getName,\n          \"str_pivot\"),\n        (children: Seq[Expression]) => new MLBStrPivot(children.head, children(1))))\n  }\n}\n```\n\n### Function Implement\n\n```scala\n// left is fully string\ncase class MLBStrPivot(left: Expression, right: Expression, child: Expression) extends RuntimeReplaceable {\n\n  //size(array_union(array('1', '2', '3'), array('2','1'))) = size(array('1', '2', '3'))\n  def this(left: Expression, right: Expression) = {\n    this(left, right,  \n    EqualTo\n      (\n        Size(ArrayUnion(StringSplit(left, Literal(\",\"), Literal(-1)), StringSplit(right, Literal(\",\"), Literal(-1))), false),\n        Size(StringSplit(left, Literal(\",\"), Literal(-1)))\n      )\n    )\n  }\n\n  override def flatArguments: Iterator[Any] = Iterator(left, right)\n  override def exprsReplaced: Seq[Expression] = Seq(left, right)\n  // 用上面实现的Express进行替换\t\n  override protected def withNewChildInternal(newChild: Expression): MLBStrPivot = copy(child = newChild)\n}\n```","source":"_posts/bigdata/spark/利用RuntimeReplaceable实现Spark Native function.md","raw":"---\ntitle: 利用RuntimeReplaceable实现Spark Native function\ntags:\n  - spark\ncategories:\n  - - bigdata\n    - spark\nabbrlink: 7631\ndate: 2023-09-27 15:55:27\nupdated: 2023-09-27 15:55:27\ncover:\ntop_img:\ndescription:\nkeywords:\n---\n\n## 关于Spark Native Function\n\n在Spark中实现自定义函数，有多种方式：\n\n- 1、实现Hive UDF，Spark是兼容Hive UDF的，简单易用，支持纯SQL环境，因此这可能是使用最为广泛的。\n- 2、实现Spark-SQL UDF，需要嵌入到代码中，因此也主要用在代码中，目前还不支持纯SQL环境。\n- 3、通过拓展SparkSessionExtensions，基本等价于Spark Built-in内置函数，可以充分利用Spark catalyst优化器和Codegen，从而带来可观的性能提升，这里称之为Spark Native Function。但是这种方式也是实现最为困难的，需要对SQL解析器、优化器等有一定的理解。同时网上关于这种方式的资料几乎没有，Spark官方文档中也是根本没有提及这种方式，足以说明这种方式较高的门槛。\n\n## 应用场景：RuntimeReplaceable\n\nSpark已经内置足够多的UDF，已经可以满足绝大部分的应用场景。\n\n剩下的不能满足的应用场景中，其中很大一部分可以通过组合这些内置的函数，来满足。因此也就带来一个问题，就是有时候应用场景非常复杂，需要组合几十种函数，而Spark-SQL也不支持存储过程，最后导致SQL非常长，难以理解阅读，从而难以维护。\n\n而通过实现`RuntimeReplaceable`类型Spark Native Function，可以完美的解决我们的问题。`RuntimeReplaceable`是通过用我们自定义的函数Express替换掉抽象语法树中的函数Express，主要用于兼容不同数据库系统函数别名，也正好满足我们的应用场景。\n\n\n\n## 上代码\n\n在这个例子中，我们实现了一个`str_pivot` Spark Native Function，该函数解决的应用场景如下：\n\n> 有这样一个用逗号分隔的字符串`c1,c2,c3`包含三个元素c1、c2、c3，这三个元素通过排列组合，顺序不同也是一种组合，共有16中组合，例如：c1，c1c2，c2c1，c1c2c3等等。\n>\n> 给出另一个字符串`c2c1`，判断这个字符串是不是其中一个排列组合。这就是`str_pivot`函数要实现的。\n>\n> 我们可以通过下面这个算法实现：\n>\n> `size(array_union(array('1', '2', '3'), array('2','1'))) = size(array('1', '2', '3'))`\n\n### driver\n\n```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.execution.CodegenMode\n\n/**\n * org.apache.spark.sql.catalyst.analysis.FunctionRegistry\n *\n * org.apache.spark.sql.catalyst.expressions.Length\n *\n * -- CodeGen\n * org.apache.spark.sql.catalyst.expressions.UnaryMathExpression\n */\nobject StringPiovtFunctionDriver {\n  val sql = \"select str_pivot('1,2,3,4', '1,2')\"\n  val sql_udf = \"select str_pivot_udf('1,2,3,4', '1,2')\"\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder()\n      .master(\"local[1]\")\n      .appName(\"SparkNativeFunctionInject\")\n      .withExtensions(new FunctionSparkExtension)\n      .getOrCreate()\n\t\n    // UDF方式实现，对比执行计划等\n    spark.udf.register(\"str_pivot_udf\",\n      (left: String, right: String) => {\n        left.split(\",\").union(right.split(\",\")).toSet.size == left.split(\",\").length\n      }\n    )\n\n    spark.sql(sql).show()\n    spark.sql(sql).explain(true)\n    spark.sql(sql).explain(CodegenMode.name)\n\n    spark.sql(sql_udf).show()\n    spark.sql(sql_udf).explain(true)\n    spark.sql(sql_udf).explain(CodegenMode.name)\n\n  }\n}\n\n```\n\n### 拓展SparkSessionExtensions，injectFunction\n\n```scala\nimport org.apache.spark.sql.catalyst.FunctionIdentifier\nimport org.apache.spark.sql.catalyst.analysis.FunctionRegistry.FunctionBuilder\nimport org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionInfo}\nimport org.apache.spark.sql.{MLBStrPivot, SparkSessionExtensions, StringLength, StringPivot}\n\nclass FunctionSparkExtension extends (SparkSessionExtensions => Unit){\n  override def apply(extensions: SparkSessionExtensions): Unit = {\n    extensions.injectFunction(\n      (new FunctionIdentifier(\"str_pivot\"),\n        new ExpressionInfo(classOf[MLBStrPivot].getName,\n          \"str_pivot\"),\n        (children: Seq[Expression]) => new MLBStrPivot(children.head, children(1))))\n  }\n}\n```\n\n### Function Implement\n\n```scala\n// left is fully string\ncase class MLBStrPivot(left: Expression, right: Expression, child: Expression) extends RuntimeReplaceable {\n\n  //size(array_union(array('1', '2', '3'), array('2','1'))) = size(array('1', '2', '3'))\n  def this(left: Expression, right: Expression) = {\n    this(left, right,  \n    EqualTo\n      (\n        Size(ArrayUnion(StringSplit(left, Literal(\",\"), Literal(-1)), StringSplit(right, Literal(\",\"), Literal(-1))), false),\n        Size(StringSplit(left, Literal(\",\"), Literal(-1)))\n      )\n    )\n  }\n\n  override def flatArguments: Iterator[Any] = Iterator(left, right)\n  override def exprsReplaced: Seq[Expression] = Seq(left, right)\n  // 用上面实现的Express进行替换\t\n  override protected def withNewChildInternal(newChild: Expression): MLBStrPivot = copy(child = newChild)\n}\n```","slug":"bigdata/spark/利用RuntimeReplaceable实现Spark Native function","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt4004c8j5m1wvidu30","content":"<h2 id=\"关于Spark-Native-Function\"><a href=\"#关于Spark-Native-Function\" class=\"headerlink\" title=\"关于Spark Native Function\"></a>关于Spark Native Function</h2><p>在Spark中实现自定义函数，有多种方式：</p>\n<ul>\n<li>1、实现Hive UDF，Spark是兼容Hive UDF的，简单易用，支持纯SQL环境，因此这可能是使用最为广泛的。</li>\n<li>2、实现Spark-SQL UDF，需要嵌入到代码中，因此也主要用在代码中，目前还不支持纯SQL环境。</li>\n<li>3、通过拓展SparkSessionExtensions，基本等价于Spark Built-in内置函数，可以充分利用Spark catalyst优化器和Codegen，从而带来可观的性能提升，这里称之为Spark Native Function。但是这种方式也是实现最为困难的，需要对SQL解析器、优化器等有一定的理解。同时网上关于这种方式的资料几乎没有，Spark官方文档中也是根本没有提及这种方式，足以说明这种方式较高的门槛。</li>\n</ul>\n<h2 id=\"应用场景：RuntimeReplaceable\"><a href=\"#应用场景：RuntimeReplaceable\" class=\"headerlink\" title=\"应用场景：RuntimeReplaceable\"></a>应用场景：RuntimeReplaceable</h2><p>Spark已经内置足够多的UDF，已经可以满足绝大部分的应用场景。</p>\n<p>剩下的不能满足的应用场景中，其中很大一部分可以通过组合这些内置的函数，来满足。因此也就带来一个问题，就是有时候应用场景非常复杂，需要组合几十种函数，而Spark-SQL也不支持存储过程，最后导致SQL非常长，难以理解阅读，从而难以维护。</p>\n<p>而通过实现<code>RuntimeReplaceable</code>类型Spark Native Function，可以完美的解决我们的问题。<code>RuntimeReplaceable</code>是通过用我们自定义的函数Express替换掉抽象语法树中的函数Express，主要用于兼容不同数据库系统函数别名，也正好满足我们的应用场景。</p>\n<h2 id=\"上代码\"><a href=\"#上代码\" class=\"headerlink\" title=\"上代码\"></a>上代码</h2><p>在这个例子中，我们实现了一个<code>str_pivot</code> Spark Native Function，该函数解决的应用场景如下：</p>\n<blockquote>\n<p>有这样一个用逗号分隔的字符串<code>c1,c2,c3</code>包含三个元素c1、c2、c3，这三个元素通过排列组合，顺序不同也是一种组合，共有16中组合，例如：c1，c1c2，c2c1，c1c2c3等等。</p>\n<p>给出另一个字符串<code>c2c1</code>，判断这个字符串是不是其中一个排列组合。这就是<code>str_pivot</code>函数要实现的。</p>\n<p>我们可以通过下面这个算法实现：</p>\n<p><code>size(array_union(array(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;), array(&#39;2&#39;,&#39;1&#39;))) = size(array(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;))</code></p>\n</blockquote>\n<h3 id=\"driver\"><a href=\"#driver\" class=\"headerlink\" title=\"driver\"></a>driver</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.execution.<span class=\"type\">CodegenMode</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.analysis.FunctionRegistry</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.expressions.Length</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * -- CodeGen</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.expressions.UnaryMathExpression</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">StringPiovtFunctionDriver</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> sql = <span class=\"string\">&quot;select str_pivot(&#x27;1,2,3,4&#x27;, &#x27;1,2&#x27;)&quot;</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> sql_udf = <span class=\"string\">&quot;select str_pivot_udf(&#x27;1,2,3,4&#x27;, &#x27;1,2&#x27;)&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .master(<span class=\"string\">&quot;local[1]&quot;</span>)</span><br><span class=\"line\">      .appName(<span class=\"string\">&quot;SparkNativeFunctionInject&quot;</span>)</span><br><span class=\"line\">      .withExtensions(<span class=\"keyword\">new</span> <span class=\"type\">FunctionSparkExtension</span>)</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"comment\">// UDF方式实现，对比执行计划等</span></span><br><span class=\"line\">    spark.udf.register(<span class=\"string\">&quot;str_pivot_udf&quot;</span>,</span><br><span class=\"line\">      (left: <span class=\"type\">String</span>, right: <span class=\"type\">String</span>) =&gt; &#123;</span><br><span class=\"line\">        left.split(<span class=\"string\">&quot;,&quot;</span>).union(right.split(<span class=\"string\">&quot;,&quot;</span>)).toSet.size == left.split(<span class=\"string\">&quot;,&quot;</span>).length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    spark.sql(sql).show()</span><br><span class=\"line\">    spark.sql(sql).explain(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    spark.sql(sql).explain(<span class=\"type\">CodegenMode</span>.name)</span><br><span class=\"line\"></span><br><span class=\"line\">    spark.sql(sql_udf).show()</span><br><span class=\"line\">    spark.sql(sql_udf).explain(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    spark.sql(sql_udf).explain(<span class=\"type\">CodegenMode</span>.name)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"拓展SparkSessionExtensions，injectFunction\"><a href=\"#拓展SparkSessionExtensions，injectFunction\" class=\"headerlink\" title=\"拓展SparkSessionExtensions，injectFunction\"></a>拓展SparkSessionExtensions，injectFunction</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.<span class=\"type\">FunctionIdentifier</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.analysis.<span class=\"type\">FunctionRegistry</span>.<span class=\"type\">FunctionBuilder</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class=\"type\">Expression</span>, <span class=\"type\">ExpressionInfo</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.&#123;<span class=\"type\">MLBStrPivot</span>, <span class=\"type\">SparkSessionExtensions</span>, <span class=\"type\">StringLength</span>, <span class=\"type\">StringPivot</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FunctionSparkExtension</span> <span class=\"keyword\">extends</span> (<span class=\"params\"><span class=\"type\">SparkSessionExtensions</span> =&gt; <span class=\"type\">Unit</span></span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(extensions: <span class=\"type\">SparkSessionExtensions</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    extensions.injectFunction(</span><br><span class=\"line\">      (<span class=\"keyword\">new</span> <span class=\"type\">FunctionIdentifier</span>(<span class=\"string\">&quot;str_pivot&quot;</span>),</span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"type\">ExpressionInfo</span>(classOf[<span class=\"type\">MLBStrPivot</span>].getName,</span><br><span class=\"line\">          <span class=\"string\">&quot;str_pivot&quot;</span>),</span><br><span class=\"line\">        (children: <span class=\"type\">Seq</span>[<span class=\"type\">Expression</span>]) =&gt; <span class=\"keyword\">new</span> <span class=\"type\">MLBStrPivot</span>(children.head, children(<span class=\"number\">1</span>))))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Function-Implement\"><a href=\"#Function-Implement\" class=\"headerlink\" title=\"Function Implement\"></a>Function Implement</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// left is fully string</span></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLBStrPivot</span>(<span class=\"params\">left: <span class=\"type\">Expression</span>, right: <span class=\"type\">Expression</span>, child: <span class=\"type\">Expression</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">RuntimeReplaceable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//size(array_union(array(&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;), array(&#x27;2&#x27;,&#x27;1&#x27;))) = size(array(&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;))</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(left: <span class=\"type\">Expression</span>, right: <span class=\"type\">Expression</span>) = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(left, right,  </span><br><span class=\"line\">    <span class=\"type\">EqualTo</span></span><br><span class=\"line\">      (</span><br><span class=\"line\">        <span class=\"type\">Size</span>(<span class=\"type\">ArrayUnion</span>(<span class=\"type\">StringSplit</span>(left, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>)), <span class=\"type\">StringSplit</span>(right, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>))), <span class=\"literal\">false</span>),</span><br><span class=\"line\">        <span class=\"type\">Size</span>(<span class=\"type\">StringSplit</span>(left, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>)))</span><br><span class=\"line\">      )</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatArguments</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Any</span>] = <span class=\"type\">Iterator</span>(left, right)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">exprsReplaced</span></span>: <span class=\"type\">Seq</span>[<span class=\"type\">Expression</span>] = <span class=\"type\">Seq</span>(left, right)</span><br><span class=\"line\">  <span class=\"comment\">// 用上面实现的Express进行替换\t</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">withNewChildInternal</span></span>(newChild: <span class=\"type\">Expression</span>): <span class=\"type\">MLBStrPivot</span> = copy(child = newChild)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"关于Spark-Native-Function\"><a href=\"#关于Spark-Native-Function\" class=\"headerlink\" title=\"关于Spark Native Function\"></a>关于Spark Native Function</h2><p>在Spark中实现自定义函数，有多种方式：</p>\n<ul>\n<li>1、实现Hive UDF，Spark是兼容Hive UDF的，简单易用，支持纯SQL环境，因此这可能是使用最为广泛的。</li>\n<li>2、实现Spark-SQL UDF，需要嵌入到代码中，因此也主要用在代码中，目前还不支持纯SQL环境。</li>\n<li>3、通过拓展SparkSessionExtensions，基本等价于Spark Built-in内置函数，可以充分利用Spark catalyst优化器和Codegen，从而带来可观的性能提升，这里称之为Spark Native Function。但是这种方式也是实现最为困难的，需要对SQL解析器、优化器等有一定的理解。同时网上关于这种方式的资料几乎没有，Spark官方文档中也是根本没有提及这种方式，足以说明这种方式较高的门槛。</li>\n</ul>\n<h2 id=\"应用场景：RuntimeReplaceable\"><a href=\"#应用场景：RuntimeReplaceable\" class=\"headerlink\" title=\"应用场景：RuntimeReplaceable\"></a>应用场景：RuntimeReplaceable</h2><p>Spark已经内置足够多的UDF，已经可以满足绝大部分的应用场景。</p>\n<p>剩下的不能满足的应用场景中，其中很大一部分可以通过组合这些内置的函数，来满足。因此也就带来一个问题，就是有时候应用场景非常复杂，需要组合几十种函数，而Spark-SQL也不支持存储过程，最后导致SQL非常长，难以理解阅读，从而难以维护。</p>\n<p>而通过实现<code>RuntimeReplaceable</code>类型Spark Native Function，可以完美的解决我们的问题。<code>RuntimeReplaceable</code>是通过用我们自定义的函数Express替换掉抽象语法树中的函数Express，主要用于兼容不同数据库系统函数别名，也正好满足我们的应用场景。</p>\n<h2 id=\"上代码\"><a href=\"#上代码\" class=\"headerlink\" title=\"上代码\"></a>上代码</h2><p>在这个例子中，我们实现了一个<code>str_pivot</code> Spark Native Function，该函数解决的应用场景如下：</p>\n<blockquote>\n<p>有这样一个用逗号分隔的字符串<code>c1,c2,c3</code>包含三个元素c1、c2、c3，这三个元素通过排列组合，顺序不同也是一种组合，共有16中组合，例如：c1，c1c2，c2c1，c1c2c3等等。</p>\n<p>给出另一个字符串<code>c2c1</code>，判断这个字符串是不是其中一个排列组合。这就是<code>str_pivot</code>函数要实现的。</p>\n<p>我们可以通过下面这个算法实现：</p>\n<p><code>size(array_union(array(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;), array(&#39;2&#39;,&#39;1&#39;))) = size(array(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;))</code></p>\n</blockquote>\n<h3 id=\"driver\"><a href=\"#driver\" class=\"headerlink\" title=\"driver\"></a>driver</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.execution.<span class=\"type\">CodegenMode</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.analysis.FunctionRegistry</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.expressions.Length</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * -- CodeGen</span></span><br><span class=\"line\"><span class=\"comment\"> * org.apache.spark.sql.catalyst.expressions.UnaryMathExpression</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">StringPiovtFunctionDriver</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> sql = <span class=\"string\">&quot;select str_pivot(&#x27;1,2,3,4&#x27;, &#x27;1,2&#x27;)&quot;</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> sql_udf = <span class=\"string\">&quot;select str_pivot_udf(&#x27;1,2,3,4&#x27;, &#x27;1,2&#x27;)&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .master(<span class=\"string\">&quot;local[1]&quot;</span>)</span><br><span class=\"line\">      .appName(<span class=\"string\">&quot;SparkNativeFunctionInject&quot;</span>)</span><br><span class=\"line\">      .withExtensions(<span class=\"keyword\">new</span> <span class=\"type\">FunctionSparkExtension</span>)</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"comment\">// UDF方式实现，对比执行计划等</span></span><br><span class=\"line\">    spark.udf.register(<span class=\"string\">&quot;str_pivot_udf&quot;</span>,</span><br><span class=\"line\">      (left: <span class=\"type\">String</span>, right: <span class=\"type\">String</span>) =&gt; &#123;</span><br><span class=\"line\">        left.split(<span class=\"string\">&quot;,&quot;</span>).union(right.split(<span class=\"string\">&quot;,&quot;</span>)).toSet.size == left.split(<span class=\"string\">&quot;,&quot;</span>).length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    spark.sql(sql).show()</span><br><span class=\"line\">    spark.sql(sql).explain(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    spark.sql(sql).explain(<span class=\"type\">CodegenMode</span>.name)</span><br><span class=\"line\"></span><br><span class=\"line\">    spark.sql(sql_udf).show()</span><br><span class=\"line\">    spark.sql(sql_udf).explain(<span class=\"literal\">true</span>)</span><br><span class=\"line\">    spark.sql(sql_udf).explain(<span class=\"type\">CodegenMode</span>.name)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"拓展SparkSessionExtensions，injectFunction\"><a href=\"#拓展SparkSessionExtensions，injectFunction\" class=\"headerlink\" title=\"拓展SparkSessionExtensions，injectFunction\"></a>拓展SparkSessionExtensions，injectFunction</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.<span class=\"type\">FunctionIdentifier</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.analysis.<span class=\"type\">FunctionRegistry</span>.<span class=\"type\">FunctionBuilder</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class=\"type\">Expression</span>, <span class=\"type\">ExpressionInfo</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.&#123;<span class=\"type\">MLBStrPivot</span>, <span class=\"type\">SparkSessionExtensions</span>, <span class=\"type\">StringLength</span>, <span class=\"type\">StringPivot</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FunctionSparkExtension</span> <span class=\"keyword\">extends</span> (<span class=\"params\"><span class=\"type\">SparkSessionExtensions</span> =&gt; <span class=\"type\">Unit</span></span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(extensions: <span class=\"type\">SparkSessionExtensions</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    extensions.injectFunction(</span><br><span class=\"line\">      (<span class=\"keyword\">new</span> <span class=\"type\">FunctionIdentifier</span>(<span class=\"string\">&quot;str_pivot&quot;</span>),</span><br><span class=\"line\">        <span class=\"keyword\">new</span> <span class=\"type\">ExpressionInfo</span>(classOf[<span class=\"type\">MLBStrPivot</span>].getName,</span><br><span class=\"line\">          <span class=\"string\">&quot;str_pivot&quot;</span>),</span><br><span class=\"line\">        (children: <span class=\"type\">Seq</span>[<span class=\"type\">Expression</span>]) =&gt; <span class=\"keyword\">new</span> <span class=\"type\">MLBStrPivot</span>(children.head, children(<span class=\"number\">1</span>))))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Function-Implement\"><a href=\"#Function-Implement\" class=\"headerlink\" title=\"Function Implement\"></a>Function Implement</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// left is fully string</span></span><br><span class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLBStrPivot</span>(<span class=\"params\">left: <span class=\"type\">Expression</span>, right: <span class=\"type\">Expression</span>, child: <span class=\"type\">Expression</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">RuntimeReplaceable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//size(array_union(array(&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;), array(&#x27;2&#x27;,&#x27;1&#x27;))) = size(array(&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;))</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(left: <span class=\"type\">Expression</span>, right: <span class=\"type\">Expression</span>) = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(left, right,  </span><br><span class=\"line\">    <span class=\"type\">EqualTo</span></span><br><span class=\"line\">      (</span><br><span class=\"line\">        <span class=\"type\">Size</span>(<span class=\"type\">ArrayUnion</span>(<span class=\"type\">StringSplit</span>(left, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>)), <span class=\"type\">StringSplit</span>(right, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>))), <span class=\"literal\">false</span>),</span><br><span class=\"line\">        <span class=\"type\">Size</span>(<span class=\"type\">StringSplit</span>(left, <span class=\"type\">Literal</span>(<span class=\"string\">&quot;,&quot;</span>), <span class=\"type\">Literal</span>(<span class=\"number\">-1</span>)))</span><br><span class=\"line\">      )</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatArguments</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Any</span>] = <span class=\"type\">Iterator</span>(left, right)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">exprsReplaced</span></span>: <span class=\"type\">Seq</span>[<span class=\"type\">Expression</span>] = <span class=\"type\">Seq</span>(left, right)</span><br><span class=\"line\">  <span class=\"comment\">// 用上面实现的Express进行替换\t</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">withNewChildInternal</span></span>(newChild: <span class=\"type\">Expression</span>): <span class=\"type\">MLBStrPivot</span> = copy(child = newChild)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Flink-SQL-Client与Hive集成问题指南","abbrlink":5143,"date":"2022-09-13T02:51:08.000Z","updated":"2022-09-13T02:52:08.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n>只需要下载对应的`flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar`,放在{FLINK_HOME}/lib下面即可。\n>\n>不需要下载什么`flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar`放在{FLINK_HOME}/lib下面,现在Flink通过\n>\n>```shell\n>export HADOOP_CLASSPATH=`hadoop classpath`\n>```\n>\n>来寻找Hadoop相关的Jar包。\n>\n>\n>\n>当Flink on YARN时，还需要在{FLINK_HOME}/lib中添加以下依赖：\n>\n>```shell\n>要么是这个：\n>       flink-shaded-hadoop-2-uber-2.7.5-8.0.jar\n>\n>要么是，可以直接从hadoop/share/hadoop/mapreduce/等目录拷过来：\n>      hadoop-common-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-common-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-core-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-hs-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.1.jar\n>```\n>\n>\n\n## ENV\n\n- Flink ：1.14.5\n- Hadoop：3.2.3\n- Hive：3.1.2+\n\n\n\n## Error\n\n- `Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V`\n\n  解压`flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar`,直接将里面的com/google文件夹删除\n\n- 其他找不到Hadoop相关Jar包的错误，\n\n  一般都是因为\n\n  ```shell\n  export HADOOP_CLASSPATH=`hadoop classpath`\n  ```\n\n​      没有设置成功，不仅sql-client.sh要在此环境下启动，Standalone的Flink也要在此环境下启动。\n\n- 执行\n\n  ```shell\n  echo `hadoop classpath`\n  ```\n\n  后，下面这样才算成功\n\n  ```\n  /opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/common/lib:/opt/module/hadoop-3.2.3/share/hadoop/common/sources:/opt/module/hadoop-3.2.3/share/hadoop/common/webapps:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/sources:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/webapps:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib-examples:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib:/opt/module/hadoop-3.2.3/share/hadoop/yarn/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/test:/opt/module/hadoop-3.2.3/share/hadoop/yarn/timelineservice:/opt/module/hadoop-3.2.3/share/hadoop/yarn/webapps:/opt/module/hadoop-3.2.3/share/hadoop/yarn/yarn-service-examples\n  ```\n\n  \n\n  而这样不算成功：\n\n  ```\n  /opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*\n  ```\n\n  \n\n- quit;退出sql-client时报错：\n\n  ```\n  Exception in thread \"Thread-4\" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.\n  ```\n\n  可以在在 flink 配置文件里 flink-conf.yaml设置`classloader.check-leaked-classloader: false`\n","source":"_posts/bigdata/flink/Flink-SQL-Client与Hive集成问题指南.md","raw":"---\ntitle: Flink-SQL-Client与Hive集成问题指南\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\nabbrlink: 5143\ndate: 2022-09-13 10:51:08\nupdated: 2022-09-13 10:52:08\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n>只需要下载对应的`flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar`,放在{FLINK_HOME}/lib下面即可。\n>\n>不需要下载什么`flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar`放在{FLINK_HOME}/lib下面,现在Flink通过\n>\n>```shell\n>export HADOOP_CLASSPATH=`hadoop classpath`\n>```\n>\n>来寻找Hadoop相关的Jar包。\n>\n>\n>\n>当Flink on YARN时，还需要在{FLINK_HOME}/lib中添加以下依赖：\n>\n>```shell\n>要么是这个：\n>       flink-shaded-hadoop-2-uber-2.7.5-8.0.jar\n>\n>要么是，可以直接从hadoop/share/hadoop/mapreduce/等目录拷过来：\n>      hadoop-common-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-common-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-core-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-hs-3.0.0-cdh6.3.1.jar\n>      hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.1.jar\n>```\n>\n>\n\n## ENV\n\n- Flink ：1.14.5\n- Hadoop：3.2.3\n- Hive：3.1.2+\n\n\n\n## Error\n\n- `Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V`\n\n  解压`flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar`,直接将里面的com/google文件夹删除\n\n- 其他找不到Hadoop相关Jar包的错误，\n\n  一般都是因为\n\n  ```shell\n  export HADOOP_CLASSPATH=`hadoop classpath`\n  ```\n\n​      没有设置成功，不仅sql-client.sh要在此环境下启动，Standalone的Flink也要在此环境下启动。\n\n- 执行\n\n  ```shell\n  echo `hadoop classpath`\n  ```\n\n  后，下面这样才算成功\n\n  ```\n  /opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/common/lib:/opt/module/hadoop-3.2.3/share/hadoop/common/sources:/opt/module/hadoop-3.2.3/share/hadoop/common/webapps:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/sources:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/webapps:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib-examples:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib:/opt/module/hadoop-3.2.3/share/hadoop/yarn/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/test:/opt/module/hadoop-3.2.3/share/hadoop/yarn/timelineservice:/opt/module/hadoop-3.2.3/share/hadoop/yarn/webapps:/opt/module/hadoop-3.2.3/share/hadoop/yarn/yarn-service-examples\n  ```\n\n  \n\n  而这样不算成功：\n\n  ```\n  /opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*\n  ```\n\n  \n\n- quit;退出sql-client时报错：\n\n  ```\n  Exception in thread \"Thread-4\" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.\n  ```\n\n  可以在在 flink 配置文件里 flink-conf.yaml设置`classloader.check-leaked-classloader: false`\n","slug":"bigdata/flink/Flink-SQL-Client与Hive集成问题指南","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt4004g8j5m2r4megu8","content":"<blockquote>\n<p>只需要下载对应的<code>flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar</code>,放在{FLINK_HOME}&#x2F;lib下面即可。</p>\n<p>不需要下载什么<code>flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar</code>放在{FLINK_HOME}&#x2F;lib下面,现在Flink通过</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\"><span class=\"built_in\">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span></span><br></pre></td></tr></table></figure>\n\n<p>来寻找Hadoop相关的Jar包。</p>\n<p>当Flink on YARN时，还需要在{FLINK_HOME}&#x2F;lib中添加以下依赖：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">要么是这个：</span></span><br><span class=\"line\">      flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">要么是，可以直接从hadoop/share/hadoop/mapreduce/等目录拷过来：</span></span><br><span class=\"line\">     hadoop-common-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-common-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-core-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-hs-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.1.jar</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h2 id=\"ENV\"><a href=\"#ENV\" class=\"headerlink\" title=\"ENV\"></a>ENV</h2><ul>\n<li>Flink ：1.14.5</li>\n<li>Hadoop：3.2.3</li>\n<li>Hive：3.1.2+</li>\n</ul>\n<h2 id=\"Error\"><a href=\"#Error\" class=\"headerlink\" title=\"Error\"></a>Error</h2><ul>\n<li><p><code>Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</code></p>\n<p>解压<code>flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar</code>,直接将里面的com&#x2F;google文件夹删除</p>\n</li>\n<li><p>其他找不到Hadoop相关Jar包的错误，</p>\n<p>一般都是因为</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>​      没有设置成功，不仅sql-client.sh要在此环境下启动，Standalone的Flink也要在此环境下启动。</p>\n<ul>\n<li><p>执行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo `hadoop classpath`</span><br></pre></td></tr></table></figure>\n\n<p>后，下面这样才算成功</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/common/lib:/opt/module/hadoop-3.2.3/share/hadoop/common/sources:/opt/module/hadoop-3.2.3/share/hadoop/common/webapps:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/sources:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/webapps:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib-examples:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib:/opt/module/hadoop-3.2.3/share/hadoop/yarn/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/test:/opt/module/hadoop-3.2.3/share/hadoop/yarn/timelineservice:/opt/module/hadoop-3.2.3/share/hadoop/yarn/webapps:/opt/module/hadoop-3.2.3/share/hadoop/yarn/yarn-service-examples</span><br></pre></td></tr></table></figure>\n\n\n\n<p>而这样不算成功：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>quit;退出sql-client时报错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;Thread-4&quot; java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.</span><br></pre></td></tr></table></figure>\n\n<p>可以在在 flink 配置文件里 flink-conf.yaml设置<code>classloader.check-leaked-classloader: false</code></p>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>只需要下载对应的<code>flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar</code>,放在{FLINK_HOME}&#x2F;lib下面即可。</p>\n<p>不需要下载什么<code>flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar</code>放在{FLINK_HOME}&#x2F;lib下面,现在Flink通过</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\"><span class=\"built_in\">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span></span><br></pre></td></tr></table></figure>\n\n<p>来寻找Hadoop相关的Jar包。</p>\n<p>当Flink on YARN时，还需要在{FLINK_HOME}&#x2F;lib中添加以下依赖：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">要么是这个：</span></span><br><span class=\"line\">      flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">要么是，可以直接从hadoop/share/hadoop/mapreduce/等目录拷过来：</span></span><br><span class=\"line\">     hadoop-common-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-common-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-core-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-hs-3.0.0-cdh6.3.1.jar</span><br><span class=\"line\">     hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.1.jar</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h2 id=\"ENV\"><a href=\"#ENV\" class=\"headerlink\" title=\"ENV\"></a>ENV</h2><ul>\n<li>Flink ：1.14.5</li>\n<li>Hadoop：3.2.3</li>\n<li>Hive：3.1.2+</li>\n</ul>\n<h2 id=\"Error\"><a href=\"#Error\" class=\"headerlink\" title=\"Error\"></a>Error</h2><ul>\n<li><p><code>Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</code></p>\n<p>解压<code>flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar</code>,直接将里面的com&#x2F;google文件夹删除</p>\n</li>\n<li><p>其他找不到Hadoop相关Jar包的错误，</p>\n<p>一般都是因为</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>​      没有设置成功，不仅sql-client.sh要在此环境下启动，Standalone的Flink也要在此环境下启动。</p>\n<ul>\n<li><p>执行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo `hadoop classpath`</span><br></pre></td></tr></table></figure>\n\n<p>后，下面这样才算成功</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/common/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/common/lib:/opt/module/hadoop-3.2.3/share/hadoop/common/sources:/opt/module/hadoop-3.2.3/share/hadoop/common/webapps:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/sources:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/webapps:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/jdiff:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib-examples:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib:/opt/module/hadoop-3.2.3/share/hadoop/yarn/sources:/opt/module/hadoop-3.2.3/share/hadoop/yarn/test:/opt/module/hadoop-3.2.3/share/hadoop/yarn/timelineservice:/opt/module/hadoop-3.2.3/share/hadoop/yarn/webapps:/opt/module/hadoop-3.2.3/share/hadoop/yarn/yarn-service-examples</span><br></pre></td></tr></table></figure>\n\n\n\n<p>而这样不算成功：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/module/hadoop-3.2.3/etc/hadoop:/opt/module/hadoop-3.2.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/common/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn:/opt/module/hadoop-3.2.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.3/share/hadoop/yarn/*</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>quit;退出sql-client时报错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;Thread-4&quot; java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration &#x27;classloader.check-leaked-classloader&#x27;.</span><br></pre></td></tr></table></figure>\n\n<p>可以在在 flink 配置文件里 flink-conf.yaml设置<code>classloader.check-leaked-classloader: false</code></p>\n</li>\n</ul>\n"},{"title":"初入Flink Table && SQL","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":46784,"date":"2022-08-13T09:55:05.000Z","updated":"2022-08-13T09:55:05.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n# QuickStart\n\n- Table API 和 SQL 需要引入的依赖有两个：planner 和 bridge。\n\n  ```xml\n      <dependency>\n          <groupId>org.apache.flink</groupId>\n          <artifactId>flink-table-api-scala-bridge_${scala.version}</artifactId>\n          <version>${flink.version}</version>\n      </dependency>\n      <dependency>\n          <groupId>org.apache.flink</groupId>\n          <artifactId>flink-table-planner_${scala.version}</artifactId>\n          <version>${flink.version}</version>\n      </dependency>\n  ```\n\n- > 老版本planner已经被废除，只剩下blink\n  >\n  > The old planner has been removed in Flink 1.14. Please upgrade your table program to use the default planner (previously called the 'blink' planner).\n\n# Flink CDC SQL Demo\n\n- 1、下载Flink，下载`flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar`依赖包，并将它们放到目录 `{flink_home}/lib/` 下.\n\n- 2、在 MySQL 数据库中准备数据，创建数据库和表 `products`，`orders`，并插入数据\n\n  ```sql\n  -- MySQL\n  CREATE DATABASE mydb;\n  USE mydb;\n  CREATE TABLE products (\n    id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description VARCHAR(512)\n  );\n  ALTER TABLE products AUTO_INCREMENT = 101;\n  \n  INSERT INTO products\n  VALUES (default,\"scooter\",\"Small 2-wheel scooter\"),\n         (default,\"car battery\",\"12V car battery\"),\n         (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40 to #3\"),\n         (default,\"hammer\",\"12oz carpenter's hammer\"),\n         (default,\"hammer\",\"14oz carpenter's hammer\"),\n         (default,\"hammer\",\"16oz carpenter's hammer\"),\n         (default,\"rocks\",\"box of assorted rocks\"),\n         (default,\"jacket\",\"water resistent black wind breaker\"),\n         (default,\"spare tire\",\"24 inch spare tire\");\n  \n  CREATE TABLE orders (\n    order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    order_date DATETIME NOT NULL,\n    customer_name VARCHAR(255) NOT NULL,\n    price DECIMAL(10, 5) NOT NULL,\n    product_id INTEGER NOT NULL,\n    order_status BOOLEAN NOT NULL -- Whether order has been placed\n  ) AUTO_INCREMENT = 10001;\n  \n  INSERT INTO orders\n  VALUES (default, '2020-07-30 10:08:22', 'Jark', 50.50, 102, false),\n         (default, '2020-07-30 10:11:09', 'Sally', 15.00, 105, false),\n         (default, '2020-07-30 12:00:30', 'Edward', 25.25, 106, false);\n  ```\n\n- 3、启动 Flink 集群和 Flink SQL CLI\n\n  > ./bin/start-cluster.sh\n  > ./bin/sql-client.sh\n\n- 4、在 Flink SQL CLI 中使用 Flink DDL 创建表\n\n  >首先，开启 checkpoint，每隔3秒做一次 checkpoint\n  >\n  >```\n  >-- Flink SQL                   \n  >Flink SQL> SET execution.checkpointing.interval = 3s;\n  >```\n  >\n  >然后, 对于数据库中的表 `products`, `orders`, `shipments`， 使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据\n  >\n  >```sql\n  >-- Flink SQL\n  >Flink SQL> CREATE TABLE products (\n  >    id INT,\n  >    name STRING,\n  >    description STRING,\n  >    PRIMARY KEY (id) NOT ENFORCED\n  >  ) WITH (\n  >    'connector' = 'mysql-cdc',\n  >    'hostname' = 'localhost',\n  >    'port' = '3306',\n  >    'username' = 'root',\n  >    'password' = '1234',\n  >    'database-name' = 'mydb',\n  >    'table-name' = 'products'\n  >  );\n  >\n  >Flink SQL> CREATE TABLE orders (\n  >   order_id INT,\n  >   order_date TIMESTAMP(0),\n  >   customer_name STRING,\n  >   price DECIMAL(10, 5),\n  >   product_id INT,\n  >   order_status BOOLEAN,\n  >   PRIMARY KEY (order_id) NOT ENFORCED\n  > ) WITH (\n  >   'connector' = 'mysql-cdc',\n  >   'hostname' = 'localhost',\n  >   'port' = '3306',\n  >   'username' = 'root',\n  >   'password' = '1234',\n  >   'database-name' = 'mydb',\n  >   'table-name' = 'orders'\n  > );\n  >```\n\n\n\n# SQL Client\n\n- CLI 为维护和可视化结果提供**三种模式**。\n\n- **表格模式**（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'table';\n  ```\n\n  **变更日志模式**（changelog mode）不会实体化和可视化结果，而是由插入（`+`）和撤销（`-`）组成的持续查询产生结果流。\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'changelog';\n  ```\n\n  **Tableau模式**（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(`execution.type`)：\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'tableau';\n  ```\n\n\n\n# MySQL开启binlog\n\n- 找到my.cnf文件\n\n  > mysql --help | grep 'Default options' -A 1\n\n```yaml\n#第一种方式:\n#开启binlog日志\nlog_bin=ON\n#binlog日志的基本文件名\nlog_bin_basename=/var/lib/mysql/mysql-bin\n#binlog文件的索引文件，管理所有binlog文件\nlog_bin_index=/var/lib/mysql/mysql-bin.index\n#配置serverid\nserver-id=1\n\n#第二种方式:\n#此一行等同于上面log_bin三行\nlog-bin=/var/lib/mysql/mysql-bin\n#配置serverid\nserver-id=1\n\n# Demo\nserver-id=1\nlog-bin=mysql-bin\nbinlog_format=row\nbinlog-do-db=mydb\n```\n\n\n\n# Code Repo\n\n- ```scala\n      // 从命令参数中读取hostname和port\n      val paramTool: ParameterTool = ParameterTool.fromArgs(args)\n      val hostname: String = paramTool.get(\"host\")\n      val port: Int = paramTool.getInt(\"port\")\n  ```\n","source":"_posts/bigdata/flink/初入Flink-Table@SQL.md","raw":"---\ntitle: 初入Flink Table && SQL\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 46784\ndate: 2022-08-13 17:55:05\nupdated: 2022-08-13 17:55:05\ncover:\ndescription:\nkeywords:\n---\n\n# QuickStart\n\n- Table API 和 SQL 需要引入的依赖有两个：planner 和 bridge。\n\n  ```xml\n      <dependency>\n          <groupId>org.apache.flink</groupId>\n          <artifactId>flink-table-api-scala-bridge_${scala.version}</artifactId>\n          <version>${flink.version}</version>\n      </dependency>\n      <dependency>\n          <groupId>org.apache.flink</groupId>\n          <artifactId>flink-table-planner_${scala.version}</artifactId>\n          <version>${flink.version}</version>\n      </dependency>\n  ```\n\n- > 老版本planner已经被废除，只剩下blink\n  >\n  > The old planner has been removed in Flink 1.14. Please upgrade your table program to use the default planner (previously called the 'blink' planner).\n\n# Flink CDC SQL Demo\n\n- 1、下载Flink，下载`flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar`依赖包，并将它们放到目录 `{flink_home}/lib/` 下.\n\n- 2、在 MySQL 数据库中准备数据，创建数据库和表 `products`，`orders`，并插入数据\n\n  ```sql\n  -- MySQL\n  CREATE DATABASE mydb;\n  USE mydb;\n  CREATE TABLE products (\n    id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description VARCHAR(512)\n  );\n  ALTER TABLE products AUTO_INCREMENT = 101;\n  \n  INSERT INTO products\n  VALUES (default,\"scooter\",\"Small 2-wheel scooter\"),\n         (default,\"car battery\",\"12V car battery\"),\n         (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40 to #3\"),\n         (default,\"hammer\",\"12oz carpenter's hammer\"),\n         (default,\"hammer\",\"14oz carpenter's hammer\"),\n         (default,\"hammer\",\"16oz carpenter's hammer\"),\n         (default,\"rocks\",\"box of assorted rocks\"),\n         (default,\"jacket\",\"water resistent black wind breaker\"),\n         (default,\"spare tire\",\"24 inch spare tire\");\n  \n  CREATE TABLE orders (\n    order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    order_date DATETIME NOT NULL,\n    customer_name VARCHAR(255) NOT NULL,\n    price DECIMAL(10, 5) NOT NULL,\n    product_id INTEGER NOT NULL,\n    order_status BOOLEAN NOT NULL -- Whether order has been placed\n  ) AUTO_INCREMENT = 10001;\n  \n  INSERT INTO orders\n  VALUES (default, '2020-07-30 10:08:22', 'Jark', 50.50, 102, false),\n         (default, '2020-07-30 10:11:09', 'Sally', 15.00, 105, false),\n         (default, '2020-07-30 12:00:30', 'Edward', 25.25, 106, false);\n  ```\n\n- 3、启动 Flink 集群和 Flink SQL CLI\n\n  > ./bin/start-cluster.sh\n  > ./bin/sql-client.sh\n\n- 4、在 Flink SQL CLI 中使用 Flink DDL 创建表\n\n  >首先，开启 checkpoint，每隔3秒做一次 checkpoint\n  >\n  >```\n  >-- Flink SQL                   \n  >Flink SQL> SET execution.checkpointing.interval = 3s;\n  >```\n  >\n  >然后, 对于数据库中的表 `products`, `orders`, `shipments`， 使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据\n  >\n  >```sql\n  >-- Flink SQL\n  >Flink SQL> CREATE TABLE products (\n  >    id INT,\n  >    name STRING,\n  >    description STRING,\n  >    PRIMARY KEY (id) NOT ENFORCED\n  >  ) WITH (\n  >    'connector' = 'mysql-cdc',\n  >    'hostname' = 'localhost',\n  >    'port' = '3306',\n  >    'username' = 'root',\n  >    'password' = '1234',\n  >    'database-name' = 'mydb',\n  >    'table-name' = 'products'\n  >  );\n  >\n  >Flink SQL> CREATE TABLE orders (\n  >   order_id INT,\n  >   order_date TIMESTAMP(0),\n  >   customer_name STRING,\n  >   price DECIMAL(10, 5),\n  >   product_id INT,\n  >   order_status BOOLEAN,\n  >   PRIMARY KEY (order_id) NOT ENFORCED\n  > ) WITH (\n  >   'connector' = 'mysql-cdc',\n  >   'hostname' = 'localhost',\n  >   'port' = '3306',\n  >   'username' = 'root',\n  >   'password' = '1234',\n  >   'database-name' = 'mydb',\n  >   'table-name' = 'orders'\n  > );\n  >```\n\n\n\n# SQL Client\n\n- CLI 为维护和可视化结果提供**三种模式**。\n\n- **表格模式**（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'table';\n  ```\n\n  **变更日志模式**（changelog mode）不会实体化和可视化结果，而是由插入（`+`）和撤销（`-`）组成的持续查询产生结果流。\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'changelog';\n  ```\n\n  **Tableau模式**（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(`execution.type`)：\n\n  ```text\n  SET 'sql-client.execution.result-mode' = 'tableau';\n  ```\n\n\n\n# MySQL开启binlog\n\n- 找到my.cnf文件\n\n  > mysql --help | grep 'Default options' -A 1\n\n```yaml\n#第一种方式:\n#开启binlog日志\nlog_bin=ON\n#binlog日志的基本文件名\nlog_bin_basename=/var/lib/mysql/mysql-bin\n#binlog文件的索引文件，管理所有binlog文件\nlog_bin_index=/var/lib/mysql/mysql-bin.index\n#配置serverid\nserver-id=1\n\n#第二种方式:\n#此一行等同于上面log_bin三行\nlog-bin=/var/lib/mysql/mysql-bin\n#配置serverid\nserver-id=1\n\n# Demo\nserver-id=1\nlog-bin=mysql-bin\nbinlog_format=row\nbinlog-do-db=mydb\n```\n\n\n\n# Code Repo\n\n- ```scala\n      // 从命令参数中读取hostname和port\n      val paramTool: ParameterTool = ParameterTool.fromArgs(args)\n      val hostname: String = paramTool.get(\"host\")\n      val port: Int = paramTool.getInt(\"port\")\n  ```\n","slug":"bigdata/flink/初入Flink-Table@SQL","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004k8j5ma8n7axdy","content":"<h1 id=\"QuickStart\"><a href=\"#QuickStart\" class=\"headerlink\" title=\"QuickStart\"></a>QuickStart</h1><ul>\n<li><p>Table API 和 SQL 需要引入的依赖有两个：planner 和 bridge。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-api-scala-bridge_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><blockquote>\n<p>老版本planner已经被废除，只剩下blink</p>\n<p>The old planner has been removed in Flink 1.14. Please upgrade your table program to use the default planner (previously called the ‘blink’ planner).</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"Flink-CDC-SQL-Demo\"><a href=\"#Flink-CDC-SQL-Demo\" class=\"headerlink\" title=\"Flink CDC SQL Demo\"></a>Flink CDC SQL Demo</h1><ul>\n<li><p>1、下载Flink，下载<code>flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar</code>依赖包，并将它们放到目录 <code>&#123;flink_home&#125;/lib/</code> 下.</p>\n</li>\n<li><p>2、在 MySQL 数据库中准备数据，创建数据库和表 <code>products</code>，<code>orders</code>，并插入数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- MySQL</span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE mydb;</span><br><span class=\"line\">USE mydb;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> products (</span><br><span class=\"line\">  id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">  name <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  description <span class=\"type\">VARCHAR</span>(<span class=\"number\">512</span>)</span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> products AUTO_INCREMENT <span class=\"operator\">=</span> <span class=\"number\">101</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> products</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"keyword\">default</span>,&quot;scooter&quot;,&quot;Small 2-wheel scooter&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;car battery&quot;,&quot;12V car battery&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;12-pack drill bits&quot;,&quot;12-pack of drill bits with sizes ranging from #40 to #3&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;12oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;14oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;16oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;rocks&quot;,&quot;box of assorted rocks&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;jacket&quot;,&quot;water resistent black wind breaker&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;spare tire&quot;,&quot;24 inch spare tire&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> orders (</span><br><span class=\"line\">  order_id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">  order_date DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  customer_name <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  price <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">5</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  product_id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  order_status <span class=\"type\">BOOLEAN</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"comment\">-- Whether order has been placed</span></span><br><span class=\"line\">) AUTO_INCREMENT <span class=\"operator\">=</span> <span class=\"number\">10001</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> orders</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 10:08:22&#x27;</span>, <span class=\"string\">&#x27;Jark&#x27;</span>, <span class=\"number\">50.50</span>, <span class=\"number\">102</span>, <span class=\"literal\">false</span>),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 10:11:09&#x27;</span>, <span class=\"string\">&#x27;Sally&#x27;</span>, <span class=\"number\">15.00</span>, <span class=\"number\">105</span>, <span class=\"literal\">false</span>),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 12:00:30&#x27;</span>, <span class=\"string\">&#x27;Edward&#x27;</span>, <span class=\"number\">25.25</span>, <span class=\"number\">106</span>, <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、启动 Flink 集群和 Flink SQL CLI</p>\n<blockquote>\n<p>.&#x2F;bin&#x2F;start-cluster.sh<br>.&#x2F;bin&#x2F;sql-client.sh</p>\n</blockquote>\n</li>\n<li><p>4、在 Flink SQL CLI 中使用 Flink DDL 创建表</p>\n<blockquote>\n<p>首先，开启 checkpoint，每隔3秒做一次 checkpoint</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;-- Flink SQL                   </span><br><span class=\"line\">&gt;Flink SQL&gt; SET execution.checkpointing.interval = 3s;</span><br></pre></td></tr></table></figure>\n\n<p>然后, 对于数据库中的表 <code>products</code>, <code>orders</code>, <code>shipments</code>， 使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"operator\">&gt;</span><span class=\"comment\">-- Flink SQL</span></span><br><span class=\"line\"> <span class=\"operator\">&gt;</span>Flink <span class=\"keyword\">SQL</span><span class=\"operator\">&gt;</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> products (</span><br><span class=\"line\">   id <span class=\"type\">INT</span>,</span><br><span class=\"line\">   name STRING,</span><br><span class=\"line\">   description STRING,</span><br><span class=\"line\">   <span class=\"keyword\">PRIMARY</span> KEY (id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\"> ) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">   <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mysql-cdc&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;hostname&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;localhost&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;port&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;3306&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1234&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;database-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mydb&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;table-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;products&#x27;</span></span><br><span class=\"line\"> );</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"operator\">&gt;</span>Flink <span class=\"keyword\">SQL</span><span class=\"operator\">&gt;</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> orders (</span><br><span class=\"line\">  order_id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  order_date <span class=\"type\">TIMESTAMP</span>(<span class=\"number\">0</span>),</span><br><span class=\"line\">  customer_name STRING,</span><br><span class=\"line\">  price <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">  product_id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  order_status <span class=\"type\">BOOLEAN</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (order_id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mysql-cdc&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;hostname&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;localhost&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;port&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;3306&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1234&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;database-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mydb&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;table-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;orders&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></blockquote>\n</li>\n</ul>\n<h1 id=\"SQL-Client\"><a href=\"#SQL-Client\" class=\"headerlink\" title=\"SQL Client\"></a>SQL Client</h1><ul>\n<li><p>CLI 为维护和可视化结果提供<strong>三种模式</strong>。</p>\n</li>\n<li><p><strong>表格模式</strong>（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;table&#x27;;</span><br></pre></td></tr></table></figure>\n\n<p><strong>变更日志模式</strong>（changelog mode）不会实体化和可视化结果，而是由插入（<code>+</code>）和撤销（<code>-</code>）组成的持续查询产生结果流。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;changelog&#x27;;</span><br></pre></td></tr></table></figure>\n\n<p><strong>Tableau模式</strong>（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(<code>execution.type</code>)：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;tableau&#x27;;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"MySQL开启binlog\"><a href=\"#MySQL开启binlog\" class=\"headerlink\" title=\"MySQL开启binlog\"></a>MySQL开启binlog</h1><ul>\n<li><p>找到my.cnf文件</p>\n<blockquote>\n<p>mysql –help | grep ‘Default options’ -A 1</p>\n</blockquote>\n</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#第一种方式:</span></span><br><span class=\"line\"><span class=\"comment\">#开启binlog日志</span></span><br><span class=\"line\"><span class=\"string\">log_bin=ON</span></span><br><span class=\"line\"><span class=\"comment\">#binlog日志的基本文件名</span></span><br><span class=\"line\"><span class=\"string\">log_bin_basename=/var/lib/mysql/mysql-bin</span></span><br><span class=\"line\"><span class=\"comment\">#binlog文件的索引文件，管理所有binlog文件</span></span><br><span class=\"line\"><span class=\"string\">log_bin_index=/var/lib/mysql/mysql-bin.index</span></span><br><span class=\"line\"><span class=\"comment\">#配置serverid</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#第二种方式:</span></span><br><span class=\"line\"><span class=\"comment\">#此一行等同于上面log_bin三行</span></span><br><span class=\"line\"><span class=\"string\">log-bin=/var/lib/mysql/mysql-bin</span></span><br><span class=\"line\"><span class=\"comment\">#配置serverid</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Demo</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"><span class=\"string\">log-bin=mysql-bin</span></span><br><span class=\"line\"><span class=\"string\">binlog_format=row</span></span><br><span class=\"line\"><span class=\"string\">binlog-do-db=mydb</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"Code-Repo\"><a href=\"#Code-Repo\" class=\"headerlink\" title=\"Code Repo\"></a>Code Repo</h1><ul>\n<li><pre><code class=\"scala\">    // 从命令参数中读取hostname和port\n    val paramTool: ParameterTool = ParameterTool.fromArgs(args)\n    val hostname: String = paramTool.get(&quot;host&quot;)\n    val port: Int = paramTool.getInt(&quot;port&quot;)\n</code></pre>\n</li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h1 id=\"QuickStart\"><a href=\"#QuickStart\" class=\"headerlink\" title=\"QuickStart\"></a>QuickStart</h1><ul>\n<li><p>Table API 和 SQL 需要引入的依赖有两个：planner 和 bridge。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-api-scala-bridge_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><blockquote>\n<p>老版本planner已经被废除，只剩下blink</p>\n<p>The old planner has been removed in Flink 1.14. Please upgrade your table program to use the default planner (previously called the ‘blink’ planner).</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"Flink-CDC-SQL-Demo\"><a href=\"#Flink-CDC-SQL-Demo\" class=\"headerlink\" title=\"Flink CDC SQL Demo\"></a>Flink CDC SQL Demo</h1><ul>\n<li><p>1、下载Flink，下载<code>flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar</code>依赖包，并将它们放到目录 <code>&#123;flink_home&#125;/lib/</code> 下.</p>\n</li>\n<li><p>2、在 MySQL 数据库中准备数据，创建数据库和表 <code>products</code>，<code>orders</code>，并插入数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- MySQL</span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE mydb;</span><br><span class=\"line\">USE mydb;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> products (</span><br><span class=\"line\">  id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">  name <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  description <span class=\"type\">VARCHAR</span>(<span class=\"number\">512</span>)</span><br><span class=\"line\">);</span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> products AUTO_INCREMENT <span class=\"operator\">=</span> <span class=\"number\">101</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> products</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"keyword\">default</span>,&quot;scooter&quot;,&quot;Small 2-wheel scooter&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;car battery&quot;,&quot;12V car battery&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;12-pack drill bits&quot;,&quot;12-pack of drill bits with sizes ranging from #40 to #3&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;12oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;14oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;hammer&quot;,&quot;16oz carpenter&#x27;s hammer&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;rocks&quot;,&quot;box of assorted rocks&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;jacket&quot;,&quot;water resistent black wind breaker&quot;),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>,&quot;spare tire&quot;,&quot;24 inch spare tire&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> orders (</span><br><span class=\"line\">  order_id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">  order_date DATETIME <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  customer_name <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  price <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">5</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  product_id <span class=\"type\">INTEGER</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  order_status <span class=\"type\">BOOLEAN</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"comment\">-- Whether order has been placed</span></span><br><span class=\"line\">) AUTO_INCREMENT <span class=\"operator\">=</span> <span class=\"number\">10001</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> orders</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 10:08:22&#x27;</span>, <span class=\"string\">&#x27;Jark&#x27;</span>, <span class=\"number\">50.50</span>, <span class=\"number\">102</span>, <span class=\"literal\">false</span>),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 10:11:09&#x27;</span>, <span class=\"string\">&#x27;Sally&#x27;</span>, <span class=\"number\">15.00</span>, <span class=\"number\">105</span>, <span class=\"literal\">false</span>),</span><br><span class=\"line\">       (<span class=\"keyword\">default</span>, <span class=\"string\">&#x27;2020-07-30 12:00:30&#x27;</span>, <span class=\"string\">&#x27;Edward&#x27;</span>, <span class=\"number\">25.25</span>, <span class=\"number\">106</span>, <span class=\"literal\">false</span>);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、启动 Flink 集群和 Flink SQL CLI</p>\n<blockquote>\n<p>.&#x2F;bin&#x2F;start-cluster.sh<br>.&#x2F;bin&#x2F;sql-client.sh</p>\n</blockquote>\n</li>\n<li><p>4、在 Flink SQL CLI 中使用 Flink DDL 创建表</p>\n<blockquote>\n<p>首先，开启 checkpoint，每隔3秒做一次 checkpoint</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;-- Flink SQL                   </span><br><span class=\"line\">&gt;Flink SQL&gt; SET execution.checkpointing.interval = 3s;</span><br></pre></td></tr></table></figure>\n\n<p>然后, 对于数据库中的表 <code>products</code>, <code>orders</code>, <code>shipments</code>， 使用 Flink SQL CLI 创建对应的表，用于同步这些底层数据库表的数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"operator\">&gt;</span><span class=\"comment\">-- Flink SQL</span></span><br><span class=\"line\"> <span class=\"operator\">&gt;</span>Flink <span class=\"keyword\">SQL</span><span class=\"operator\">&gt;</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> products (</span><br><span class=\"line\">   id <span class=\"type\">INT</span>,</span><br><span class=\"line\">   name STRING,</span><br><span class=\"line\">   description STRING,</span><br><span class=\"line\">   <span class=\"keyword\">PRIMARY</span> KEY (id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\"> ) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">   <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mysql-cdc&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;hostname&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;localhost&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;port&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;3306&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1234&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;database-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mydb&#x27;</span>,</span><br><span class=\"line\">   <span class=\"string\">&#x27;table-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;products&#x27;</span></span><br><span class=\"line\"> );</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"operator\">&gt;</span>Flink <span class=\"keyword\">SQL</span><span class=\"operator\">&gt;</span> <span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> orders (</span><br><span class=\"line\">  order_id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  order_date <span class=\"type\">TIMESTAMP</span>(<span class=\"number\">0</span>),</span><br><span class=\"line\">  customer_name STRING,</span><br><span class=\"line\">  price <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">  product_id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  order_status <span class=\"type\">BOOLEAN</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (order_id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mysql-cdc&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;hostname&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;localhost&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;port&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;3306&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1234&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;database-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;mydb&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;table-name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;orders&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></blockquote>\n</li>\n</ul>\n<h1 id=\"SQL-Client\"><a href=\"#SQL-Client\" class=\"headerlink\" title=\"SQL Client\"></a>SQL Client</h1><ul>\n<li><p>CLI 为维护和可视化结果提供<strong>三种模式</strong>。</p>\n</li>\n<li><p><strong>表格模式</strong>（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;table&#x27;;</span><br></pre></td></tr></table></figure>\n\n<p><strong>变更日志模式</strong>（changelog mode）不会实体化和可视化结果，而是由插入（<code>+</code>）和撤销（<code>-</code>）组成的持续查询产生结果流。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;changelog&#x27;;</span><br></pre></td></tr></table></figure>\n\n<p><strong>Tableau模式</strong>（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(<code>execution.type</code>)：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;tableau&#x27;;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"MySQL开启binlog\"><a href=\"#MySQL开启binlog\" class=\"headerlink\" title=\"MySQL开启binlog\"></a>MySQL开启binlog</h1><ul>\n<li><p>找到my.cnf文件</p>\n<blockquote>\n<p>mysql –help | grep ‘Default options’ -A 1</p>\n</blockquote>\n</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#第一种方式:</span></span><br><span class=\"line\"><span class=\"comment\">#开启binlog日志</span></span><br><span class=\"line\"><span class=\"string\">log_bin=ON</span></span><br><span class=\"line\"><span class=\"comment\">#binlog日志的基本文件名</span></span><br><span class=\"line\"><span class=\"string\">log_bin_basename=/var/lib/mysql/mysql-bin</span></span><br><span class=\"line\"><span class=\"comment\">#binlog文件的索引文件，管理所有binlog文件</span></span><br><span class=\"line\"><span class=\"string\">log_bin_index=/var/lib/mysql/mysql-bin.index</span></span><br><span class=\"line\"><span class=\"comment\">#配置serverid</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#第二种方式:</span></span><br><span class=\"line\"><span class=\"comment\">#此一行等同于上面log_bin三行</span></span><br><span class=\"line\"><span class=\"string\">log-bin=/var/lib/mysql/mysql-bin</span></span><br><span class=\"line\"><span class=\"comment\">#配置serverid</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Demo</span></span><br><span class=\"line\"><span class=\"string\">server-id=1</span></span><br><span class=\"line\"><span class=\"string\">log-bin=mysql-bin</span></span><br><span class=\"line\"><span class=\"string\">binlog_format=row</span></span><br><span class=\"line\"><span class=\"string\">binlog-do-db=mydb</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"Code-Repo\"><a href=\"#Code-Repo\" class=\"headerlink\" title=\"Code Repo\"></a>Code Repo</h1><ul>\n<li><pre><code class=\"scala\">    // 从命令参数中读取hostname和port\n    val paramTool: ParameterTool = ParameterTool.fromArgs(args)\n    val hostname: String = paramTool.get(&quot;host&quot;)\n    val port: Int = paramTool.getInt(&quot;port&quot;)\n</code></pre>\n</li>\n</ul>\n"},{"title":"Flink Cluster With YARN","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":44798,"date":"2022-08-08T11:54:07.000Z","updated":"2022-08-08T11:54:07.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n>在YARN部署模式中，有三种部署方式：\n>\n>- in Application Mode\n>- in Session Mode\n>- in a Per-Job Mode (deprecated)\n\n# YARN模式\n\n独立（Standalone）模式由 Flink 自身提供资源，无需其他框架，这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但我们知道，Flink 是大数据计算框架，不是资源调度框架，这并不是它的强项；所以还是应该让专业的框架做专业的事，和其他资源调度框架集成更靠谱。而在目前大数据生态中，国内应用最为广泛的资源管理平台就是 YARN 了。所以接下来我们就将学习，在强大的 YARN 平台上 Flink 是如何集成部署的。\n\n整体来说，YARN 上部署的过程是：客户端把 Flink 应用提交给 Yarn 的ResourceManager, Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配TaskManager 资源。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1659960821751-9ec31d52-d839-445c-8aa4-e6f9af4d2ca8.png)\n\n## 相关准备和配置\n\n在 Flink1.8.0 之前的版本，想要以 YARN 模式部署 Flink 任务时，需要 Flink 是有 Hadoop 支持的。从 Flink 1.8 版本开始，不再提供基于 Hadoop 编译的安装包，若需要Hadoop 的环境支持，需要自行在官网下载 Hadoop 相关版本的组件flink-shaded-hadoop-2-uber-2.7.5-10.0.jar， 并将该组件上传至 Flink 的 lib 目录下。在 Flink 1.11.0 版本之后，增加了很多重要新特性，其中就包括增加了对Hadoop3.0.0 以及更高版本Hadoop 的支持，不再提供“flink-shaded-hadoop-*” jar 包，而是通过配置环境变量完成与 YARN 集群的对接。\n\n在将 Flink 任务部署至 YARN 集群之前，需要确认集群是否安装有Hadoop，保证 Hadoop\n\n版本至少在 2.2 以上，并且集群中安装有 HDFS 服务。具体配置步骤如下：\n\n（1）下载并解压安装包，并将解压后的安装包重命名为flink-1.13.0-yarn，本节的相关操作都将默认在此安装路径下执行。\n\n（2）配置环境变量，增加环境变量配置如下：\n\n```sh\n$ sudo vim /etc/profile.d/my_env.sh \nHADOOP_HOME=/opt/module/hadoop-2.7.5\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport HADOOP_CLASSPATH=`hadoop classpath`\n```\n这里必须保证设置了环境变量HADOOP_CLASSPATH。\n\n（3）启动Hadoop 集群，包括 HDFS 和 YARN\n（4）进入 conf 目录，修改 flink-conf.yaml 文件，修改以下配置，若在提交命令中不特定指明，这些配置将作为默认配置。\n```\n$ cd /opt/module/flink-1.13.0-yarn/conf/\n$ vim flink-conf.yaml \njobmanager.memory.process.size: 1600m \ntaskmanager.memory.process.size: 1728m \ntaskmanager.numberOfTaskSlots: 8\nparallelism.default: 1\n```\n\n## 应用模式部署\n\n应用模式同样非常简单，与单作业模式类似，直接执行 flink run-application 命令即可。\n\n（1)执行命令提交作业。\n\n```sh\n$ bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount\nFlinkTutorial-1.0-SNAPSHOT.jar\n```\n\n（2）在命令行中查看或取消作业。\n\n```shell\n$./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY\n$./bin/flink cancel\t-t\tyarn-application -Dyarn.application.id=application_XXXX_YY <jobId>\n```\n\n（3） 也可以通过yarn.provided.lib.dirs 配置选项指定位置，将 jar 上传到远程。\n\n```sh\n$ ./bin/flink run-application -t yarn-application -Dyarn.provided.lib.dirs=\"hdfs://myhdfs/my-remote-flink-dist-dir\" \nhdfs://myhdfs/jars/my-application.jar\n```\n\n\n这种方式下 jar 可以预先上传到 HDFS，而不需要单独发送到集群，这就使得作业提交更加轻量了。\n\n## 高可用\n\nYARN 模式的高可用和独立模式（Standalone）的高可用原理不一样。\n\nStandalone 模式中, 同时启动多个 JobManager, 一个为“领导者”（leader），其他为“后备”（standby）, 当 leader 挂了, 其他的才会有一个成为 leader。\n\n而 YARN 的高可用是只启动一个 Jobmanager, 当这个 Jobmanager 挂了之后, YARN 会再次启动一个, 所以其实是利用的 YARN 的重试次数来实现的高可用。\n\n（1） 在 yarn-site.xml 中配置。\n\n```xml\n<property>\n<name>yarn.resourcemanager.am.max-attempts</name>\n<value>4</value>\n<description>\nThe maximum number of application master execution attempts.\n</description>\n</property>\n```\n\n注意: 配置完不要忘记分发, 和重启 YARN。\n\n（2） 在 flink-conf.yaml 中配置。\n\n```yaml\nyarn.application-attempts: 3 \nhigh-availability: zookeeper\nhigh-availability.storageDir: hdfs://hadoop102:9820/flink/yarn/ha \nhigh-availability.zookeeper.quorum: hadoop102:2181,hadoop103:2181,hadoop104:2181\nhigh-availability.zookeeper.path.root: /flink-yarn\n```\n\n（3） 启动 yarn-session。\n\n（4） 杀死 JobManager, 查看复活情况。\n\n注意: yarn-site.xml 中配置的是 JobManager 重启次数的上限, flink-conf.xml 中的次数应该小于这个值。\n","source":"_posts/bigdata/flink/搭建Flink集群.md","raw":"---\ntitle: Flink Cluster With YARN\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 44798\ndate: 2022-08-08 19:54:07\nupdated: 2022-08-08 19:54:07\ncover:\ndescription:\nkeywords:\n---\n\n>在YARN部署模式中，有三种部署方式：\n>\n>- in Application Mode\n>- in Session Mode\n>- in a Per-Job Mode (deprecated)\n\n# YARN模式\n\n独立（Standalone）模式由 Flink 自身提供资源，无需其他框架，这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但我们知道，Flink 是大数据计算框架，不是资源调度框架，这并不是它的强项；所以还是应该让专业的框架做专业的事，和其他资源调度框架集成更靠谱。而在目前大数据生态中，国内应用最为广泛的资源管理平台就是 YARN 了。所以接下来我们就将学习，在强大的 YARN 平台上 Flink 是如何集成部署的。\n\n整体来说，YARN 上部署的过程是：客户端把 Flink 应用提交给 Yarn 的ResourceManager, Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配TaskManager 资源。\n\n ![image.png](https://cdn.nlark.com/yuque/0/2022/png/2500465/1659960821751-9ec31d52-d839-445c-8aa4-e6f9af4d2ca8.png)\n\n## 相关准备和配置\n\n在 Flink1.8.0 之前的版本，想要以 YARN 模式部署 Flink 任务时，需要 Flink 是有 Hadoop 支持的。从 Flink 1.8 版本开始，不再提供基于 Hadoop 编译的安装包，若需要Hadoop 的环境支持，需要自行在官网下载 Hadoop 相关版本的组件flink-shaded-hadoop-2-uber-2.7.5-10.0.jar， 并将该组件上传至 Flink 的 lib 目录下。在 Flink 1.11.0 版本之后，增加了很多重要新特性，其中就包括增加了对Hadoop3.0.0 以及更高版本Hadoop 的支持，不再提供“flink-shaded-hadoop-*” jar 包，而是通过配置环境变量完成与 YARN 集群的对接。\n\n在将 Flink 任务部署至 YARN 集群之前，需要确认集群是否安装有Hadoop，保证 Hadoop\n\n版本至少在 2.2 以上，并且集群中安装有 HDFS 服务。具体配置步骤如下：\n\n（1）下载并解压安装包，并将解压后的安装包重命名为flink-1.13.0-yarn，本节的相关操作都将默认在此安装路径下执行。\n\n（2）配置环境变量，增加环境变量配置如下：\n\n```sh\n$ sudo vim /etc/profile.d/my_env.sh \nHADOOP_HOME=/opt/module/hadoop-2.7.5\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport HADOOP_CLASSPATH=`hadoop classpath`\n```\n这里必须保证设置了环境变量HADOOP_CLASSPATH。\n\n（3）启动Hadoop 集群，包括 HDFS 和 YARN\n（4）进入 conf 目录，修改 flink-conf.yaml 文件，修改以下配置，若在提交命令中不特定指明，这些配置将作为默认配置。\n```\n$ cd /opt/module/flink-1.13.0-yarn/conf/\n$ vim flink-conf.yaml \njobmanager.memory.process.size: 1600m \ntaskmanager.memory.process.size: 1728m \ntaskmanager.numberOfTaskSlots: 8\nparallelism.default: 1\n```\n\n## 应用模式部署\n\n应用模式同样非常简单，与单作业模式类似，直接执行 flink run-application 命令即可。\n\n（1)执行命令提交作业。\n\n```sh\n$ bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount\nFlinkTutorial-1.0-SNAPSHOT.jar\n```\n\n（2）在命令行中查看或取消作业。\n\n```shell\n$./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY\n$./bin/flink cancel\t-t\tyarn-application -Dyarn.application.id=application_XXXX_YY <jobId>\n```\n\n（3） 也可以通过yarn.provided.lib.dirs 配置选项指定位置，将 jar 上传到远程。\n\n```sh\n$ ./bin/flink run-application -t yarn-application -Dyarn.provided.lib.dirs=\"hdfs://myhdfs/my-remote-flink-dist-dir\" \nhdfs://myhdfs/jars/my-application.jar\n```\n\n\n这种方式下 jar 可以预先上传到 HDFS，而不需要单独发送到集群，这就使得作业提交更加轻量了。\n\n## 高可用\n\nYARN 模式的高可用和独立模式（Standalone）的高可用原理不一样。\n\nStandalone 模式中, 同时启动多个 JobManager, 一个为“领导者”（leader），其他为“后备”（standby）, 当 leader 挂了, 其他的才会有一个成为 leader。\n\n而 YARN 的高可用是只启动一个 Jobmanager, 当这个 Jobmanager 挂了之后, YARN 会再次启动一个, 所以其实是利用的 YARN 的重试次数来实现的高可用。\n\n（1） 在 yarn-site.xml 中配置。\n\n```xml\n<property>\n<name>yarn.resourcemanager.am.max-attempts</name>\n<value>4</value>\n<description>\nThe maximum number of application master execution attempts.\n</description>\n</property>\n```\n\n注意: 配置完不要忘记分发, 和重启 YARN。\n\n（2） 在 flink-conf.yaml 中配置。\n\n```yaml\nyarn.application-attempts: 3 \nhigh-availability: zookeeper\nhigh-availability.storageDir: hdfs://hadoop102:9820/flink/yarn/ha \nhigh-availability.zookeeper.quorum: hadoop102:2181,hadoop103:2181,hadoop104:2181\nhigh-availability.zookeeper.path.root: /flink-yarn\n```\n\n（3） 启动 yarn-session。\n\n（4） 杀死 JobManager, 查看复活情况。\n\n注意: yarn-site.xml 中配置的是 JobManager 重启次数的上限, flink-conf.xml 中的次数应该小于这个值。\n","slug":"bigdata/flink/搭建Flink集群","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004n8j5mcnu7buzu","content":"<blockquote>\n<p>在YARN部署模式中，有三种部署方式：</p>\n<ul>\n<li>in Application Mode</li>\n<li>in Session Mode</li>\n<li>in a Per-Job Mode (deprecated)</li>\n</ul>\n</blockquote>\n<h1 id=\"YARN模式\"><a href=\"#YARN模式\" class=\"headerlink\" title=\"YARN模式\"></a>YARN模式</h1><p>独立（Standalone）模式由 Flink 自身提供资源，无需其他框架，这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但我们知道，Flink 是大数据计算框架，不是资源调度框架，这并不是它的强项；所以还是应该让专业的框架做专业的事，和其他资源调度框架集成更靠谱。而在目前大数据生态中，国内应用最为广泛的资源管理平台就是 YARN 了。所以接下来我们就将学习，在强大的 YARN 平台上 Flink 是如何集成部署的。</p>\n<p>整体来说，YARN 上部署的过程是：客户端把 Flink 应用提交给 Yarn 的ResourceManager, Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配TaskManager 资源。</p>\n<p> <img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1659960821751-9ec31d52-d839-445c-8aa4-e6f9af4d2ca8.png\" alt=\"image.png\"></p>\n<h2 id=\"相关准备和配置\"><a href=\"#相关准备和配置\" class=\"headerlink\" title=\"相关准备和配置\"></a>相关准备和配置</h2><p>在 Flink1.8.0 之前的版本，想要以 YARN 模式部署 Flink 任务时，需要 Flink 是有 Hadoop 支持的。从 Flink 1.8 版本开始，不再提供基于 Hadoop 编译的安装包，若需要Hadoop 的环境支持，需要自行在官网下载 Hadoop 相关版本的组件flink-shaded-hadoop-2-uber-2.7.5-10.0.jar， 并将该组件上传至 Flink 的 lib 目录下。在 Flink 1.11.0 版本之后，增加了很多重要新特性，其中就包括增加了对Hadoop3.0.0 以及更高版本Hadoop 的支持，不再提供“flink-shaded-hadoop-*” jar 包，而是通过配置环境变量完成与 YARN 集群的对接。</p>\n<p>在将 Flink 任务部署至 YARN 集群之前，需要确认集群是否安装有Hadoop，保证 Hadoop</p>\n<p>版本至少在 2.2 以上，并且集群中安装有 HDFS 服务。具体配置步骤如下：</p>\n<p>（1）下载并解压安装包，并将解压后的安装包重命名为flink-1.13.0-yarn，本节的相关操作都将默认在此安装路径下执行。</p>\n<p>（2）配置环境变量，增加环境变量配置如下：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo vim /etc/profile.d/my_env.sh </span><br><span class=\"line\">HADOOP_HOME=/opt/module/hadoop-2.7.5</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/bin:<span class=\"variable\">$HADOOP_HOME</span>/sbin</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_CONF_DIR=<span class=\"variable\">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure>\n<p>这里必须保证设置了环境变量HADOOP_CLASSPATH。</p>\n<p>（3）启动Hadoop 集群，包括 HDFS 和 YARN<br>（4）进入 conf 目录，修改 flink-conf.yaml 文件，修改以下配置，若在提交命令中不特定指明，这些配置将作为默认配置。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd /opt/module/flink-1.13.0-yarn/conf/</span><br><span class=\"line\">$ vim flink-conf.yaml </span><br><span class=\"line\">jobmanager.memory.process.size: 1600m </span><br><span class=\"line\">taskmanager.memory.process.size: 1728m </span><br><span class=\"line\">taskmanager.numberOfTaskSlots: 8</span><br><span class=\"line\">parallelism.default: 1</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"应用模式部署\"><a href=\"#应用模式部署\" class=\"headerlink\" title=\"应用模式部署\"></a>应用模式部署</h2><p>应用模式同样非常简单，与单作业模式类似，直接执行 flink run-application 命令即可。</p>\n<p>（1)执行命令提交作业。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount</span><br><span class=\"line\">FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n<p>（2）在命令行中查看或取消作业。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">./bin/flink cancel\t-t\tyarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>（3） 也可以通过yarn.provided.lib.dirs 配置选项指定位置，将 jar 上传到远程。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ./bin/flink run-application -t yarn-application -Dyarn.provided.lib.dirs=<span class=\"string\">&quot;hdfs://myhdfs/my-remote-flink-dist-dir&quot;</span> </span><br><span class=\"line\">hdfs://myhdfs/jars/my-application.jar</span><br></pre></td></tr></table></figure>\n\n\n<p>这种方式下 jar 可以预先上传到 HDFS，而不需要单独发送到集群，这就使得作业提交更加轻量了。</p>\n<h2 id=\"高可用\"><a href=\"#高可用\" class=\"headerlink\" title=\"高可用\"></a>高可用</h2><p>YARN 模式的高可用和独立模式（Standalone）的高可用原理不一样。</p>\n<p>Standalone 模式中, 同时启动多个 JobManager, 一个为“领导者”（leader），其他为“后备”（standby）, 当 leader 挂了, 其他的才会有一个成为 leader。</p>\n<p>而 YARN 的高可用是只启动一个 Jobmanager, 当这个 Jobmanager 挂了之后, YARN 会再次启动一个, 所以其实是利用的 YARN 的重试次数来实现的高可用。</p>\n<p>（1） 在 yarn-site.xml 中配置。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">The maximum number of application master execution attempts.</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>注意: 配置完不要忘记分发, 和重启 YARN。</p>\n<p>（2） 在 flink-conf.yaml 中配置。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">yarn.application-attempts:</span> <span class=\"number\">3</span> </span><br><span class=\"line\"><span class=\"attr\">high-availability:</span> <span class=\"string\">zookeeper</span></span><br><span class=\"line\"><span class=\"attr\">high-availability.storageDir:</span> <span class=\"string\">hdfs://hadoop102:9820/flink/yarn/ha</span> </span><br><span class=\"line\"><span class=\"attr\">high-availability.zookeeper.quorum:</span> <span class=\"string\">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br><span class=\"line\"><span class=\"attr\">high-availability.zookeeper.path.root:</span> <span class=\"string\">/flink-yarn</span></span><br></pre></td></tr></table></figure>\n\n<p>（3） 启动 yarn-session。</p>\n<p>（4） 杀死 JobManager, 查看复活情况。</p>\n<p>注意: yarn-site.xml 中配置的是 JobManager 重启次数的上限, flink-conf.xml 中的次数应该小于这个值。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>在YARN部署模式中，有三种部署方式：</p>\n<ul>\n<li>in Application Mode</li>\n<li>in Session Mode</li>\n<li>in a Per-Job Mode (deprecated)</li>\n</ul>\n</blockquote>\n<h1 id=\"YARN模式\"><a href=\"#YARN模式\" class=\"headerlink\" title=\"YARN模式\"></a>YARN模式</h1><p>独立（Standalone）模式由 Flink 自身提供资源，无需其他框架，这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但我们知道，Flink 是大数据计算框架，不是资源调度框架，这并不是它的强项；所以还是应该让专业的框架做专业的事，和其他资源调度框架集成更靠谱。而在目前大数据生态中，国内应用最为广泛的资源管理平台就是 YARN 了。所以接下来我们就将学习，在强大的 YARN 平台上 Flink 是如何集成部署的。</p>\n<p>整体来说，YARN 上部署的过程是：客户端把 Flink 应用提交给 Yarn 的ResourceManager, Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配TaskManager 资源。</p>\n<p> <img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1659960821751-9ec31d52-d839-445c-8aa4-e6f9af4d2ca8.png\" alt=\"image.png\"></p>\n<h2 id=\"相关准备和配置\"><a href=\"#相关准备和配置\" class=\"headerlink\" title=\"相关准备和配置\"></a>相关准备和配置</h2><p>在 Flink1.8.0 之前的版本，想要以 YARN 模式部署 Flink 任务时，需要 Flink 是有 Hadoop 支持的。从 Flink 1.8 版本开始，不再提供基于 Hadoop 编译的安装包，若需要Hadoop 的环境支持，需要自行在官网下载 Hadoop 相关版本的组件flink-shaded-hadoop-2-uber-2.7.5-10.0.jar， 并将该组件上传至 Flink 的 lib 目录下。在 Flink 1.11.0 版本之后，增加了很多重要新特性，其中就包括增加了对Hadoop3.0.0 以及更高版本Hadoop 的支持，不再提供“flink-shaded-hadoop-*” jar 包，而是通过配置环境变量完成与 YARN 集群的对接。</p>\n<p>在将 Flink 任务部署至 YARN 集群之前，需要确认集群是否安装有Hadoop，保证 Hadoop</p>\n<p>版本至少在 2.2 以上，并且集群中安装有 HDFS 服务。具体配置步骤如下：</p>\n<p>（1）下载并解压安装包，并将解压后的安装包重命名为flink-1.13.0-yarn，本节的相关操作都将默认在此安装路径下执行。</p>\n<p>（2）配置环境变量，增加环境变量配置如下：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo vim /etc/profile.d/my_env.sh </span><br><span class=\"line\">HADOOP_HOME=/opt/module/hadoop-2.7.5</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$HADOOP_HOME</span>/bin:<span class=\"variable\">$HADOOP_HOME</span>/sbin</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_CONF_DIR=<span class=\"variable\">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class=\"line\"><span class=\"built_in\">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure>\n<p>这里必须保证设置了环境变量HADOOP_CLASSPATH。</p>\n<p>（3）启动Hadoop 集群，包括 HDFS 和 YARN<br>（4）进入 conf 目录，修改 flink-conf.yaml 文件，修改以下配置，若在提交命令中不特定指明，这些配置将作为默认配置。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd /opt/module/flink-1.13.0-yarn/conf/</span><br><span class=\"line\">$ vim flink-conf.yaml </span><br><span class=\"line\">jobmanager.memory.process.size: 1600m </span><br><span class=\"line\">taskmanager.memory.process.size: 1728m </span><br><span class=\"line\">taskmanager.numberOfTaskSlots: 8</span><br><span class=\"line\">parallelism.default: 1</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"应用模式部署\"><a href=\"#应用模式部署\" class=\"headerlink\" title=\"应用模式部署\"></a>应用模式部署</h2><p>应用模式同样非常简单，与单作业模式类似，直接执行 flink run-application 命令即可。</p>\n<p>（1)执行命令提交作业。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount</span><br><span class=\"line\">FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n<p>（2）在命令行中查看或取消作业。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$</span><span class=\"language-bash\">./bin/flink cancel\t-t\tyarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>（3） 也可以通过yarn.provided.lib.dirs 配置选项指定位置，将 jar 上传到远程。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ./bin/flink run-application -t yarn-application -Dyarn.provided.lib.dirs=<span class=\"string\">&quot;hdfs://myhdfs/my-remote-flink-dist-dir&quot;</span> </span><br><span class=\"line\">hdfs://myhdfs/jars/my-application.jar</span><br></pre></td></tr></table></figure>\n\n\n<p>这种方式下 jar 可以预先上传到 HDFS，而不需要单独发送到集群，这就使得作业提交更加轻量了。</p>\n<h2 id=\"高可用\"><a href=\"#高可用\" class=\"headerlink\" title=\"高可用\"></a>高可用</h2><p>YARN 模式的高可用和独立模式（Standalone）的高可用原理不一样。</p>\n<p>Standalone 模式中, 同时启动多个 JobManager, 一个为“领导者”（leader），其他为“后备”（standby）, 当 leader 挂了, 其他的才会有一个成为 leader。</p>\n<p>而 YARN 的高可用是只启动一个 Jobmanager, 当这个 Jobmanager 挂了之后, YARN 会再次启动一个, 所以其实是利用的 YARN 的重试次数来实现的高可用。</p>\n<p>（1） 在 yarn-site.xml 中配置。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>4<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">The maximum number of application master execution attempts.</span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>注意: 配置完不要忘记分发, 和重启 YARN。</p>\n<p>（2） 在 flink-conf.yaml 中配置。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">yarn.application-attempts:</span> <span class=\"number\">3</span> </span><br><span class=\"line\"><span class=\"attr\">high-availability:</span> <span class=\"string\">zookeeper</span></span><br><span class=\"line\"><span class=\"attr\">high-availability.storageDir:</span> <span class=\"string\">hdfs://hadoop102:9820/flink/yarn/ha</span> </span><br><span class=\"line\"><span class=\"attr\">high-availability.zookeeper.quorum:</span> <span class=\"string\">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br><span class=\"line\"><span class=\"attr\">high-availability.zookeeper.path.root:</span> <span class=\"string\">/flink-yarn</span></span><br></pre></td></tr></table></figure>\n\n<p>（3） 启动 yarn-session。</p>\n<p>（4） 杀死 JobManager, 查看复活情况。</p>\n<p>注意: yarn-site.xml 中配置的是 JobManager 重启次数的上限, flink-conf.xml 中的次数应该小于这个值。</p>\n"},{"title":"Flink调优","top_img":"/img/bg/banner.gif","abbrlink":54105,"date":"2022-08-08T12:23:04.000Z","updated":"2022-08-08T12:23:04.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n# 内存设置（1CPU配置4G内存）\n\n> bin/flink run \\\n>\n> -t yarn-per-job \\\n>\n> -d \\\n>\n> -p 5 \\ 指定并行度\n>\n> -Dyarn.application.queue=test \\ 指定yarn队列\n>\n> -Djobmanager.memory.process.size=2048mb \\ JM2~4G足够\n>\n> -Dtaskmanager.memory.process.size=6144mb \\ 单个TM2~8G足够\n>\n> -Dtaskmanager.numberOfTaskSlots=2 \\ **与容器核数1core：1slot或1core：2slot**\n>\n> -c com.atguigu.app.dwd.LogBaseApp \\\n>\n> /opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar\n\nFlink是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS/TPS来描述数据情况。\n\n##  TaskManager 内存模型  \n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654665961981-86c260ab-5310-4674-ac61-6a1d1f738f18.png)\n\n### 1、内存模型详解\n\n#### JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head\n\n1）JVMmetaspace：JVM 元空间\n\ntaskmanager.memory.jvm-metaspace.size，默认 256mb\n\n\n\n2）JVMover-head执行开销：JVM执行时自身所需要的内容，包括线程堆栈、IO、编译缓存等所使用的内存。\n\ntaskmanager.memory.jvm-overhead.fraction，默认 0.1\n\ntaskmanager.memory.jvm-overhead.min，默认 192mb\n\ntaskmanager.memory.jvm-overhead.max，默认 1gb\n\n\n\n**总进程内存\\*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min/max**\n\n**大小**\n\n\n\n#### 框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。\n\n堆内：taskmanager.memory.framework.heap.size，默认 128MB\n\n堆外：taskmanager.memory.framework.off-heap.size，默认 128MB\n\n#### Task内存：Task执行用户代码时所使用的内存\n\n堆内：taskmanager.memory.task.heap.size，默认 none，由 Flink 内存扣除掉其他部分的内存得到。\n\n堆外：taskmanager.memory.task.off-heap.size，默认 0，表示不使用堆外内存\n\n#### 网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\n\n**堆外：**\n\ntaskmanager.memory.network.fraction，默认 0.1\n\ntaskmanager.memory.network.min，默认 64mb\n\ntaskmanager.memory.network.max，默认 1gb\n\n**Flink 内存\\*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min/max大小**\n\n  \n\n#### 托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。\n\n堆外：taskmanager.memory.managed.fraction，默认 0.4\n\ntaskmanager.memory.managed.size，默认 none\n\n**如果 size 没指定，则等于 Flink 内存\\*fraction**\n\n## 2、案例分析  \n\n基于Yarn模式，一般参数指定的是总进程内存，taskmanager.memory.process.size，比如指定为 4G，每一块内存得到大小如下：\n\n（1）计算 Flink 内存\n\nJVM 元空间 256m\n\nJVM 执行开销： 4g*0.1=409.6m，在[192m,1g]之间，最终结果 409.6m\n\nFlink 内存=4g-256m-409.6m=3430.4m\n\n（2）网络内存=3430.4m*0.1=343.04m，在[64m,1g]之间，最终结果 343.04m\n\n（3）托管内存=3430.4m*0.4=1372.16m\n\n（4）框架内存，堆内和堆外都是 128m\n\n（5）Task堆内内存=3430.4m-128m-128m-343.04m-1372.16m=1459.2m\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667261844-9b48b348-1bcb-4ca8-b556-f63a3680cf83.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667279269-d43c4812-9561-433a-83fe-a8d70b5fb5b9.png)\n\n### 所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\n\n## 合理利用 cpu 资源\n\nYarn 的**容量调度器**默认情况下是使用“DefaultResourceCalculator”分配策略，只根据内存调度资源，所以在 Yarn 的资源管理页面上看到每个容器的 vcore 个数还是 1。\n\n可以修改策略为 DominantResourceCalculator，该资源计算器在计算资源的时候会综合考虑 cpu 和内存的情况。在capacity-scheduler.xml 中修改属性:\n\n```xml\n<property>\n  <name>yarn.scheduler.capacity.resource-calculator</name>\n  <!-- <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value> -->\n  <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>\n</property>\n```\n\n### 1.1.1    使用DefaultResourceCalculator 策略\n\n```shell\nbin/flink run \\\n-t yarn-per-job \\\n-d \\\n-p 5 \\\n-Drest.flamegraph.enabled=true \\\n-Dyarn.application.queue=test \\\n-Djobmanager.memory.process.size=1024mb \\\n-Dtaskmanager.memory.process.size=4096mb \\\n-Dtaskmanager.numberOfTaskSlots=2 \\\n-c com.atguigu.flink.tuning.UvDemo \\\n/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n```\n\n可以看到一个容器只有一个 vcore：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668251950-033e4bc0-4b65-4fe8-b309-45a29956922b.png)\n\n### 1.1.2    使用DominantResourceCalculator 策略\n\n修改后 yarn 配置后，分发配置并重启 yarn，再次提交 flink 作业：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=4096mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo\\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n看到容器的 vcore 数变了:\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668344371-82744e2d-89b2-4fab-8a09-f77475df1088.png)\n\nJobManager1 个，占用 1 个容器，vcore=1\n\nTaskManager3 个，占用 3 个容器，每个容器 vcore=2，总 vcore=2*3=6，因为默认单个容器的 vcore 数=单 TM 的slot 数\n\n### 1.1.3    使用 DominantResourceCalculator 策略并指定容器**vcore 数**\n\n指定yarn 容器的 vcore 数，提交：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Dyarn.containers.vcores=3\\\n>\n>  -Djobmanager.memory.process.size=1024mb \\ -Dtaskmanager.memory.process.size=4096mb \\ -Dtaskmanager.numberOfTaskSlots=2 \\ -c com.atguigu.flink.tuning.UvDemo \\ /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar  \n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668509233-7292aa6f-0e54-4799-ba38-ab72659ef824.png)\n\nJobManager1 个，占用 1 个容器，vcore=1\n\nTaskManager3 个，占用 3 个容器，每个容器vcore =3，总 vcore=3*3=9\n\n# RocksDB大状态调优\n\nRocksDB 是基于 LSM Tree 实现的（类似HBase），写数据都是先缓存到内存中，所以RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record/s。**使用RocksDB 时，状态大小仅受可用磁盘空间量的限制，性能瓶颈主要在于 RocksDB对磁盘的读请求，每次读写操作都必须对数据进行反序列化或者序列化。**所以当处理性能不够时，仅需要横向扩展并行度即可提高整个Job 的吞吐量。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654669015363-a35261ab-d4ff-4068-a013-eecfe78a5c7d.png)\n\n\n\n从 Flink1.10 开始，Flink 默认将 RocksDB 的内存大小配置为每个 taskslot 的托管内存。调试内存性能的问题主要是通过调整配置项 taskmanager.memory.managed.size或者 taskmanager.memory.managed.fraction以增加 Flink 的托管内存(即堆外的托管内存)。进一步可以调整一些参数进行高级性能调优，这些参数也可以在应用程序中通过RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory)指定。下面介绍\n\n提高资源利用率的几个重要配置：\n\n### 2.1.1   开启State访问性能监控\n\nFlink 1.13 中引入了 State 访问的性能监控，即 latency trackig state。此功能不局限于 StateBackend 的类型，自定义实现的 StateBackend 也可以复用此功能。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654670053632-0e169f44-1340-4202-ab6a-bd9a6173a14a.png)\n\nState访问性能监控会产生一定的性能影响，所以，默认每 100次做一次取样(sample)，对不同的 StateBackend 性能损失影响不同：\n\n- 对于 RocksDBStateBackend，性能损失大概在 1% 左右\n- 对于 HeapStateBackend，性能损失最多可达 10%\n\n```yaml\nstate.backend.latency-track.keyed-state-enabled：true #启用访问状态的性能监控 \nstate.backend.latency-track.sample-interval: 100 #采样间隔 \nstate.backend.latency-track.history-size: 128 #保留的采样数据个数，越大越精确 \nstate.backend.latency-track.state-name-as-variable: true #将状态名作为变量  \n```\n\n正常开启第一个参数即可。\n\n> bin/flink run \\\n>\n> -t yarn-per-job \\\n>\n> -d \\\n>\n> -p 5 \\\n>\n> -Drest.flamegraph.enabled=true \\\n>\n> -Dyarn.application.queue=test \\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=4096mb \\\n>\n> -Dtaskmanager.numberOfTaskSlots=2 \\\n>\n>  -Dstate.backend.latency-track.keyed-state-enabled=true \\ \n>\n> -c com.atguigu.flink.tuning.RocksdbTuning \\ /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar  \n\n### 2.1.2    开启增量检查点和本地恢复\n\n1）开启增量检查点\n\nRocksDB 是目前唯一可用于支持有状态流处理应用程序增量检查点的状态后端，可以修改参数开启增量检查点：\n\nstate.backend.incremental: true #默认 false，改为 true。 \n\n或代码中指定 new EmbeddedRocksDBStateBackend(true)  \n\n2）开启本地恢复\n\n当 Flink任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs拉取数据。本地恢复目前仅涵盖键控类型的状态后端（RocksDB），MemoryStateBackend不支持本地恢复并忽略此选项。\n\nstate.backend.local-recovery:true\n\n### 2.1.3    调整预定义选项\n\nFlink针对不同的设置为 RocksDB提供了一些预定义的选项集合,其中包含了后续提到的一些参数，如果调整预定义选项后还达不到预期，再去调整后面的 block、writebuffer等参数。\n\n当 前 支 持 的 预 定 义 选 项 有   DEFAULT 、 SPINNING_DISK_OPTIMIZED 、\n\nSPINNING_DISK_OPTIMIZED_HIGH_MEM 或FLASH_SSD_OPTIMIZED。有条件上 SSD\n\n的，可以指定为 FLASH_SSD_OPTIMIZED\n\n state.backend.rocksdb.predefined-options： SPINNING_DISK_OPTIMIZED_HIGH_MEM #设置为机械硬盘+内存模式  \n\n### 2.1.4    增大 block 缓存\n\n整个 RocksDB 共享一个 blockcache，读数据时内存的 cache 大小，该参数越大读\n\n数据时缓存命中率越高，默认大小为8MB，建议设置到64~256MB。\n\nstate.backend.rocksdb.block.cache-size:64m     #默认8m  \n\n### 2.1.5    增大writebuffer 和 level 阈值大小\n\nRocksDB 中，每个 State 使用一个 ColumnFamily，每个 ColumnFamily 使用独占的 writebuffer，默认 64MB，建议调大。\n\n调整这个参数通常要适当增加 L1层的大小阈值 max-size-level-base，默认 256m。\n\n该值太小会造成能存放的 SST 文件过少，层级变多造成查找困难，太大会造成文件过多，合并困难。建议设为 target_file_size_base（默认 64MB） 的倍数，且不能太小，例如 5~10倍，即 320~640MB。\n\nstate.backend.rocksdb.writebuffer.size: 128m\n\nstate.backend.rocksdb.compaction.level.max-size-level-base:320m   \n\n### 2.1.6    增大write buffer 数量\n\n每个 ColumnFamily对应的 writebuffer 最大数量，这实际上是内存中“只读内存表“的最大数量，默认值是 2。对于机械磁盘来说，如果内存足够大，可以调大到 5左右\n\nstate.backend.rocksdb.writebuffer.count:5                                                                     \n\n### 2.1.7    增大后台线程数和writebuffer 合并数\n\n1）增大线程数\n\n用于后台 flush和合并 sst文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4等更大的值\n\nstate.backend.rocksdb.thread.num: 4                                                                             \n\n2）增大writebuffer 最小合并数\n\n将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 最小数量，默认\n\n值为 1，可以调成 3。\n\nstate.backend.rocksdb.writebuffer.number-to-merge:3                                             \n\n### 2.1.8    开启分区索引功能\n\nFlink1.13 中对 RocksDB 增加了分区索引功能，复用了 RocksDB 的partitionedIndex&filter 功能，简单来说就是对 RocksDB 的 partitionedIndex 做了多级索引。也就是将内存中的最上层常驻，下层根据需要再 load回来，这样就大大降低了数据 Swap竞争。线上测试中，相对于**内存比较小**的场景中，性能提升 10 倍左右。如果在内存管控下 Rocksdb 性能不如预期的话，这也能成为一个性能优化点。\n\nstate.backend.rocksdb.memory.partitioned-index-filters:true   #默认false                \n\n\n\n**2.1.9**    **参数设定案例**\n\n```sh\nbin/flinkrun\\\n-tyarn-per-job\\\n-d\\\n-p5\\\n-Drest.flamegraph.enabled=true\\\n-Dyarn.application.queue=test\\\n-Djobmanager.memory.process.size=1024mb \\\n-Dtaskmanager.memory.process.size=4096mb\\\n-Dtaskmanager.numberOfTaskSlots=2\\\n-Dstate.backend.incremental=true\\\n-Dstate.backend.local-recovery=true\\\n-Dstate.backend.rocksdb.predefined-options=SPINNING_DISK_OPTIMIZED_HIGH_MEM\\\n-Dstate.backend.rocksdb.block.cache-size=64m\\\n-Dstate.backend.rocksdb.writebuffer.size=128m\\\n-Dstate.backend.rocksdb.compaction.level.max-size-level-base=320m\\\n-Dstate.backend.rocksdb.writebuffer.count=5 \\\n-Dstate.backend.rocksdb.thread.num=4\\\n-Dstate.backend.rocksdb.writebuffer.number-to-merge=3\\\n-Dstate.backend.rocksdb.memory.partitioned-index-filters=true\\\n-Dstate.backend.latency-track.keyed-state-enabled=true\\\n-ccom.atguigu.flink.tuning.RocksdbTuning\\\n/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n```\n\n\n\n### 设置本地 RocksDB 多目录\n\n在flink-conf.yaml 中配置：\n\n```plain\nstate.backend.rocksdb.localdir: /data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb\n```\n\n\n\n注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。**当设置多个 RocksDB 本地磁盘目录时，Flink 会****随机选择****要使用的目录，所以就可能存在三个并行度共用同一目录的情况。**如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。\n\n**当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。**\n\n如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。**由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘IO性能消耗确实比较大。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662632337-7fe1e6c6-5fe2-412e-82e8-77f3c81458b7.png)\n\n如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj磁盘。可以看到 sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。**如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下**，**从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662673431-6575b710-490c-49c4-bec7-f4b7964b3fc7.png)\n\n- **state.backend.incremental：**开启增量检查点，默认false，改为true。\n- **state.backend.rocksdb.predefined-options：**SPINNING_DISK_OPTIMIZED_HIGH_MEM设置为机械硬盘+内存模式，有条件上SSD，指定为FLASH_SSD_OPTIMIZED\n- **state.backend.rocksdb.block.cache-size**: 整个 RocksDB 共享一个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。\n- **state.backend.rocksdb.thread.num**: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。\n- **state.backend.rocksdb.writebuffer.size**: RocksDB 中，每个 State 使用一个 Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M\n- **state.backend.rocksdb.writebuffer.count**: 每个 Column Family 对应的 writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5 左右\n- **state.backend.rocksdb.writebuffer.number-to-merge**: 将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成3。\n- **state.backend.local-recovery**: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据\n\n## Checkpoint设置\n\n一般我们的 Checkpoint 时间间隔可以设置为分钟级别（1~5分钟），例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5~10 分钟一次Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次Checkpoint 之间至少暂停 4或8 分钟。\n\n同时，也需要考虑时效性的要求,需要在时效性和性能之间做一个平衡，如果时效性要求高，结合 end- to-end 时长，设置秒级或毫秒级。\n\n如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看 Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。\n\nRocksDB相关参数在1.3中已说明，可以在flink-conf.yaml指定，也可以在Job的代码中调用API单独指定，这里不再列出。\n\n```scala\n// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint\nRocksDBStateBackend rocksDBStateBackend = new RocksDBStateBackend(\"hdfs://hadoop102:8020/flink/checkpoints\", true);\nenv.setStateBackend(rocksDBStateBackend);\n\n// 开启Checkpoint，间隔为 3 分钟\nenv.enableCheckpointing(TimeUnit.MINUTES.toMillis(3));\n// 配置 Checkpoint\nCheckpointConfig checkpointConf = env.getCheckpointConfig();\ncheckpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)\n// 最小间隔 4分钟\ncheckpointConf.setMinPauseBetweenCheckpoints(TimeUnit.MINUTES.toMillis(4))\n// 超时时间 10分钟\ncheckpointConf.setCheckpointTimeout(TimeUnit.MINUTES.toMillis(10));\n// 保存checkpoint\ncheckpointConf.enableExternalizedCheckpoints(\nCheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n```\n\n# 反压处理\n\n## 3.1 概述\n\nFlink 网络流控及反压的介绍：\n\nhttps://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626\n\n### 3.1.1    反压的理解\n\n简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。\n\n反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。\n\n### 3.1.2    反压的危害\n\n反压如果不能得到正确的处理，可能会影响到 checkpoint时长和 state大小，甚至可能会导致资源耗尽甚至系统崩溃。\n\n- 1）影响 checkpoint 时长：barrier 不会越过普通数据，数据处理被阻塞也会导致checkpointbarrier 流经整个数据管道的时长变长，导致 checkpoint 总体时间（End toEndDuration）变长。\n- 2）影响 state 大小：barrier 对齐时，接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到 state 里面，导致 checkpoint 变大。\n\n这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint**超时失败**，而 state 大小同样可能拖慢 checkpoint 甚至导致 **OOM**（使用 Heap-basedStateBackend）或者物理内存使用**超出容器资源**（使用 RocksDBStateBackend）的稳定性问题。\n\n**因此，我们在生产中要尽量避免出现反压的情况。**\n\n## 3.2 定位反压节点\n\n解决反压首先要做的是定位到造成反压的节点，排查的时候，先把operatorchain 禁用，方便定位到具体算子。\n\n\n\n提交UvDemo:\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n### 3.2.1    利用 FlinkWebUI 定位\n\nFlinkWebUI 的反压监控提供了 SubTask 级别的反压监控，1.13 版本以前是通过周期性对  Task  线程的栈信息采样，得到线程被阻塞在请求  Buffer（意味着被下游队列阻塞）\n\n的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1以下则为 OK，0.1\n\n至 0.5为 LOW，而超过 0.5则为 HIGH。\n\nFlink1.13 优化了反压检测的逻辑（使用基于任务 Mailbox计时，而不在再于堆栈采样），并且重新实现了作业图的 UI展示：Flink现在在 UI 上通过颜色和数值来展示繁忙和反压的程度。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674284140-b680f841-3ad4-4250-87fd-8c331333f1f5.png)\n\n1）通过WebUI看到 Map算子处于反压：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674446026-5ec8c33c-cadc-44c9-9d00-b644899f52d6.png)\n\n3）分析瓶颈算子\n\n如果处于反压状态，那么有两种可能性：\n\n（1）  该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。这种情况，该节点是反压的根源节点，它是从 SourceTask到 Sink Task 的第一个出现反压的节点。**（很少出现，表现为：反压算子一进多出，后面的算子处理速度慢，从这个反压算子开始，后面的算子都反压了。图示，绿色为反压节点：**\n\n**（OK-> OK->** **反** **->反 -> 反 ）**\n\n**一进多出，输入缓存区使用率可能高也可能低，输出缓存区使用率高**\n\n（2）  下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。这种情况，需要继续排查下游节点，一直找到第一个为OK的一般就是根源节点。**（表现为：这个反压算子处理速度慢，阻塞了前面的算子，导致前面的算子反压了，其后面的算子表现为不反压。图示，绿色为反压节点：**\n\n​      **（反 -> 反 ->** **OK**-> OK-> OK）\n\n**输入缓存区使用率高，输出缓存区使用率低**\n\n总体来看，如果我们找到第一个出现反压的节点，反压根源要么是就这个节点，要么是它紧接着的下游节点。\n\n通常来讲，第二种情况更常见。如果无法确定，还需要结合 Metrics进一步判断。\n\n### 3.2.2    利用 Metrics 定位\n\n监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为\n\n有用的是以下几个 Metrics:\n\n| **Metris**                        | **描述**                        |\n| --------------------------------- | ------------------------------- |\n| outPoolUsage                      | 发送端 Buffer 的使用率          |\n| inPoolUsage                       | 接收端 Buffer 的使用率          |\n| floatingBuffersUsage（1.9 以上）  | 接收端 FloatingBuffer 的使用率  |\n| exclusiveBuffersUsage（1.9 以上） | 接收端 ExclusiveBuffer 的使用率 |\n\n其中 inPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage。\n\n#### 1）根据指标分析反压\n\n分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(1.9 以上):\n\n|                                            | **outPoolUsage** **低**                                      | **outPoolUsage** **高**                    |\n| ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------ |\n| **inPoolUsage** **低**                     | 正常                                                         | 被下游反压，处于临时情况（还没传递到上游） |\n| 可能是反压的根源，一条输入多条输出的场景   |                                                              |                                            |\n| **inPoolUsage** **高**                     | 如果上游所有 outPoolUsage 都是低，有可能最终可能导致反压（还没传递到上游） | 被下游反压                                 |\n| 如果上游的 outPoolUsage 是高，则为反压根源 |                                                              |                                            |\n\n#### 2）可以进一步分析数据传输\n\nFlink1.9 及以上版本，还可以根据 floatingBuffersUsage/exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游Subtask 的数据传输。\n\n在流量较大时，Channel  的  ExclusiveBuffer  可能会被写满，此时  Flink  会向  BufferPool 申请剩余的 FloatingBuffer。这些 **FloatingBuffer 属于备用 Buffer。**\n\n\n\n|                                                              | **exclusiveBuffersUsage** **低**        | **exclusiveBuffersUsage** **高**                  |\n| ------------------------------------------------------------ | --------------------------------------- | ------------------------------------------------- |\n| **floatingBuffersUsage** **低**所有上游**outPoolUsage** **低** | 正常                                    |                                                   |\n| **floatingBuffersUsage** **低**上游某个**outPoolUsage** **高** | 潜在的网络瓶颈                          |                                                   |\n| **floatingBuffersUsage**高所有上游**outPoolUsage** **低**    | 最终对部分inputChannel 反压（正在传递） | 最终对大多数或所有   inputChannel反压（正在传递） |\n| **floatingBuffersUsage**高上游某个**outPoolUsage** **高**    | 只对部分 inputChannel 反压              | 对大多数或所有 inputChannel 反压                  |\n\n总结：\n\n- 1）floatingBuffersUsage 为高，则表明反压正在传导至上游\n- 2）同时 exclusiveBuffersUsage 为低，则表明可能有倾斜\n\n\n\n比如，floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数\n\nchannel 占用了大部分的 FloatingBuffer。\n\n## 3.3 反压的原因及处理\n\n注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。\n\n定位到反压节点后，分析造成原因的办法主要是观察 TaskThread。按照下面的顺序，一步一步去排查。\n\n### 3.3.1    查看是否数据倾斜\n\n**在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI各**\n\n**个 SubTask 的 RecordsSent 和 RecordReceived 来确认，另外 Checkpointdetail里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675365111-f2598a4c-7ae6-4c6b-852b-a2c31b53623e.png)\n\n（关于数据倾斜的详细解决方案，会在下一章节详细讨论）\n\n### 3.3.2    使用火焰图分析\n\n如果不是数据倾斜，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题），需要找到瓶颈算子中的哪部分计算逻辑消耗巨大。\n\n最有用的办法就是对 TaskManager 进行 CPUprofile，从中我们可以分析到 TaskThread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面；如果不是的话要看 TaskThread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是checkpoint 或者 GC 等系统活动导致的暂时系统暂停。\n\n#### 1）开启火焰图功能\n\nFlink1.13直接在 WebUI提供 JVM的 CPU 火焰图，这将大大简化性能瓶颈的分析，默认是不开启的，需要修改参数：\n\nrest.flamegraph.enabled:true#默认false                                                                          \n\n\n\n也可以在提交时指定：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n#### 2）WebUI 查看火焰图\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675647317-7df4c4eb-e01f-4637-9d0e-a9980331f2c2.png)\n\n火焰图是通过对堆栈跟踪进行多次采样来构建的。每个方法调用都由一个条形表示，其中条形的长度与其在样本中出现的次数成正比。\n\n- On-CPU: 处于 [RUNNABLE, NEW]状态的线程\n- Off-CPU: 处于 [TIMED_WAITING, WAITING, BLOCKED]的线程，用于查看在样本中发现的阻塞调用。\n\n#### 3）分析火焰图\n\n颜色没有特殊含义，具体查看：\n\n- 纵向是调用链，从下往上，顶部就是正在执行的函数\n- 横向是样本出现次数，可以理解为执行时长。\n\n**看顶层的哪个函数占据的宽度最大。只要有\"平顶\"（plateaus），就表示该函数可能存在性能问题。**\n\n如果是 Flink1.13 以前的版本，可以手动做火焰图：\n\n如何生成火焰图：http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/\n\n### 3.3.3    分析GC 情况\n\nTaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManagerJVM 各区内存不合理导致的频繁 FullGC 甚至失联。通常建议使用默认的 G1 垃圾回收器。\n\n可以通过打印 GC 日志（-XX:+PrintGCDetails），使用 GC 分析器（GCViewer 工具）来验证是否处于这种情况。\n\n\n\n- 在 Flink 提交脚本中,设置 JVM 参数，打印 GC 日志：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Denv.java.opts=\"-XX:+PrintGCDetails-XX:+PrintGCDateStamps\"\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n\n\n- 下载 GC 日志的方式：\n\n因为是 onyarn 模式，运行的节点一个一个找比较麻烦。可以打开 WebUI，选择JobManager 或者 TaskManager，点击 Stdout，即可看到 GC 日志，点击下载按钮即可将 GC日志通过 HTTP的方式下载下来。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654679097595-18b82b7c-8bd5-4d21-b720-44c795ce377a.png)\n\n- 分析 GC 日志：\n\n通过 GC 日志分析出单个 FlinkTaskmanager 堆总大小、年轻代、老年代分配的内存空间、FullGC 后老年代剩余大小等，相关指标定义可以去 Github 具体查看。\n\nGCViewer 地址：https://github.com/chewiebug/GCViewer\n\nLinux 下分析：\n\njava -jargcviewer_1.3.4.jargc.log                                                                                    \n\nWindows 下分析：\n\n直接双击gcviewer_1.3.4.jar，打开GUI界面，选择gc的log打开         \n\n​                      \n\n扩展：最重要的指标是FullGC 后，老年代剩余大小这个指标，按照《Java 性能优化权威指南》这本书 Java 堆大小计算法则，设 FullGC 后老年代剩余大小空间为 M，那么堆的大小建议 3~4 倍 M，新生代为 1~1.5 倍 M，老年代应为 2~3 倍 M。\n\n### 3.3.4    外部组件交互\n\n如果发现我们的 Source端数据读取性能比较低或者 Sink端写入性能较差，需要检查第三方组件是否遇到瓶颈，还有就是做维表join时的性能问题。\n\n例如：\n\nKafka集群是否需要扩容，Kafka 连接器是否并行度较低\n\nHBase的 rowkey 是否遇到热点问题，是否请求处理不过来\n\nClickHouse并发能力较弱，是否达到瓶颈\n\n……\n\n关于第三方组件的性能问题，需要结合具体的组件来分析，最常用的思路：\n\n- 1）异步 io+热缓存来优化读写性能\n- 2）先攒批再读写维表join参考：\n\nhttps://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3\n\nhttps://www.jianshu.com/p/a62fa483ff54\n\n\n\n# 四、数据倾斜\n\n## 4.1  判断是否存在数据倾斜\n\n相同 Task 的多个 Subtask 中， 个别 Subtask 接收到的数据量明显大于其他Subtask 接收到的数据量，通过 FlinkWebUI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692839400-88f4eb2d-9389-4011-a676-2f6da336cb39.png)\n\n另外， 有时 Checkpointdetail 里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。\n\n## 4.2 数据倾斜的解决\n\n### 4.2.1    keyBy 后的聚合操作存在数据倾斜\n\n#### 1）为什么不能直接用二次聚合来处理（没有卵用）\n\nFlink是实时流处理，如果keyby之后的聚合操作存在数据倾斜，且没有开窗口（没攒批）的情况下，简单的认为使用两阶段聚合，是不能解决问题的。因为这个时候Flink是来一条处理一条，且向下游发送一条结果，对于原来 keyby的维度（第二阶段聚合）来讲，数据量并没有减少，且结果重复计算（非 FlinkSQL，未使用回撤流），如下图所示：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692995562-f3b6caac-04e3-45ac-87bc-92286cb10e2b.png)\n\n#### 2）使用 LocalKeyBy 的思想\n\n在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后，再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。类似 MapReduce中 Combiner的思想，但是这要求聚合操作必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。从 FlinkLocalKeyBy实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。\n\n#### 实现方式：\n\n- DataStreamAPI 需要自己写代码实现\n- SQL 可以指定参数，开启miniBatch 和 LocalGlobal 功能（推荐，后续介绍）\n\n### 4.1.1    keyBy之前发生数据倾斜\n\n如果 keyBy 之前就存在数据倾斜，上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。\n\n对于不存在 keyBy 的 Flink 任务也会出现该情况。\n\n这种情况，需要让 Flink 任务强制进行shuffle。使用 shuffle、rebalance 或 rescale\n\n算子即可将数据均匀分配，从而解决数据倾斜的问题。\n\n### 4.1.2    keyBy 后的窗口聚合操作存在数据倾斜\n\n因为使用了窗口，变成了有界数据（攒批）的处理，窗口默认是触发时才会输出一条结果发往下游，所以可以使用两阶段聚合的方式：\n\n#### 1）实现思路：\n\n- 第一阶段聚合：key拼接随机数前缀或后缀，进行 keyby、开窗、聚合\n\n**注意：聚合完不再是 WindowedStream，要获取 WindowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）**\n\n- 第二阶段聚合：按照原来的 key 及windowEnd 作keyby、聚合\n\nSQL写法参考：https://zhuanlan.zhihu.com/p/197299746\n","source":"_posts/bigdata/flink/Flink调优.md","raw":"---\ntitle: Flink调优\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\ntop_img: /img/bg/banner.gif\nabbrlink: 54105\ndate: 2022-08-08 20:23:04\nupdated: 2022-08-08 20:23:04\ncover:\ndescription:\nkeywords:\n---\n\n# 内存设置（1CPU配置4G内存）\n\n> bin/flink run \\\n>\n> -t yarn-per-job \\\n>\n> -d \\\n>\n> -p 5 \\ 指定并行度\n>\n> -Dyarn.application.queue=test \\ 指定yarn队列\n>\n> -Djobmanager.memory.process.size=2048mb \\ JM2~4G足够\n>\n> -Dtaskmanager.memory.process.size=6144mb \\ 单个TM2~8G足够\n>\n> -Dtaskmanager.numberOfTaskSlots=2 \\ **与容器核数1core：1slot或1core：2slot**\n>\n> -c com.atguigu.app.dwd.LogBaseApp \\\n>\n> /opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar\n\nFlink是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS/TPS来描述数据情况。\n\n##  TaskManager 内存模型  \n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654665961981-86c260ab-5310-4674-ac61-6a1d1f738f18.png)\n\n### 1、内存模型详解\n\n#### JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head\n\n1）JVMmetaspace：JVM 元空间\n\ntaskmanager.memory.jvm-metaspace.size，默认 256mb\n\n\n\n2）JVMover-head执行开销：JVM执行时自身所需要的内容，包括线程堆栈、IO、编译缓存等所使用的内存。\n\ntaskmanager.memory.jvm-overhead.fraction，默认 0.1\n\ntaskmanager.memory.jvm-overhead.min，默认 192mb\n\ntaskmanager.memory.jvm-overhead.max，默认 1gb\n\n\n\n**总进程内存\\*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min/max**\n\n**大小**\n\n\n\n#### 框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。\n\n堆内：taskmanager.memory.framework.heap.size，默认 128MB\n\n堆外：taskmanager.memory.framework.off-heap.size，默认 128MB\n\n#### Task内存：Task执行用户代码时所使用的内存\n\n堆内：taskmanager.memory.task.heap.size，默认 none，由 Flink 内存扣除掉其他部分的内存得到。\n\n堆外：taskmanager.memory.task.off-heap.size，默认 0，表示不使用堆外内存\n\n#### 网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\n\n**堆外：**\n\ntaskmanager.memory.network.fraction，默认 0.1\n\ntaskmanager.memory.network.min，默认 64mb\n\ntaskmanager.memory.network.max，默认 1gb\n\n**Flink 内存\\*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min/max大小**\n\n  \n\n#### 托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。\n\n堆外：taskmanager.memory.managed.fraction，默认 0.4\n\ntaskmanager.memory.managed.size，默认 none\n\n**如果 size 没指定，则等于 Flink 内存\\*fraction**\n\n## 2、案例分析  \n\n基于Yarn模式，一般参数指定的是总进程内存，taskmanager.memory.process.size，比如指定为 4G，每一块内存得到大小如下：\n\n（1）计算 Flink 内存\n\nJVM 元空间 256m\n\nJVM 执行开销： 4g*0.1=409.6m，在[192m,1g]之间，最终结果 409.6m\n\nFlink 内存=4g-256m-409.6m=3430.4m\n\n（2）网络内存=3430.4m*0.1=343.04m，在[64m,1g]之间，最终结果 343.04m\n\n（3）托管内存=3430.4m*0.4=1372.16m\n\n（4）框架内存，堆内和堆外都是 128m\n\n（5）Task堆内内存=3430.4m-128m-128m-343.04m-1372.16m=1459.2m\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667261844-9b48b348-1bcb-4ca8-b556-f63a3680cf83.png)\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667279269-d43c4812-9561-433a-83fe-a8d70b5fb5b9.png)\n\n### 所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\n\n## 合理利用 cpu 资源\n\nYarn 的**容量调度器**默认情况下是使用“DefaultResourceCalculator”分配策略，只根据内存调度资源，所以在 Yarn 的资源管理页面上看到每个容器的 vcore 个数还是 1。\n\n可以修改策略为 DominantResourceCalculator，该资源计算器在计算资源的时候会综合考虑 cpu 和内存的情况。在capacity-scheduler.xml 中修改属性:\n\n```xml\n<property>\n  <name>yarn.scheduler.capacity.resource-calculator</name>\n  <!-- <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value> -->\n  <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>\n</property>\n```\n\n### 1.1.1    使用DefaultResourceCalculator 策略\n\n```shell\nbin/flink run \\\n-t yarn-per-job \\\n-d \\\n-p 5 \\\n-Drest.flamegraph.enabled=true \\\n-Dyarn.application.queue=test \\\n-Djobmanager.memory.process.size=1024mb \\\n-Dtaskmanager.memory.process.size=4096mb \\\n-Dtaskmanager.numberOfTaskSlots=2 \\\n-c com.atguigu.flink.tuning.UvDemo \\\n/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n```\n\n可以看到一个容器只有一个 vcore：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668251950-033e4bc0-4b65-4fe8-b309-45a29956922b.png)\n\n### 1.1.2    使用DominantResourceCalculator 策略\n\n修改后 yarn 配置后，分发配置并重启 yarn，再次提交 flink 作业：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=4096mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo\\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n看到容器的 vcore 数变了:\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668344371-82744e2d-89b2-4fab-8a09-f77475df1088.png)\n\nJobManager1 个，占用 1 个容器，vcore=1\n\nTaskManager3 个，占用 3 个容器，每个容器 vcore=2，总 vcore=2*3=6，因为默认单个容器的 vcore 数=单 TM 的slot 数\n\n### 1.1.3    使用 DominantResourceCalculator 策略并指定容器**vcore 数**\n\n指定yarn 容器的 vcore 数，提交：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Dyarn.containers.vcores=3\\\n>\n>  -Djobmanager.memory.process.size=1024mb \\ -Dtaskmanager.memory.process.size=4096mb \\ -Dtaskmanager.numberOfTaskSlots=2 \\ -c com.atguigu.flink.tuning.UvDemo \\ /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar  \n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668509233-7292aa6f-0e54-4799-ba38-ab72659ef824.png)\n\nJobManager1 个，占用 1 个容器，vcore=1\n\nTaskManager3 个，占用 3 个容器，每个容器vcore =3，总 vcore=3*3=9\n\n# RocksDB大状态调优\n\nRocksDB 是基于 LSM Tree 实现的（类似HBase），写数据都是先缓存到内存中，所以RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record/s。**使用RocksDB 时，状态大小仅受可用磁盘空间量的限制，性能瓶颈主要在于 RocksDB对磁盘的读请求，每次读写操作都必须对数据进行反序列化或者序列化。**所以当处理性能不够时，仅需要横向扩展并行度即可提高整个Job 的吞吐量。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654669015363-a35261ab-d4ff-4068-a013-eecfe78a5c7d.png)\n\n\n\n从 Flink1.10 开始，Flink 默认将 RocksDB 的内存大小配置为每个 taskslot 的托管内存。调试内存性能的问题主要是通过调整配置项 taskmanager.memory.managed.size或者 taskmanager.memory.managed.fraction以增加 Flink 的托管内存(即堆外的托管内存)。进一步可以调整一些参数进行高级性能调优，这些参数也可以在应用程序中通过RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory)指定。下面介绍\n\n提高资源利用率的几个重要配置：\n\n### 2.1.1   开启State访问性能监控\n\nFlink 1.13 中引入了 State 访问的性能监控，即 latency trackig state。此功能不局限于 StateBackend 的类型，自定义实现的 StateBackend 也可以复用此功能。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654670053632-0e169f44-1340-4202-ab6a-bd9a6173a14a.png)\n\nState访问性能监控会产生一定的性能影响，所以，默认每 100次做一次取样(sample)，对不同的 StateBackend 性能损失影响不同：\n\n- 对于 RocksDBStateBackend，性能损失大概在 1% 左右\n- 对于 HeapStateBackend，性能损失最多可达 10%\n\n```yaml\nstate.backend.latency-track.keyed-state-enabled：true #启用访问状态的性能监控 \nstate.backend.latency-track.sample-interval: 100 #采样间隔 \nstate.backend.latency-track.history-size: 128 #保留的采样数据个数，越大越精确 \nstate.backend.latency-track.state-name-as-variable: true #将状态名作为变量  \n```\n\n正常开启第一个参数即可。\n\n> bin/flink run \\\n>\n> -t yarn-per-job \\\n>\n> -d \\\n>\n> -p 5 \\\n>\n> -Drest.flamegraph.enabled=true \\\n>\n> -Dyarn.application.queue=test \\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=4096mb \\\n>\n> -Dtaskmanager.numberOfTaskSlots=2 \\\n>\n>  -Dstate.backend.latency-track.keyed-state-enabled=true \\ \n>\n> -c com.atguigu.flink.tuning.RocksdbTuning \\ /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar  \n\n### 2.1.2    开启增量检查点和本地恢复\n\n1）开启增量检查点\n\nRocksDB 是目前唯一可用于支持有状态流处理应用程序增量检查点的状态后端，可以修改参数开启增量检查点：\n\nstate.backend.incremental: true #默认 false，改为 true。 \n\n或代码中指定 new EmbeddedRocksDBStateBackend(true)  \n\n2）开启本地恢复\n\n当 Flink任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs拉取数据。本地恢复目前仅涵盖键控类型的状态后端（RocksDB），MemoryStateBackend不支持本地恢复并忽略此选项。\n\nstate.backend.local-recovery:true\n\n### 2.1.3    调整预定义选项\n\nFlink针对不同的设置为 RocksDB提供了一些预定义的选项集合,其中包含了后续提到的一些参数，如果调整预定义选项后还达不到预期，再去调整后面的 block、writebuffer等参数。\n\n当 前 支 持 的 预 定 义 选 项 有   DEFAULT 、 SPINNING_DISK_OPTIMIZED 、\n\nSPINNING_DISK_OPTIMIZED_HIGH_MEM 或FLASH_SSD_OPTIMIZED。有条件上 SSD\n\n的，可以指定为 FLASH_SSD_OPTIMIZED\n\n state.backend.rocksdb.predefined-options： SPINNING_DISK_OPTIMIZED_HIGH_MEM #设置为机械硬盘+内存模式  \n\n### 2.1.4    增大 block 缓存\n\n整个 RocksDB 共享一个 blockcache，读数据时内存的 cache 大小，该参数越大读\n\n数据时缓存命中率越高，默认大小为8MB，建议设置到64~256MB。\n\nstate.backend.rocksdb.block.cache-size:64m     #默认8m  \n\n### 2.1.5    增大writebuffer 和 level 阈值大小\n\nRocksDB 中，每个 State 使用一个 ColumnFamily，每个 ColumnFamily 使用独占的 writebuffer，默认 64MB，建议调大。\n\n调整这个参数通常要适当增加 L1层的大小阈值 max-size-level-base，默认 256m。\n\n该值太小会造成能存放的 SST 文件过少，层级变多造成查找困难，太大会造成文件过多，合并困难。建议设为 target_file_size_base（默认 64MB） 的倍数，且不能太小，例如 5~10倍，即 320~640MB。\n\nstate.backend.rocksdb.writebuffer.size: 128m\n\nstate.backend.rocksdb.compaction.level.max-size-level-base:320m   \n\n### 2.1.6    增大write buffer 数量\n\n每个 ColumnFamily对应的 writebuffer 最大数量，这实际上是内存中“只读内存表“的最大数量，默认值是 2。对于机械磁盘来说，如果内存足够大，可以调大到 5左右\n\nstate.backend.rocksdb.writebuffer.count:5                                                                     \n\n### 2.1.7    增大后台线程数和writebuffer 合并数\n\n1）增大线程数\n\n用于后台 flush和合并 sst文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4等更大的值\n\nstate.backend.rocksdb.thread.num: 4                                                                             \n\n2）增大writebuffer 最小合并数\n\n将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 最小数量，默认\n\n值为 1，可以调成 3。\n\nstate.backend.rocksdb.writebuffer.number-to-merge:3                                             \n\n### 2.1.8    开启分区索引功能\n\nFlink1.13 中对 RocksDB 增加了分区索引功能，复用了 RocksDB 的partitionedIndex&filter 功能，简单来说就是对 RocksDB 的 partitionedIndex 做了多级索引。也就是将内存中的最上层常驻，下层根据需要再 load回来，这样就大大降低了数据 Swap竞争。线上测试中，相对于**内存比较小**的场景中，性能提升 10 倍左右。如果在内存管控下 Rocksdb 性能不如预期的话，这也能成为一个性能优化点。\n\nstate.backend.rocksdb.memory.partitioned-index-filters:true   #默认false                \n\n\n\n**2.1.9**    **参数设定案例**\n\n```sh\nbin/flinkrun\\\n-tyarn-per-job\\\n-d\\\n-p5\\\n-Drest.flamegraph.enabled=true\\\n-Dyarn.application.queue=test\\\n-Djobmanager.memory.process.size=1024mb \\\n-Dtaskmanager.memory.process.size=4096mb\\\n-Dtaskmanager.numberOfTaskSlots=2\\\n-Dstate.backend.incremental=true\\\n-Dstate.backend.local-recovery=true\\\n-Dstate.backend.rocksdb.predefined-options=SPINNING_DISK_OPTIMIZED_HIGH_MEM\\\n-Dstate.backend.rocksdb.block.cache-size=64m\\\n-Dstate.backend.rocksdb.writebuffer.size=128m\\\n-Dstate.backend.rocksdb.compaction.level.max-size-level-base=320m\\\n-Dstate.backend.rocksdb.writebuffer.count=5 \\\n-Dstate.backend.rocksdb.thread.num=4\\\n-Dstate.backend.rocksdb.writebuffer.number-to-merge=3\\\n-Dstate.backend.rocksdb.memory.partitioned-index-filters=true\\\n-Dstate.backend.latency-track.keyed-state-enabled=true\\\n-ccom.atguigu.flink.tuning.RocksdbTuning\\\n/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n```\n\n\n\n### 设置本地 RocksDB 多目录\n\n在flink-conf.yaml 中配置：\n\n```plain\nstate.backend.rocksdb.localdir: /data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb\n```\n\n\n\n注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。**当设置多个 RocksDB 本地磁盘目录时，Flink 会****随机选择****要使用的目录，所以就可能存在三个并行度共用同一目录的情况。**如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。\n\n**当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。**\n\n如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。**由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘IO性能消耗确实比较大。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662632337-7fe1e6c6-5fe2-412e-82e8-77f3c81458b7.png)\n\n如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj磁盘。可以看到 sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。**如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下**，**从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662673431-6575b710-490c-49c4-bec7-f4b7964b3fc7.png)\n\n- **state.backend.incremental：**开启增量检查点，默认false，改为true。\n- **state.backend.rocksdb.predefined-options：**SPINNING_DISK_OPTIMIZED_HIGH_MEM设置为机械硬盘+内存模式，有条件上SSD，指定为FLASH_SSD_OPTIMIZED\n- **state.backend.rocksdb.block.cache-size**: 整个 RocksDB 共享一个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。\n- **state.backend.rocksdb.thread.num**: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。\n- **state.backend.rocksdb.writebuffer.size**: RocksDB 中，每个 State 使用一个 Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M\n- **state.backend.rocksdb.writebuffer.count**: 每个 Column Family 对应的 writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5 左右\n- **state.backend.rocksdb.writebuffer.number-to-merge**: 将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成3。\n- **state.backend.local-recovery**: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据\n\n## Checkpoint设置\n\n一般我们的 Checkpoint 时间间隔可以设置为分钟级别（1~5分钟），例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5~10 分钟一次Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次Checkpoint 之间至少暂停 4或8 分钟。\n\n同时，也需要考虑时效性的要求,需要在时效性和性能之间做一个平衡，如果时效性要求高，结合 end- to-end 时长，设置秒级或毫秒级。\n\n如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看 Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。\n\nRocksDB相关参数在1.3中已说明，可以在flink-conf.yaml指定，也可以在Job的代码中调用API单独指定，这里不再列出。\n\n```scala\n// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint\nRocksDBStateBackend rocksDBStateBackend = new RocksDBStateBackend(\"hdfs://hadoop102:8020/flink/checkpoints\", true);\nenv.setStateBackend(rocksDBStateBackend);\n\n// 开启Checkpoint，间隔为 3 分钟\nenv.enableCheckpointing(TimeUnit.MINUTES.toMillis(3));\n// 配置 Checkpoint\nCheckpointConfig checkpointConf = env.getCheckpointConfig();\ncheckpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)\n// 最小间隔 4分钟\ncheckpointConf.setMinPauseBetweenCheckpoints(TimeUnit.MINUTES.toMillis(4))\n// 超时时间 10分钟\ncheckpointConf.setCheckpointTimeout(TimeUnit.MINUTES.toMillis(10));\n// 保存checkpoint\ncheckpointConf.enableExternalizedCheckpoints(\nCheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n```\n\n# 反压处理\n\n## 3.1 概述\n\nFlink 网络流控及反压的介绍：\n\nhttps://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626\n\n### 3.1.1    反压的理解\n\n简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。\n\n反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。\n\n### 3.1.2    反压的危害\n\n反压如果不能得到正确的处理，可能会影响到 checkpoint时长和 state大小，甚至可能会导致资源耗尽甚至系统崩溃。\n\n- 1）影响 checkpoint 时长：barrier 不会越过普通数据，数据处理被阻塞也会导致checkpointbarrier 流经整个数据管道的时长变长，导致 checkpoint 总体时间（End toEndDuration）变长。\n- 2）影响 state 大小：barrier 对齐时，接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到 state 里面，导致 checkpoint 变大。\n\n这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint**超时失败**，而 state 大小同样可能拖慢 checkpoint 甚至导致 **OOM**（使用 Heap-basedStateBackend）或者物理内存使用**超出容器资源**（使用 RocksDBStateBackend）的稳定性问题。\n\n**因此，我们在生产中要尽量避免出现反压的情况。**\n\n## 3.2 定位反压节点\n\n解决反压首先要做的是定位到造成反压的节点，排查的时候，先把operatorchain 禁用，方便定位到具体算子。\n\n\n\n提交UvDemo:\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n### 3.2.1    利用 FlinkWebUI 定位\n\nFlinkWebUI 的反压监控提供了 SubTask 级别的反压监控，1.13 版本以前是通过周期性对  Task  线程的栈信息采样，得到线程被阻塞在请求  Buffer（意味着被下游队列阻塞）\n\n的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1以下则为 OK，0.1\n\n至 0.5为 LOW，而超过 0.5则为 HIGH。\n\nFlink1.13 优化了反压检测的逻辑（使用基于任务 Mailbox计时，而不在再于堆栈采样），并且重新实现了作业图的 UI展示：Flink现在在 UI 上通过颜色和数值来展示繁忙和反压的程度。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674284140-b680f841-3ad4-4250-87fd-8c331333f1f5.png)\n\n1）通过WebUI看到 Map算子处于反压：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674446026-5ec8c33c-cadc-44c9-9d00-b644899f52d6.png)\n\n3）分析瓶颈算子\n\n如果处于反压状态，那么有两种可能性：\n\n（1）  该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。这种情况，该节点是反压的根源节点，它是从 SourceTask到 Sink Task 的第一个出现反压的节点。**（很少出现，表现为：反压算子一进多出，后面的算子处理速度慢，从这个反压算子开始，后面的算子都反压了。图示，绿色为反压节点：**\n\n**（OK-> OK->** **反** **->反 -> 反 ）**\n\n**一进多出，输入缓存区使用率可能高也可能低，输出缓存区使用率高**\n\n（2）  下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。这种情况，需要继续排查下游节点，一直找到第一个为OK的一般就是根源节点。**（表现为：这个反压算子处理速度慢，阻塞了前面的算子，导致前面的算子反压了，其后面的算子表现为不反压。图示，绿色为反压节点：**\n\n​      **（反 -> 反 ->** **OK**-> OK-> OK）\n\n**输入缓存区使用率高，输出缓存区使用率低**\n\n总体来看，如果我们找到第一个出现反压的节点，反压根源要么是就这个节点，要么是它紧接着的下游节点。\n\n通常来讲，第二种情况更常见。如果无法确定，还需要结合 Metrics进一步判断。\n\n### 3.2.2    利用 Metrics 定位\n\n监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为\n\n有用的是以下几个 Metrics:\n\n| **Metris**                        | **描述**                        |\n| --------------------------------- | ------------------------------- |\n| outPoolUsage                      | 发送端 Buffer 的使用率          |\n| inPoolUsage                       | 接收端 Buffer 的使用率          |\n| floatingBuffersUsage（1.9 以上）  | 接收端 FloatingBuffer 的使用率  |\n| exclusiveBuffersUsage（1.9 以上） | 接收端 ExclusiveBuffer 的使用率 |\n\n其中 inPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage。\n\n#### 1）根据指标分析反压\n\n分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(1.9 以上):\n\n|                                            | **outPoolUsage** **低**                                      | **outPoolUsage** **高**                    |\n| ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------ |\n| **inPoolUsage** **低**                     | 正常                                                         | 被下游反压，处于临时情况（还没传递到上游） |\n| 可能是反压的根源，一条输入多条输出的场景   |                                                              |                                            |\n| **inPoolUsage** **高**                     | 如果上游所有 outPoolUsage 都是低，有可能最终可能导致反压（还没传递到上游） | 被下游反压                                 |\n| 如果上游的 outPoolUsage 是高，则为反压根源 |                                                              |                                            |\n\n#### 2）可以进一步分析数据传输\n\nFlink1.9 及以上版本，还可以根据 floatingBuffersUsage/exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游Subtask 的数据传输。\n\n在流量较大时，Channel  的  ExclusiveBuffer  可能会被写满，此时  Flink  会向  BufferPool 申请剩余的 FloatingBuffer。这些 **FloatingBuffer 属于备用 Buffer。**\n\n\n\n|                                                              | **exclusiveBuffersUsage** **低**        | **exclusiveBuffersUsage** **高**                  |\n| ------------------------------------------------------------ | --------------------------------------- | ------------------------------------------------- |\n| **floatingBuffersUsage** **低**所有上游**outPoolUsage** **低** | 正常                                    |                                                   |\n| **floatingBuffersUsage** **低**上游某个**outPoolUsage** **高** | 潜在的网络瓶颈                          |                                                   |\n| **floatingBuffersUsage**高所有上游**outPoolUsage** **低**    | 最终对部分inputChannel 反压（正在传递） | 最终对大多数或所有   inputChannel反压（正在传递） |\n| **floatingBuffersUsage**高上游某个**outPoolUsage** **高**    | 只对部分 inputChannel 反压              | 对大多数或所有 inputChannel 反压                  |\n\n总结：\n\n- 1）floatingBuffersUsage 为高，则表明反压正在传导至上游\n- 2）同时 exclusiveBuffersUsage 为低，则表明可能有倾斜\n\n\n\n比如，floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数\n\nchannel 占用了大部分的 FloatingBuffer。\n\n## 3.3 反压的原因及处理\n\n注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。\n\n定位到反压节点后，分析造成原因的办法主要是观察 TaskThread。按照下面的顺序，一步一步去排查。\n\n### 3.3.1    查看是否数据倾斜\n\n**在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI各**\n\n**个 SubTask 的 RecordsSent 和 RecordReceived 来确认，另外 Checkpointdetail里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。**\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675365111-f2598a4c-7ae6-4c6b-852b-a2c31b53623e.png)\n\n（关于数据倾斜的详细解决方案，会在下一章节详细讨论）\n\n### 3.3.2    使用火焰图分析\n\n如果不是数据倾斜，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题），需要找到瓶颈算子中的哪部分计算逻辑消耗巨大。\n\n最有用的办法就是对 TaskManager 进行 CPUprofile，从中我们可以分析到 TaskThread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面；如果不是的话要看 TaskThread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是checkpoint 或者 GC 等系统活动导致的暂时系统暂停。\n\n#### 1）开启火焰图功能\n\nFlink1.13直接在 WebUI提供 JVM的 CPU 火焰图，这将大大简化性能瓶颈的分析，默认是不开启的，需要修改参数：\n\nrest.flamegraph.enabled:true#默认false                                                                          \n\n\n\n也可以在提交时指定：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n#### 2）WebUI 查看火焰图\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675647317-7df4c4eb-e01f-4637-9d0e-a9980331f2c2.png)\n\n火焰图是通过对堆栈跟踪进行多次采样来构建的。每个方法调用都由一个条形表示，其中条形的长度与其在样本中出现的次数成正比。\n\n- On-CPU: 处于 [RUNNABLE, NEW]状态的线程\n- Off-CPU: 处于 [TIMED_WAITING, WAITING, BLOCKED]的线程，用于查看在样本中发现的阻塞调用。\n\n#### 3）分析火焰图\n\n颜色没有特殊含义，具体查看：\n\n- 纵向是调用链，从下往上，顶部就是正在执行的函数\n- 横向是样本出现次数，可以理解为执行时长。\n\n**看顶层的哪个函数占据的宽度最大。只要有\"平顶\"（plateaus），就表示该函数可能存在性能问题。**\n\n如果是 Flink1.13 以前的版本，可以手动做火焰图：\n\n如何生成火焰图：http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/\n\n### 3.3.3    分析GC 情况\n\nTaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManagerJVM 各区内存不合理导致的频繁 FullGC 甚至失联。通常建议使用默认的 G1 垃圾回收器。\n\n可以通过打印 GC 日志（-XX:+PrintGCDetails），使用 GC 分析器（GCViewer 工具）来验证是否处于这种情况。\n\n\n\n- 在 Flink 提交脚本中,设置 JVM 参数，打印 GC 日志：\n\n> bin/flinkrun\\\n>\n> -tyarn-per-job\\\n>\n> -d\\\n>\n> -p5 \\\n>\n> -Drest.flamegraph.enabled=true\\\n>\n> -Denv.java.opts=\"-XX:+PrintGCDetails-XX:+PrintGCDateStamps\"\\\n>\n> -Dyarn.application.queue=test\\\n>\n> -Djobmanager.memory.process.size=1024mb \\\n>\n> -Dtaskmanager.memory.process.size=2048mb\\\n>\n> -Dtaskmanager.numberOfTaskSlots=2\\\n>\n> -ccom.atguigu.flink.tuning.UvDemo \\\n>\n> /opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar\n\n\n\n- 下载 GC 日志的方式：\n\n因为是 onyarn 模式，运行的节点一个一个找比较麻烦。可以打开 WebUI，选择JobManager 或者 TaskManager，点击 Stdout，即可看到 GC 日志，点击下载按钮即可将 GC日志通过 HTTP的方式下载下来。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654679097595-18b82b7c-8bd5-4d21-b720-44c795ce377a.png)\n\n- 分析 GC 日志：\n\n通过 GC 日志分析出单个 FlinkTaskmanager 堆总大小、年轻代、老年代分配的内存空间、FullGC 后老年代剩余大小等，相关指标定义可以去 Github 具体查看。\n\nGCViewer 地址：https://github.com/chewiebug/GCViewer\n\nLinux 下分析：\n\njava -jargcviewer_1.3.4.jargc.log                                                                                    \n\nWindows 下分析：\n\n直接双击gcviewer_1.3.4.jar，打开GUI界面，选择gc的log打开         \n\n​                      \n\n扩展：最重要的指标是FullGC 后，老年代剩余大小这个指标，按照《Java 性能优化权威指南》这本书 Java 堆大小计算法则，设 FullGC 后老年代剩余大小空间为 M，那么堆的大小建议 3~4 倍 M，新生代为 1~1.5 倍 M，老年代应为 2~3 倍 M。\n\n### 3.3.4    外部组件交互\n\n如果发现我们的 Source端数据读取性能比较低或者 Sink端写入性能较差，需要检查第三方组件是否遇到瓶颈，还有就是做维表join时的性能问题。\n\n例如：\n\nKafka集群是否需要扩容，Kafka 连接器是否并行度较低\n\nHBase的 rowkey 是否遇到热点问题，是否请求处理不过来\n\nClickHouse并发能力较弱，是否达到瓶颈\n\n……\n\n关于第三方组件的性能问题，需要结合具体的组件来分析，最常用的思路：\n\n- 1）异步 io+热缓存来优化读写性能\n- 2）先攒批再读写维表join参考：\n\nhttps://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3\n\nhttps://www.jianshu.com/p/a62fa483ff54\n\n\n\n# 四、数据倾斜\n\n## 4.1  判断是否存在数据倾斜\n\n相同 Task 的多个 Subtask 中， 个别 Subtask 接收到的数据量明显大于其他Subtask 接收到的数据量，通过 FlinkWebUI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692839400-88f4eb2d-9389-4011-a676-2f6da336cb39.png)\n\n另外， 有时 Checkpointdetail 里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。\n\n## 4.2 数据倾斜的解决\n\n### 4.2.1    keyBy 后的聚合操作存在数据倾斜\n\n#### 1）为什么不能直接用二次聚合来处理（没有卵用）\n\nFlink是实时流处理，如果keyby之后的聚合操作存在数据倾斜，且没有开窗口（没攒批）的情况下，简单的认为使用两阶段聚合，是不能解决问题的。因为这个时候Flink是来一条处理一条，且向下游发送一条结果，对于原来 keyby的维度（第二阶段聚合）来讲，数据量并没有减少，且结果重复计算（非 FlinkSQL，未使用回撤流），如下图所示：\n\n![img](https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692995562-f3b6caac-04e3-45ac-87bc-92286cb10e2b.png)\n\n#### 2）使用 LocalKeyBy 的思想\n\n在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后，再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。类似 MapReduce中 Combiner的思想，但是这要求聚合操作必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。从 FlinkLocalKeyBy实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。\n\n#### 实现方式：\n\n- DataStreamAPI 需要自己写代码实现\n- SQL 可以指定参数，开启miniBatch 和 LocalGlobal 功能（推荐，后续介绍）\n\n### 4.1.1    keyBy之前发生数据倾斜\n\n如果 keyBy 之前就存在数据倾斜，上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。\n\n对于不存在 keyBy 的 Flink 任务也会出现该情况。\n\n这种情况，需要让 Flink 任务强制进行shuffle。使用 shuffle、rebalance 或 rescale\n\n算子即可将数据均匀分配，从而解决数据倾斜的问题。\n\n### 4.1.2    keyBy 后的窗口聚合操作存在数据倾斜\n\n因为使用了窗口，变成了有界数据（攒批）的处理，窗口默认是触发时才会输出一条结果发往下游，所以可以使用两阶段聚合的方式：\n\n#### 1）实现思路：\n\n- 第一阶段聚合：key拼接随机数前缀或后缀，进行 keyby、开窗、聚合\n\n**注意：聚合完不再是 WindowedStream，要获取 WindowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）**\n\n- 第二阶段聚合：按照原来的 key 及windowEnd 作keyby、聚合\n\nSQL写法参考：https://zhuanlan.zhihu.com/p/197299746\n","slug":"bigdata/flink/Flink调优","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004q8j5m4z9zfdy6","content":"<h1 id=\"内存设置（1CPU配置4G内存）\"><a href=\"#内存设置（1CPU配置4G内存）\" class=\"headerlink\" title=\"内存设置（1CPU配置4G内存）\"></a>内存设置（1CPU配置4G内存）</h1><blockquote>\n<p>bin&#x2F;flink run \\</p>\n<p>-t yarn-per-job \\</p>\n<p>-d \\</p>\n<p>-p 5 \\ 指定并行度</p>\n<p>-Dyarn.application.queue&#x3D;test \\ 指定yarn队列</p>\n<p>-Djobmanager.memory.process.size&#x3D;2048mb \\ JM2~4G足够</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;6144mb \\ 单个TM2~8G足够</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2 \\ <strong>与容器核数1core：1slot或1core：2slot</strong></p>\n<p>-c com.atguigu.app.dwd.LogBaseApp \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;gmall-flink&#x2F;gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar</p>\n</blockquote>\n<p>Flink是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS&#x2F;TPS来描述数据情况。</p>\n<h2 id=\"TaskManager-内存模型\"><a href=\"#TaskManager-内存模型\" class=\"headerlink\" title=\"TaskManager 内存模型\"></a>TaskManager 内存模型</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654665961981-86c260ab-5310-4674-ac61-6a1d1f738f18.png\" alt=\"img\"></p>\n<h3 id=\"1、内存模型详解\"><a href=\"#1、内存模型详解\" class=\"headerlink\" title=\"1、内存模型详解\"></a>1、内存模型详解</h3><h4 id=\"JVM-特定内存：JVM-本身使用的内存，包含-JVM-的-metaspace-和-over-head\"><a href=\"#JVM-特定内存：JVM-本身使用的内存，包含-JVM-的-metaspace-和-over-head\" class=\"headerlink\" title=\"JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head\"></a>JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head</h4><p>1）JVMmetaspace：JVM 元空间</p>\n<p>taskmanager.memory.jvm-metaspace.size，默认 256mb</p>\n<p>2）JVMover-head执行开销：JVM执行时自身所需要的内容，包括线程堆栈、IO、编译缓存等所使用的内存。</p>\n<p>taskmanager.memory.jvm-overhead.fraction，默认 0.1</p>\n<p>taskmanager.memory.jvm-overhead.min，默认 192mb</p>\n<p>taskmanager.memory.jvm-overhead.max，默认 1gb</p>\n<p><strong>总进程内存*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min&#x2F;max</strong></p>\n<p><strong>大小</strong></p>\n<h4 id=\"框架内存：Flink-框架，即-TaskManager-本身所占用的内存，不计入-Slot-的资源中。\"><a href=\"#框架内存：Flink-框架，即-TaskManager-本身所占用的内存，不计入-Slot-的资源中。\" class=\"headerlink\" title=\"框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。\"></a>框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。</h4><p>堆内：taskmanager.memory.framework.heap.size，默认 128MB</p>\n<p>堆外：taskmanager.memory.framework.off-heap.size，默认 128MB</p>\n<h4 id=\"Task内存：Task执行用户代码时所使用的内存\"><a href=\"#Task内存：Task执行用户代码时所使用的内存\" class=\"headerlink\" title=\"Task内存：Task执行用户代码时所使用的内存\"></a>Task内存：Task执行用户代码时所使用的内存</h4><p>堆内：taskmanager.memory.task.heap.size，默认 none，由 Flink 内存扣除掉其他部分的内存得到。</p>\n<p>堆外：taskmanager.memory.task.off-heap.size，默认 0，表示不使用堆外内存</p>\n<h4 id=\"网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\"><a href=\"#网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\" class=\"headerlink\" title=\"网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\"></a>网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区</h4><p><strong>堆外：</strong></p>\n<p>taskmanager.memory.network.fraction，默认 0.1</p>\n<p>taskmanager.memory.network.min，默认 64mb</p>\n<p>taskmanager.memory.network.max，默认 1gb</p>\n<p><strong>Flink 内存*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min&#x2F;max大小</strong></p>\n<h4 id=\"托管内存：用于-RocksDBStateBackend-的本地内存和批的排序、哈希表、缓存中间结果。\"><a href=\"#托管内存：用于-RocksDBStateBackend-的本地内存和批的排序、哈希表、缓存中间结果。\" class=\"headerlink\" title=\"托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。\"></a>托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。</h4><p>堆外：taskmanager.memory.managed.fraction，默认 0.4</p>\n<p>taskmanager.memory.managed.size，默认 none</p>\n<p><strong>如果 size 没指定，则等于 Flink 内存*fraction</strong></p>\n<h2 id=\"2、案例分析\"><a href=\"#2、案例分析\" class=\"headerlink\" title=\"2、案例分析\"></a>2、案例分析</h2><p>基于Yarn模式，一般参数指定的是总进程内存，taskmanager.memory.process.size，比如指定为 4G，每一块内存得到大小如下：</p>\n<p>（1）计算 Flink 内存</p>\n<p>JVM 元空间 256m</p>\n<p>JVM 执行开销： 4g*0.1&#x3D;409.6m，在[192m,1g]之间，最终结果 409.6m</p>\n<p>Flink 内存&#x3D;4g-256m-409.6m&#x3D;3430.4m</p>\n<p>（2）网络内存&#x3D;3430.4m*0.1&#x3D;343.04m，在[64m,1g]之间，最终结果 343.04m</p>\n<p>（3）托管内存&#x3D;3430.4m*0.4&#x3D;1372.16m</p>\n<p>（4）框架内存，堆内和堆外都是 128m</p>\n<p>（5）Task堆内内存&#x3D;3430.4m-128m-128m-343.04m-1372.16m&#x3D;1459.2m</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667261844-9b48b348-1bcb-4ca8-b556-f63a3680cf83.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667279269-d43c4812-9561-433a-83fe-a8d70b5fb5b9.png\" alt=\"img\"></p>\n<h3 id=\"所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\"><a href=\"#所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\" class=\"headerlink\" title=\"所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\"></a>所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。</h3><h2 id=\"合理利用-cpu-资源\"><a href=\"#合理利用-cpu-资源\" class=\"headerlink\" title=\"合理利用 cpu 资源\"></a>合理利用 cpu 资源</h2><p>Yarn 的<strong>容量调度器</strong>默认情况下是使用“DefaultResourceCalculator”分配策略，只根据内存调度资源，所以在 Yarn 的资源管理页面上看到每个容器的 vcore 个数还是 1。</p>\n<p>可以修改策略为 DominantResourceCalculator，该资源计算器在计算资源的时候会综合考虑 cpu 和内存的情况。在capacity-scheduler.xml 中修改属性:</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.capacity.resource-calculator<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- &lt;value&gt;org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator&lt;/value&gt; --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"1-1-1-使用DefaultResourceCalculator-策略\"><a href=\"#1-1-1-使用DefaultResourceCalculator-策略\" class=\"headerlink\" title=\"1.1.1    使用DefaultResourceCalculator 策略\"></a>1.1.1    使用DefaultResourceCalculator 策略</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flink run \\</span><br><span class=\"line\">-t yarn-per-job \\</span><br><span class=\"line\">-d \\</span><br><span class=\"line\">-p 5 \\</span><br><span class=\"line\">-Drest.flamegraph.enabled=true \\</span><br><span class=\"line\">-Dyarn.application.queue=test \\</span><br><span class=\"line\">-Djobmanager.memory.process.size=1024mb \\</span><br><span class=\"line\">-Dtaskmanager.memory.process.size=4096mb \\</span><br><span class=\"line\">-Dtaskmanager.numberOfTaskSlots=2 \\</span><br><span class=\"line\">-c com.atguigu.flink.tuning.UvDemo \\</span><br><span class=\"line\">/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n<p>可以看到一个容器只有一个 vcore：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668251950-033e4bc0-4b65-4fe8-b309-45a29956922b.png\" alt=\"img\"></p>\n<h3 id=\"1-1-2-使用DominantResourceCalculator-策略\"><a href=\"#1-1-2-使用DominantResourceCalculator-策略\" class=\"headerlink\" title=\"1.1.2    使用DominantResourceCalculator 策略\"></a>1.1.2    使用DominantResourceCalculator 策略</h3><p>修改后 yarn 配置后，分发配置并重启 yarn，再次提交 flink 作业：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;4096mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo\\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<p>看到容器的 vcore 数变了:</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668344371-82744e2d-89b2-4fab-8a09-f77475df1088.png\" alt=\"img\"></p>\n<p>JobManager1 个，占用 1 个容器，vcore&#x3D;1</p>\n<p>TaskManager3 个，占用 3 个容器，每个容器 vcore&#x3D;2，总 vcore&#x3D;2*3&#x3D;6，因为默认单个容器的 vcore 数&#x3D;单 TM 的slot 数</p>\n<h3 id=\"1-1-3-使用-DominantResourceCalculator-策略并指定容器vcore-数\"><a href=\"#1-1-3-使用-DominantResourceCalculator-策略并指定容器vcore-数\" class=\"headerlink\" title=\"1.1.3    使用 DominantResourceCalculator 策略并指定容器vcore 数\"></a>1.1.3    使用 DominantResourceCalculator 策略并指定容器<strong>vcore 数</strong></h3><p>指定yarn 容器的 vcore 数，提交：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Dyarn.containers.vcores&#x3D;3\\</p>\n<p> -Djobmanager.memory.process.size&#x3D;1024mb \\ -Dtaskmanager.memory.process.size&#x3D;4096mb \\ -Dtaskmanager.numberOfTaskSlots&#x3D;2 \\ -c com.atguigu.flink.tuning.UvDemo \\ &#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar  </p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668509233-7292aa6f-0e54-4799-ba38-ab72659ef824.png\" alt=\"img\"></p>\n<p>JobManager1 个，占用 1 个容器，vcore&#x3D;1</p>\n<p>TaskManager3 个，占用 3 个容器，每个容器vcore &#x3D;3，总 vcore&#x3D;3*3&#x3D;9</p>\n<h1 id=\"RocksDB大状态调优\"><a href=\"#RocksDB大状态调优\" class=\"headerlink\" title=\"RocksDB大状态调优\"></a>RocksDB大状态调优</h1><p>RocksDB 是基于 LSM Tree 实现的（类似HBase），写数据都是先缓存到内存中，所以RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record&#x2F;s。<strong>使用RocksDB 时，状态大小仅受可用磁盘空间量的限制，性能瓶颈主要在于 RocksDB对磁盘的读请求，每次读写操作都必须对数据进行反序列化或者序列化。</strong>所以当处理性能不够时，仅需要横向扩展并行度即可提高整个Job 的吞吐量。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654669015363-a35261ab-d4ff-4068-a013-eecfe78a5c7d.png\" alt=\"img\"></p>\n<p>从 Flink1.10 开始，Flink 默认将 RocksDB 的内存大小配置为每个 taskslot 的托管内存。调试内存性能的问题主要是通过调整配置项 taskmanager.memory.managed.size或者 taskmanager.memory.managed.fraction以增加 Flink 的托管内存(即堆外的托管内存)。进一步可以调整一些参数进行高级性能调优，这些参数也可以在应用程序中通过RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory)指定。下面介绍</p>\n<p>提高资源利用率的几个重要配置：</p>\n<h3 id=\"2-1-1-开启State访问性能监控\"><a href=\"#2-1-1-开启State访问性能监控\" class=\"headerlink\" title=\"2.1.1   开启State访问性能监控\"></a>2.1.1   开启State访问性能监控</h3><p>Flink 1.13 中引入了 State 访问的性能监控，即 latency trackig state。此功能不局限于 StateBackend 的类型，自定义实现的 StateBackend 也可以复用此功能。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654670053632-0e169f44-1340-4202-ab6a-bd9a6173a14a.png\" alt=\"img\"></p>\n<p>State访问性能监控会产生一定的性能影响，所以，默认每 100次做一次取样(sample)，对不同的 StateBackend 性能损失影响不同：</p>\n<ul>\n<li>对于 RocksDBStateBackend，性能损失大概在 1% 左右</li>\n<li>对于 HeapStateBackend，性能损失最多可达 10%</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">state.backend.latency-track.keyed-state-enabled：true</span> <span class=\"comment\">#启用访问状态的性能监控 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.sample-interval:</span> <span class=\"number\">100</span> <span class=\"comment\">#采样间隔 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.history-size:</span> <span class=\"number\">128</span> <span class=\"comment\">#保留的采样数据个数，越大越精确 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.state-name-as-variable:</span> <span class=\"literal\">true</span> <span class=\"comment\">#将状态名作为变量  </span></span><br></pre></td></tr></table></figure>\n\n<p>正常开启第一个参数即可。</p>\n<blockquote>\n<p>bin&#x2F;flink run \\</p>\n<p>-t yarn-per-job \\</p>\n<p>-d \\</p>\n<p>-p 5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true \\</p>\n<p>-Dyarn.application.queue&#x3D;test \\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;4096mb \\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2 \\</p>\n<p> -Dstate.backend.latency-track.keyed-state-enabled&#x3D;true \\ </p>\n<p>-c com.atguigu.flink.tuning.RocksdbTuning \\ &#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar  </p>\n</blockquote>\n<h3 id=\"2-1-2-开启增量检查点和本地恢复\"><a href=\"#2-1-2-开启增量检查点和本地恢复\" class=\"headerlink\" title=\"2.1.2    开启增量检查点和本地恢复\"></a>2.1.2    开启增量检查点和本地恢复</h3><p>1）开启增量检查点</p>\n<p>RocksDB 是目前唯一可用于支持有状态流处理应用程序增量检查点的状态后端，可以修改参数开启增量检查点：</p>\n<p>state.backend.incremental: true #默认 false，改为 true。 </p>\n<p>或代码中指定 new EmbeddedRocksDBStateBackend(true)  </p>\n<p>2）开启本地恢复</p>\n<p>当 Flink任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs拉取数据。本地恢复目前仅涵盖键控类型的状态后端（RocksDB），MemoryStateBackend不支持本地恢复并忽略此选项。</p>\n<p>state.backend.local-recovery:true</p>\n<h3 id=\"2-1-3-调整预定义选项\"><a href=\"#2-1-3-调整预定义选项\" class=\"headerlink\" title=\"2.1.3    调整预定义选项\"></a>2.1.3    调整预定义选项</h3><p>Flink针对不同的设置为 RocksDB提供了一些预定义的选项集合,其中包含了后续提到的一些参数，如果调整预定义选项后还达不到预期，再去调整后面的 block、writebuffer等参数。</p>\n<p>当 前 支 持 的 预 定 义 选 项 有   DEFAULT 、 SPINNING_DISK_OPTIMIZED 、</p>\n<p>SPINNING_DISK_OPTIMIZED_HIGH_MEM 或FLASH_SSD_OPTIMIZED。有条件上 SSD</p>\n<p>的，可以指定为 FLASH_SSD_OPTIMIZED</p>\n<p> state.backend.rocksdb.predefined-options： SPINNING_DISK_OPTIMIZED_HIGH_MEM #设置为机械硬盘+内存模式  </p>\n<h3 id=\"2-1-4-增大-block-缓存\"><a href=\"#2-1-4-增大-block-缓存\" class=\"headerlink\" title=\"2.1.4    增大 block 缓存\"></a>2.1.4    增大 block 缓存</h3><p>整个 RocksDB 共享一个 blockcache，读数据时内存的 cache 大小，该参数越大读</p>\n<p>数据时缓存命中率越高，默认大小为8MB，建议设置到64~256MB。</p>\n<p>state.backend.rocksdb.block.cache-size:64m     #默认8m  </p>\n<h3 id=\"2-1-5-增大writebuffer-和-level-阈值大小\"><a href=\"#2-1-5-增大writebuffer-和-level-阈值大小\" class=\"headerlink\" title=\"2.1.5    增大writebuffer 和 level 阈值大小\"></a>2.1.5    增大writebuffer 和 level 阈值大小</h3><p>RocksDB 中，每个 State 使用一个 ColumnFamily，每个 ColumnFamily 使用独占的 writebuffer，默认 64MB，建议调大。</p>\n<p>调整这个参数通常要适当增加 L1层的大小阈值 max-size-level-base，默认 256m。</p>\n<p>该值太小会造成能存放的 SST 文件过少，层级变多造成查找困难，太大会造成文件过多，合并困难。建议设为 target_file_size_base（默认 64MB） 的倍数，且不能太小，例如 5<del>10倍，即 320</del>640MB。</p>\n<p>state.backend.rocksdb.writebuffer.size: 128m</p>\n<p>state.backend.rocksdb.compaction.level.max-size-level-base:320m   </p>\n<h3 id=\"2-1-6-增大write-buffer-数量\"><a href=\"#2-1-6-增大write-buffer-数量\" class=\"headerlink\" title=\"2.1.6    增大write buffer 数量\"></a>2.1.6    增大write buffer 数量</h3><p>每个 ColumnFamily对应的 writebuffer 最大数量，这实际上是内存中“只读内存表“的最大数量，默认值是 2。对于机械磁盘来说，如果内存足够大，可以调大到 5左右</p>\n<p>state.backend.rocksdb.writebuffer.count:5                                                                     </p>\n<h3 id=\"2-1-7-增大后台线程数和writebuffer-合并数\"><a href=\"#2-1-7-增大后台线程数和writebuffer-合并数\" class=\"headerlink\" title=\"2.1.7    增大后台线程数和writebuffer 合并数\"></a>2.1.7    增大后台线程数和writebuffer 合并数</h3><p>1）增大线程数</p>\n<p>用于后台 flush和合并 sst文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4等更大的值</p>\n<p>state.backend.rocksdb.thread.num: 4                                                                             </p>\n<p>2）增大writebuffer 最小合并数</p>\n<p>将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 最小数量，默认</p>\n<p>值为 1，可以调成 3。</p>\n<p>state.backend.rocksdb.writebuffer.number-to-merge:3                                             </p>\n<h3 id=\"2-1-8-开启分区索引功能\"><a href=\"#2-1-8-开启分区索引功能\" class=\"headerlink\" title=\"2.1.8    开启分区索引功能\"></a>2.1.8    开启分区索引功能</h3><p>Flink1.13 中对 RocksDB 增加了分区索引功能，复用了 RocksDB 的partitionedIndex&amp;filter 功能，简单来说就是对 RocksDB 的 partitionedIndex 做了多级索引。也就是将内存中的最上层常驻，下层根据需要再 load回来，这样就大大降低了数据 Swap竞争。线上测试中，相对于<strong>内存比较小</strong>的场景中，性能提升 10 倍左右。如果在内存管控下 Rocksdb 性能不如预期的话，这也能成为一个性能优化点。</p>\n<p>state.backend.rocksdb.memory.partitioned-index-filters:true   #默认false                </p>\n<p><strong>2.1.9</strong>    <strong>参数设定案例</strong></p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flinkrun\\</span><br><span class=\"line\">-tyarn-per-job\\</span><br><span class=\"line\">-d\\</span><br><span class=\"line\">-p5\\</span><br><span class=\"line\">-Drest.flamegraph.enabled=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dyarn.application.queue=<span class=\"built_in\">test</span>\\</span><br><span class=\"line\">-Djobmanager.memory.process.size=1024mb \\</span><br><span class=\"line\">-Dtaskmanager.memory.process.size=4096mb\\</span><br><span class=\"line\">-Dtaskmanager.numberOfTaskSlots=2\\</span><br><span class=\"line\">-Dstate.backend.incremental=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.local-recovery=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.predefined-options=SPINNING_DISK_OPTIMIZED_HIGH_MEM\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.block.cache-size=64m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.size=128m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.compaction.level.max-size-level-base=320m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.count=5 \\</span><br><span class=\"line\">-Dstate.backend.rocksdb.thread.num=4\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.number-to-merge=3\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.memory.partitioned-index-filters=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.latency-track.keyed-state-enabled=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-ccom.atguigu.flink.tuning.RocksdbTuning\\</span><br><span class=\"line\">/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"设置本地-RocksDB-多目录\"><a href=\"#设置本地-RocksDB-多目录\" class=\"headerlink\" title=\"设置本地 RocksDB 多目录\"></a>设置本地 RocksDB 多目录</h3><p>在flink-conf.yaml 中配置：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">state.backend.rocksdb.localdir: /data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb</span><br></pre></td></tr></table></figure>\n\n\n\n<p>注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。<strong>当设置多个 RocksDB 本地磁盘目录时，Flink 会<strong><strong>随机选择</strong></strong>要使用的目录，所以就可能存在三个并行度共用同一目录的情况。</strong>如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。</p>\n<p><strong>当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。</strong></p>\n<p>如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。<strong>由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘IO性能消耗确实比较大。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662632337-7fe1e6c6-5fe2-412e-82e8-77f3c81458b7.png\" alt=\"img\"></p>\n<p>如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj磁盘。可以看到 sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。<strong>如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下</strong>，<strong>从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662673431-6575b710-490c-49c4-bec7-f4b7964b3fc7.png\" alt=\"img\"></p>\n<ul>\n<li><strong>state.backend.incremental：</strong>开启增量检查点，默认false，改为true。</li>\n<li><strong>state.backend.rocksdb.predefined-options：</strong>SPINNING_DISK_OPTIMIZED_HIGH_MEM设置为机械硬盘+内存模式，有条件上SSD，指定为FLASH_SSD_OPTIMIZED</li>\n<li><strong>state.backend.rocksdb.block.cache-size</strong>: 整个 RocksDB 共享一个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。</li>\n<li><strong>state.backend.rocksdb.thread.num</strong>: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。</li>\n<li><strong>state.backend.rocksdb.writebuffer.size</strong>: RocksDB 中，每个 State 使用一个 Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M</li>\n<li><strong>state.backend.rocksdb.writebuffer.count</strong>: 每个 Column Family 对应的 writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5 左右</li>\n<li><strong>state.backend.rocksdb.writebuffer.number-to-merge</strong>: 将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成3。</li>\n<li><strong>state.backend.local-recovery</strong>: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据</li>\n</ul>\n<h2 id=\"Checkpoint设置\"><a href=\"#Checkpoint设置\" class=\"headerlink\" title=\"Checkpoint设置\"></a>Checkpoint设置</h2><p>一般我们的 Checkpoint 时间间隔可以设置为分钟级别（1<del>5分钟），例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5</del>10 分钟一次Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次Checkpoint 之间至少暂停 4或8 分钟。</p>\n<p>同时，也需要考虑时效性的要求,需要在时效性和性能之间做一个平衡，如果时效性要求高，结合 end- to-end 时长，设置秒级或毫秒级。</p>\n<p>如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看 Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。</p>\n<p>RocksDB相关参数在1.3中已说明，可以在flink-conf.yaml指定，也可以在Job的代码中调用API单独指定，这里不再列出。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint</span></span><br><span class=\"line\"><span class=\"type\">RocksDBStateBackend</span> rocksDBStateBackend = <span class=\"keyword\">new</span> <span class=\"type\">RocksDBStateBackend</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020/flink/checkpoints&quot;</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">env.setStateBackend(rocksDBStateBackend);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 开启Checkpoint，间隔为 3 分钟</span></span><br><span class=\"line\">env.enableCheckpointing(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">3</span>));</span><br><span class=\"line\"><span class=\"comment\">// 配置 Checkpoint</span></span><br><span class=\"line\"><span class=\"type\">CheckpointConfig</span> checkpointConf = env.getCheckpointConfig();</span><br><span class=\"line\">checkpointConf.setCheckpointingMode(<span class=\"type\">CheckpointingMode</span>.<span class=\"type\">EXACTLY_ONCE</span>)</span><br><span class=\"line\"><span class=\"comment\">// 最小间隔 4分钟</span></span><br><span class=\"line\">checkpointConf.setMinPauseBetweenCheckpoints(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"comment\">// 超时时间 10分钟</span></span><br><span class=\"line\">checkpointConf.setCheckpointTimeout(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">10</span>));</span><br><span class=\"line\"><span class=\"comment\">// 保存checkpoint</span></span><br><span class=\"line\">checkpointConf.enableExternalizedCheckpoints(</span><br><span class=\"line\"><span class=\"type\">CheckpointConfig</span>.<span class=\"type\">ExternalizedCheckpointCleanup</span>.<span class=\"type\">RETAIN_ON_CANCELLATION</span>);</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"反压处理\"><a href=\"#反压处理\" class=\"headerlink\" title=\"反压处理\"></a>反压处理</h1><h2 id=\"3-1-概述\"><a href=\"#3-1-概述\" class=\"headerlink\" title=\"3.1 概述\"></a>3.1 概述</h2><p>Flink 网络流控及反压的介绍：</p>\n<p><a href=\"https://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626\">https://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626</a></p>\n<h3 id=\"3-1-1-反压的理解\"><a href=\"#3-1-1-反压的理解\" class=\"headerlink\" title=\"3.1.1    反压的理解\"></a>3.1.1    反压的理解</h3><p>简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。</p>\n<p>反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。</p>\n<h3 id=\"3-1-2-反压的危害\"><a href=\"#3-1-2-反压的危害\" class=\"headerlink\" title=\"3.1.2    反压的危害\"></a>3.1.2    反压的危害</h3><p>反压如果不能得到正确的处理，可能会影响到 checkpoint时长和 state大小，甚至可能会导致资源耗尽甚至系统崩溃。</p>\n<ul>\n<li>1）影响 checkpoint 时长：barrier 不会越过普通数据，数据处理被阻塞也会导致checkpointbarrier 流经整个数据管道的时长变长，导致 checkpoint 总体时间（End toEndDuration）变长。</li>\n<li>2）影响 state 大小：barrier 对齐时，接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到 state 里面，导致 checkpoint 变大。</li>\n</ul>\n<p>这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint<strong>超时失败</strong>，而 state 大小同样可能拖慢 checkpoint 甚至导致 <strong>OOM</strong>（使用 Heap-basedStateBackend）或者物理内存使用<strong>超出容器资源</strong>（使用 RocksDBStateBackend）的稳定性问题。</p>\n<p><strong>因此，我们在生产中要尽量避免出现反压的情况。</strong></p>\n<h2 id=\"3-2-定位反压节点\"><a href=\"#3-2-定位反压节点\" class=\"headerlink\" title=\"3.2 定位反压节点\"></a>3.2 定位反压节点</h2><p>解决反压首先要做的是定位到造成反压的节点，排查的时候，先把operatorchain 禁用，方便定位到具体算子。</p>\n<p>提交UvDemo:</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<h3 id=\"3-2-1-利用-FlinkWebUI-定位\"><a href=\"#3-2-1-利用-FlinkWebUI-定位\" class=\"headerlink\" title=\"3.2.1    利用 FlinkWebUI 定位\"></a>3.2.1    利用 FlinkWebUI 定位</h3><p>FlinkWebUI 的反压监控提供了 SubTask 级别的反压监控，1.13 版本以前是通过周期性对  Task  线程的栈信息采样，得到线程被阻塞在请求  Buffer（意味着被下游队列阻塞）</p>\n<p>的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1以下则为 OK，0.1</p>\n<p>至 0.5为 LOW，而超过 0.5则为 HIGH。</p>\n<p>Flink1.13 优化了反压检测的逻辑（使用基于任务 Mailbox计时，而不在再于堆栈采样），并且重新实现了作业图的 UI展示：Flink现在在 UI 上通过颜色和数值来展示繁忙和反压的程度。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674284140-b680f841-3ad4-4250-87fd-8c331333f1f5.png\" alt=\"img\"></p>\n<p>1）通过WebUI看到 Map算子处于反压：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674446026-5ec8c33c-cadc-44c9-9d00-b644899f52d6.png\" alt=\"img\"></p>\n<p>3）分析瓶颈算子</p>\n<p>如果处于反压状态，那么有两种可能性：</p>\n<p>（1）  该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。这种情况，该节点是反压的根源节点，它是从 SourceTask到 Sink Task 的第一个出现反压的节点。<strong>（很少出现，表现为：反压算子一进多出，后面的算子处理速度慢，从这个反压算子开始，后面的算子都反压了。图示，绿色为反压节点：</strong></p>\n<p><strong>（OK-&gt; OK-&gt;</strong> <strong>反</strong> <strong>-&gt;反 -&gt; 反 ）</strong></p>\n<p><strong>一进多出，输入缓存区使用率可能高也可能低，输出缓存区使用率高</strong></p>\n<p>（2）  下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。这种情况，需要继续排查下游节点，一直找到第一个为OK的一般就是根源节点。<strong>（表现为：这个反压算子处理速度慢，阻塞了前面的算子，导致前面的算子反压了，其后面的算子表现为不反压。图示，绿色为反压节点：</strong></p>\n<p>​      <strong>（反 -&gt; 反 -&gt;</strong> <strong>OK</strong>-&gt; OK-&gt; OK）</p>\n<p><strong>输入缓存区使用率高，输出缓存区使用率低</strong></p>\n<p>总体来看，如果我们找到第一个出现反压的节点，反压根源要么是就这个节点，要么是它紧接着的下游节点。</p>\n<p>通常来讲，第二种情况更常见。如果无法确定，还需要结合 Metrics进一步判断。</p>\n<h3 id=\"3-2-2-利用-Metrics-定位\"><a href=\"#3-2-2-利用-Metrics-定位\" class=\"headerlink\" title=\"3.2.2    利用 Metrics 定位\"></a>3.2.2    利用 Metrics 定位</h3><p>监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为</p>\n<p>有用的是以下几个 Metrics:</p>\n<table>\n<thead>\n<tr>\n<th><strong>Metris</strong></th>\n<th><strong>描述</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>outPoolUsage</td>\n<td>发送端 Buffer 的使用率</td>\n</tr>\n<tr>\n<td>inPoolUsage</td>\n<td>接收端 Buffer 的使用率</td>\n</tr>\n<tr>\n<td>floatingBuffersUsage（1.9 以上）</td>\n<td>接收端 FloatingBuffer 的使用率</td>\n</tr>\n<tr>\n<td>exclusiveBuffersUsage（1.9 以上）</td>\n<td>接收端 ExclusiveBuffer 的使用率</td>\n</tr>\n</tbody></table>\n<p>其中 inPoolUsage &#x3D; floatingBuffersUsage + exclusiveBuffersUsage。</p>\n<h4 id=\"1）根据指标分析反压\"><a href=\"#1）根据指标分析反压\" class=\"headerlink\" title=\"1）根据指标分析反压\"></a>1）根据指标分析反压</h4><p>分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(1.9 以上):</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>outPoolUsage</strong> <strong>低</strong></th>\n<th><strong>outPoolUsage</strong> <strong>高</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>inPoolUsage</strong> <strong>低</strong></td>\n<td>正常</td>\n<td>被下游反压，处于临时情况（还没传递到上游）</td>\n</tr>\n<tr>\n<td>可能是反压的根源，一条输入多条输出的场景</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>inPoolUsage</strong> <strong>高</strong></td>\n<td>如果上游所有 outPoolUsage 都是低，有可能最终可能导致反压（还没传递到上游）</td>\n<td>被下游反压</td>\n</tr>\n<tr>\n<td>如果上游的 outPoolUsage 是高，则为反压根源</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h4 id=\"2）可以进一步分析数据传输\"><a href=\"#2）可以进一步分析数据传输\" class=\"headerlink\" title=\"2）可以进一步分析数据传输\"></a>2）可以进一步分析数据传输</h4><p>Flink1.9 及以上版本，还可以根据 floatingBuffersUsage&#x2F;exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游Subtask 的数据传输。</p>\n<p>在流量较大时，Channel  的  ExclusiveBuffer  可能会被写满，此时  Flink  会向  BufferPool 申请剩余的 FloatingBuffer。这些 <strong>FloatingBuffer 属于备用 Buffer。</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>exclusiveBuffersUsage</strong> <strong>低</strong></th>\n<th><strong>exclusiveBuffersUsage</strong> <strong>高</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>floatingBuffersUsage</strong> <strong>低</strong>所有上游<strong>outPoolUsage</strong> <strong>低</strong></td>\n<td>正常</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong> <strong>低</strong>上游某个<strong>outPoolUsage</strong> <strong>高</strong></td>\n<td>潜在的网络瓶颈</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong>高所有上游<strong>outPoolUsage</strong> <strong>低</strong></td>\n<td>最终对部分inputChannel 反压（正在传递）</td>\n<td>最终对大多数或所有   inputChannel反压（正在传递）</td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong>高上游某个<strong>outPoolUsage</strong> <strong>高</strong></td>\n<td>只对部分 inputChannel 反压</td>\n<td>对大多数或所有 inputChannel 反压</td>\n</tr>\n</tbody></table>\n<p>总结：</p>\n<ul>\n<li>1）floatingBuffersUsage 为高，则表明反压正在传导至上游</li>\n<li>2）同时 exclusiveBuffersUsage 为低，则表明可能有倾斜</li>\n</ul>\n<p>比如，floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数</p>\n<p>channel 占用了大部分的 FloatingBuffer。</p>\n<h2 id=\"3-3-反压的原因及处理\"><a href=\"#3-3-反压的原因及处理\" class=\"headerlink\" title=\"3.3 反压的原因及处理\"></a>3.3 反压的原因及处理</h2><p>注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。</p>\n<p>定位到反压节点后，分析造成原因的办法主要是观察 TaskThread。按照下面的顺序，一步一步去排查。</p>\n<h3 id=\"3-3-1-查看是否数据倾斜\"><a href=\"#3-3-1-查看是否数据倾斜\" class=\"headerlink\" title=\"3.3.1    查看是否数据倾斜\"></a>3.3.1    查看是否数据倾斜</h3><p><strong>在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI各</strong></p>\n<p><strong>个 SubTask 的 RecordsSent 和 RecordReceived 来确认，另外 Checkpointdetail里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675365111-f2598a4c-7ae6-4c6b-852b-a2c31b53623e.png\" alt=\"img\"></p>\n<p>（关于数据倾斜的详细解决方案，会在下一章节详细讨论）</p>\n<h3 id=\"3-3-2-使用火焰图分析\"><a href=\"#3-3-2-使用火焰图分析\" class=\"headerlink\" title=\"3.3.2    使用火焰图分析\"></a>3.3.2    使用火焰图分析</h3><p>如果不是数据倾斜，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题），需要找到瓶颈算子中的哪部分计算逻辑消耗巨大。</p>\n<p>最有用的办法就是对 TaskManager 进行 CPUprofile，从中我们可以分析到 TaskThread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面；如果不是的话要看 TaskThread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是checkpoint 或者 GC 等系统活动导致的暂时系统暂停。</p>\n<h4 id=\"1）开启火焰图功能\"><a href=\"#1）开启火焰图功能\" class=\"headerlink\" title=\"1）开启火焰图功能\"></a>1）开启火焰图功能</h4><p>Flink1.13直接在 WebUI提供 JVM的 CPU 火焰图，这将大大简化性能瓶颈的分析，默认是不开启的，需要修改参数：</p>\n<p>rest.flamegraph.enabled:true#默认false                                                                          </p>\n<p>也可以在提交时指定：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<h4 id=\"2）WebUI-查看火焰图\"><a href=\"#2）WebUI-查看火焰图\" class=\"headerlink\" title=\"2）WebUI 查看火焰图\"></a>2）WebUI 查看火焰图</h4><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675647317-7df4c4eb-e01f-4637-9d0e-a9980331f2c2.png\" alt=\"img\"></p>\n<p>火焰图是通过对堆栈跟踪进行多次采样来构建的。每个方法调用都由一个条形表示，其中条形的长度与其在样本中出现的次数成正比。</p>\n<ul>\n<li>On-CPU: 处于 [RUNNABLE, NEW]状态的线程</li>\n<li>Off-CPU: 处于 [TIMED_WAITING, WAITING, BLOCKED]的线程，用于查看在样本中发现的阻塞调用。</li>\n</ul>\n<h4 id=\"3）分析火焰图\"><a href=\"#3）分析火焰图\" class=\"headerlink\" title=\"3）分析火焰图\"></a>3）分析火焰图</h4><p>颜色没有特殊含义，具体查看：</p>\n<ul>\n<li>纵向是调用链，从下往上，顶部就是正在执行的函数</li>\n<li>横向是样本出现次数，可以理解为执行时长。</li>\n</ul>\n<p><strong>看顶层的哪个函数占据的宽度最大。只要有”平顶”（plateaus），就表示该函数可能存在性能问题。</strong></p>\n<p>如果是 Flink1.13 以前的版本，可以手动做火焰图：</p>\n<p>如何生成火焰图：<a href=\"http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/\">http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/</a></p>\n<h3 id=\"3-3-3-分析GC-情况\"><a href=\"#3-3-3-分析GC-情况\" class=\"headerlink\" title=\"3.3.3    分析GC 情况\"></a>3.3.3    分析GC 情况</h3><p>TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManagerJVM 各区内存不合理导致的频繁 FullGC 甚至失联。通常建议使用默认的 G1 垃圾回收器。</p>\n<p>可以通过打印 GC 日志（-XX:+PrintGCDetails），使用 GC 分析器（GCViewer 工具）来验证是否处于这种情况。</p>\n<ul>\n<li>在 Flink 提交脚本中,设置 JVM 参数，打印 GC 日志：</li>\n</ul>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Denv.java.opts&#x3D;”-XX:+PrintGCDetails-XX:+PrintGCDateStamps”\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<ul>\n<li>下载 GC 日志的方式：</li>\n</ul>\n<p>因为是 onyarn 模式，运行的节点一个一个找比较麻烦。可以打开 WebUI，选择JobManager 或者 TaskManager，点击 Stdout，即可看到 GC 日志，点击下载按钮即可将 GC日志通过 HTTP的方式下载下来。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654679097595-18b82b7c-8bd5-4d21-b720-44c795ce377a.png\" alt=\"img\"></p>\n<ul>\n<li>分析 GC 日志：</li>\n</ul>\n<p>通过 GC 日志分析出单个 FlinkTaskmanager 堆总大小、年轻代、老年代分配的内存空间、FullGC 后老年代剩余大小等，相关指标定义可以去 Github 具体查看。</p>\n<p>GCViewer 地址：<a href=\"https://github.com/chewiebug/GCViewer\">https://github.com/chewiebug/GCViewer</a></p>\n<p>Linux 下分析：</p>\n<p>java -jargcviewer_1.3.4.jargc.log                                                                                    </p>\n<p>Windows 下分析：</p>\n<p>直接双击gcviewer_1.3.4.jar，打开GUI界面，选择gc的log打开         </p>\n<p>​                      </p>\n<p>扩展：最重要的指标是FullGC 后，老年代剩余大小这个指标，按照《Java 性能优化权威指南》这本书 Java 堆大小计算法则，设 FullGC 后老年代剩余大小空间为 M，那么堆的大小建议 3<del>4 倍 M，新生代为 1</del>1.5 倍 M，老年代应为 2~3 倍 M。</p>\n<h3 id=\"3-3-4-外部组件交互\"><a href=\"#3-3-4-外部组件交互\" class=\"headerlink\" title=\"3.3.4    外部组件交互\"></a>3.3.4    外部组件交互</h3><p>如果发现我们的 Source端数据读取性能比较低或者 Sink端写入性能较差，需要检查第三方组件是否遇到瓶颈，还有就是做维表join时的性能问题。</p>\n<p>例如：</p>\n<p>Kafka集群是否需要扩容，Kafka 连接器是否并行度较低</p>\n<p>HBase的 rowkey 是否遇到热点问题，是否请求处理不过来</p>\n<p>ClickHouse并发能力较弱，是否达到瓶颈</p>\n<p>……</p>\n<p>关于第三方组件的性能问题，需要结合具体的组件来分析，最常用的思路：</p>\n<ul>\n<li>1）异步 io+热缓存来优化读写性能</li>\n<li>2）先攒批再读写维表join参考：</li>\n</ul>\n<p><a href=\"https://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3\">https://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3</a></p>\n<p><a href=\"https://www.jianshu.com/p/a62fa483ff54\">https://www.jianshu.com/p/a62fa483ff54</a></p>\n<h1 id=\"四、数据倾斜\"><a href=\"#四、数据倾斜\" class=\"headerlink\" title=\"四、数据倾斜\"></a>四、数据倾斜</h1><h2 id=\"4-1-判断是否存在数据倾斜\"><a href=\"#4-1-判断是否存在数据倾斜\" class=\"headerlink\" title=\"4.1  判断是否存在数据倾斜\"></a>4.1  判断是否存在数据倾斜</h2><p>相同 Task 的多个 Subtask 中， 个别 Subtask 接收到的数据量明显大于其他Subtask 接收到的数据量，通过 FlinkWebUI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692839400-88f4eb2d-9389-4011-a676-2f6da336cb39.png\" alt=\"img\"></p>\n<p>另外， 有时 Checkpointdetail 里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。</p>\n<h2 id=\"4-2-数据倾斜的解决\"><a href=\"#4-2-数据倾斜的解决\" class=\"headerlink\" title=\"4.2 数据倾斜的解决\"></a>4.2 数据倾斜的解决</h2><h3 id=\"4-2-1-keyBy-后的聚合操作存在数据倾斜\"><a href=\"#4-2-1-keyBy-后的聚合操作存在数据倾斜\" class=\"headerlink\" title=\"4.2.1    keyBy 后的聚合操作存在数据倾斜\"></a>4.2.1    keyBy 后的聚合操作存在数据倾斜</h3><h4 id=\"1）为什么不能直接用二次聚合来处理（没有卵用）\"><a href=\"#1）为什么不能直接用二次聚合来处理（没有卵用）\" class=\"headerlink\" title=\"1）为什么不能直接用二次聚合来处理（没有卵用）\"></a>1）为什么不能直接用二次聚合来处理（没有卵用）</h4><p>Flink是实时流处理，如果keyby之后的聚合操作存在数据倾斜，且没有开窗口（没攒批）的情况下，简单的认为使用两阶段聚合，是不能解决问题的。因为这个时候Flink是来一条处理一条，且向下游发送一条结果，对于原来 keyby的维度（第二阶段聚合）来讲，数据量并没有减少，且结果重复计算（非 FlinkSQL，未使用回撤流），如下图所示：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692995562-f3b6caac-04e3-45ac-87bc-92286cb10e2b.png\" alt=\"img\"></p>\n<h4 id=\"2）使用-LocalKeyBy-的思想\"><a href=\"#2）使用-LocalKeyBy-的思想\" class=\"headerlink\" title=\"2）使用 LocalKeyBy 的思想\"></a>2）使用 LocalKeyBy 的思想</h4><p>在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后，再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。类似 MapReduce中 Combiner的思想，但是这要求聚合操作必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。从 FlinkLocalKeyBy实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。</p>\n<h4 id=\"实现方式：\"><a href=\"#实现方式：\" class=\"headerlink\" title=\"实现方式：\"></a>实现方式：</h4><ul>\n<li>DataStreamAPI 需要自己写代码实现</li>\n<li>SQL 可以指定参数，开启miniBatch 和 LocalGlobal 功能（推荐，后续介绍）</li>\n</ul>\n<h3 id=\"4-1-1-keyBy之前发生数据倾斜\"><a href=\"#4-1-1-keyBy之前发生数据倾斜\" class=\"headerlink\" title=\"4.1.1    keyBy之前发生数据倾斜\"></a>4.1.1    keyBy之前发生数据倾斜</h3><p>如果 keyBy 之前就存在数据倾斜，上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。</p>\n<p>对于不存在 keyBy 的 Flink 任务也会出现该情况。</p>\n<p>这种情况，需要让 Flink 任务强制进行shuffle。使用 shuffle、rebalance 或 rescale</p>\n<p>算子即可将数据均匀分配，从而解决数据倾斜的问题。</p>\n<h3 id=\"4-1-2-keyBy-后的窗口聚合操作存在数据倾斜\"><a href=\"#4-1-2-keyBy-后的窗口聚合操作存在数据倾斜\" class=\"headerlink\" title=\"4.1.2    keyBy 后的窗口聚合操作存在数据倾斜\"></a>4.1.2    keyBy 后的窗口聚合操作存在数据倾斜</h3><p>因为使用了窗口，变成了有界数据（攒批）的处理，窗口默认是触发时才会输出一条结果发往下游，所以可以使用两阶段聚合的方式：</p>\n<h4 id=\"1）实现思路：\"><a href=\"#1）实现思路：\" class=\"headerlink\" title=\"1）实现思路：\"></a>1）实现思路：</h4><ul>\n<li>第一阶段聚合：key拼接随机数前缀或后缀，进行 keyby、开窗、聚合</li>\n</ul>\n<p><strong>注意：聚合完不再是 WindowedStream，要获取 WindowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）</strong></p>\n<ul>\n<li>第二阶段聚合：按照原来的 key 及windowEnd 作keyby、聚合</li>\n</ul>\n<p>SQL写法参考：<a href=\"https://zhuanlan.zhihu.com/p/197299746\">https://zhuanlan.zhihu.com/p/197299746</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h1 id=\"内存设置（1CPU配置4G内存）\"><a href=\"#内存设置（1CPU配置4G内存）\" class=\"headerlink\" title=\"内存设置（1CPU配置4G内存）\"></a>内存设置（1CPU配置4G内存）</h1><blockquote>\n<p>bin&#x2F;flink run \\</p>\n<p>-t yarn-per-job \\</p>\n<p>-d \\</p>\n<p>-p 5 \\ 指定并行度</p>\n<p>-Dyarn.application.queue&#x3D;test \\ 指定yarn队列</p>\n<p>-Djobmanager.memory.process.size&#x3D;2048mb \\ JM2~4G足够</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;6144mb \\ 单个TM2~8G足够</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2 \\ <strong>与容器核数1core：1slot或1core：2slot</strong></p>\n<p>-c com.atguigu.app.dwd.LogBaseApp \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;gmall-flink&#x2F;gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar</p>\n</blockquote>\n<p>Flink是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS&#x2F;TPS来描述数据情况。</p>\n<h2 id=\"TaskManager-内存模型\"><a href=\"#TaskManager-内存模型\" class=\"headerlink\" title=\"TaskManager 内存模型\"></a>TaskManager 内存模型</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654665961981-86c260ab-5310-4674-ac61-6a1d1f738f18.png\" alt=\"img\"></p>\n<h3 id=\"1、内存模型详解\"><a href=\"#1、内存模型详解\" class=\"headerlink\" title=\"1、内存模型详解\"></a>1、内存模型详解</h3><h4 id=\"JVM-特定内存：JVM-本身使用的内存，包含-JVM-的-metaspace-和-over-head\"><a href=\"#JVM-特定内存：JVM-本身使用的内存，包含-JVM-的-metaspace-和-over-head\" class=\"headerlink\" title=\"JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head\"></a>JVM 特定内存：JVM 本身使用的内存，包含 JVM 的 metaspace 和 over-head</h4><p>1）JVMmetaspace：JVM 元空间</p>\n<p>taskmanager.memory.jvm-metaspace.size，默认 256mb</p>\n<p>2）JVMover-head执行开销：JVM执行时自身所需要的内容，包括线程堆栈、IO、编译缓存等所使用的内存。</p>\n<p>taskmanager.memory.jvm-overhead.fraction，默认 0.1</p>\n<p>taskmanager.memory.jvm-overhead.min，默认 192mb</p>\n<p>taskmanager.memory.jvm-overhead.max，默认 1gb</p>\n<p><strong>总进程内存*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min&#x2F;max</strong></p>\n<p><strong>大小</strong></p>\n<h4 id=\"框架内存：Flink-框架，即-TaskManager-本身所占用的内存，不计入-Slot-的资源中。\"><a href=\"#框架内存：Flink-框架，即-TaskManager-本身所占用的内存，不计入-Slot-的资源中。\" class=\"headerlink\" title=\"框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。\"></a>框架内存：Flink 框架，即 TaskManager 本身所占用的内存，不计入 Slot 的资源中。</h4><p>堆内：taskmanager.memory.framework.heap.size，默认 128MB</p>\n<p>堆外：taskmanager.memory.framework.off-heap.size，默认 128MB</p>\n<h4 id=\"Task内存：Task执行用户代码时所使用的内存\"><a href=\"#Task内存：Task执行用户代码时所使用的内存\" class=\"headerlink\" title=\"Task内存：Task执行用户代码时所使用的内存\"></a>Task内存：Task执行用户代码时所使用的内存</h4><p>堆内：taskmanager.memory.task.heap.size，默认 none，由 Flink 内存扣除掉其他部分的内存得到。</p>\n<p>堆外：taskmanager.memory.task.off-heap.size，默认 0，表示不使用堆外内存</p>\n<h4 id=\"网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\"><a href=\"#网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\" class=\"headerlink\" title=\"网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区\"></a>网络内存：网络数据交换所使用的堆外内存大小，如网络数据交换缓冲区</h4><p><strong>堆外：</strong></p>\n<p>taskmanager.memory.network.fraction，默认 0.1</p>\n<p>taskmanager.memory.network.min，默认 64mb</p>\n<p>taskmanager.memory.network.max，默认 1gb</p>\n<p><strong>Flink 内存*fraction，如果小于配置的 min（或大于配置的 max）大小，则使用 min&#x2F;max大小</strong></p>\n<h4 id=\"托管内存：用于-RocksDBStateBackend-的本地内存和批的排序、哈希表、缓存中间结果。\"><a href=\"#托管内存：用于-RocksDBStateBackend-的本地内存和批的排序、哈希表、缓存中间结果。\" class=\"headerlink\" title=\"托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。\"></a>托管内存：用于 RocksDBStateBackend 的本地内存和批的排序、哈希表、缓存中间结果。</h4><p>堆外：taskmanager.memory.managed.fraction，默认 0.4</p>\n<p>taskmanager.memory.managed.size，默认 none</p>\n<p><strong>如果 size 没指定，则等于 Flink 内存*fraction</strong></p>\n<h2 id=\"2、案例分析\"><a href=\"#2、案例分析\" class=\"headerlink\" title=\"2、案例分析\"></a>2、案例分析</h2><p>基于Yarn模式，一般参数指定的是总进程内存，taskmanager.memory.process.size，比如指定为 4G，每一块内存得到大小如下：</p>\n<p>（1）计算 Flink 内存</p>\n<p>JVM 元空间 256m</p>\n<p>JVM 执行开销： 4g*0.1&#x3D;409.6m，在[192m,1g]之间，最终结果 409.6m</p>\n<p>Flink 内存&#x3D;4g-256m-409.6m&#x3D;3430.4m</p>\n<p>（2）网络内存&#x3D;3430.4m*0.1&#x3D;343.04m，在[64m,1g]之间，最终结果 343.04m</p>\n<p>（3）托管内存&#x3D;3430.4m*0.4&#x3D;1372.16m</p>\n<p>（4）框架内存，堆内和堆外都是 128m</p>\n<p>（5）Task堆内内存&#x3D;3430.4m-128m-128m-343.04m-1372.16m&#x3D;1459.2m</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667261844-9b48b348-1bcb-4ca8-b556-f63a3680cf83.png\" alt=\"img\"></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654667279269-d43c4812-9561-433a-83fe-a8d70b5fb5b9.png\" alt=\"img\"></p>\n<h3 id=\"所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\"><a href=\"#所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\" class=\"headerlink\" title=\"所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。\"></a>所以进程内存给多大，每一部分内存需不需要调整，可以看内存的使用率来调整。</h3><h2 id=\"合理利用-cpu-资源\"><a href=\"#合理利用-cpu-资源\" class=\"headerlink\" title=\"合理利用 cpu 资源\"></a>合理利用 cpu 资源</h2><p>Yarn 的<strong>容量调度器</strong>默认情况下是使用“DefaultResourceCalculator”分配策略，只根据内存调度资源，所以在 Yarn 的资源管理页面上看到每个容器的 vcore 个数还是 1。</p>\n<p>可以修改策略为 DominantResourceCalculator，该资源计算器在计算资源的时候会综合考虑 cpu 和内存的情况。在capacity-scheduler.xml 中修改属性:</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.capacity.resource-calculator<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- &lt;value&gt;org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator&lt;/value&gt; --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"1-1-1-使用DefaultResourceCalculator-策略\"><a href=\"#1-1-1-使用DefaultResourceCalculator-策略\" class=\"headerlink\" title=\"1.1.1    使用DefaultResourceCalculator 策略\"></a>1.1.1    使用DefaultResourceCalculator 策略</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flink run \\</span><br><span class=\"line\">-t yarn-per-job \\</span><br><span class=\"line\">-d \\</span><br><span class=\"line\">-p 5 \\</span><br><span class=\"line\">-Drest.flamegraph.enabled=true \\</span><br><span class=\"line\">-Dyarn.application.queue=test \\</span><br><span class=\"line\">-Djobmanager.memory.process.size=1024mb \\</span><br><span class=\"line\">-Dtaskmanager.memory.process.size=4096mb \\</span><br><span class=\"line\">-Dtaskmanager.numberOfTaskSlots=2 \\</span><br><span class=\"line\">-c com.atguigu.flink.tuning.UvDemo \\</span><br><span class=\"line\">/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n<p>可以看到一个容器只有一个 vcore：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668251950-033e4bc0-4b65-4fe8-b309-45a29956922b.png\" alt=\"img\"></p>\n<h3 id=\"1-1-2-使用DominantResourceCalculator-策略\"><a href=\"#1-1-2-使用DominantResourceCalculator-策略\" class=\"headerlink\" title=\"1.1.2    使用DominantResourceCalculator 策略\"></a>1.1.2    使用DominantResourceCalculator 策略</h3><p>修改后 yarn 配置后，分发配置并重启 yarn，再次提交 flink 作业：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;4096mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo\\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<p>看到容器的 vcore 数变了:</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668344371-82744e2d-89b2-4fab-8a09-f77475df1088.png\" alt=\"img\"></p>\n<p>JobManager1 个，占用 1 个容器，vcore&#x3D;1</p>\n<p>TaskManager3 个，占用 3 个容器，每个容器 vcore&#x3D;2，总 vcore&#x3D;2*3&#x3D;6，因为默认单个容器的 vcore 数&#x3D;单 TM 的slot 数</p>\n<h3 id=\"1-1-3-使用-DominantResourceCalculator-策略并指定容器vcore-数\"><a href=\"#1-1-3-使用-DominantResourceCalculator-策略并指定容器vcore-数\" class=\"headerlink\" title=\"1.1.3    使用 DominantResourceCalculator 策略并指定容器vcore 数\"></a>1.1.3    使用 DominantResourceCalculator 策略并指定容器<strong>vcore 数</strong></h3><p>指定yarn 容器的 vcore 数，提交：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Dyarn.containers.vcores&#x3D;3\\</p>\n<p> -Djobmanager.memory.process.size&#x3D;1024mb \\ -Dtaskmanager.memory.process.size&#x3D;4096mb \\ -Dtaskmanager.numberOfTaskSlots&#x3D;2 \\ -c com.atguigu.flink.tuning.UvDemo \\ &#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar  </p>\n</blockquote>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654668509233-7292aa6f-0e54-4799-ba38-ab72659ef824.png\" alt=\"img\"></p>\n<p>JobManager1 个，占用 1 个容器，vcore&#x3D;1</p>\n<p>TaskManager3 个，占用 3 个容器，每个容器vcore &#x3D;3，总 vcore&#x3D;3*3&#x3D;9</p>\n<h1 id=\"RocksDB大状态调优\"><a href=\"#RocksDB大状态调优\" class=\"headerlink\" title=\"RocksDB大状态调优\"></a>RocksDB大状态调优</h1><p>RocksDB 是基于 LSM Tree 实现的（类似HBase），写数据都是先缓存到内存中，所以RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record&#x2F;s。<strong>使用RocksDB 时，状态大小仅受可用磁盘空间量的限制，性能瓶颈主要在于 RocksDB对磁盘的读请求，每次读写操作都必须对数据进行反序列化或者序列化。</strong>所以当处理性能不够时，仅需要横向扩展并行度即可提高整个Job 的吞吐量。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654669015363-a35261ab-d4ff-4068-a013-eecfe78a5c7d.png\" alt=\"img\"></p>\n<p>从 Flink1.10 开始，Flink 默认将 RocksDB 的内存大小配置为每个 taskslot 的托管内存。调试内存性能的问题主要是通过调整配置项 taskmanager.memory.managed.size或者 taskmanager.memory.managed.fraction以增加 Flink 的托管内存(即堆外的托管内存)。进一步可以调整一些参数进行高级性能调优，这些参数也可以在应用程序中通过RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory)指定。下面介绍</p>\n<p>提高资源利用率的几个重要配置：</p>\n<h3 id=\"2-1-1-开启State访问性能监控\"><a href=\"#2-1-1-开启State访问性能监控\" class=\"headerlink\" title=\"2.1.1   开启State访问性能监控\"></a>2.1.1   开启State访问性能监控</h3><p>Flink 1.13 中引入了 State 访问的性能监控，即 latency trackig state。此功能不局限于 StateBackend 的类型，自定义实现的 StateBackend 也可以复用此功能。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654670053632-0e169f44-1340-4202-ab6a-bd9a6173a14a.png\" alt=\"img\"></p>\n<p>State访问性能监控会产生一定的性能影响，所以，默认每 100次做一次取样(sample)，对不同的 StateBackend 性能损失影响不同：</p>\n<ul>\n<li>对于 RocksDBStateBackend，性能损失大概在 1% 左右</li>\n<li>对于 HeapStateBackend，性能损失最多可达 10%</li>\n</ul>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">state.backend.latency-track.keyed-state-enabled：true</span> <span class=\"comment\">#启用访问状态的性能监控 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.sample-interval:</span> <span class=\"number\">100</span> <span class=\"comment\">#采样间隔 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.history-size:</span> <span class=\"number\">128</span> <span class=\"comment\">#保留的采样数据个数，越大越精确 </span></span><br><span class=\"line\"><span class=\"attr\">state.backend.latency-track.state-name-as-variable:</span> <span class=\"literal\">true</span> <span class=\"comment\">#将状态名作为变量  </span></span><br></pre></td></tr></table></figure>\n\n<p>正常开启第一个参数即可。</p>\n<blockquote>\n<p>bin&#x2F;flink run \\</p>\n<p>-t yarn-per-job \\</p>\n<p>-d \\</p>\n<p>-p 5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true \\</p>\n<p>-Dyarn.application.queue&#x3D;test \\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;4096mb \\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2 \\</p>\n<p> -Dstate.backend.latency-track.keyed-state-enabled&#x3D;true \\ </p>\n<p>-c com.atguigu.flink.tuning.RocksdbTuning \\ &#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar  </p>\n</blockquote>\n<h3 id=\"2-1-2-开启增量检查点和本地恢复\"><a href=\"#2-1-2-开启增量检查点和本地恢复\" class=\"headerlink\" title=\"2.1.2    开启增量检查点和本地恢复\"></a>2.1.2    开启增量检查点和本地恢复</h3><p>1）开启增量检查点</p>\n<p>RocksDB 是目前唯一可用于支持有状态流处理应用程序增量检查点的状态后端，可以修改参数开启增量检查点：</p>\n<p>state.backend.incremental: true #默认 false，改为 true。 </p>\n<p>或代码中指定 new EmbeddedRocksDBStateBackend(true)  </p>\n<p>2）开启本地恢复</p>\n<p>当 Flink任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs拉取数据。本地恢复目前仅涵盖键控类型的状态后端（RocksDB），MemoryStateBackend不支持本地恢复并忽略此选项。</p>\n<p>state.backend.local-recovery:true</p>\n<h3 id=\"2-1-3-调整预定义选项\"><a href=\"#2-1-3-调整预定义选项\" class=\"headerlink\" title=\"2.1.3    调整预定义选项\"></a>2.1.3    调整预定义选项</h3><p>Flink针对不同的设置为 RocksDB提供了一些预定义的选项集合,其中包含了后续提到的一些参数，如果调整预定义选项后还达不到预期，再去调整后面的 block、writebuffer等参数。</p>\n<p>当 前 支 持 的 预 定 义 选 项 有   DEFAULT 、 SPINNING_DISK_OPTIMIZED 、</p>\n<p>SPINNING_DISK_OPTIMIZED_HIGH_MEM 或FLASH_SSD_OPTIMIZED。有条件上 SSD</p>\n<p>的，可以指定为 FLASH_SSD_OPTIMIZED</p>\n<p> state.backend.rocksdb.predefined-options： SPINNING_DISK_OPTIMIZED_HIGH_MEM #设置为机械硬盘+内存模式  </p>\n<h3 id=\"2-1-4-增大-block-缓存\"><a href=\"#2-1-4-增大-block-缓存\" class=\"headerlink\" title=\"2.1.4    增大 block 缓存\"></a>2.1.4    增大 block 缓存</h3><p>整个 RocksDB 共享一个 blockcache，读数据时内存的 cache 大小，该参数越大读</p>\n<p>数据时缓存命中率越高，默认大小为8MB，建议设置到64~256MB。</p>\n<p>state.backend.rocksdb.block.cache-size:64m     #默认8m  </p>\n<h3 id=\"2-1-5-增大writebuffer-和-level-阈值大小\"><a href=\"#2-1-5-增大writebuffer-和-level-阈值大小\" class=\"headerlink\" title=\"2.1.5    增大writebuffer 和 level 阈值大小\"></a>2.1.5    增大writebuffer 和 level 阈值大小</h3><p>RocksDB 中，每个 State 使用一个 ColumnFamily，每个 ColumnFamily 使用独占的 writebuffer，默认 64MB，建议调大。</p>\n<p>调整这个参数通常要适当增加 L1层的大小阈值 max-size-level-base，默认 256m。</p>\n<p>该值太小会造成能存放的 SST 文件过少，层级变多造成查找困难，太大会造成文件过多，合并困难。建议设为 target_file_size_base（默认 64MB） 的倍数，且不能太小，例如 5<del>10倍，即 320</del>640MB。</p>\n<p>state.backend.rocksdb.writebuffer.size: 128m</p>\n<p>state.backend.rocksdb.compaction.level.max-size-level-base:320m   </p>\n<h3 id=\"2-1-6-增大write-buffer-数量\"><a href=\"#2-1-6-增大write-buffer-数量\" class=\"headerlink\" title=\"2.1.6    增大write buffer 数量\"></a>2.1.6    增大write buffer 数量</h3><p>每个 ColumnFamily对应的 writebuffer 最大数量，这实际上是内存中“只读内存表“的最大数量，默认值是 2。对于机械磁盘来说，如果内存足够大，可以调大到 5左右</p>\n<p>state.backend.rocksdb.writebuffer.count:5                                                                     </p>\n<h3 id=\"2-1-7-增大后台线程数和writebuffer-合并数\"><a href=\"#2-1-7-增大后台线程数和writebuffer-合并数\" class=\"headerlink\" title=\"2.1.7    增大后台线程数和writebuffer 合并数\"></a>2.1.7    增大后台线程数和writebuffer 合并数</h3><p>1）增大线程数</p>\n<p>用于后台 flush和合并 sst文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4等更大的值</p>\n<p>state.backend.rocksdb.thread.num: 4                                                                             </p>\n<p>2）增大writebuffer 最小合并数</p>\n<p>将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 最小数量，默认</p>\n<p>值为 1，可以调成 3。</p>\n<p>state.backend.rocksdb.writebuffer.number-to-merge:3                                             </p>\n<h3 id=\"2-1-8-开启分区索引功能\"><a href=\"#2-1-8-开启分区索引功能\" class=\"headerlink\" title=\"2.1.8    开启分区索引功能\"></a>2.1.8    开启分区索引功能</h3><p>Flink1.13 中对 RocksDB 增加了分区索引功能，复用了 RocksDB 的partitionedIndex&amp;filter 功能，简单来说就是对 RocksDB 的 partitionedIndex 做了多级索引。也就是将内存中的最上层常驻，下层根据需要再 load回来，这样就大大降低了数据 Swap竞争。线上测试中，相对于<strong>内存比较小</strong>的场景中，性能提升 10 倍左右。如果在内存管控下 Rocksdb 性能不如预期的话，这也能成为一个性能优化点。</p>\n<p>state.backend.rocksdb.memory.partitioned-index-filters:true   #默认false                </p>\n<p><strong>2.1.9</strong>    <strong>参数设定案例</strong></p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flinkrun\\</span><br><span class=\"line\">-tyarn-per-job\\</span><br><span class=\"line\">-d\\</span><br><span class=\"line\">-p5\\</span><br><span class=\"line\">-Drest.flamegraph.enabled=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dyarn.application.queue=<span class=\"built_in\">test</span>\\</span><br><span class=\"line\">-Djobmanager.memory.process.size=1024mb \\</span><br><span class=\"line\">-Dtaskmanager.memory.process.size=4096mb\\</span><br><span class=\"line\">-Dtaskmanager.numberOfTaskSlots=2\\</span><br><span class=\"line\">-Dstate.backend.incremental=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.local-recovery=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.predefined-options=SPINNING_DISK_OPTIMIZED_HIGH_MEM\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.block.cache-size=64m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.size=128m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.compaction.level.max-size-level-base=320m\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.count=5 \\</span><br><span class=\"line\">-Dstate.backend.rocksdb.thread.num=4\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.writebuffer.number-to-merge=3\\</span><br><span class=\"line\">-Dstate.backend.rocksdb.memory.partitioned-index-filters=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-Dstate.backend.latency-track.keyed-state-enabled=<span class=\"literal\">true</span>\\</span><br><span class=\"line\">-ccom.atguigu.flink.tuning.RocksdbTuning\\</span><br><span class=\"line\">/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"设置本地-RocksDB-多目录\"><a href=\"#设置本地-RocksDB-多目录\" class=\"headerlink\" title=\"设置本地 RocksDB 多目录\"></a>设置本地 RocksDB 多目录</h3><p>在flink-conf.yaml 中配置：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">state.backend.rocksdb.localdir: /data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb</span><br></pre></td></tr></table></figure>\n\n\n\n<p>注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。<strong>当设置多个 RocksDB 本地磁盘目录时，Flink 会<strong><strong>随机选择</strong></strong>要使用的目录，所以就可能存在三个并行度共用同一目录的情况。</strong>如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。</p>\n<p><strong>当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。</strong></p>\n<p>如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。<strong>由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘IO性能消耗确实比较大。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662632337-7fe1e6c6-5fe2-412e-82e8-77f3c81458b7.png\" alt=\"img\"></p>\n<p>如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj磁盘。可以看到 sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。<strong>如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下</strong>，<strong>从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654662673431-6575b710-490c-49c4-bec7-f4b7964b3fc7.png\" alt=\"img\"></p>\n<ul>\n<li><strong>state.backend.incremental：</strong>开启增量检查点，默认false，改为true。</li>\n<li><strong>state.backend.rocksdb.predefined-options：</strong>SPINNING_DISK_OPTIMIZED_HIGH_MEM设置为机械硬盘+内存模式，有条件上SSD，指定为FLASH_SSD_OPTIMIZED</li>\n<li><strong>state.backend.rocksdb.block.cache-size</strong>: 整个 RocksDB 共享一个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。</li>\n<li><strong>state.backend.rocksdb.thread.num</strong>: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。</li>\n<li><strong>state.backend.rocksdb.writebuffer.size</strong>: RocksDB 中，每个 State 使用一个 Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M</li>\n<li><strong>state.backend.rocksdb.writebuffer.count</strong>: 每个 Column Family 对应的 writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5 左右</li>\n<li><strong>state.backend.rocksdb.writebuffer.number-to-merge</strong>: 将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成3。</li>\n<li><strong>state.backend.local-recovery</strong>: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据</li>\n</ul>\n<h2 id=\"Checkpoint设置\"><a href=\"#Checkpoint设置\" class=\"headerlink\" title=\"Checkpoint设置\"></a>Checkpoint设置</h2><p>一般我们的 Checkpoint 时间间隔可以设置为分钟级别（1<del>5分钟），例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5</del>10 分钟一次Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次Checkpoint 之间至少暂停 4或8 分钟。</p>\n<p>同时，也需要考虑时效性的要求,需要在时效性和性能之间做一个平衡，如果时效性要求高，结合 end- to-end 时长，设置秒级或毫秒级。</p>\n<p>如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看 Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。</p>\n<p>RocksDB相关参数在1.3中已说明，可以在flink-conf.yaml指定，也可以在Job的代码中调用API单独指定，这里不再列出。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint</span></span><br><span class=\"line\"><span class=\"type\">RocksDBStateBackend</span> rocksDBStateBackend = <span class=\"keyword\">new</span> <span class=\"type\">RocksDBStateBackend</span>(<span class=\"string\">&quot;hdfs://hadoop102:8020/flink/checkpoints&quot;</span>, <span class=\"literal\">true</span>);</span><br><span class=\"line\">env.setStateBackend(rocksDBStateBackend);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 开启Checkpoint，间隔为 3 分钟</span></span><br><span class=\"line\">env.enableCheckpointing(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">3</span>));</span><br><span class=\"line\"><span class=\"comment\">// 配置 Checkpoint</span></span><br><span class=\"line\"><span class=\"type\">CheckpointConfig</span> checkpointConf = env.getCheckpointConfig();</span><br><span class=\"line\">checkpointConf.setCheckpointingMode(<span class=\"type\">CheckpointingMode</span>.<span class=\"type\">EXACTLY_ONCE</span>)</span><br><span class=\"line\"><span class=\"comment\">// 最小间隔 4分钟</span></span><br><span class=\"line\">checkpointConf.setMinPauseBetweenCheckpoints(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"comment\">// 超时时间 10分钟</span></span><br><span class=\"line\">checkpointConf.setCheckpointTimeout(<span class=\"type\">TimeUnit</span>.<span class=\"type\">MINUTES</span>.toMillis(<span class=\"number\">10</span>));</span><br><span class=\"line\"><span class=\"comment\">// 保存checkpoint</span></span><br><span class=\"line\">checkpointConf.enableExternalizedCheckpoints(</span><br><span class=\"line\"><span class=\"type\">CheckpointConfig</span>.<span class=\"type\">ExternalizedCheckpointCleanup</span>.<span class=\"type\">RETAIN_ON_CANCELLATION</span>);</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"反压处理\"><a href=\"#反压处理\" class=\"headerlink\" title=\"反压处理\"></a>反压处理</h1><h2 id=\"3-1-概述\"><a href=\"#3-1-概述\" class=\"headerlink\" title=\"3.1 概述\"></a>3.1 概述</h2><p>Flink 网络流控及反压的介绍：</p>\n<p><a href=\"https://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626\">https://flink-learning.org.cn/article/detail/138316d1556f8f9d34e517d04d670626</a></p>\n<h3 id=\"3-1-1-反压的理解\"><a href=\"#3-1-1-反压的理解\" class=\"headerlink\" title=\"3.1.1    反压的理解\"></a>3.1.1    反压的理解</h3><p>简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。</p>\n<p>反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。</p>\n<h3 id=\"3-1-2-反压的危害\"><a href=\"#3-1-2-反压的危害\" class=\"headerlink\" title=\"3.1.2    反压的危害\"></a>3.1.2    反压的危害</h3><p>反压如果不能得到正确的处理，可能会影响到 checkpoint时长和 state大小，甚至可能会导致资源耗尽甚至系统崩溃。</p>\n<ul>\n<li>1）影响 checkpoint 时长：barrier 不会越过普通数据，数据处理被阻塞也会导致checkpointbarrier 流经整个数据管道的时长变长，导致 checkpoint 总体时间（End toEndDuration）变长。</li>\n<li>2）影响 state 大小：barrier 对齐时，接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到 state 里面，导致 checkpoint 变大。</li>\n</ul>\n<p>这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint<strong>超时失败</strong>，而 state 大小同样可能拖慢 checkpoint 甚至导致 <strong>OOM</strong>（使用 Heap-basedStateBackend）或者物理内存使用<strong>超出容器资源</strong>（使用 RocksDBStateBackend）的稳定性问题。</p>\n<p><strong>因此，我们在生产中要尽量避免出现反压的情况。</strong></p>\n<h2 id=\"3-2-定位反压节点\"><a href=\"#3-2-定位反压节点\" class=\"headerlink\" title=\"3.2 定位反压节点\"></a>3.2 定位反压节点</h2><p>解决反压首先要做的是定位到造成反压的节点，排查的时候，先把operatorchain 禁用，方便定位到具体算子。</p>\n<p>提交UvDemo:</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<h3 id=\"3-2-1-利用-FlinkWebUI-定位\"><a href=\"#3-2-1-利用-FlinkWebUI-定位\" class=\"headerlink\" title=\"3.2.1    利用 FlinkWebUI 定位\"></a>3.2.1    利用 FlinkWebUI 定位</h3><p>FlinkWebUI 的反压监控提供了 SubTask 级别的反压监控，1.13 版本以前是通过周期性对  Task  线程的栈信息采样，得到线程被阻塞在请求  Buffer（意味着被下游队列阻塞）</p>\n<p>的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1以下则为 OK，0.1</p>\n<p>至 0.5为 LOW，而超过 0.5则为 HIGH。</p>\n<p>Flink1.13 优化了反压检测的逻辑（使用基于任务 Mailbox计时，而不在再于堆栈采样），并且重新实现了作业图的 UI展示：Flink现在在 UI 上通过颜色和数值来展示繁忙和反压的程度。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674284140-b680f841-3ad4-4250-87fd-8c331333f1f5.png\" alt=\"img\"></p>\n<p>1）通过WebUI看到 Map算子处于反压：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654674446026-5ec8c33c-cadc-44c9-9d00-b644899f52d6.png\" alt=\"img\"></p>\n<p>3）分析瓶颈算子</p>\n<p>如果处于反压状态，那么有两种可能性：</p>\n<p>（1）  该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。这种情况，该节点是反压的根源节点，它是从 SourceTask到 Sink Task 的第一个出现反压的节点。<strong>（很少出现，表现为：反压算子一进多出，后面的算子处理速度慢，从这个反压算子开始，后面的算子都反压了。图示，绿色为反压节点：</strong></p>\n<p><strong>（OK-&gt; OK-&gt;</strong> <strong>反</strong> <strong>-&gt;反 -&gt; 反 ）</strong></p>\n<p><strong>一进多出，输入缓存区使用率可能高也可能低，输出缓存区使用率高</strong></p>\n<p>（2）  下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。这种情况，需要继续排查下游节点，一直找到第一个为OK的一般就是根源节点。<strong>（表现为：这个反压算子处理速度慢，阻塞了前面的算子，导致前面的算子反压了，其后面的算子表现为不反压。图示，绿色为反压节点：</strong></p>\n<p>​      <strong>（反 -&gt; 反 -&gt;</strong> <strong>OK</strong>-&gt; OK-&gt; OK）</p>\n<p><strong>输入缓存区使用率高，输出缓存区使用率低</strong></p>\n<p>总体来看，如果我们找到第一个出现反压的节点，反压根源要么是就这个节点，要么是它紧接着的下游节点。</p>\n<p>通常来讲，第二种情况更常见。如果无法确定，还需要结合 Metrics进一步判断。</p>\n<h3 id=\"3-2-2-利用-Metrics-定位\"><a href=\"#3-2-2-利用-Metrics-定位\" class=\"headerlink\" title=\"3.2.2    利用 Metrics 定位\"></a>3.2.2    利用 Metrics 定位</h3><p>监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为</p>\n<p>有用的是以下几个 Metrics:</p>\n<table>\n<thead>\n<tr>\n<th><strong>Metris</strong></th>\n<th><strong>描述</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>outPoolUsage</td>\n<td>发送端 Buffer 的使用率</td>\n</tr>\n<tr>\n<td>inPoolUsage</td>\n<td>接收端 Buffer 的使用率</td>\n</tr>\n<tr>\n<td>floatingBuffersUsage（1.9 以上）</td>\n<td>接收端 FloatingBuffer 的使用率</td>\n</tr>\n<tr>\n<td>exclusiveBuffersUsage（1.9 以上）</td>\n<td>接收端 ExclusiveBuffer 的使用率</td>\n</tr>\n</tbody></table>\n<p>其中 inPoolUsage &#x3D; floatingBuffersUsage + exclusiveBuffersUsage。</p>\n<h4 id=\"1）根据指标分析反压\"><a href=\"#1）根据指标分析反压\" class=\"headerlink\" title=\"1）根据指标分析反压\"></a>1）根据指标分析反压</h4><p>分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(1.9 以上):</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>outPoolUsage</strong> <strong>低</strong></th>\n<th><strong>outPoolUsage</strong> <strong>高</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>inPoolUsage</strong> <strong>低</strong></td>\n<td>正常</td>\n<td>被下游反压，处于临时情况（还没传递到上游）</td>\n</tr>\n<tr>\n<td>可能是反压的根源，一条输入多条输出的场景</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>inPoolUsage</strong> <strong>高</strong></td>\n<td>如果上游所有 outPoolUsage 都是低，有可能最终可能导致反压（还没传递到上游）</td>\n<td>被下游反压</td>\n</tr>\n<tr>\n<td>如果上游的 outPoolUsage 是高，则为反压根源</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h4 id=\"2）可以进一步分析数据传输\"><a href=\"#2）可以进一步分析数据传输\" class=\"headerlink\" title=\"2）可以进一步分析数据传输\"></a>2）可以进一步分析数据传输</h4><p>Flink1.9 及以上版本，还可以根据 floatingBuffersUsage&#x2F;exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游Subtask 的数据传输。</p>\n<p>在流量较大时，Channel  的  ExclusiveBuffer  可能会被写满，此时  Flink  会向  BufferPool 申请剩余的 FloatingBuffer。这些 <strong>FloatingBuffer 属于备用 Buffer。</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>exclusiveBuffersUsage</strong> <strong>低</strong></th>\n<th><strong>exclusiveBuffersUsage</strong> <strong>高</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>floatingBuffersUsage</strong> <strong>低</strong>所有上游<strong>outPoolUsage</strong> <strong>低</strong></td>\n<td>正常</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong> <strong>低</strong>上游某个<strong>outPoolUsage</strong> <strong>高</strong></td>\n<td>潜在的网络瓶颈</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong>高所有上游<strong>outPoolUsage</strong> <strong>低</strong></td>\n<td>最终对部分inputChannel 反压（正在传递）</td>\n<td>最终对大多数或所有   inputChannel反压（正在传递）</td>\n</tr>\n<tr>\n<td><strong>floatingBuffersUsage</strong>高上游某个<strong>outPoolUsage</strong> <strong>高</strong></td>\n<td>只对部分 inputChannel 反压</td>\n<td>对大多数或所有 inputChannel 反压</td>\n</tr>\n</tbody></table>\n<p>总结：</p>\n<ul>\n<li>1）floatingBuffersUsage 为高，则表明反压正在传导至上游</li>\n<li>2）同时 exclusiveBuffersUsage 为低，则表明可能有倾斜</li>\n</ul>\n<p>比如，floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数</p>\n<p>channel 占用了大部分的 FloatingBuffer。</p>\n<h2 id=\"3-3-反压的原因及处理\"><a href=\"#3-3-反压的原因及处理\" class=\"headerlink\" title=\"3.3 反压的原因及处理\"></a>3.3 反压的原因及处理</h2><p>注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。</p>\n<p>定位到反压节点后，分析造成原因的办法主要是观察 TaskThread。按照下面的顺序，一步一步去排查。</p>\n<h3 id=\"3-3-1-查看是否数据倾斜\"><a href=\"#3-3-1-查看是否数据倾斜\" class=\"headerlink\" title=\"3.3.1    查看是否数据倾斜\"></a>3.3.1    查看是否数据倾斜</h3><p><strong>在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI各</strong></p>\n<p><strong>个 SubTask 的 RecordsSent 和 RecordReceived 来确认，另外 Checkpointdetail里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。</strong></p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675365111-f2598a4c-7ae6-4c6b-852b-a2c31b53623e.png\" alt=\"img\"></p>\n<p>（关于数据倾斜的详细解决方案，会在下一章节详细讨论）</p>\n<h3 id=\"3-3-2-使用火焰图分析\"><a href=\"#3-3-2-使用火焰图分析\" class=\"headerlink\" title=\"3.3.2    使用火焰图分析\"></a>3.3.2    使用火焰图分析</h3><p>如果不是数据倾斜，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题），需要找到瓶颈算子中的哪部分计算逻辑消耗巨大。</p>\n<p>最有用的办法就是对 TaskManager 进行 CPUprofile，从中我们可以分析到 TaskThread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面；如果不是的话要看 TaskThread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是checkpoint 或者 GC 等系统活动导致的暂时系统暂停。</p>\n<h4 id=\"1）开启火焰图功能\"><a href=\"#1）开启火焰图功能\" class=\"headerlink\" title=\"1）开启火焰图功能\"></a>1）开启火焰图功能</h4><p>Flink1.13直接在 WebUI提供 JVM的 CPU 火焰图，这将大大简化性能瓶颈的分析，默认是不开启的，需要修改参数：</p>\n<p>rest.flamegraph.enabled:true#默认false                                                                          </p>\n<p>也可以在提交时指定：</p>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<h4 id=\"2）WebUI-查看火焰图\"><a href=\"#2）WebUI-查看火焰图\" class=\"headerlink\" title=\"2）WebUI 查看火焰图\"></a>2）WebUI 查看火焰图</h4><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654675647317-7df4c4eb-e01f-4637-9d0e-a9980331f2c2.png\" alt=\"img\"></p>\n<p>火焰图是通过对堆栈跟踪进行多次采样来构建的。每个方法调用都由一个条形表示，其中条形的长度与其在样本中出现的次数成正比。</p>\n<ul>\n<li>On-CPU: 处于 [RUNNABLE, NEW]状态的线程</li>\n<li>Off-CPU: 处于 [TIMED_WAITING, WAITING, BLOCKED]的线程，用于查看在样本中发现的阻塞调用。</li>\n</ul>\n<h4 id=\"3）分析火焰图\"><a href=\"#3）分析火焰图\" class=\"headerlink\" title=\"3）分析火焰图\"></a>3）分析火焰图</h4><p>颜色没有特殊含义，具体查看：</p>\n<ul>\n<li>纵向是调用链，从下往上，顶部就是正在执行的函数</li>\n<li>横向是样本出现次数，可以理解为执行时长。</li>\n</ul>\n<p><strong>看顶层的哪个函数占据的宽度最大。只要有”平顶”（plateaus），就表示该函数可能存在性能问题。</strong></p>\n<p>如果是 Flink1.13 以前的版本，可以手动做火焰图：</p>\n<p>如何生成火焰图：<a href=\"http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/\">http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/</a></p>\n<h3 id=\"3-3-3-分析GC-情况\"><a href=\"#3-3-3-分析GC-情况\" class=\"headerlink\" title=\"3.3.3    分析GC 情况\"></a>3.3.3    分析GC 情况</h3><p>TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManagerJVM 各区内存不合理导致的频繁 FullGC 甚至失联。通常建议使用默认的 G1 垃圾回收器。</p>\n<p>可以通过打印 GC 日志（-XX:+PrintGCDetails），使用 GC 分析器（GCViewer 工具）来验证是否处于这种情况。</p>\n<ul>\n<li>在 Flink 提交脚本中,设置 JVM 参数，打印 GC 日志：</li>\n</ul>\n<blockquote>\n<p>bin&#x2F;flinkrun\\</p>\n<p>-tyarn-per-job\\</p>\n<p>-d\\</p>\n<p>-p5 \\</p>\n<p>-Drest.flamegraph.enabled&#x3D;true\\</p>\n<p>-Denv.java.opts&#x3D;”-XX:+PrintGCDetails-XX:+PrintGCDateStamps”\\</p>\n<p>-Dyarn.application.queue&#x3D;test\\</p>\n<p>-Djobmanager.memory.process.size&#x3D;1024mb \\</p>\n<p>-Dtaskmanager.memory.process.size&#x3D;2048mb\\</p>\n<p>-Dtaskmanager.numberOfTaskSlots&#x3D;2\\</p>\n<p>-ccom.atguigu.flink.tuning.UvDemo \\</p>\n<p>&#x2F;opt&#x2F;module&#x2F;flink-1.13.1&#x2F;myjar&#x2F;flink-tuning-1.0-SNAPSHOT.jar</p>\n</blockquote>\n<ul>\n<li>下载 GC 日志的方式：</li>\n</ul>\n<p>因为是 onyarn 模式，运行的节点一个一个找比较麻烦。可以打开 WebUI，选择JobManager 或者 TaskManager，点击 Stdout，即可看到 GC 日志，点击下载按钮即可将 GC日志通过 HTTP的方式下载下来。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654679097595-18b82b7c-8bd5-4d21-b720-44c795ce377a.png\" alt=\"img\"></p>\n<ul>\n<li>分析 GC 日志：</li>\n</ul>\n<p>通过 GC 日志分析出单个 FlinkTaskmanager 堆总大小、年轻代、老年代分配的内存空间、FullGC 后老年代剩余大小等，相关指标定义可以去 Github 具体查看。</p>\n<p>GCViewer 地址：<a href=\"https://github.com/chewiebug/GCViewer\">https://github.com/chewiebug/GCViewer</a></p>\n<p>Linux 下分析：</p>\n<p>java -jargcviewer_1.3.4.jargc.log                                                                                    </p>\n<p>Windows 下分析：</p>\n<p>直接双击gcviewer_1.3.4.jar，打开GUI界面，选择gc的log打开         </p>\n<p>​                      </p>\n<p>扩展：最重要的指标是FullGC 后，老年代剩余大小这个指标，按照《Java 性能优化权威指南》这本书 Java 堆大小计算法则，设 FullGC 后老年代剩余大小空间为 M，那么堆的大小建议 3<del>4 倍 M，新生代为 1</del>1.5 倍 M，老年代应为 2~3 倍 M。</p>\n<h3 id=\"3-3-4-外部组件交互\"><a href=\"#3-3-4-外部组件交互\" class=\"headerlink\" title=\"3.3.4    外部组件交互\"></a>3.3.4    外部组件交互</h3><p>如果发现我们的 Source端数据读取性能比较低或者 Sink端写入性能较差，需要检查第三方组件是否遇到瓶颈，还有就是做维表join时的性能问题。</p>\n<p>例如：</p>\n<p>Kafka集群是否需要扩容，Kafka 连接器是否并行度较低</p>\n<p>HBase的 rowkey 是否遇到热点问题，是否请求处理不过来</p>\n<p>ClickHouse并发能力较弱，是否达到瓶颈</p>\n<p>……</p>\n<p>关于第三方组件的性能问题，需要结合具体的组件来分析，最常用的思路：</p>\n<ul>\n<li>1）异步 io+热缓存来优化读写性能</li>\n<li>2）先攒批再读写维表join参考：</li>\n</ul>\n<p><a href=\"https://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3\">https://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3</a></p>\n<p><a href=\"https://www.jianshu.com/p/a62fa483ff54\">https://www.jianshu.com/p/a62fa483ff54</a></p>\n<h1 id=\"四、数据倾斜\"><a href=\"#四、数据倾斜\" class=\"headerlink\" title=\"四、数据倾斜\"></a>四、数据倾斜</h1><h2 id=\"4-1-判断是否存在数据倾斜\"><a href=\"#4-1-判断是否存在数据倾斜\" class=\"headerlink\" title=\"4.1  判断是否存在数据倾斜\"></a>4.1  判断是否存在数据倾斜</h2><p>相同 Task 的多个 Subtask 中， 个别 Subtask 接收到的数据量明显大于其他Subtask 接收到的数据量，通过 FlinkWebUI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692839400-88f4eb2d-9389-4011-a676-2f6da336cb39.png\" alt=\"img\"></p>\n<p>另外， 有时 Checkpointdetail 里不同 SubTask 的 Statesize 也是一个分析数据倾斜的有用指标。</p>\n<h2 id=\"4-2-数据倾斜的解决\"><a href=\"#4-2-数据倾斜的解决\" class=\"headerlink\" title=\"4.2 数据倾斜的解决\"></a>4.2 数据倾斜的解决</h2><h3 id=\"4-2-1-keyBy-后的聚合操作存在数据倾斜\"><a href=\"#4-2-1-keyBy-后的聚合操作存在数据倾斜\" class=\"headerlink\" title=\"4.2.1    keyBy 后的聚合操作存在数据倾斜\"></a>4.2.1    keyBy 后的聚合操作存在数据倾斜</h3><h4 id=\"1）为什么不能直接用二次聚合来处理（没有卵用）\"><a href=\"#1）为什么不能直接用二次聚合来处理（没有卵用）\" class=\"headerlink\" title=\"1）为什么不能直接用二次聚合来处理（没有卵用）\"></a>1）为什么不能直接用二次聚合来处理（没有卵用）</h4><p>Flink是实时流处理，如果keyby之后的聚合操作存在数据倾斜，且没有开窗口（没攒批）的情况下，简单的认为使用两阶段聚合，是不能解决问题的。因为这个时候Flink是来一条处理一条，且向下游发送一条结果，对于原来 keyby的维度（第二阶段聚合）来讲，数据量并没有减少，且结果重复计算（非 FlinkSQL，未使用回撤流），如下图所示：</p>\n<p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/2500465/1654692995562-f3b6caac-04e3-45ac-87bc-92286cb10e2b.png\" alt=\"img\"></p>\n<h4 id=\"2）使用-LocalKeyBy-的思想\"><a href=\"#2）使用-LocalKeyBy-的思想\" class=\"headerlink\" title=\"2）使用 LocalKeyBy 的思想\"></a>2）使用 LocalKeyBy 的思想</h4><p>在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后，再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。类似 MapReduce中 Combiner的思想，但是这要求聚合操作必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。从 FlinkLocalKeyBy实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。</p>\n<h4 id=\"实现方式：\"><a href=\"#实现方式：\" class=\"headerlink\" title=\"实现方式：\"></a>实现方式：</h4><ul>\n<li>DataStreamAPI 需要自己写代码实现</li>\n<li>SQL 可以指定参数，开启miniBatch 和 LocalGlobal 功能（推荐，后续介绍）</li>\n</ul>\n<h3 id=\"4-1-1-keyBy之前发生数据倾斜\"><a href=\"#4-1-1-keyBy之前发生数据倾斜\" class=\"headerlink\" title=\"4.1.1    keyBy之前发生数据倾斜\"></a>4.1.1    keyBy之前发生数据倾斜</h3><p>如果 keyBy 之前就存在数据倾斜，上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。</p>\n<p>对于不存在 keyBy 的 Flink 任务也会出现该情况。</p>\n<p>这种情况，需要让 Flink 任务强制进行shuffle。使用 shuffle、rebalance 或 rescale</p>\n<p>算子即可将数据均匀分配，从而解决数据倾斜的问题。</p>\n<h3 id=\"4-1-2-keyBy-后的窗口聚合操作存在数据倾斜\"><a href=\"#4-1-2-keyBy-后的窗口聚合操作存在数据倾斜\" class=\"headerlink\" title=\"4.1.2    keyBy 后的窗口聚合操作存在数据倾斜\"></a>4.1.2    keyBy 后的窗口聚合操作存在数据倾斜</h3><p>因为使用了窗口，变成了有界数据（攒批）的处理，窗口默认是触发时才会输出一条结果发往下游，所以可以使用两阶段聚合的方式：</p>\n<h4 id=\"1）实现思路：\"><a href=\"#1）实现思路：\" class=\"headerlink\" title=\"1）实现思路：\"></a>1）实现思路：</h4><ul>\n<li>第一阶段聚合：key拼接随机数前缀或后缀，进行 keyby、开窗、聚合</li>\n</ul>\n<p><strong>注意：聚合完不再是 WindowedStream，要获取 WindowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）</strong></p>\n<ul>\n<li>第二阶段聚合：按照原来的 key 及windowEnd 作keyby、聚合</li>\n</ul>\n<p>SQL写法参考：<a href=\"https://zhuanlan.zhihu.com/p/197299746\">https://zhuanlan.zhihu.com/p/197299746</a></p>\n"},{"title":"测试Flink Doris Connector","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":13792,"date":"2022-08-06T09:57:25.000Z","updated":"2022-08-06T09:57:25.000Z","cover":"https://dogefs.s3.ladydaily.com/tzk/storage/20210302234030.png","description":null,"keywords":null,"_content":"\n> 利用Flink Doris Connector将Kafka中的数据实时导入到Doris\n>\n> 该Connector支持Flink SQL和DataStream API\n>\n> **注意：**\n>\n> 1. 修改和删除只支持在 Unique Key 模型上\n> 2. 目前的删除是支持 Flink CDC 的方式接入数据实现自动删除，如果是其他数据接入的方式删除需要自己实现。Flink CDC 的数据删除使用方式参照本文档最后一节\n\n\n\n### Maven\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-java</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-streaming-java_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-clients_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<!-- flink table -->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n\n<!-- flink-doris-connector -->\n<dependency>\n  <groupId>org.apache.doris</groupId>\n  <artifactId>flink-doris-connector-1.14_2.12</artifactId>\n  <version>1.1.0</version>\n</dependency>  \n```\n\n\n\n### 参数配置\n\nFlink Doris Connector Sink 的内部实现是通过 `Stream Load` 服务向 Doris 写入数据, 同时也支持 `Stream Load` 请求参数的配置设置，具体参数可参考[这里](https://doris.apache.org/zh-CN/docs/data-operate/import/import-way/stream-load-manual)，配置方法如下：\n\n- SQL 使用 `WITH` 参数 `sink.properties.` 配置\n- DataStream 使用方法`DorisExecutionOptions.builder().setStreamLoadProp(Properties)`配置\n\n\n\n### 示例\n\n```scala\npackage cn.jxau.yuan\n\n\nimport cn.jxau.yuan.common.Config\nimport org.apache.doris.flink.cfg.{DorisExecutionOptions, DorisOptions, DorisReadOptions}\nimport org.apache.doris.flink.sink.DorisSink\nimport org.apache.doris.flink.sink.writer.SimpleStringSerializer\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.connector.kafka.source.KafkaSource\nimport org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer\nimport org.apache.flink.streaming.api.environment.CheckpointConfig\nimport org.apache.flink.streaming.api.scala._\n\nimport java.util.Properties\n\n\nobject KafkaConnectTest {\n\n  def main(args: Array[String]): Unit = {\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n    env.enableCheckpointing(10000)\n    env.getCheckpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)\n\n\n    val kafkaSource: KafkaSource[String] = KafkaSource.builder[String]\n      .setBootstrapServers(Config.broker)\n      .setTopics(\"input-topic\")\n      .setGroupId(\"my-group\")\n      .setStartingOffsets(OffsetsInitializer.earliest)\n      .setValueOnlyDeserializer(new SimpleStringSchema)\n      .build\n\n    env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks[String](), \"Kafka Source\")\n      .sinkTo(buildDorisSink())\n\n    env.execute()\n  }\n\n  def buildDorisSink(): DorisSink[String]  = {\n    //sink config\n    val builder: DorisSink.Builder[String] = DorisSink.builder();\n    val dorisBuilder: DorisOptions.Builder = DorisOptions.builder();\n    dorisBuilder.setFenodes(\"127.0.0.1:8030\")\n      .setTableIdentifier(\"db.table\")\n      .setUsername(\"root\")\n      .setPassword(\"password\");\n\n\n    val pro: Properties = new Properties();\n    //json data format\n    pro.setProperty(\"format\", \"json\");\n    pro.setProperty(\"read_json_by_line\", \"true\");\n\n\n    val executionOptions: DorisExecutionOptions  = DorisExecutionOptions.builder()\n      .setLabelPrefix(\"label-doris\") //streamload label prefix,\n      .setStreamLoadProp(pro)\n      .build()\n\n    builder.setDorisReadOptions(DorisReadOptions.builder().build())\n      .setDorisExecutionOptions(executionOptions)\n      .setSerializer(new SimpleStringSerializer()) //serialize according to string\n      .setDorisOptions(dorisBuilder.build())\n      .build()\n  }\n}\n\n```\n\n## Flink Table && SQL Maven\n\n我们想要在代码中使用Table API，必须引入相关的依赖。\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n\n```\n\n这里的依赖是一个 Java 的“桥接器”（bridge），主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。\n\n如果我们希望在本地的集成开发环境（IDE）里运行 Table API 和 SQL，还需要引入以下依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n    </dependency>\n    <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n```\n\n这里主要添加的依赖是一个“计划器”（planner），它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的 blink planner。由于 Flink 安装包的 lib 目录下会自带planner，所以在生产集群环境中提交的作业不需要打包这个赖。而在Table API 的内部实现上，部分相关的代码是用 Scala 实现的，所以还需要额外添加一个 Scala 版流处理的相关依赖。\n另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-common</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n```\n\n","source":"_posts/bigdata/flink/测试Flink-Doris-Connector.md","raw":"---\ntitle: 测试Flink Doris Connector\ntags:\n  - Doris\ncategories:\n  - - bigdata\n    - Doris\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 13792\ndate: 2022-08-06 17:57:25\nupdated: 2022-08-06 17:57:25\ncover:\ndescription:\nkeywords:\n---\n\n> 利用Flink Doris Connector将Kafka中的数据实时导入到Doris\n>\n> 该Connector支持Flink SQL和DataStream API\n>\n> **注意：**\n>\n> 1. 修改和删除只支持在 Unique Key 模型上\n> 2. 目前的删除是支持 Flink CDC 的方式接入数据实现自动删除，如果是其他数据接入的方式删除需要自己实现。Flink CDC 的数据删除使用方式参照本文档最后一节\n\n\n\n### Maven\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-java</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-streaming-java_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-clients_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n<!-- flink table -->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner_${scala.version}</artifactId>\n    <version>${flink.version}</version>\n    <scope>provided</scope>\n</dependency>\n\n<!-- flink-doris-connector -->\n<dependency>\n  <groupId>org.apache.doris</groupId>\n  <artifactId>flink-doris-connector-1.14_2.12</artifactId>\n  <version>1.1.0</version>\n</dependency>  \n```\n\n\n\n### 参数配置\n\nFlink Doris Connector Sink 的内部实现是通过 `Stream Load` 服务向 Doris 写入数据, 同时也支持 `Stream Load` 请求参数的配置设置，具体参数可参考[这里](https://doris.apache.org/zh-CN/docs/data-operate/import/import-way/stream-load-manual)，配置方法如下：\n\n- SQL 使用 `WITH` 参数 `sink.properties.` 配置\n- DataStream 使用方法`DorisExecutionOptions.builder().setStreamLoadProp(Properties)`配置\n\n\n\n### 示例\n\n```scala\npackage cn.jxau.yuan\n\n\nimport cn.jxau.yuan.common.Config\nimport org.apache.doris.flink.cfg.{DorisExecutionOptions, DorisOptions, DorisReadOptions}\nimport org.apache.doris.flink.sink.DorisSink\nimport org.apache.doris.flink.sink.writer.SimpleStringSerializer\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.connector.kafka.source.KafkaSource\nimport org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer\nimport org.apache.flink.streaming.api.environment.CheckpointConfig\nimport org.apache.flink.streaming.api.scala._\n\nimport java.util.Properties\n\n\nobject KafkaConnectTest {\n\n  def main(args: Array[String]): Unit = {\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n    env.enableCheckpointing(10000)\n    env.getCheckpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)\n\n\n    val kafkaSource: KafkaSource[String] = KafkaSource.builder[String]\n      .setBootstrapServers(Config.broker)\n      .setTopics(\"input-topic\")\n      .setGroupId(\"my-group\")\n      .setStartingOffsets(OffsetsInitializer.earliest)\n      .setValueOnlyDeserializer(new SimpleStringSchema)\n      .build\n\n    env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks[String](), \"Kafka Source\")\n      .sinkTo(buildDorisSink())\n\n    env.execute()\n  }\n\n  def buildDorisSink(): DorisSink[String]  = {\n    //sink config\n    val builder: DorisSink.Builder[String] = DorisSink.builder();\n    val dorisBuilder: DorisOptions.Builder = DorisOptions.builder();\n    dorisBuilder.setFenodes(\"127.0.0.1:8030\")\n      .setTableIdentifier(\"db.table\")\n      .setUsername(\"root\")\n      .setPassword(\"password\");\n\n\n    val pro: Properties = new Properties();\n    //json data format\n    pro.setProperty(\"format\", \"json\");\n    pro.setProperty(\"read_json_by_line\", \"true\");\n\n\n    val executionOptions: DorisExecutionOptions  = DorisExecutionOptions.builder()\n      .setLabelPrefix(\"label-doris\") //streamload label prefix,\n      .setStreamLoadProp(pro)\n      .build()\n\n    builder.setDorisReadOptions(DorisReadOptions.builder().build())\n      .setDorisExecutionOptions(executionOptions)\n      .setSerializer(new SimpleStringSerializer()) //serialize according to string\n      .setDorisOptions(dorisBuilder.build())\n      .build()\n  }\n}\n\n```\n\n## Flink Table && SQL Maven\n\n我们想要在代码中使用Table API，必须引入相关的依赖。\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n\n```\n\n这里的依赖是一个 Java 的“桥接器”（bridge），主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。\n\n如果我们希望在本地的集成开发环境（IDE）里运行 Table API 和 SQL，还需要引入以下依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n    </dependency>\n    <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n```\n\n这里主要添加的依赖是一个“计划器”（planner），它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的 blink planner。由于 Flink 安装包的 lib 目录下会自带planner，所以在生产集群环境中提交的作业不需要打包这个赖。而在Table API 的内部实现上，部分相关的代码是用 Scala 实现的，所以还需要额外添加一个 Scala 版流处理的相关依赖。\n另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖：\n\n```xml\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-common</artifactId>\n    <version>${flink.version}</version>\n</dependency>\n```\n\n","slug":"bigdata/flink/测试Flink-Doris-Connector","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004s8j5mbfzkci1d","content":"<blockquote>\n<p>利用Flink Doris Connector将Kafka中的数据实时导入到Doris</p>\n<p>该Connector支持Flink SQL和DataStream API</p>\n<p><strong>注意：</strong></p>\n<ol>\n<li>修改和删除只支持在 Unique Key 模型上</li>\n<li>目前的删除是支持 Flink CDC 的方式接入数据实现自动删除，如果是其他数据接入的方式删除需要自己实现。Flink CDC 的数据删除使用方式参照本文档最后一节</li>\n</ol>\n</blockquote>\n<h3 id=\"Maven\"><a href=\"#Maven\" class=\"headerlink\" title=\"Maven\"></a>Maven</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-java<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-clients_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- flink table --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- flink-doris-connector --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.doris<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-doris-connector-1.14_2.12<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.1.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span>  </span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h3><p>Flink Doris Connector Sink 的内部实现是通过 <code>Stream Load</code> 服务向 Doris 写入数据, 同时也支持 <code>Stream Load</code> 请求参数的配置设置，具体参数可参考<a href=\"https://doris.apache.org/zh-CN/docs/data-operate/import/import-way/stream-load-manual\">这里</a>，配置方法如下：</p>\n<ul>\n<li>SQL 使用 <code>WITH</code> 参数 <code>sink.properties.</code> 配置</li>\n<li>DataStream 使用方法<code>DorisExecutionOptions.builder().setStreamLoadProp(Properties)</code>配置</li>\n</ul>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau.yuan</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> cn.jxau.yuan.common.<span class=\"type\">Config</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.cfg.&#123;<span class=\"type\">DorisExecutionOptions</span>, <span class=\"type\">DorisOptions</span>, <span class=\"type\">DorisReadOptions</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.sink.<span class=\"type\">DorisSink</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.sink.writer.<span class=\"type\">SimpleStringSerializer</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.eventtime.<span class=\"type\">WatermarkStrategy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.serialization.<span class=\"type\">SimpleStringSchema</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.connector.kafka.source.<span class=\"type\">KafkaSource</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.connector.kafka.source.enumerator.initializer.<span class=\"type\">OffsetsInitializer</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.<span class=\"type\">CheckpointConfig</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.scala._</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.<span class=\"type\">Properties</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">KafkaConnectTest</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> env: <span class=\"type\">StreamExecutionEnvironment</span> = <span class=\"type\">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class=\"line\">    env.enableCheckpointing(<span class=\"number\">10000</span>)</span><br><span class=\"line\">    env.getCheckpointConfig.setExternalizedCheckpointCleanup(<span class=\"type\">CheckpointConfig</span>.<span class=\"type\">ExternalizedCheckpointCleanup</span>.<span class=\"type\">RETAIN_ON_CANCELLATION</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> kafkaSource: <span class=\"type\">KafkaSource</span>[<span class=\"type\">String</span>] = <span class=\"type\">KafkaSource</span>.builder[<span class=\"type\">String</span>]</span><br><span class=\"line\">      .setBootstrapServers(<span class=\"type\">Config</span>.broker)</span><br><span class=\"line\">      .setTopics(<span class=\"string\">&quot;input-topic&quot;</span>)</span><br><span class=\"line\">      .setGroupId(<span class=\"string\">&quot;my-group&quot;</span>)</span><br><span class=\"line\">      .setStartingOffsets(<span class=\"type\">OffsetsInitializer</span>.earliest)</span><br><span class=\"line\">      .setValueOnlyDeserializer(<span class=\"keyword\">new</span> <span class=\"type\">SimpleStringSchema</span>)</span><br><span class=\"line\">      .build</span><br><span class=\"line\"></span><br><span class=\"line\">    env.fromSource(kafkaSource, <span class=\"type\">WatermarkStrategy</span>.noWatermarks[<span class=\"type\">String</span>](), <span class=\"string\">&quot;Kafka Source&quot;</span>)</span><br><span class=\"line\">      .sinkTo(buildDorisSink())</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">buildDorisSink</span></span>(): <span class=\"type\">DorisSink</span>[<span class=\"type\">String</span>]  = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//sink config</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> builder: <span class=\"type\">DorisSink</span>.<span class=\"type\">Builder</span>[<span class=\"type\">String</span>] = <span class=\"type\">DorisSink</span>.builder();</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dorisBuilder: <span class=\"type\">DorisOptions</span>.<span class=\"type\">Builder</span> = <span class=\"type\">DorisOptions</span>.builder();</span><br><span class=\"line\">    dorisBuilder.setFenodes(<span class=\"string\">&quot;127.0.0.1:8030&quot;</span>)</span><br><span class=\"line\">      .setTableIdentifier(<span class=\"string\">&quot;db.table&quot;</span>)</span><br><span class=\"line\">      .setUsername(<span class=\"string\">&quot;root&quot;</span>)</span><br><span class=\"line\">      .setPassword(<span class=\"string\">&quot;password&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> pro: <span class=\"type\">Properties</span> = <span class=\"keyword\">new</span> <span class=\"type\">Properties</span>();</span><br><span class=\"line\">    <span class=\"comment\">//json data format</span></span><br><span class=\"line\">    pro.setProperty(<span class=\"string\">&quot;format&quot;</span>, <span class=\"string\">&quot;json&quot;</span>);</span><br><span class=\"line\">    pro.setProperty(<span class=\"string\">&quot;read_json_by_line&quot;</span>, <span class=\"string\">&quot;true&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> executionOptions: <span class=\"type\">DorisExecutionOptions</span>  = <span class=\"type\">DorisExecutionOptions</span>.builder()</span><br><span class=\"line\">      .setLabelPrefix(<span class=\"string\">&quot;label-doris&quot;</span>) <span class=\"comment\">//streamload label prefix,</span></span><br><span class=\"line\">      .setStreamLoadProp(pro)</span><br><span class=\"line\">      .build()</span><br><span class=\"line\"></span><br><span class=\"line\">    builder.setDorisReadOptions(<span class=\"type\">DorisReadOptions</span>.builder().build())</span><br><span class=\"line\">      .setDorisExecutionOptions(executionOptions)</span><br><span class=\"line\">      .setSerializer(<span class=\"keyword\">new</span> <span class=\"type\">SimpleStringSerializer</span>()) <span class=\"comment\">//serialize according to string</span></span><br><span class=\"line\">      .setDorisOptions(dorisBuilder.build())</span><br><span class=\"line\">      .build()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Flink-Table-amp-amp-SQL-Maven\"><a href=\"#Flink-Table-amp-amp-SQL-Maven\" class=\"headerlink\" title=\"Flink Table &amp;&amp; SQL Maven\"></a>Flink Table &amp;&amp; SQL Maven</h2><p>我们想要在代码中使用Table API，必须引入相关的依赖。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-api-java-bridge_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>这里的依赖是一个 Java 的“桥接器”（bridge），主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。</p>\n<p>如果我们希望在本地的集成开发环境（IDE）里运行 Table API 和 SQL，还需要引入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>这里主要添加的依赖是一个“计划器”（planner），它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的 blink planner。由于 Flink 安装包的 lib 目录下会自带planner，所以在生产集群环境中提交的作业不需要打包这个赖。而在Table API 的内部实现上，部分相关的代码是用 Scala 实现的，所以还需要额外添加一个 Scala 版流处理的相关依赖。<br>另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-common<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<blockquote>\n<p>利用Flink Doris Connector将Kafka中的数据实时导入到Doris</p>\n<p>该Connector支持Flink SQL和DataStream API</p>\n<p><strong>注意：</strong></p>\n<ol>\n<li>修改和删除只支持在 Unique Key 模型上</li>\n<li>目前的删除是支持 Flink CDC 的方式接入数据实现自动删除，如果是其他数据接入的方式删除需要自己实现。Flink CDC 的数据删除使用方式参照本文档最后一节</li>\n</ol>\n</blockquote>\n<h3 id=\"Maven\"><a href=\"#Maven\" class=\"headerlink\" title=\"Maven\"></a>Maven</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-java<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-clients_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- flink table --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>provided<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- flink-doris-connector --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.doris<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-doris-connector-1.14_2.12<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.1.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span>  </span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h3><p>Flink Doris Connector Sink 的内部实现是通过 <code>Stream Load</code> 服务向 Doris 写入数据, 同时也支持 <code>Stream Load</code> 请求参数的配置设置，具体参数可参考<a href=\"https://doris.apache.org/zh-CN/docs/data-operate/import/import-way/stream-load-manual\">这里</a>，配置方法如下：</p>\n<ul>\n<li>SQL 使用 <code>WITH</code> 参数 <code>sink.properties.</code> 配置</li>\n<li>DataStream 使用方法<code>DorisExecutionOptions.builder().setStreamLoadProp(Properties)</code>配置</li>\n</ul>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> cn.jxau.yuan</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> cn.jxau.yuan.common.<span class=\"type\">Config</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.cfg.&#123;<span class=\"type\">DorisExecutionOptions</span>, <span class=\"type\">DorisOptions</span>, <span class=\"type\">DorisReadOptions</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.sink.<span class=\"type\">DorisSink</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.doris.flink.sink.writer.<span class=\"type\">SimpleStringSerializer</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.eventtime.<span class=\"type\">WatermarkStrategy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.serialization.<span class=\"type\">SimpleStringSchema</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.connector.kafka.source.<span class=\"type\">KafkaSource</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.connector.kafka.source.enumerator.initializer.<span class=\"type\">OffsetsInitializer</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.<span class=\"type\">CheckpointConfig</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.scala._</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.<span class=\"type\">Properties</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">KafkaConnectTest</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> env: <span class=\"type\">StreamExecutionEnvironment</span> = <span class=\"type\">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class=\"line\">    env.enableCheckpointing(<span class=\"number\">10000</span>)</span><br><span class=\"line\">    env.getCheckpointConfig.setExternalizedCheckpointCleanup(<span class=\"type\">CheckpointConfig</span>.<span class=\"type\">ExternalizedCheckpointCleanup</span>.<span class=\"type\">RETAIN_ON_CANCELLATION</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> kafkaSource: <span class=\"type\">KafkaSource</span>[<span class=\"type\">String</span>] = <span class=\"type\">KafkaSource</span>.builder[<span class=\"type\">String</span>]</span><br><span class=\"line\">      .setBootstrapServers(<span class=\"type\">Config</span>.broker)</span><br><span class=\"line\">      .setTopics(<span class=\"string\">&quot;input-topic&quot;</span>)</span><br><span class=\"line\">      .setGroupId(<span class=\"string\">&quot;my-group&quot;</span>)</span><br><span class=\"line\">      .setStartingOffsets(<span class=\"type\">OffsetsInitializer</span>.earliest)</span><br><span class=\"line\">      .setValueOnlyDeserializer(<span class=\"keyword\">new</span> <span class=\"type\">SimpleStringSchema</span>)</span><br><span class=\"line\">      .build</span><br><span class=\"line\"></span><br><span class=\"line\">    env.fromSource(kafkaSource, <span class=\"type\">WatermarkStrategy</span>.noWatermarks[<span class=\"type\">String</span>](), <span class=\"string\">&quot;Kafka Source&quot;</span>)</span><br><span class=\"line\">      .sinkTo(buildDorisSink())</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">buildDorisSink</span></span>(): <span class=\"type\">DorisSink</span>[<span class=\"type\">String</span>]  = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//sink config</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> builder: <span class=\"type\">DorisSink</span>.<span class=\"type\">Builder</span>[<span class=\"type\">String</span>] = <span class=\"type\">DorisSink</span>.builder();</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dorisBuilder: <span class=\"type\">DorisOptions</span>.<span class=\"type\">Builder</span> = <span class=\"type\">DorisOptions</span>.builder();</span><br><span class=\"line\">    dorisBuilder.setFenodes(<span class=\"string\">&quot;127.0.0.1:8030&quot;</span>)</span><br><span class=\"line\">      .setTableIdentifier(<span class=\"string\">&quot;db.table&quot;</span>)</span><br><span class=\"line\">      .setUsername(<span class=\"string\">&quot;root&quot;</span>)</span><br><span class=\"line\">      .setPassword(<span class=\"string\">&quot;password&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> pro: <span class=\"type\">Properties</span> = <span class=\"keyword\">new</span> <span class=\"type\">Properties</span>();</span><br><span class=\"line\">    <span class=\"comment\">//json data format</span></span><br><span class=\"line\">    pro.setProperty(<span class=\"string\">&quot;format&quot;</span>, <span class=\"string\">&quot;json&quot;</span>);</span><br><span class=\"line\">    pro.setProperty(<span class=\"string\">&quot;read_json_by_line&quot;</span>, <span class=\"string\">&quot;true&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> executionOptions: <span class=\"type\">DorisExecutionOptions</span>  = <span class=\"type\">DorisExecutionOptions</span>.builder()</span><br><span class=\"line\">      .setLabelPrefix(<span class=\"string\">&quot;label-doris&quot;</span>) <span class=\"comment\">//streamload label prefix,</span></span><br><span class=\"line\">      .setStreamLoadProp(pro)</span><br><span class=\"line\">      .build()</span><br><span class=\"line\"></span><br><span class=\"line\">    builder.setDorisReadOptions(<span class=\"type\">DorisReadOptions</span>.builder().build())</span><br><span class=\"line\">      .setDorisExecutionOptions(executionOptions)</span><br><span class=\"line\">      .setSerializer(<span class=\"keyword\">new</span> <span class=\"type\">SimpleStringSerializer</span>()) <span class=\"comment\">//serialize according to string</span></span><br><span class=\"line\">      .setDorisOptions(dorisBuilder.build())</span><br><span class=\"line\">      .build()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Flink-Table-amp-amp-SQL-Maven\"><a href=\"#Flink-Table-amp-amp-SQL-Maven\" class=\"headerlink\" title=\"Flink Table &amp;&amp; SQL Maven\"></a>Flink Table &amp;&amp; SQL Maven</h2><p>我们想要在代码中使用Table API，必须引入相关的依赖。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-api-java-bridge_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>这里的依赖是一个 Java 的“桥接器”（bridge），主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。</p>\n<p>如果我们希望在本地的集成开发环境（IDE）里运行 Table API 和 SQL，还需要引入以下依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>这里主要添加的依赖是一个“计划器”（planner），它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的 blink planner。由于 Flink 安装包的 lib 目录下会自带planner，所以在生产集群环境中提交的作业不需要打包这个赖。而在Table API 的内部实现上，部分相关的代码是用 Scala 实现的，所以还需要额外添加一个 Scala 版流处理的相关依赖。<br>另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.flink<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>flink-table-common<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>$&#123;flink.version&#125;<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n"},{"title":"优化Flink ogg-json format","top_img":"/img/bg/banner.gif","abbrlink":5296,"date":"2023-08-08T12:23:04.000Z","updated":"2023-08-08T12:23:04.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## 前言\n\n最近发现从kafka同步到Paimon中的数据不正确。具体表现为，明明数据库中某条记录已经Update了，但是Paimon中的同一条记录没有同步更新。经过一系列的排查发现，是由于公司ogg json格式不统一，导致Flink ogg-json format解析失败，同时因为配置了`ogg-json.ignore-parse-errors = true`，最终导致整条ogg更新Record被丢弃，没有发送到下流的Paimon。\n\n\n\n## 代码记录\n\n`org.apache.flink.formats.json.ogg.OggJsonDeserializationSchema#deserialize(byte[], org.apache.flink.util.Collector<org.apache.flink.table.data.RowData>)`\n\n```java\n    @Override\n    public void deserialize(byte[] message, Collector<RowData> out) throws IOException {\n        if (message == null || message.length == 0) {\n            // skip tombstone messages\n            return;\n        }\n        try {\n            final JsonNode root = jsonDeserializer.deserializeToJsonNode(message);\n            GenericRowData row = (GenericRowData) jsonDeserializer.convertToRowData(root);\n\n            GenericRowData before = (GenericRowData) row.getField(0);\n            GenericRowData after = (GenericRowData) row.getField(1);\n            String op = row.getField(2).toString();\n            if (OP_CREATE.equals(op)) {\n                after.setRowKind(RowKind.INSERT);\n                emitRow(row, after, out);\n            } else if (OP_UPDATE.equals(op)) {\n                if (before == null) {\n                    throw new IllegalStateException(\n                            String.format(REPLICA_IDENTITY_EXCEPTION, \"UPDATE\"));\n                }\n\n                // for case: \"before\":{}\n                if (!root.get(\"before\").isEmpty()) {\n                    before.setRowKind(RowKind.UPDATE_BEFORE);\n                    emitRow(row, before, out);\n                }\n\n                after.setRowKind(RowKind.UPDATE_AFTER);\n                emitRow(row, after, out);\n            } else if (OP_DELETE.equals(op)) {\n                if (before == null) {\n                    throw new IllegalStateException(\n                            String.format(REPLICA_IDENTITY_EXCEPTION, \"DELETE\"));\n                }\n                before.setRowKind(RowKind.DELETE);\n                emitRow(row, before, out);\n            } else {\n                if (!ignoreParseErrors) {\n                    throw new IOException(\n                            format(\n                                    \"Unknown \\\"op_type\\\" value \\\"%s\\\". The Ogg JSON message is '%s'\",\n                                    op, new String(message)));\n                }\n            }\n        } catch (Throwable t) {\n            // a big try catch to protect the processing.\n            if (!ignoreParseErrors) {\n                throw new IOException(\n                        format(\"Corrupt Ogg JSON message '%s'.\", new String(message)), t);\n            }\n        }\n    }\n```\n\n\n\n## PR\n\nhttps://github.com/apache/flink/pull/23102\n","source":"_posts/bigdata/flink/增强Flink ogg-json format.md","raw":"---\ntitle: 优化Flink ogg-json format\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\ntop_img: /img/bg/banner.gif\nabbrlink: 5296\ndate: 2023-08-08 20:23:04\nupdated: 2023-08-08 20:23:04\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n最近发现从kafka同步到Paimon中的数据不正确。具体表现为，明明数据库中某条记录已经Update了，但是Paimon中的同一条记录没有同步更新。经过一系列的排查发现，是由于公司ogg json格式不统一，导致Flink ogg-json format解析失败，同时因为配置了`ogg-json.ignore-parse-errors = true`，最终导致整条ogg更新Record被丢弃，没有发送到下流的Paimon。\n\n\n\n## 代码记录\n\n`org.apache.flink.formats.json.ogg.OggJsonDeserializationSchema#deserialize(byte[], org.apache.flink.util.Collector<org.apache.flink.table.data.RowData>)`\n\n```java\n    @Override\n    public void deserialize(byte[] message, Collector<RowData> out) throws IOException {\n        if (message == null || message.length == 0) {\n            // skip tombstone messages\n            return;\n        }\n        try {\n            final JsonNode root = jsonDeserializer.deserializeToJsonNode(message);\n            GenericRowData row = (GenericRowData) jsonDeserializer.convertToRowData(root);\n\n            GenericRowData before = (GenericRowData) row.getField(0);\n            GenericRowData after = (GenericRowData) row.getField(1);\n            String op = row.getField(2).toString();\n            if (OP_CREATE.equals(op)) {\n                after.setRowKind(RowKind.INSERT);\n                emitRow(row, after, out);\n            } else if (OP_UPDATE.equals(op)) {\n                if (before == null) {\n                    throw new IllegalStateException(\n                            String.format(REPLICA_IDENTITY_EXCEPTION, \"UPDATE\"));\n                }\n\n                // for case: \"before\":{}\n                if (!root.get(\"before\").isEmpty()) {\n                    before.setRowKind(RowKind.UPDATE_BEFORE);\n                    emitRow(row, before, out);\n                }\n\n                after.setRowKind(RowKind.UPDATE_AFTER);\n                emitRow(row, after, out);\n            } else if (OP_DELETE.equals(op)) {\n                if (before == null) {\n                    throw new IllegalStateException(\n                            String.format(REPLICA_IDENTITY_EXCEPTION, \"DELETE\"));\n                }\n                before.setRowKind(RowKind.DELETE);\n                emitRow(row, before, out);\n            } else {\n                if (!ignoreParseErrors) {\n                    throw new IOException(\n                            format(\n                                    \"Unknown \\\"op_type\\\" value \\\"%s\\\". The Ogg JSON message is '%s'\",\n                                    op, new String(message)));\n                }\n            }\n        } catch (Throwable t) {\n            // a big try catch to protect the processing.\n            if (!ignoreParseErrors) {\n                throw new IOException(\n                        format(\"Corrupt Ogg JSON message '%s'.\", new String(message)), t);\n            }\n        }\n    }\n```\n\n\n\n## PR\n\nhttps://github.com/apache/flink/pull/23102\n","slug":"bigdata/flink/增强Flink ogg-json format","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004v8j5m252tgt9i","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近发现从kafka同步到Paimon中的数据不正确。具体表现为，明明数据库中某条记录已经Update了，但是Paimon中的同一条记录没有同步更新。经过一系列的排查发现，是由于公司ogg json格式不统一，导致Flink ogg-json format解析失败，同时因为配置了<code>ogg-json.ignore-parse-errors = true</code>，最终导致整条ogg更新Record被丢弃，没有发送到下流的Paimon。</p>\n<h2 id=\"代码记录\"><a href=\"#代码记录\" class=\"headerlink\" title=\"代码记录\"></a>代码记录</h2><p><code>org.apache.flink.formats.json.ogg.OggJsonDeserializationSchema#deserialize(byte[], org.apache.flink.util.Collector&lt;org.apache.flink.table.data.RowData&gt;)</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">deserialize</span><span class=\"params\">(<span class=\"type\">byte</span>[] message, Collector&lt;RowData&gt; out)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (message == <span class=\"literal\">null</span> || message.length == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// skip tombstone messages</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">JsonNode</span> <span class=\"variable\">root</span> <span class=\"operator\">=</span> jsonDeserializer.deserializeToJsonNode(message);</span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">row</span> <span class=\"operator\">=</span> (GenericRowData) jsonDeserializer.convertToRowData(root);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">before</span> <span class=\"operator\">=</span> (GenericRowData) row.getField(<span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">after</span> <span class=\"operator\">=</span> (GenericRowData) row.getField(<span class=\"number\">1</span>);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">op</span> <span class=\"operator\">=</span> row.getField(<span class=\"number\">2</span>).toString();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (OP_CREATE.equals(op)) &#123;</span><br><span class=\"line\">            after.setRowKind(RowKind.INSERT);</span><br><span class=\"line\">            emitRow(row, after, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (OP_UPDATE.equals(op)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (before == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(</span><br><span class=\"line\">                        String.format(REPLICA_IDENTITY_EXCEPTION, <span class=\"string\">&quot;UPDATE&quot;</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// for case: &quot;before&quot;:&#123;&#125;</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!root.get(<span class=\"string\">&quot;before&quot;</span>).isEmpty()) &#123;</span><br><span class=\"line\">                before.setRowKind(RowKind.UPDATE_BEFORE);</span><br><span class=\"line\">                emitRow(row, before, out);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            after.setRowKind(RowKind.UPDATE_AFTER);</span><br><span class=\"line\">            emitRow(row, after, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (OP_DELETE.equals(op)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (before == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(</span><br><span class=\"line\">                        String.format(REPLICA_IDENTITY_EXCEPTION, <span class=\"string\">&quot;DELETE&quot;</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            before.setRowKind(RowKind.DELETE);</span><br><span class=\"line\">            emitRow(row, before, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!ignoreParseErrors) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IOException</span>(</span><br><span class=\"line\">                        format(</span><br><span class=\"line\">                                <span class=\"string\">&quot;Unknown \\&quot;op_type\\&quot; value \\&quot;%s\\&quot;. The Ogg JSON message is &#x27;%s&#x27;&quot;</span>,</span><br><span class=\"line\">                                op, <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(message)));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (Throwable t) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// a big try catch to protect the processing.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!ignoreParseErrors) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IOException</span>(</span><br><span class=\"line\">                    format(<span class=\"string\">&quot;Corrupt Ogg JSON message &#x27;%s&#x27;.&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(message)), t);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"PR\"><a href=\"#PR\" class=\"headerlink\" title=\"PR\"></a>PR</h2><p><a href=\"https://github.com/apache/flink/pull/23102\">https://github.com/apache/flink/pull/23102</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近发现从kafka同步到Paimon中的数据不正确。具体表现为，明明数据库中某条记录已经Update了，但是Paimon中的同一条记录没有同步更新。经过一系列的排查发现，是由于公司ogg json格式不统一，导致Flink ogg-json format解析失败，同时因为配置了<code>ogg-json.ignore-parse-errors = true</code>，最终导致整条ogg更新Record被丢弃，没有发送到下流的Paimon。</p>\n<h2 id=\"代码记录\"><a href=\"#代码记录\" class=\"headerlink\" title=\"代码记录\"></a>代码记录</h2><p><code>org.apache.flink.formats.json.ogg.OggJsonDeserializationSchema#deserialize(byte[], org.apache.flink.util.Collector&lt;org.apache.flink.table.data.RowData&gt;)</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">deserialize</span><span class=\"params\">(<span class=\"type\">byte</span>[] message, Collector&lt;RowData&gt; out)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (message == <span class=\"literal\">null</span> || message.length == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// skip tombstone messages</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">JsonNode</span> <span class=\"variable\">root</span> <span class=\"operator\">=</span> jsonDeserializer.deserializeToJsonNode(message);</span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">row</span> <span class=\"operator\">=</span> (GenericRowData) jsonDeserializer.convertToRowData(root);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">before</span> <span class=\"operator\">=</span> (GenericRowData) row.getField(<span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"type\">GenericRowData</span> <span class=\"variable\">after</span> <span class=\"operator\">=</span> (GenericRowData) row.getField(<span class=\"number\">1</span>);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">op</span> <span class=\"operator\">=</span> row.getField(<span class=\"number\">2</span>).toString();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (OP_CREATE.equals(op)) &#123;</span><br><span class=\"line\">            after.setRowKind(RowKind.INSERT);</span><br><span class=\"line\">            emitRow(row, after, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (OP_UPDATE.equals(op)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (before == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(</span><br><span class=\"line\">                        String.format(REPLICA_IDENTITY_EXCEPTION, <span class=\"string\">&quot;UPDATE&quot;</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// for case: &quot;before&quot;:&#123;&#125;</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!root.get(<span class=\"string\">&quot;before&quot;</span>).isEmpty()) &#123;</span><br><span class=\"line\">                before.setRowKind(RowKind.UPDATE_BEFORE);</span><br><span class=\"line\">                emitRow(row, before, out);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            after.setRowKind(RowKind.UPDATE_AFTER);</span><br><span class=\"line\">            emitRow(row, after, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (OP_DELETE.equals(op)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (before == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IllegalStateException</span>(</span><br><span class=\"line\">                        String.format(REPLICA_IDENTITY_EXCEPTION, <span class=\"string\">&quot;DELETE&quot;</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            before.setRowKind(RowKind.DELETE);</span><br><span class=\"line\">            emitRow(row, before, out);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!ignoreParseErrors) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IOException</span>(</span><br><span class=\"line\">                        format(</span><br><span class=\"line\">                                <span class=\"string\">&quot;Unknown \\&quot;op_type\\&quot; value \\&quot;%s\\&quot;. The Ogg JSON message is &#x27;%s&#x27;&quot;</span>,</span><br><span class=\"line\">                                op, <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(message)));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (Throwable t) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// a big try catch to protect the processing.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!ignoreParseErrors) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"title class_\">IOException</span>(</span><br><span class=\"line\">                    format(<span class=\"string\">&quot;Corrupt Ogg JSON message &#x27;%s&#x27;.&quot;</span>, <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(message)), t);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"PR\"><a href=\"#PR\" class=\"headerlink\" title=\"PR\"></a>PR</h2><p><a href=\"https://github.com/apache/flink/pull/23102\">https://github.com/apache/flink/pull/23102</a></p>\n"},{"title":"通过Flink-SQL，将Kafka中的Oracle-CDC-Log同步到Doris","top_img":"linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)","abbrlink":38552,"date":"2022-08-14T15:14:56.000Z","updated":"2022-08-14T15:14:56.000Z","cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n## 前言\n- Oracle的binlog日志已经由DBA通过OGG同步到Kafka中了，因此用不到Flink CDC\n- 同步到Kafka中的JSON样式\n  ```json\n  {\n  \"before\": {\n    \"id\": 111,\n    \"name\": \"scooter\",\n    \"description\": \"Big 2-wheel scooter\",\n    \"weight\": 5.18\n  },\n  \"after\": {\n    \"id\": 111,\n    \"name\": \"scooter\",\n    \"description\": \"Big 2-wheel scooter\",\n    \"weight\": 5.15\n  },\n  \"op_type\": \"U\",\n  \"op_ts\": \"2020-05-13 15:40:06.000000\",\n  \"current_ts\": \"2020-05-13 15:40:07.000000\",\n  \"primary_keys\": [\n    \"id\"\n  ],\n  \"pos\": \"00000000000000000000143\",\n  \"table\": \"PRODUCTS\"\n  }\n  ```\n\n## Flink SQL\n> 需要下载以下Jar包，放在{flink_home}/lib/下\n> flink-sql-connector-kafka_2.12-1.14.5.jar\n> flink-json-1.15.1.jar\n> flink-doris-connector-1.14_2.12-1.1.0.jar\n\n- 开启CheckPoint：`SET 'execution.checkpointing.interval' = '10min';`\n\n- 创建Kafka数据源表，设置`'format' = 'ogg-json'`，只有`org.apache.flink.flink-json-1.15.1`中以上才支持ogg-json fromat\n```sql\nCREATE TABLE topic_products (\n  id INT,\n  name STRING,\n  description STRING,\n  weight DECIMAL(10, 2)\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'products_ogg_1',\n  'properties.bootstrap.servers' = '172.30.160.5:9092',\n  'properties.group.id' = 'testGroup',\n  'format' = 'ogg-json',\n  'scan.startup.mode' = 'earliest-offset',\n  'ogg-json.ignore-parse-errors' = 'true'\n);\n```\n\n- 创建Doris-Sink表\n```sql\nCREATE TABLE doris_sink (\nid INT,\nname STRING,\ndescription STRING,\nweight DECIMAL(10, 2)\n)\nWITH (\n  'connector' = 'doris',\n  'fenodes' = '172.30.160.5:8030',\n  'table.identifier' = 'test.product',\n  'username' = 'root',\n  'password' = '',\n  'sink.properties.format' = 'json',\n  'sink.properties.read_json_by_line' = 'true',\n  'sink.enable-delete' = 'true',\n  'sink.label-prefix' = 'doris_label'\n);\n```\n\n- 执行`INSERT into doris_sink select * from topic_products;`语句，写入Doris\n\n## Code Repo\n\n> 1. **bin/sql-client.sh embedded -i init_file -f file -s yarn-session** \n> 2. Execute SQL Files \n\n```sql\n-- Define available catalogs\n\nCREATE CATALOG MyCatalog\n  WITH (\n    'type' = 'hive'\n  );\n\nUSE CATALOG MyCatalog;\n\n-- Define available database\n\nCREATE DATABASE MyDatabase;\n\nUSE MyDatabase;\n\n-- Define TABLE\n\nCREATE TABLE MyTable(\n  MyField1 INT,\n  MyField2 STRING\n) WITH (\n  'connector' = 'filesystem',\n  'path' = '/path/to/something',\n  'format' = 'csv'\n);\n\n-- Define VIEW\n\nCREATE VIEW MyCustomView AS SELECT MyField2 FROM MyTable;\n\n-- Define user-defined functions here.\n\nCREATE FUNCTION foo.bar.AggregateUDF AS myUDF;\n\n-- Properties that change the fundamental execution behavior of a table program.\n\nSET 'execution.runtime-mode' = 'streaming'; -- execution mode either 'batch' or 'streaming'\nSET 'sql-client.execution.result-mode' = 'table'; -- available values: 'table', 'changelog' and 'tableau'\nSET 'sql-client.execution.max-table-result.rows' = '10000'; -- optional: maximum number of maintained rows\nSET 'parallelism.default' = '1'; -- optional: Flink's parallelism (1 by default)\nSET 'pipeline.auto-watermark-interval' = '200'; --optional: interval for periodic watermarks\nSET 'pipeline.max-parallelism' = '10'; -- optional: Flink's maximum parallelism\nSET 'table.exec.state.ttl' = '1000'; -- optional: table program's idle state time\nSET 'restart-strategy' = 'fixed-delay';\n\n-- Configuration options for adjusting and tuning table programs.\n\nSET 'table.optimizer.join-reorder-enabled' = 'true';\nSET 'table.exec.spill-compression.enabled' = 'true';\nSET 'table.exec.spill-compression.block-size' = '128kb';\n```\n\n\n\n```sql\nCREATE TEMPORARY TABLE users (\n  user_id BIGINT,\n  user_name STRING,\n  user_level STRING,\n  region STRING,\n  PRIMARY KEY (user_id) NOT ENFORCED\n) WITH (\n  'connector' = 'upsert-kafka',\n  'topic' = 'users',\n  'properties.bootstrap.servers' = '...',\n  'key.format' = 'csv',\n  'value.format' = 'avro'\n);\n\n-- set sync mode\nSET 'table.dml-sync' = 'true';\n\n-- set the job name\nSET 'pipeline.name' = 'SqlJob';\n\n-- set the queue that the job submit to\nSET 'yarn.application.queue' = 'root';\n\n-- set the job parallelism\nSET 'parallelism.default' = '100';\n\n-- restore from the specific savepoint path\nSET 'execution.savepoint.path' = '/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab';\n\nINSERT INTO pageviews_enriched\nSELECT *\nFROM pageviews AS p\nLEFT JOIN users FOR SYSTEM_TIME AS OF p.proctime AS u\nON p.user_id = u.user_id;\n```\n\n","source":"_posts/bigdata/flink/通过Flink-SQL，将Kafka中的Oracle-CDC-Log同步到Doris.md","raw":"---\ntitle: 通过Flink-SQL，将Kafka中的Oracle-CDC-Log同步到Doris\ntags:\n  - Flink\ncategories:\n  - - bigdata\n    - Flink\ntop_img: 'linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)'\nabbrlink: 38552\ndate: 2022-08-14 23:14:56\nupdated: 2022-08-14 23:14:56\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n- Oracle的binlog日志已经由DBA通过OGG同步到Kafka中了，因此用不到Flink CDC\n- 同步到Kafka中的JSON样式\n  ```json\n  {\n  \"before\": {\n    \"id\": 111,\n    \"name\": \"scooter\",\n    \"description\": \"Big 2-wheel scooter\",\n    \"weight\": 5.18\n  },\n  \"after\": {\n    \"id\": 111,\n    \"name\": \"scooter\",\n    \"description\": \"Big 2-wheel scooter\",\n    \"weight\": 5.15\n  },\n  \"op_type\": \"U\",\n  \"op_ts\": \"2020-05-13 15:40:06.000000\",\n  \"current_ts\": \"2020-05-13 15:40:07.000000\",\n  \"primary_keys\": [\n    \"id\"\n  ],\n  \"pos\": \"00000000000000000000143\",\n  \"table\": \"PRODUCTS\"\n  }\n  ```\n\n## Flink SQL\n> 需要下载以下Jar包，放在{flink_home}/lib/下\n> flink-sql-connector-kafka_2.12-1.14.5.jar\n> flink-json-1.15.1.jar\n> flink-doris-connector-1.14_2.12-1.1.0.jar\n\n- 开启CheckPoint：`SET 'execution.checkpointing.interval' = '10min';`\n\n- 创建Kafka数据源表，设置`'format' = 'ogg-json'`，只有`org.apache.flink.flink-json-1.15.1`中以上才支持ogg-json fromat\n```sql\nCREATE TABLE topic_products (\n  id INT,\n  name STRING,\n  description STRING,\n  weight DECIMAL(10, 2)\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'products_ogg_1',\n  'properties.bootstrap.servers' = '172.30.160.5:9092',\n  'properties.group.id' = 'testGroup',\n  'format' = 'ogg-json',\n  'scan.startup.mode' = 'earliest-offset',\n  'ogg-json.ignore-parse-errors' = 'true'\n);\n```\n\n- 创建Doris-Sink表\n```sql\nCREATE TABLE doris_sink (\nid INT,\nname STRING,\ndescription STRING,\nweight DECIMAL(10, 2)\n)\nWITH (\n  'connector' = 'doris',\n  'fenodes' = '172.30.160.5:8030',\n  'table.identifier' = 'test.product',\n  'username' = 'root',\n  'password' = '',\n  'sink.properties.format' = 'json',\n  'sink.properties.read_json_by_line' = 'true',\n  'sink.enable-delete' = 'true',\n  'sink.label-prefix' = 'doris_label'\n);\n```\n\n- 执行`INSERT into doris_sink select * from topic_products;`语句，写入Doris\n\n## Code Repo\n\n> 1. **bin/sql-client.sh embedded -i init_file -f file -s yarn-session** \n> 2. Execute SQL Files \n\n```sql\n-- Define available catalogs\n\nCREATE CATALOG MyCatalog\n  WITH (\n    'type' = 'hive'\n  );\n\nUSE CATALOG MyCatalog;\n\n-- Define available database\n\nCREATE DATABASE MyDatabase;\n\nUSE MyDatabase;\n\n-- Define TABLE\n\nCREATE TABLE MyTable(\n  MyField1 INT,\n  MyField2 STRING\n) WITH (\n  'connector' = 'filesystem',\n  'path' = '/path/to/something',\n  'format' = 'csv'\n);\n\n-- Define VIEW\n\nCREATE VIEW MyCustomView AS SELECT MyField2 FROM MyTable;\n\n-- Define user-defined functions here.\n\nCREATE FUNCTION foo.bar.AggregateUDF AS myUDF;\n\n-- Properties that change the fundamental execution behavior of a table program.\n\nSET 'execution.runtime-mode' = 'streaming'; -- execution mode either 'batch' or 'streaming'\nSET 'sql-client.execution.result-mode' = 'table'; -- available values: 'table', 'changelog' and 'tableau'\nSET 'sql-client.execution.max-table-result.rows' = '10000'; -- optional: maximum number of maintained rows\nSET 'parallelism.default' = '1'; -- optional: Flink's parallelism (1 by default)\nSET 'pipeline.auto-watermark-interval' = '200'; --optional: interval for periodic watermarks\nSET 'pipeline.max-parallelism' = '10'; -- optional: Flink's maximum parallelism\nSET 'table.exec.state.ttl' = '1000'; -- optional: table program's idle state time\nSET 'restart-strategy' = 'fixed-delay';\n\n-- Configuration options for adjusting and tuning table programs.\n\nSET 'table.optimizer.join-reorder-enabled' = 'true';\nSET 'table.exec.spill-compression.enabled' = 'true';\nSET 'table.exec.spill-compression.block-size' = '128kb';\n```\n\n\n\n```sql\nCREATE TEMPORARY TABLE users (\n  user_id BIGINT,\n  user_name STRING,\n  user_level STRING,\n  region STRING,\n  PRIMARY KEY (user_id) NOT ENFORCED\n) WITH (\n  'connector' = 'upsert-kafka',\n  'topic' = 'users',\n  'properties.bootstrap.servers' = '...',\n  'key.format' = 'csv',\n  'value.format' = 'avro'\n);\n\n-- set sync mode\nSET 'table.dml-sync' = 'true';\n\n-- set the job name\nSET 'pipeline.name' = 'SqlJob';\n\n-- set the queue that the job submit to\nSET 'yarn.application.queue' = 'root';\n\n-- set the job parallelism\nSET 'parallelism.default' = '100';\n\n-- restore from the specific savepoint path\nSET 'execution.savepoint.path' = '/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab';\n\nINSERT INTO pageviews_enriched\nSELECT *\nFROM pageviews AS p\nLEFT JOIN users FOR SYSTEM_TIME AS OF p.proctime AS u\nON p.user_id = u.user_id;\n```\n\n","slug":"bigdata/flink/通过Flink-SQL，将Kafka中的Oracle-CDC-Log同步到Doris","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt5004y8j5m7065cmv3","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><ul>\n<li>Oracle的binlog日志已经由DBA通过OGG同步到Kafka中了，因此用不到Flink CDC</li>\n<li>同步到Kafka中的JSON样式<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">&quot;before&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;id&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">111</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;name&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;description&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Big 2-wheel scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;weight&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">5.18</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;after&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;id&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">111</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;name&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;description&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Big 2-wheel scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;weight&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">5.15</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;op_type&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;U&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;op_ts&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2020-05-13 15:40:06.000000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;current_ts&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2020-05-13 15:40:07.000000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;primary_keys&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">  <span class=\"string\">&quot;id&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;pos&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;00000000000000000000143&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;table&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;PRODUCTS&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"Flink-SQL\"><a href=\"#Flink-SQL\" class=\"headerlink\" title=\"Flink SQL\"></a>Flink SQL</h2><blockquote>\n<p>需要下载以下Jar包，放在{flink_home}&#x2F;lib&#x2F;下<br>flink-sql-connector-kafka_2.12-1.14.5.jar<br>flink-json-1.15.1.jar<br>flink-doris-connector-1.14_2.12-1.1.0.jar</p>\n</blockquote>\n<ul>\n<li><p>开启CheckPoint：<code>SET &#39;execution.checkpointing.interval&#39; = &#39;10min&#39;;</code></p>\n</li>\n<li><p>创建Kafka数据源表，设置<code>&#39;format&#39; = &#39;ogg-json&#39;</code>，只有<code>org.apache.flink.flink-json-1.15.1</code>中以上才支持ogg-json fromat</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> topic_products (</span><br><span class=\"line\">  id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  name STRING,</span><br><span class=\"line\">  description STRING,</span><br><span class=\"line\">  weight <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;kafka&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;topic&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;products_ogg_1&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.bootstrap.servers&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;172.30.160.5:9092&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.group.id&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;testGroup&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;ogg-json&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;scan.startup.mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;earliest-offset&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;ogg-json.ignore-parse-errors&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建Doris-Sink表</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> doris_sink (</span><br><span class=\"line\">id <span class=\"type\">INT</span>,</span><br><span class=\"line\">name STRING,</span><br><span class=\"line\">description STRING,</span><br><span class=\"line\">weight <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;doris&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;fenodes&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;172.30.160.5:8030&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;table.identifier&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;test.product&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.properties.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;json&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.properties.read_json_by_line&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.enable-delete&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.label-prefix&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;doris_label&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行<code>INSERT into doris_sink select * from topic_products;</code>语句，写入Doris</p>\n</li>\n</ul>\n<h2 id=\"Code-Repo\"><a href=\"#Code-Repo\" class=\"headerlink\" title=\"Code Repo\"></a>Code Repo</h2><blockquote>\n<ol>\n<li><strong>bin&#x2F;sql-client.sh embedded -i init_file -f file -s yarn-session</strong> </li>\n<li>Execute SQL Files</li>\n</ol>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Define available catalogs</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> CATALOG MyCatalog</span><br><span class=\"line\">  <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">    <span class=\"string\">&#x27;type&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;hive&#x27;</span></span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\">USE CATALOG MyCatalog;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define available database</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE MyDatabase;</span><br><span class=\"line\"></span><br><span class=\"line\">USE MyDatabase;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define TABLE</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> MyTable(</span><br><span class=\"line\">  MyField1 <span class=\"type\">INT</span>,</span><br><span class=\"line\">  MyField2 STRING</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;filesystem&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;path&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;/path/to/something&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;csv&#x27;</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define VIEW</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">VIEW</span> MyCustomView <span class=\"keyword\">AS</span> <span class=\"keyword\">SELECT</span> MyField2 <span class=\"keyword\">FROM</span> MyTable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define user-defined functions here.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">FUNCTION</span> foo.bar.AggregateUDF <span class=\"keyword\">AS</span> myUDF;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Properties that change the fundamental execution behavior of a table program.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;execution.runtime-mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;streaming&#x27;</span>; <span class=\"comment\">-- execution mode either &#x27;batch&#x27; or &#x27;streaming&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;sql-client.execution.result-mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;table&#x27;</span>; <span class=\"comment\">-- available values: &#x27;table&#x27;, &#x27;changelog&#x27; and &#x27;tableau&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;sql-client.execution.max-table-result.rows&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;10000&#x27;</span>; <span class=\"comment\">-- optional: maximum number of maintained rows</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;parallelism.default&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1&#x27;</span>; <span class=\"comment\">-- optional: Flink&#x27;s parallelism (1 by default)</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.auto-watermark-interval&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;200&#x27;</span>; <span class=\"comment\">--optional: interval for periodic watermarks</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.max-parallelism&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;10&#x27;</span>; <span class=\"comment\">-- optional: Flink&#x27;s maximum parallelism</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.state.ttl&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1000&#x27;</span>; <span class=\"comment\">-- optional: table program&#x27;s idle state time</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;restart-strategy&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;fixed-delay&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Configuration options for adjusting and tuning table programs.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.optimizer.join-reorder-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.spill-compression.enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.spill-compression.block-size&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;128kb&#x27;</span>;</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> TEMPORARY <span class=\"keyword\">TABLE</span> users (</span><br><span class=\"line\">  user_id <span class=\"type\">BIGINT</span>,</span><br><span class=\"line\">  user_name STRING,</span><br><span class=\"line\">  user_level STRING,</span><br><span class=\"line\">  region STRING,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (user_id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;upsert-kafka&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;topic&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;users&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.bootstrap.servers&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;...&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;key.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;csv&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;value.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;avro&#x27;</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set sync mode</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.dml-sync&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the job name</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;SqlJob&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the queue that the job submit to</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;yarn.application.queue&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the job parallelism</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;parallelism.default&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;100&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- restore from the specific savepoint path</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;execution.savepoint.path&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> pageviews_enriched</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> pageviews <span class=\"keyword\">AS</span> p</span><br><span class=\"line\"><span class=\"keyword\">LEFT</span> <span class=\"keyword\">JOIN</span> users <span class=\"keyword\">FOR</span> <span class=\"built_in\">SYSTEM_TIME</span> <span class=\"keyword\">AS</span> <span class=\"keyword\">OF</span> p.proctime <span class=\"keyword\">AS</span> u</span><br><span class=\"line\"><span class=\"keyword\">ON</span> p.user_id <span class=\"operator\">=</span> u.user_id;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><ul>\n<li>Oracle的binlog日志已经由DBA通过OGG同步到Kafka中了，因此用不到Flink CDC</li>\n<li>同步到Kafka中的JSON样式<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span></span><br><span class=\"line\"><span class=\"attr\">&quot;before&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;id&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">111</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;name&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;description&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Big 2-wheel scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;weight&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">5.18</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;after&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">&#123;</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;id&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">111</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;name&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;description&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;Big 2-wheel scooter&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">  <span class=\"attr\">&quot;weight&quot;</span><span class=\"punctuation\">:</span> <span class=\"number\">5.15</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;op_type&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;U&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;op_ts&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2020-05-13 15:40:06.000000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;current_ts&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;2020-05-13 15:40:07.000000&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;primary_keys&quot;</span><span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span></span><br><span class=\"line\">  <span class=\"string\">&quot;id&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;pos&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;00000000000000000000143&quot;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\"><span class=\"attr\">&quot;table&quot;</span><span class=\"punctuation\">:</span> <span class=\"string\">&quot;PRODUCTS&quot;</span></span><br><span class=\"line\"><span class=\"punctuation\">&#125;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"Flink-SQL\"><a href=\"#Flink-SQL\" class=\"headerlink\" title=\"Flink SQL\"></a>Flink SQL</h2><blockquote>\n<p>需要下载以下Jar包，放在{flink_home}&#x2F;lib&#x2F;下<br>flink-sql-connector-kafka_2.12-1.14.5.jar<br>flink-json-1.15.1.jar<br>flink-doris-connector-1.14_2.12-1.1.0.jar</p>\n</blockquote>\n<ul>\n<li><p>开启CheckPoint：<code>SET &#39;execution.checkpointing.interval&#39; = &#39;10min&#39;;</code></p>\n</li>\n<li><p>创建Kafka数据源表，设置<code>&#39;format&#39; = &#39;ogg-json&#39;</code>，只有<code>org.apache.flink.flink-json-1.15.1</code>中以上才支持ogg-json fromat</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> topic_products (</span><br><span class=\"line\">  id <span class=\"type\">INT</span>,</span><br><span class=\"line\">  name STRING,</span><br><span class=\"line\">  description STRING,</span><br><span class=\"line\">  weight <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;kafka&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;topic&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;products_ogg_1&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.bootstrap.servers&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;172.30.160.5:9092&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.group.id&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;testGroup&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;ogg-json&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;scan.startup.mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;earliest-offset&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;ogg-json.ignore-parse-errors&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建Doris-Sink表</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> doris_sink (</span><br><span class=\"line\">id <span class=\"type\">INT</span>,</span><br><span class=\"line\">name STRING,</span><br><span class=\"line\">description STRING,</span><br><span class=\"line\">weight <span class=\"type\">DECIMAL</span>(<span class=\"number\">10</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;doris&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;fenodes&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;172.30.160.5:8030&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;table.identifier&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;test.product&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;username&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;password&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.properties.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;json&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.properties.read_json_by_line&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.enable-delete&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;sink.label-prefix&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;doris_label&#x27;</span></span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行<code>INSERT into doris_sink select * from topic_products;</code>语句，写入Doris</p>\n</li>\n</ul>\n<h2 id=\"Code-Repo\"><a href=\"#Code-Repo\" class=\"headerlink\" title=\"Code Repo\"></a>Code Repo</h2><blockquote>\n<ol>\n<li><strong>bin&#x2F;sql-client.sh embedded -i init_file -f file -s yarn-session</strong> </li>\n<li>Execute SQL Files</li>\n</ol>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Define available catalogs</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> CATALOG MyCatalog</span><br><span class=\"line\">  <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">    <span class=\"string\">&#x27;type&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;hive&#x27;</span></span><br><span class=\"line\">  );</span><br><span class=\"line\"></span><br><span class=\"line\">USE CATALOG MyCatalog;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define available database</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE MyDatabase;</span><br><span class=\"line\"></span><br><span class=\"line\">USE MyDatabase;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define TABLE</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> MyTable(</span><br><span class=\"line\">  MyField1 <span class=\"type\">INT</span>,</span><br><span class=\"line\">  MyField2 STRING</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;filesystem&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;path&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;/path/to/something&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;csv&#x27;</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define VIEW</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">VIEW</span> MyCustomView <span class=\"keyword\">AS</span> <span class=\"keyword\">SELECT</span> MyField2 <span class=\"keyword\">FROM</span> MyTable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Define user-defined functions here.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">FUNCTION</span> foo.bar.AggregateUDF <span class=\"keyword\">AS</span> myUDF;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Properties that change the fundamental execution behavior of a table program.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;execution.runtime-mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;streaming&#x27;</span>; <span class=\"comment\">-- execution mode either &#x27;batch&#x27; or &#x27;streaming&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;sql-client.execution.result-mode&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;table&#x27;</span>; <span class=\"comment\">-- available values: &#x27;table&#x27;, &#x27;changelog&#x27; and &#x27;tableau&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;sql-client.execution.max-table-result.rows&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;10000&#x27;</span>; <span class=\"comment\">-- optional: maximum number of maintained rows</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;parallelism.default&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1&#x27;</span>; <span class=\"comment\">-- optional: Flink&#x27;s parallelism (1 by default)</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.auto-watermark-interval&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;200&#x27;</span>; <span class=\"comment\">--optional: interval for periodic watermarks</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.max-parallelism&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;10&#x27;</span>; <span class=\"comment\">-- optional: Flink&#x27;s maximum parallelism</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.state.ttl&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;1000&#x27;</span>; <span class=\"comment\">-- optional: table program&#x27;s idle state time</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;restart-strategy&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;fixed-delay&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Configuration options for adjusting and tuning table programs.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.optimizer.join-reorder-enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.spill-compression.enabled&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.exec.spill-compression.block-size&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;128kb&#x27;</span>;</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> TEMPORARY <span class=\"keyword\">TABLE</span> users (</span><br><span class=\"line\">  user_id <span class=\"type\">BIGINT</span>,</span><br><span class=\"line\">  user_name STRING,</span><br><span class=\"line\">  user_level STRING,</span><br><span class=\"line\">  region STRING,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (user_id) <span class=\"keyword\">NOT</span> ENFORCED</span><br><span class=\"line\">) <span class=\"keyword\">WITH</span> (</span><br><span class=\"line\">  <span class=\"string\">&#x27;connector&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;upsert-kafka&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;topic&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;users&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;properties.bootstrap.servers&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;...&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;key.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;csv&#x27;</span>,</span><br><span class=\"line\">  <span class=\"string\">&#x27;value.format&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;avro&#x27;</span></span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set sync mode</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;table.dml-sync&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;true&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the job name</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;pipeline.name&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;SqlJob&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the queue that the job submit to</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;yarn.application.queue&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;root&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- set the job parallelism</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;parallelism.default&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;100&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- restore from the specific savepoint path</span></span><br><span class=\"line\"><span class=\"keyword\">SET</span> <span class=\"string\">&#x27;execution.savepoint.path&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> pageviews_enriched</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> pageviews <span class=\"keyword\">AS</span> p</span><br><span class=\"line\"><span class=\"keyword\">LEFT</span> <span class=\"keyword\">JOIN</span> users <span class=\"keyword\">FOR</span> <span class=\"built_in\">SYSTEM_TIME</span> <span class=\"keyword\">AS</span> <span class=\"keyword\">OF</span> p.proctime <span class=\"keyword\">AS</span> u</span><br><span class=\"line\"><span class=\"keyword\">ON</span> p.user_id <span class=\"operator\">=</span> u.user_id;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Scala特殊语法指南","abbrlink":5351,"date":"2023-03-08T11:54:07.000Z","updated":"2022-03-08T11:54:07.000Z","top_img":null,"cover":"https://zfh-tuchuang.oss-cn-shanghai.aliyuncs.com/img/site-backgound.jpg","description":null,"keywords":null,"_content":"\n### 柯里化\n\n在Scala中，定义了两组空括号`()()`的方法称为“柯里化方法”。这意味着该方法采用多个参数列表，每个参数列表都由一组空括号表示。\n\n例如，考虑以下方法定义：\n\n```\ndef add(a: Int)(b: Int): Int = a + b\n```\n\n在这种情况下，`add`是一个柯里化方法，它以单独的参数列表接受两个整数参数`a`和`b`。要调用此方法，您首先需要提供`a`的值，然后是`b`。这看起来像：\n\n```\nval result = add(1)(2) // result是3\n```\n\n柯里化方法在函数式编程中非常有用，可以允许部分函数应用，函数组合和其他高阶编程技术。\n\n```scala\n  /**\n   * Execute a block of code that evaluates to Unit, stop SparkContext if there is any uncaught\n   * exception\n   *\n   * NOTE: This method is to be called by the driver-side components to avoid stopping the\n   * user-started JVM process completely; in contrast, tryOrExit is to be called in the\n   * spark-started JVM process .\n   */\n  def tryOrStopSparkContext(sc: SparkContext)(block: => Unit): Unit = {\n    try {\n      block\n    } catch {\n      case e: ControlThrowable => throw e\n      case t: Throwable =>\n        val currentThreadName = Thread.currentThread().getName\n        if (sc != null) {\n          logError(s\"uncaught error in thread $currentThreadName, stopping SparkContext\", t)\n          sc.stopInNewThread()\n        }\n        if (!NonFatal(t)) {\n          logError(s\"throw uncaught fatal error in thread $currentThreadName\", t)\n          throw t\n        }\n    }\n  }\n  \n  // 调用柯里化方法\n  private val dispatchThread = new Thread(s\"spark-listener-group-$name\") {\n    setDaemon(true)\n    override def run(): Unit = Utils.tryOrStopSparkContext(sc) {\n      dispatch()\n    }\n  }\n```\n\n\n\n### 伴生对象（companion object）\n\n在Scala中，每个类都可以有一个伴生对象（companion object）。伴生对象是一个单例对象，在同一源文件中定义，并且与类具有相同的名称。类和伴生对象之间可以互相访问对方的私有成员，从而允许我们将相关的方法和功能封装在一起。\n\n通常情况下，伴生对象主要被用来定义那些不依赖于实例化对象而可以直接使用的静态成员和方法，可以将它们定义在伴生对象中。例如：\n\n```scala\nclass MyClass(val name: String, val age: Int)\n\nobject MyClass {\n  def createDefault(): MyClass = new MyClass(\"John Doe\", 30)\n}\n```\n\n在这个例子中，我们定义了一个名为`MyClass`的类，并在同一源文件中定义了一个名为`MyClass`的伴生对象。在伴生对象中，我们定义了一个名为`createDefault`的静态方法，它返回一个新创建的`MyClass`对象，该对象具有默认的名称“John Doe”和年龄30。\n\n伴生对象的另一个用途是作为工厂方法的容器，使得我们可以将某些相关的工厂方法放在同一个伴生对象中以提高可读性和可维护性。伴生对象还可以帮助我们实现静态和动态方法的多态，从而在类使用时提供更大的灵活性。\n\n值得一提的是，在Scala中没有静态成员或静态方法，伴生对象的成员和方法只是静态的外观，Scala实际上是将它们作为伴生类的私有成员实现的。\n","source":"_posts/bigdata/scala/scala语法demo.md","raw":"---\ntitle: Scala特殊语法指南\ntags:\n  - scala\ncategories:\n  - - bigdata\n    - scala\nabbrlink: 5351\ndate: 2023-03-08 19:54:07\nupdated: 2022-03-08 19:54:07\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n### 柯里化\n\n在Scala中，定义了两组空括号`()()`的方法称为“柯里化方法”。这意味着该方法采用多个参数列表，每个参数列表都由一组空括号表示。\n\n例如，考虑以下方法定义：\n\n```\ndef add(a: Int)(b: Int): Int = a + b\n```\n\n在这种情况下，`add`是一个柯里化方法，它以单独的参数列表接受两个整数参数`a`和`b`。要调用此方法，您首先需要提供`a`的值，然后是`b`。这看起来像：\n\n```\nval result = add(1)(2) // result是3\n```\n\n柯里化方法在函数式编程中非常有用，可以允许部分函数应用，函数组合和其他高阶编程技术。\n\n```scala\n  /**\n   * Execute a block of code that evaluates to Unit, stop SparkContext if there is any uncaught\n   * exception\n   *\n   * NOTE: This method is to be called by the driver-side components to avoid stopping the\n   * user-started JVM process completely; in contrast, tryOrExit is to be called in the\n   * spark-started JVM process .\n   */\n  def tryOrStopSparkContext(sc: SparkContext)(block: => Unit): Unit = {\n    try {\n      block\n    } catch {\n      case e: ControlThrowable => throw e\n      case t: Throwable =>\n        val currentThreadName = Thread.currentThread().getName\n        if (sc != null) {\n          logError(s\"uncaught error in thread $currentThreadName, stopping SparkContext\", t)\n          sc.stopInNewThread()\n        }\n        if (!NonFatal(t)) {\n          logError(s\"throw uncaught fatal error in thread $currentThreadName\", t)\n          throw t\n        }\n    }\n  }\n  \n  // 调用柯里化方法\n  private val dispatchThread = new Thread(s\"spark-listener-group-$name\") {\n    setDaemon(true)\n    override def run(): Unit = Utils.tryOrStopSparkContext(sc) {\n      dispatch()\n    }\n  }\n```\n\n\n\n### 伴生对象（companion object）\n\n在Scala中，每个类都可以有一个伴生对象（companion object）。伴生对象是一个单例对象，在同一源文件中定义，并且与类具有相同的名称。类和伴生对象之间可以互相访问对方的私有成员，从而允许我们将相关的方法和功能封装在一起。\n\n通常情况下，伴生对象主要被用来定义那些不依赖于实例化对象而可以直接使用的静态成员和方法，可以将它们定义在伴生对象中。例如：\n\n```scala\nclass MyClass(val name: String, val age: Int)\n\nobject MyClass {\n  def createDefault(): MyClass = new MyClass(\"John Doe\", 30)\n}\n```\n\n在这个例子中，我们定义了一个名为`MyClass`的类，并在同一源文件中定义了一个名为`MyClass`的伴生对象。在伴生对象中，我们定义了一个名为`createDefault`的静态方法，它返回一个新创建的`MyClass`对象，该对象具有默认的名称“John Doe”和年龄30。\n\n伴生对象的另一个用途是作为工厂方法的容器，使得我们可以将某些相关的工厂方法放在同一个伴生对象中以提高可读性和可维护性。伴生对象还可以帮助我们实现静态和动态方法的多态，从而在类使用时提供更大的灵活性。\n\n值得一提的是，在Scala中没有静态成员或静态方法，伴生对象的成员和方法只是静态的外观，Scala实际上是将它们作为伴生类的私有成员实现的。\n","slug":"bigdata/scala/scala语法demo","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vkt600518j5mgxax6s91","content":"<h3 id=\"柯里化\"><a href=\"#柯里化\" class=\"headerlink\" title=\"柯里化\"></a>柯里化</h3><p>在Scala中，定义了两组空括号<code>()()</code>的方法称为“柯里化方法”。这意味着该方法采用多个参数列表，每个参数列表都由一组空括号表示。</p>\n<p>例如，考虑以下方法定义：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def add(a: Int)(b: Int): Int = a + b</span><br></pre></td></tr></table></figure>\n\n<p>在这种情况下，<code>add</code>是一个柯里化方法，它以单独的参数列表接受两个整数参数<code>a</code>和<code>b</code>。要调用此方法，您首先需要提供<code>a</code>的值，然后是<code>b</code>。这看起来像：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = add(1)(2) // result是3</span><br></pre></td></tr></table></figure>\n\n<p>柯里化方法在函数式编程中非常有用，可以允许部分函数应用，函数组合和其他高阶编程技术。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Execute a block of code that evaluates to Unit, stop SparkContext if there is any uncaught</span></span><br><span class=\"line\"><span class=\"comment\"> * exception</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">NOTE:</span> This method is to be called by the driver-side components to avoid stopping the</span></span><br><span class=\"line\"><span class=\"comment\"> * user-started JVM process completely; in contrast, tryOrExit is to be called in the</span></span><br><span class=\"line\"><span class=\"comment\"> * spark-started JVM process .</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tryOrStopSparkContext</span></span>(sc: <span class=\"type\">SparkContext</span>)(block: =&gt; <span class=\"type\">Unit</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    block</span><br><span class=\"line\">  &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> e: <span class=\"type\">ControlThrowable</span> =&gt; <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">    <span class=\"keyword\">case</span> t: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> currentThreadName = <span class=\"type\">Thread</span>.currentThread().getName</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (sc != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s&quot;uncaught error in thread <span class=\"subst\">$currentThreadName</span>, stopping SparkContext&quot;</span>, t)</span><br><span class=\"line\">        sc.stopInNewThread()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!<span class=\"type\">NonFatal</span>(t)) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s&quot;throw uncaught fatal error in thread <span class=\"subst\">$currentThreadName</span>&quot;</span>, t)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> t</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 调用柯里化方法</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> dispatchThread = <span class=\"keyword\">new</span> <span class=\"type\">Thread</span>(<span class=\"string\">s&quot;spark-listener-group-<span class=\"subst\">$name</span>&quot;</span>) &#123;</span><br><span class=\"line\">  setDaemon(<span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class=\"line\">    dispatch()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"伴生对象（companion-object）\"><a href=\"#伴生对象（companion-object）\" class=\"headerlink\" title=\"伴生对象（companion object）\"></a>伴生对象（companion object）</h3><p>在Scala中，每个类都可以有一个伴生对象（companion object）。伴生对象是一个单例对象，在同一源文件中定义，并且与类具有相同的名称。类和伴生对象之间可以互相访问对方的私有成员，从而允许我们将相关的方法和功能封装在一起。</p>\n<p>通常情况下，伴生对象主要被用来定义那些不依赖于实例化对象而可以直接使用的静态成员和方法，可以将它们定义在伴生对象中。例如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span>(<span class=\"params\">val name: <span class=\"type\">String</span>, val age: <span class=\"type\">Int</span></span>)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">createDefault</span></span>(): <span class=\"type\">MyClass</span> = <span class=\"keyword\">new</span> <span class=\"type\">MyClass</span>(<span class=\"string\">&quot;John Doe&quot;</span>, <span class=\"number\">30</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在这个例子中，我们定义了一个名为<code>MyClass</code>的类，并在同一源文件中定义了一个名为<code>MyClass</code>的伴生对象。在伴生对象中，我们定义了一个名为<code>createDefault</code>的静态方法，它返回一个新创建的<code>MyClass</code>对象，该对象具有默认的名称“John Doe”和年龄30。</p>\n<p>伴生对象的另一个用途是作为工厂方法的容器，使得我们可以将某些相关的工厂方法放在同一个伴生对象中以提高可读性和可维护性。伴生对象还可以帮助我们实现静态和动态方法的多态，从而在类使用时提供更大的灵活性。</p>\n<p>值得一提的是，在Scala中没有静态成员或静态方法，伴生对象的成员和方法只是静态的外观，Scala实际上是将它们作为伴生类的私有成员实现的。</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h3 id=\"柯里化\"><a href=\"#柯里化\" class=\"headerlink\" title=\"柯里化\"></a>柯里化</h3><p>在Scala中，定义了两组空括号<code>()()</code>的方法称为“柯里化方法”。这意味着该方法采用多个参数列表，每个参数列表都由一组空括号表示。</p>\n<p>例如，考虑以下方法定义：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def add(a: Int)(b: Int): Int = a + b</span><br></pre></td></tr></table></figure>\n\n<p>在这种情况下，<code>add</code>是一个柯里化方法，它以单独的参数列表接受两个整数参数<code>a</code>和<code>b</code>。要调用此方法，您首先需要提供<code>a</code>的值，然后是<code>b</code>。这看起来像：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = add(1)(2) // result是3</span><br></pre></td></tr></table></figure>\n\n<p>柯里化方法在函数式编程中非常有用，可以允许部分函数应用，函数组合和其他高阶编程技术。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Execute a block of code that evaluates to Unit, stop SparkContext if there is any uncaught</span></span><br><span class=\"line\"><span class=\"comment\"> * exception</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">NOTE:</span> This method is to be called by the driver-side components to avoid stopping the</span></span><br><span class=\"line\"><span class=\"comment\"> * user-started JVM process completely; in contrast, tryOrExit is to be called in the</span></span><br><span class=\"line\"><span class=\"comment\"> * spark-started JVM process .</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tryOrStopSparkContext</span></span>(sc: <span class=\"type\">SparkContext</span>)(block: =&gt; <span class=\"type\">Unit</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    block</span><br><span class=\"line\">  &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> e: <span class=\"type\">ControlThrowable</span> =&gt; <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">    <span class=\"keyword\">case</span> t: <span class=\"type\">Throwable</span> =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> currentThreadName = <span class=\"type\">Thread</span>.currentThread().getName</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (sc != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s&quot;uncaught error in thread <span class=\"subst\">$currentThreadName</span>, stopping SparkContext&quot;</span>, t)</span><br><span class=\"line\">        sc.stopInNewThread()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!<span class=\"type\">NonFatal</span>(t)) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s&quot;throw uncaught fatal error in thread <span class=\"subst\">$currentThreadName</span>&quot;</span>, t)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> t</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 调用柯里化方法</span></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> dispatchThread = <span class=\"keyword\">new</span> <span class=\"type\">Thread</span>(<span class=\"string\">s&quot;spark-listener-group-<span class=\"subst\">$name</span>&quot;</span>) &#123;</span><br><span class=\"line\">  setDaemon(<span class=\"literal\">true</span>)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = <span class=\"type\">Utils</span>.tryOrStopSparkContext(sc) &#123;</span><br><span class=\"line\">    dispatch()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"伴生对象（companion-object）\"><a href=\"#伴生对象（companion-object）\" class=\"headerlink\" title=\"伴生对象（companion object）\"></a>伴生对象（companion object）</h3><p>在Scala中，每个类都可以有一个伴生对象（companion object）。伴生对象是一个单例对象，在同一源文件中定义，并且与类具有相同的名称。类和伴生对象之间可以互相访问对方的私有成员，从而允许我们将相关的方法和功能封装在一起。</p>\n<p>通常情况下，伴生对象主要被用来定义那些不依赖于实例化对象而可以直接使用的静态成员和方法，可以将它们定义在伴生对象中。例如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span>(<span class=\"params\">val name: <span class=\"type\">String</span>, val age: <span class=\"type\">Int</span></span>)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">createDefault</span></span>(): <span class=\"type\">MyClass</span> = <span class=\"keyword\">new</span> <span class=\"type\">MyClass</span>(<span class=\"string\">&quot;John Doe&quot;</span>, <span class=\"number\">30</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在这个例子中，我们定义了一个名为<code>MyClass</code>的类，并在同一源文件中定义了一个名为<code>MyClass</code>的伴生对象。在伴生对象中，我们定义了一个名为<code>createDefault</code>的静态方法，它返回一个新创建的<code>MyClass</code>对象，该对象具有默认的名称“John Doe”和年龄30。</p>\n<p>伴生对象的另一个用途是作为工厂方法的容器，使得我们可以将某些相关的工厂方法放在同一个伴生对象中以提高可读性和可维护性。伴生对象还可以帮助我们实现静态和动态方法的多态，从而在类使用时提供更大的灵活性。</p>\n<p>值得一提的是，在Scala中没有静态成员或静态方法，伴生对象的成员和方法只是静态的外观，Scala实际上是将它们作为伴生类的私有成员实现的。</p>\n"},{"title":"Apache Superset添加EXCLUDE函数","abbrlink":50109,"date":"2023-01-11T03:45:16.000Z","updated":"2023-01-11T03:45:16.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 前言\n\n> 因为业务需求，需要一个类似于Tableau中的exclude函数的功能。\n>\n> 例如：\n>\n> 若要查看一段时间内每个国家/地区的平均血压，但不按男性和女性进行划分，请使用 EXCLUDE 详细级别表达式 `{EXCLUDE [Sex] : AVG[Average blood pressure]}`。\n>\n> 这个函数的功能对于业务来说非常重要，但是Apache Superset中没有此类功能，因此需要修改Apache Superset源码，为其添加上这个功能！这是一个不小的挑战！\n\n\n\n## 新功能实现计划\n\n- 1、获取由用户拖拽生成的BI图表的SQL语句。\n- 2、拦截SQL语句进行改写，例如`{EXCLUDE [Sex] : AVG[blood-pressure]}`:\n  - 1、去除原SQL语句中的Sex维度信息和针对Sex的过滤条件，得到新的SQL\n  - 2、基于新的SQL发出对于blood-pressure度量来说是正确的聚合SQL，得到正确结果集\n  - 3、获取正确结果集中排除Sex维度后正确的blood-pressure度量聚合值。\n  - 4、利用正确的结果集中正确的部分，对原SQL语句得到的结果集进行更新\n\n- 3、将改写后的结果集发送回前端。\n\n# Apache Superset源码定位\n\n- Apache Superset后端以python的web框架Flask开发，代码结构清晰，代码质量优秀。\n- 添加新功能的代码集中在源码中：superset/charts/data/api.py中。主要修改的方法为`def data(self) -> Response:`\n\n## show code\n\n```python\n    @expose(\"/data\", methods=[\"POST\"])\n    @protect()\n    @statsd_metrics\n    @event_logger.log_this_with_context(\n        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.data\",\n        log_to_statsd=False,\n    )\n    def data(self) -> Response:\n        \"\"\"\n        Takes a query context constructed in the client and returns payload\n        data response for the given query.\n        ---\n        post:\n          description: >-\n            Takes a query context constructed in the client and returns payload data\n            response for the given query.\n          requestBody:\n            description: >-\n              A query context consists of a datasource from which to fetch data\n              and one or many query objects.\n            required: true\n            content:\n              application/json:\n                schema:\n                  $ref: \"#/components/schemas/ChartDataQueryContextSchema\"\n          responses:\n            200:\n              description: Query result\n              content:\n                application/json:\n                  schema:\n                    $ref: \"#/components/schemas/ChartDataResponseSchema\"\n            202:\n              description: Async job details\n              content:\n                application/json:\n                  schema:\n                    $ref: \"#/components/schemas/ChartDataAsyncResponseSchema\"\n            400:\n              $ref: '#/components/responses/400'\n            401:\n              $ref: '#/components/responses/401'\n            500:\n              $ref: '#/components/responses/500'\n        \"\"\"\n        json_body = None\n        if request.is_json:\n            json_body = request.json\n        elif request.form.get(\"form_data\"):\n            # CSV export submits regular form data\n            try:\n                json_body = json.loads(request.form[\"form_data\"])\n            except (TypeError, json.JSONDecodeError):\n                pass\n\n        if json_body is None:\n            return self.response_400(message=_(\"Request is not JSON\"))\n\n        logger.info(\"===231===\")\n        # logger.error(\"json_body......\")\n        # logger.error(json_body)\n\n        try:\n            query_context = self._create_query_context_from_form(json_body)\n            command = ChartDataCommand(query_context)\n            command.validate()\n        except DatasourceNotFound as error:\n            return self.response_404()\n        except QueryObjectValidationError as error:\n            return self.response_400(message=error.message)\n        except ValidationError as error:\n            return self.response_400(\n                message=_(\n                    \"Request is incorrect: %(error)s\", error=error.normalized_messages()\n                )\n            )\n\n        # TODO: support CSV, SQL query and other non-JSON types\n        if (\n            is_feature_enabled(\"GLOBAL_ASYNC_QUERIES\")\n            and query_context.result_format == ChartDataResultFormat.JSON\n            and query_context.result_type == ChartDataResultType.FULL\n        ):\n            return self._run_async(json_body, command)\n\n        form_data = json_body.get(\"form_data\")\n        logger.warning(\"查询成功。。。form_data\")\n        logger.warning(form_data)\n\n        try:\n            result = command.run(force_cached=False)\n            logger.info(\"result ... a\")\n            logger.info(result)\n        except ChartDataCacheLoadError as exc:\n            return self.response_422(message=exc.message)\n        except ChartDataQueryFailedError as exc:\n            return self.response_400(message=exc.message)\n\n\n        ############################# execte without symp query\n\n        json_body_without_symp = None\n\n        has_symp = False\n        # [{'col': 'shop_id', 'op': '==', 'val': '121'}]\n        filters = json_body[\"queries\"][0][\"filters\"]\n        # logger.info(filters)\n        for filter in filters:\n            if filter[\"col\"] == \"shop_id\":\n                # 去除json_body中的过滤条件，构建json_body_2,进行查询，获取正确的input聚合数据\n                filters.remove(filter)\n                has_symp = True\n\n        if has_symp :\n            json_body[\"queries\"][0][\"filters\"] = filters\n            json_body_without_symp = json_body\n\n        # logger.info(\"json_body_without_symp......\")\n        # logger.info(json_body_without_symp)\n\n        try:\n            query_context_without_symp = self._create_query_context_from_form(json_body_without_symp)\n            command_without_symp = ChartDataCommand(query_context_without_symp)\n            command_without_symp.validate()\n        except DatasourceNotFound as error:\n            return self.response_404()\n        except QueryObjectValidationError as error:\n            return self.response_400(message=error.message)\n        except ValidationError as error:\n            return self.response_400(\n                message=_(\n                    \"Request is incorrect: %(error)s\", error=error.normalized_messages()\n                )\n            )\n\n        # TODO: support CSV, SQL query and other non-JSON types\n        if (\n            is_feature_enabled(\"GLOBAL_ASYNC_QUERIES\")\n            and query_context_without_symp.result_format == ChartDataResultFormat.JSON\n            and query_context_without_symp.result_type == ChartDataResultType.FULL\n        ):\n            return self._run_async(json_body_without_symp, command_without_symp)\n\n        #############################\n\n        form_data_without_symp = json_body_without_symp.get(\"form_data\")\n\n        logger.info(\"查询成功。。。form_data_without_symp\")\n        logger.info(form_data_without_symp)\n\n\n        try:\n            result_without_symp = command_without_symp.run(force_cached=False)\n            logger.info(\"result ... b\")\n            logger.info(result_without_symp)\n        except ChartDataCacheLoadError as exc:\n            return self.response_422(message=exc.message)\n        except ChartDataQueryFailedError as exc:\n            return self.response_400(message=exc.message)\n\n        ## 重构result结果，返回前端\n\n        return self._get_data_response(\n            command, form_data=form_data, datasource=query_context.datasource\n        )\n```\n\n","source":"_posts/bigdata/superset/Apache Superset添加exclude函数.md","raw":"---\ntitle: Apache Superset添加EXCLUDE函数\ntags:\n  - Apache Superset\ncategories:\n  - - Superset\nabbrlink: 50109\ndate: 2023-01-11 11:45:16\nupdated: 2023-01-11 11:45:16\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n> 因为业务需求，需要一个类似于Tableau中的exclude函数的功能。\n>\n> 例如：\n>\n> 若要查看一段时间内每个国家/地区的平均血压，但不按男性和女性进行划分，请使用 EXCLUDE 详细级别表达式 `{EXCLUDE [Sex] : AVG[Average blood pressure]}`。\n>\n> 这个函数的功能对于业务来说非常重要，但是Apache Superset中没有此类功能，因此需要修改Apache Superset源码，为其添加上这个功能！这是一个不小的挑战！\n\n\n\n## 新功能实现计划\n\n- 1、获取由用户拖拽生成的BI图表的SQL语句。\n- 2、拦截SQL语句进行改写，例如`{EXCLUDE [Sex] : AVG[blood-pressure]}`:\n  - 1、去除原SQL语句中的Sex维度信息和针对Sex的过滤条件，得到新的SQL\n  - 2、基于新的SQL发出对于blood-pressure度量来说是正确的聚合SQL，得到正确结果集\n  - 3、获取正确结果集中排除Sex维度后正确的blood-pressure度量聚合值。\n  - 4、利用正确的结果集中正确的部分，对原SQL语句得到的结果集进行更新\n\n- 3、将改写后的结果集发送回前端。\n\n# Apache Superset源码定位\n\n- Apache Superset后端以python的web框架Flask开发，代码结构清晰，代码质量优秀。\n- 添加新功能的代码集中在源码中：superset/charts/data/api.py中。主要修改的方法为`def data(self) -> Response:`\n\n## show code\n\n```python\n    @expose(\"/data\", methods=[\"POST\"])\n    @protect()\n    @statsd_metrics\n    @event_logger.log_this_with_context(\n        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.data\",\n        log_to_statsd=False,\n    )\n    def data(self) -> Response:\n        \"\"\"\n        Takes a query context constructed in the client and returns payload\n        data response for the given query.\n        ---\n        post:\n          description: >-\n            Takes a query context constructed in the client and returns payload data\n            response for the given query.\n          requestBody:\n            description: >-\n              A query context consists of a datasource from which to fetch data\n              and one or many query objects.\n            required: true\n            content:\n              application/json:\n                schema:\n                  $ref: \"#/components/schemas/ChartDataQueryContextSchema\"\n          responses:\n            200:\n              description: Query result\n              content:\n                application/json:\n                  schema:\n                    $ref: \"#/components/schemas/ChartDataResponseSchema\"\n            202:\n              description: Async job details\n              content:\n                application/json:\n                  schema:\n                    $ref: \"#/components/schemas/ChartDataAsyncResponseSchema\"\n            400:\n              $ref: '#/components/responses/400'\n            401:\n              $ref: '#/components/responses/401'\n            500:\n              $ref: '#/components/responses/500'\n        \"\"\"\n        json_body = None\n        if request.is_json:\n            json_body = request.json\n        elif request.form.get(\"form_data\"):\n            # CSV export submits regular form data\n            try:\n                json_body = json.loads(request.form[\"form_data\"])\n            except (TypeError, json.JSONDecodeError):\n                pass\n\n        if json_body is None:\n            return self.response_400(message=_(\"Request is not JSON\"))\n\n        logger.info(\"===231===\")\n        # logger.error(\"json_body......\")\n        # logger.error(json_body)\n\n        try:\n            query_context = self._create_query_context_from_form(json_body)\n            command = ChartDataCommand(query_context)\n            command.validate()\n        except DatasourceNotFound as error:\n            return self.response_404()\n        except QueryObjectValidationError as error:\n            return self.response_400(message=error.message)\n        except ValidationError as error:\n            return self.response_400(\n                message=_(\n                    \"Request is incorrect: %(error)s\", error=error.normalized_messages()\n                )\n            )\n\n        # TODO: support CSV, SQL query and other non-JSON types\n        if (\n            is_feature_enabled(\"GLOBAL_ASYNC_QUERIES\")\n            and query_context.result_format == ChartDataResultFormat.JSON\n            and query_context.result_type == ChartDataResultType.FULL\n        ):\n            return self._run_async(json_body, command)\n\n        form_data = json_body.get(\"form_data\")\n        logger.warning(\"查询成功。。。form_data\")\n        logger.warning(form_data)\n\n        try:\n            result = command.run(force_cached=False)\n            logger.info(\"result ... a\")\n            logger.info(result)\n        except ChartDataCacheLoadError as exc:\n            return self.response_422(message=exc.message)\n        except ChartDataQueryFailedError as exc:\n            return self.response_400(message=exc.message)\n\n\n        ############################# execte without symp query\n\n        json_body_without_symp = None\n\n        has_symp = False\n        # [{'col': 'shop_id', 'op': '==', 'val': '121'}]\n        filters = json_body[\"queries\"][0][\"filters\"]\n        # logger.info(filters)\n        for filter in filters:\n            if filter[\"col\"] == \"shop_id\":\n                # 去除json_body中的过滤条件，构建json_body_2,进行查询，获取正确的input聚合数据\n                filters.remove(filter)\n                has_symp = True\n\n        if has_symp :\n            json_body[\"queries\"][0][\"filters\"] = filters\n            json_body_without_symp = json_body\n\n        # logger.info(\"json_body_without_symp......\")\n        # logger.info(json_body_without_symp)\n\n        try:\n            query_context_without_symp = self._create_query_context_from_form(json_body_without_symp)\n            command_without_symp = ChartDataCommand(query_context_without_symp)\n            command_without_symp.validate()\n        except DatasourceNotFound as error:\n            return self.response_404()\n        except QueryObjectValidationError as error:\n            return self.response_400(message=error.message)\n        except ValidationError as error:\n            return self.response_400(\n                message=_(\n                    \"Request is incorrect: %(error)s\", error=error.normalized_messages()\n                )\n            )\n\n        # TODO: support CSV, SQL query and other non-JSON types\n        if (\n            is_feature_enabled(\"GLOBAL_ASYNC_QUERIES\")\n            and query_context_without_symp.result_format == ChartDataResultFormat.JSON\n            and query_context_without_symp.result_type == ChartDataResultType.FULL\n        ):\n            return self._run_async(json_body_without_symp, command_without_symp)\n\n        #############################\n\n        form_data_without_symp = json_body_without_symp.get(\"form_data\")\n\n        logger.info(\"查询成功。。。form_data_without_symp\")\n        logger.info(form_data_without_symp)\n\n\n        try:\n            result_without_symp = command_without_symp.run(force_cached=False)\n            logger.info(\"result ... b\")\n            logger.info(result_without_symp)\n        except ChartDataCacheLoadError as exc:\n            return self.response_422(message=exc.message)\n        except ChartDataQueryFailedError as exc:\n            return self.response_400(message=exc.message)\n\n        ## 重构result结果，返回前端\n\n        return self._get_data_response(\n            command, form_data=form_data, datasource=query_context.datasource\n        )\n```\n\n","slug":"bigdata/superset/Apache Superset添加exclude函数","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vktb009u8j5m2bj6c98t","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>因为业务需求，需要一个类似于Tableau中的exclude函数的功能。</p>\n<p>例如：</p>\n<p>若要查看一段时间内每个国家&#x2F;地区的平均血压，但不按男性和女性进行划分，请使用 EXCLUDE 详细级别表达式 <code>&#123;EXCLUDE [Sex] : AVG[Average blood pressure]&#125;</code>。</p>\n<p>这个函数的功能对于业务来说非常重要，但是Apache Superset中没有此类功能，因此需要修改Apache Superset源码，为其添加上这个功能！这是一个不小的挑战！</p>\n</blockquote>\n<h2 id=\"新功能实现计划\"><a href=\"#新功能实现计划\" class=\"headerlink\" title=\"新功能实现计划\"></a>新功能实现计划</h2><ul>\n<li><p>1、获取由用户拖拽生成的BI图表的SQL语句。</p>\n</li>\n<li><p>2、拦截SQL语句进行改写，例如<code>&#123;EXCLUDE [Sex] : AVG[blood-pressure]&#125;</code>:</p>\n<ul>\n<li>1、去除原SQL语句中的Sex维度信息和针对Sex的过滤条件，得到新的SQL</li>\n<li>2、基于新的SQL发出对于blood-pressure度量来说是正确的聚合SQL，得到正确结果集</li>\n<li>3、获取正确结果集中排除Sex维度后正确的blood-pressure度量聚合值。</li>\n<li>4、利用正确的结果集中正确的部分，对原SQL语句得到的结果集进行更新</li>\n</ul>\n</li>\n<li><p>3、将改写后的结果集发送回前端。</p>\n</li>\n</ul>\n<h1 id=\"Apache-Superset源码定位\"><a href=\"#Apache-Superset源码定位\" class=\"headerlink\" title=\"Apache Superset源码定位\"></a>Apache Superset源码定位</h1><ul>\n<li>Apache Superset后端以python的web框架Flask开发，代码结构清晰，代码质量优秀。</li>\n<li>添加新功能的代码集中在源码中：superset&#x2F;charts&#x2F;data&#x2F;api.py中。主要修改的方法为<code>def data(self) -&gt; Response:</code></li>\n</ul>\n<h2 id=\"show-code\"><a href=\"#show-code\" class=\"headerlink\" title=\"show code\"></a>show code</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@expose(<span class=\"params\"><span class=\"string\">&quot;/data&quot;</span>, methods=[<span class=\"string\">&quot;POST&quot;</span>]</span>)</span></span><br><span class=\"line\"><span class=\"meta\">@protect()</span></span><br><span class=\"line\"><span class=\"meta\">@statsd_metrics</span></span><br><span class=\"line\"><span class=\"meta\">@event_logger.log_this_with_context(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">    action=<span class=\"keyword\">lambda</span> self, *args, **kwargs: <span class=\"string\">f&quot;<span class=\"subst\">&#123;self.__class__.__name__&#125;</span>.data&quot;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">    log_to_statsd=<span class=\"literal\">False</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\"></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data</span>(<span class=\"params\">self</span>) -&gt; Response:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Takes a query context constructed in the client and returns payload</span></span><br><span class=\"line\"><span class=\"string\">    data response for the given query.</span></span><br><span class=\"line\"><span class=\"string\">    ---</span></span><br><span class=\"line\"><span class=\"string\">    post:</span></span><br><span class=\"line\"><span class=\"string\">      description: &gt;-</span></span><br><span class=\"line\"><span class=\"string\">        Takes a query context constructed in the client and returns payload data</span></span><br><span class=\"line\"><span class=\"string\">        response for the given query.</span></span><br><span class=\"line\"><span class=\"string\">      requestBody:</span></span><br><span class=\"line\"><span class=\"string\">        description: &gt;-</span></span><br><span class=\"line\"><span class=\"string\">          A query context consists of a datasource from which to fetch data</span></span><br><span class=\"line\"><span class=\"string\">          and one or many query objects.</span></span><br><span class=\"line\"><span class=\"string\">        required: true</span></span><br><span class=\"line\"><span class=\"string\">        content:</span></span><br><span class=\"line\"><span class=\"string\">          application/json:</span></span><br><span class=\"line\"><span class=\"string\">            schema:</span></span><br><span class=\"line\"><span class=\"string\">              $ref: &quot;#/components/schemas/ChartDataQueryContextSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">      responses:</span></span><br><span class=\"line\"><span class=\"string\">        200:</span></span><br><span class=\"line\"><span class=\"string\">          description: Query result</span></span><br><span class=\"line\"><span class=\"string\">          content:</span></span><br><span class=\"line\"><span class=\"string\">            application/json:</span></span><br><span class=\"line\"><span class=\"string\">              schema:</span></span><br><span class=\"line\"><span class=\"string\">                $ref: &quot;#/components/schemas/ChartDataResponseSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">        202:</span></span><br><span class=\"line\"><span class=\"string\">          description: Async job details</span></span><br><span class=\"line\"><span class=\"string\">          content:</span></span><br><span class=\"line\"><span class=\"string\">            application/json:</span></span><br><span class=\"line\"><span class=\"string\">              schema:</span></span><br><span class=\"line\"><span class=\"string\">                $ref: &quot;#/components/schemas/ChartDataAsyncResponseSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">        400:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/400&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        401:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/401&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        500:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/500&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    json_body = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> request.is_json:</span><br><span class=\"line\">        json_body = request.json</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> request.form.get(<span class=\"string\">&quot;form_data&quot;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># CSV export submits regular form data</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            json_body = json.loads(request.form[<span class=\"string\">&quot;form_data&quot;</span>])</span><br><span class=\"line\">        <span class=\"keyword\">except</span> (TypeError, json.JSONDecodeError):</span><br><span class=\"line\">            <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> json_body <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=_(<span class=\"string\">&quot;Request is not JSON&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;===231===&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># logger.error(&quot;json_body......&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># logger.error(json_body)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        query_context = self._create_query_context_from_form(json_body)</span><br><span class=\"line\">        command = ChartDataCommand(query_context)</span><br><span class=\"line\">        command.validate()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> DatasourceNotFound <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_404()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> QueryObjectValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=error.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(</span><br><span class=\"line\">            message=_(</span><br><span class=\"line\">                <span class=\"string\">&quot;Request is incorrect: %(error)s&quot;</span>, error=error.normalized_messages()</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> support CSV, SQL query and other non-JSON types</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (</span><br><span class=\"line\">        is_feature_enabled(<span class=\"string\">&quot;GLOBAL_ASYNC_QUERIES&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context.result_format == ChartDataResultFormat.JSON</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context.result_type == ChartDataResultType.FULL</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._run_async(json_body, command)</span><br><span class=\"line\"></span><br><span class=\"line\">    form_data = json_body.get(<span class=\"string\">&quot;form_data&quot;</span>)</span><br><span class=\"line\">    logger.warning(<span class=\"string\">&quot;查询成功。。。form_data&quot;</span>)</span><br><span class=\"line\">    logger.warning(form_data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result = command.run(force_cached=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;result ... a&quot;</span>)</span><br><span class=\"line\">        logger.info(result)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataCacheLoadError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_422(message=exc.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataQueryFailedError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=exc.message)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">############################# execte without symp query</span></span><br><span class=\"line\"></span><br><span class=\"line\">    json_body_without_symp = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    has_symp = <span class=\"literal\">False</span></span><br><span class=\"line\">    <span class=\"comment\"># [&#123;&#x27;col&#x27;: &#x27;shop_id&#x27;, &#x27;op&#x27;: &#x27;==&#x27;, &#x27;val&#x27;: &#x27;121&#x27;&#125;]</span></span><br><span class=\"line\">    filters = json_body[<span class=\"string\">&quot;queries&quot;</span>][<span class=\"number\">0</span>][<span class=\"string\">&quot;filters&quot;</span>]</span><br><span class=\"line\">    <span class=\"comment\"># logger.info(filters)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">filter</span> <span class=\"keyword\">in</span> filters:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">filter</span>[<span class=\"string\">&quot;col&quot;</span>] == <span class=\"string\">&quot;shop_id&quot;</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 去除json_body中的过滤条件，构建json_body_2,进行查询，获取正确的input聚合数据</span></span><br><span class=\"line\">            filters.remove(<span class=\"built_in\">filter</span>)</span><br><span class=\"line\">            has_symp = <span class=\"literal\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_symp :</span><br><span class=\"line\">        json_body[<span class=\"string\">&quot;queries&quot;</span>][<span class=\"number\">0</span>][<span class=\"string\">&quot;filters&quot;</span>] = filters</span><br><span class=\"line\">        json_body_without_symp = json_body</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># logger.info(&quot;json_body_without_symp......&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># logger.info(json_body_without_symp)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        query_context_without_symp = self._create_query_context_from_form(json_body_without_symp)</span><br><span class=\"line\">        command_without_symp = ChartDataCommand(query_context_without_symp)</span><br><span class=\"line\">        command_without_symp.validate()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> DatasourceNotFound <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_404()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> QueryObjectValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=error.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(</span><br><span class=\"line\">            message=_(</span><br><span class=\"line\">                <span class=\"string\">&quot;Request is incorrect: %(error)s&quot;</span>, error=error.normalized_messages()</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> support CSV, SQL query and other non-JSON types</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (</span><br><span class=\"line\">        is_feature_enabled(<span class=\"string\">&quot;GLOBAL_ASYNC_QUERIES&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context_without_symp.result_format == ChartDataResultFormat.JSON</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context_without_symp.result_type == ChartDataResultType.FULL</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._run_async(json_body_without_symp, command_without_symp)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#############################</span></span><br><span class=\"line\"></span><br><span class=\"line\">    form_data_without_symp = json_body_without_symp.get(<span class=\"string\">&quot;form_data&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;查询成功。。。form_data_without_symp&quot;</span>)</span><br><span class=\"line\">    logger.info(form_data_without_symp)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result_without_symp = command_without_symp.run(force_cached=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;result ... b&quot;</span>)</span><br><span class=\"line\">        logger.info(result_without_symp)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataCacheLoadError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_422(message=exc.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataQueryFailedError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=exc.message)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">## 重构result结果，返回前端</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self._get_data_response(</span><br><span class=\"line\">        command, form_data=form_data, datasource=query_context.datasource</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>因为业务需求，需要一个类似于Tableau中的exclude函数的功能。</p>\n<p>例如：</p>\n<p>若要查看一段时间内每个国家&#x2F;地区的平均血压，但不按男性和女性进行划分，请使用 EXCLUDE 详细级别表达式 <code>&#123;EXCLUDE [Sex] : AVG[Average blood pressure]&#125;</code>。</p>\n<p>这个函数的功能对于业务来说非常重要，但是Apache Superset中没有此类功能，因此需要修改Apache Superset源码，为其添加上这个功能！这是一个不小的挑战！</p>\n</blockquote>\n<h2 id=\"新功能实现计划\"><a href=\"#新功能实现计划\" class=\"headerlink\" title=\"新功能实现计划\"></a>新功能实现计划</h2><ul>\n<li><p>1、获取由用户拖拽生成的BI图表的SQL语句。</p>\n</li>\n<li><p>2、拦截SQL语句进行改写，例如<code>&#123;EXCLUDE [Sex] : AVG[blood-pressure]&#125;</code>:</p>\n<ul>\n<li>1、去除原SQL语句中的Sex维度信息和针对Sex的过滤条件，得到新的SQL</li>\n<li>2、基于新的SQL发出对于blood-pressure度量来说是正确的聚合SQL，得到正确结果集</li>\n<li>3、获取正确结果集中排除Sex维度后正确的blood-pressure度量聚合值。</li>\n<li>4、利用正确的结果集中正确的部分，对原SQL语句得到的结果集进行更新</li>\n</ul>\n</li>\n<li><p>3、将改写后的结果集发送回前端。</p>\n</li>\n</ul>\n<h1 id=\"Apache-Superset源码定位\"><a href=\"#Apache-Superset源码定位\" class=\"headerlink\" title=\"Apache Superset源码定位\"></a>Apache Superset源码定位</h1><ul>\n<li>Apache Superset后端以python的web框架Flask开发，代码结构清晰，代码质量优秀。</li>\n<li>添加新功能的代码集中在源码中：superset&#x2F;charts&#x2F;data&#x2F;api.py中。主要修改的方法为<code>def data(self) -&gt; Response:</code></li>\n</ul>\n<h2 id=\"show-code\"><a href=\"#show-code\" class=\"headerlink\" title=\"show code\"></a>show code</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@expose(<span class=\"params\"><span class=\"string\">&quot;/data&quot;</span>, methods=[<span class=\"string\">&quot;POST&quot;</span>]</span>)</span></span><br><span class=\"line\"><span class=\"meta\">@protect()</span></span><br><span class=\"line\"><span class=\"meta\">@statsd_metrics</span></span><br><span class=\"line\"><span class=\"meta\">@event_logger.log_this_with_context(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">    action=<span class=\"keyword\">lambda</span> self, *args, **kwargs: <span class=\"string\">f&quot;<span class=\"subst\">&#123;self.__class__.__name__&#125;</span>.data&quot;</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\">    log_to_statsd=<span class=\"literal\">False</span>,</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"meta\"></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data</span>(<span class=\"params\">self</span>) -&gt; Response:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Takes a query context constructed in the client and returns payload</span></span><br><span class=\"line\"><span class=\"string\">    data response for the given query.</span></span><br><span class=\"line\"><span class=\"string\">    ---</span></span><br><span class=\"line\"><span class=\"string\">    post:</span></span><br><span class=\"line\"><span class=\"string\">      description: &gt;-</span></span><br><span class=\"line\"><span class=\"string\">        Takes a query context constructed in the client and returns payload data</span></span><br><span class=\"line\"><span class=\"string\">        response for the given query.</span></span><br><span class=\"line\"><span class=\"string\">      requestBody:</span></span><br><span class=\"line\"><span class=\"string\">        description: &gt;-</span></span><br><span class=\"line\"><span class=\"string\">          A query context consists of a datasource from which to fetch data</span></span><br><span class=\"line\"><span class=\"string\">          and one or many query objects.</span></span><br><span class=\"line\"><span class=\"string\">        required: true</span></span><br><span class=\"line\"><span class=\"string\">        content:</span></span><br><span class=\"line\"><span class=\"string\">          application/json:</span></span><br><span class=\"line\"><span class=\"string\">            schema:</span></span><br><span class=\"line\"><span class=\"string\">              $ref: &quot;#/components/schemas/ChartDataQueryContextSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">      responses:</span></span><br><span class=\"line\"><span class=\"string\">        200:</span></span><br><span class=\"line\"><span class=\"string\">          description: Query result</span></span><br><span class=\"line\"><span class=\"string\">          content:</span></span><br><span class=\"line\"><span class=\"string\">            application/json:</span></span><br><span class=\"line\"><span class=\"string\">              schema:</span></span><br><span class=\"line\"><span class=\"string\">                $ref: &quot;#/components/schemas/ChartDataResponseSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">        202:</span></span><br><span class=\"line\"><span class=\"string\">          description: Async job details</span></span><br><span class=\"line\"><span class=\"string\">          content:</span></span><br><span class=\"line\"><span class=\"string\">            application/json:</span></span><br><span class=\"line\"><span class=\"string\">              schema:</span></span><br><span class=\"line\"><span class=\"string\">                $ref: &quot;#/components/schemas/ChartDataAsyncResponseSchema&quot;</span></span><br><span class=\"line\"><span class=\"string\">        400:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/400&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        401:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/401&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        500:</span></span><br><span class=\"line\"><span class=\"string\">          $ref: &#x27;#/components/responses/500&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    json_body = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> request.is_json:</span><br><span class=\"line\">        json_body = request.json</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> request.form.get(<span class=\"string\">&quot;form_data&quot;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># CSV export submits regular form data</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            json_body = json.loads(request.form[<span class=\"string\">&quot;form_data&quot;</span>])</span><br><span class=\"line\">        <span class=\"keyword\">except</span> (TypeError, json.JSONDecodeError):</span><br><span class=\"line\">            <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> json_body <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=_(<span class=\"string\">&quot;Request is not JSON&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;===231===&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># logger.error(&quot;json_body......&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># logger.error(json_body)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        query_context = self._create_query_context_from_form(json_body)</span><br><span class=\"line\">        command = ChartDataCommand(query_context)</span><br><span class=\"line\">        command.validate()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> DatasourceNotFound <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_404()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> QueryObjectValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=error.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(</span><br><span class=\"line\">            message=_(</span><br><span class=\"line\">                <span class=\"string\">&quot;Request is incorrect: %(error)s&quot;</span>, error=error.normalized_messages()</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> support CSV, SQL query and other non-JSON types</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (</span><br><span class=\"line\">        is_feature_enabled(<span class=\"string\">&quot;GLOBAL_ASYNC_QUERIES&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context.result_format == ChartDataResultFormat.JSON</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context.result_type == ChartDataResultType.FULL</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._run_async(json_body, command)</span><br><span class=\"line\"></span><br><span class=\"line\">    form_data = json_body.get(<span class=\"string\">&quot;form_data&quot;</span>)</span><br><span class=\"line\">    logger.warning(<span class=\"string\">&quot;查询成功。。。form_data&quot;</span>)</span><br><span class=\"line\">    logger.warning(form_data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result = command.run(force_cached=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;result ... a&quot;</span>)</span><br><span class=\"line\">        logger.info(result)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataCacheLoadError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_422(message=exc.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataQueryFailedError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=exc.message)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">############################# execte without symp query</span></span><br><span class=\"line\"></span><br><span class=\"line\">    json_body_without_symp = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    has_symp = <span class=\"literal\">False</span></span><br><span class=\"line\">    <span class=\"comment\"># [&#123;&#x27;col&#x27;: &#x27;shop_id&#x27;, &#x27;op&#x27;: &#x27;==&#x27;, &#x27;val&#x27;: &#x27;121&#x27;&#125;]</span></span><br><span class=\"line\">    filters = json_body[<span class=\"string\">&quot;queries&quot;</span>][<span class=\"number\">0</span>][<span class=\"string\">&quot;filters&quot;</span>]</span><br><span class=\"line\">    <span class=\"comment\"># logger.info(filters)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">filter</span> <span class=\"keyword\">in</span> filters:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">filter</span>[<span class=\"string\">&quot;col&quot;</span>] == <span class=\"string\">&quot;shop_id&quot;</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 去除json_body中的过滤条件，构建json_body_2,进行查询，获取正确的input聚合数据</span></span><br><span class=\"line\">            filters.remove(<span class=\"built_in\">filter</span>)</span><br><span class=\"line\">            has_symp = <span class=\"literal\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_symp :</span><br><span class=\"line\">        json_body[<span class=\"string\">&quot;queries&quot;</span>][<span class=\"number\">0</span>][<span class=\"string\">&quot;filters&quot;</span>] = filters</span><br><span class=\"line\">        json_body_without_symp = json_body</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># logger.info(&quot;json_body_without_symp......&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># logger.info(json_body_without_symp)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        query_context_without_symp = self._create_query_context_from_form(json_body_without_symp)</span><br><span class=\"line\">        command_without_symp = ChartDataCommand(query_context_without_symp)</span><br><span class=\"line\">        command_without_symp.validate()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> DatasourceNotFound <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_404()</span><br><span class=\"line\">    <span class=\"keyword\">except</span> QueryObjectValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=error.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ValidationError <span class=\"keyword\">as</span> error:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(</span><br><span class=\"line\">            message=_(</span><br><span class=\"line\">                <span class=\"string\">&quot;Request is incorrect: %(error)s&quot;</span>, error=error.normalized_messages()</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> support CSV, SQL query and other non-JSON types</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (</span><br><span class=\"line\">        is_feature_enabled(<span class=\"string\">&quot;GLOBAL_ASYNC_QUERIES&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context_without_symp.result_format == ChartDataResultFormat.JSON</span><br><span class=\"line\">        <span class=\"keyword\">and</span> query_context_without_symp.result_type == ChartDataResultType.FULL</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._run_async(json_body_without_symp, command_without_symp)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#############################</span></span><br><span class=\"line\"></span><br><span class=\"line\">    form_data_without_symp = json_body_without_symp.get(<span class=\"string\">&quot;form_data&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;查询成功。。。form_data_without_symp&quot;</span>)</span><br><span class=\"line\">    logger.info(form_data_without_symp)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result_without_symp = command_without_symp.run(force_cached=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        logger.info(<span class=\"string\">&quot;result ... b&quot;</span>)</span><br><span class=\"line\">        logger.info(result_without_symp)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataCacheLoadError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_422(message=exc.message)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ChartDataQueryFailedError <span class=\"keyword\">as</span> exc:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.response_400(message=exc.message)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">## 重构result结果，返回前端</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> self._get_data_response(</span><br><span class=\"line\">        command, form_data=form_data, datasource=query_context.datasource</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n"},{"title":"通过阅读Flask-AppBuilder源码为Superset正确配置LDAP","abbrlink":32468,"date":"2023-05-11T03:45:16.000Z","updated":"2023-05-11T03:45:16.000Z","top_img":null,"cover":"https://dogefs.s3.ladydaily.com/tzk/storage/000651627f0c-92aec253754ab3dba99f61b49e9aa856.png","description":null,"keywords":null,"_content":"\n## 前言\n\n由于公司LDAP奇怪的信息管理，导致superset按照官方文档配置LDAP的时候，遇到一系列问题，不是认证失败，就是认证信息不合理。因此开始漫长的阅读[Flask-AppBuilder](https://github.com/dpgaspar/Flask-AppBuilder/)源码，来正确的配置LDAP。\n\n\n\n## 配置过程\n\n- 1、在superset的配置文件中，开启Flask-AppBuilder的日志输出。\n\n  ```python\n  # Whether to bump the logging level to ERROR on the flask_appbuilder package\n  # Set to False if/when debugging FAB related issues like\n  # permission management\n  SILENCE_FAB = False\n  ```\n\n- 2、正确的配置入下\n\n  ```python\n  AUTH_LDAP_SERVER = \"ldap://sdsdsds:389\"\n  AUTH_LDAP_SEARCH = \"DC=xx,DC=cc,DC=com\"\n  \n  # see flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap\n  # see flask_appbuilder.security.manager.BaseSecurityManager.auth_ldap_append_domain\n  AUTH_LDAP_APPEND_DOMAIN = \"xx.xx.com\"\n  AUTH_LDAP_SEARCH_FILTER = \"(objectClass=user)\"\n  AUTH_LDAP_UID_FIELD = \"sAMAccountName\"\n  # 由于大量用户邮箱相同，superset向数据库插入用户信息报邮箱相同的错误，导致认证失败，换一个不重复的字段\n  AUTH_LDAP_EMAIL_FIELD = \"userPrincipalName\"\n  ```\n\n- 3、进入docker，运行`superset init `,以解决Admin权限的用户也无法查看所有用户的bug。\n\n\n\n## 源码分析\n\nLDAP认证源码集中在`flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap`方法内：https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py\n\n- 1、查看源码发现，发现配置`AUTH_LDAP_APPEND_DOMAIN`这个字段可以直接通过网域账号通过认证，这样才满足我们的需求。而官方文档配置`AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD`这两个字段的方式，根本不满足我们的需求。\n\n  ```python\n      def auth_user_ldap(self, username, password):\n          \"\"\"\n          Method for authenticating user with LDAP.\n  \n          NOTE: this depends on python-ldap module\n  \n          :param username: the username\n          :param password: the password\n          \"\"\"\n          # If no username is provided, go away\n          if (username is None) or username == \"\":\n              return None\n  \n          # Search the DB for this user\n          user = self.find_user(username=username)\n  \n          # If user is not active, go away\n          if user and (not user.is_active):\n              return None\n  \n          # If user is not registered, and not self-registration, go away\n          if (not user) and (not self.auth_user_registration):\n              return None\n  \n          # Ensure python-ldap is installed\n          try:\n              import ldap\n          except ImportError:\n              log.error(\"python-ldap library is not installed\")\n              return None\n  \n          try:\n              # LDAP certificate settings\n              if self.auth_ldap_tls_cacertdir:\n                  ldap.set_option(ldap.OPT_X_TLS_CACERTDIR, self.auth_ldap_tls_cacertdir)\n              if self.auth_ldap_tls_cacertfile:\n                  ldap.set_option(\n                      ldap.OPT_X_TLS_CACERTFILE, self.auth_ldap_tls_cacertfile\n                  )\n              if self.auth_ldap_tls_certfile:\n                  ldap.set_option(ldap.OPT_X_TLS_CERTFILE, self.auth_ldap_tls_certfile)\n              if self.auth_ldap_tls_keyfile:\n                  ldap.set_option(ldap.OPT_X_TLS_KEYFILE, self.auth_ldap_tls_keyfile)\n              if self.auth_ldap_allow_self_signed:\n                  ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n                  ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n              elif self.auth_ldap_tls_demand:\n                  ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_DEMAND)\n                  ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n  \n              # Initialise LDAP connection\n              con = ldap.initialize(self.auth_ldap_server)\n              con.set_option(ldap.OPT_REFERRALS, 0)\n              if self.auth_ldap_use_tls:\n                  try:\n                      con.start_tls_s()\n                  except Exception:\n                      log.error(\n                          LOGMSG_ERR_SEC_AUTH_LDAP_TLS.format(self.auth_ldap_server)\n                      )\n                      return None\n  \n              # Define variables, so we can check if they are set in later steps\n              user_dn = None\n              user_attributes = {}\n  \n              # Flow 1 - (Indirect Search Bind):\n              #  - in this flow, special bind credentials are used to preform the\n              #    LDAP search\n              #  - in this flow, AUTH_LDAP_SEARCH must be set\n              if self.auth_ldap_bind_user:\n                  # Bind with AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD\n                  # (authorizes for LDAP search)\n                  self._ldap_bind_indirect(ldap, con)\n  \n                  # Search for `username`\n                  #  - returns the `user_dn` needed for binding to validate credentials\n                  #  - returns the `user_attributes` needed for\n                  #    AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN\n                  if self.auth_ldap_search:\n                      user_dn, user_attributes = self._search_ldap(ldap, con, username)\n                  else:\n                      log.error(\n                          \"AUTH_LDAP_SEARCH must be set when using AUTH_LDAP_BIND_USER\"\n                      )\n                      return None\n  \n                  # If search failed, go away\n                  if user_dn is None:\n                      log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))\n                      return None\n  \n                  # Bind with user_dn/password (validates credentials)\n                  if not self._ldap_bind(ldap, con, user_dn, password):\n                      if user:\n                          self.update_user_auth_stat(user, False)\n  \n                      # Invalid credentials, go away\n                      log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\n                      return None\n  \n              # Flow 2 - (Direct Search Bind):\n              #  - in this flow, the credentials provided by the end-user are used\n              #    to preform the LDAP search\n              #  - in this flow, we only search LDAP if AUTH_LDAP_SEARCH is set\n              #     - features like AUTH_USER_REGISTRATION & AUTH_ROLES_SYNC_AT_LOGIN\n              #       will only work if AUTH_LDAP_SEARCH is set\n              else:\n                  # Copy the provided username (so we can apply formatters)\n                  bind_username = username\n  \n                  # update `bind_username` by applying AUTH_LDAP_APPEND_DOMAIN\n                  #  - for Microsoft AD, which allows binding with userPrincipalName\n                  if self.auth_ldap_append_domain:\n                      bind_username = bind_username + \"@\" + self.auth_ldap_append_domain\n  \n                  # Update `bind_username` by applying AUTH_LDAP_USERNAME_FORMAT\n                  #  - for transforming the username into a DN,\n                  #    for example: \"uid=%s,ou=example,o=test\"\n                  if self.auth_ldap_username_format:\n                      bind_username = self.auth_ldap_username_format % bind_username\n  \n                  # Bind with bind_username/password\n                  # (validates credentials & authorizes for LDAP search)\n                  if not self._ldap_bind(ldap, con, bind_username, password):\n                      if user:\n                          self.update_user_auth_stat(user, False)\n  \n                      # Invalid credentials, go away\n                      log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(bind_username))\n                      return None\n  \n                  # Search for `username` (if AUTH_LDAP_SEARCH is set)\n                  #  - returns the `user_attributes`\n                  #    needed for AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN\n                  #  - we search on `username` not `bind_username`,\n                  #    because AUTH_LDAP_APPEND_DOMAIN and AUTH_LDAP_USERNAME_FORMAT\n                  #    would result in an invalid search filter\n                  if self.auth_ldap_search:\n                      user_dn, user_attributes = self._search_ldap(ldap, con, username)\n  \n                      # If search failed, go away\n                      if user_dn is None:\n                          log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))\n                          return None\n  \n              # Sync the user's roles\n              if user and user_attributes and self.auth_roles_sync_at_login:\n                  user.roles = self._ldap_calculate_user_roles(user_attributes)\n                  log.debug(\n                      \"Calculated new roles for user='{0}' as: {1}\".format(\n                          user_dn, user.roles\n                      )\n                  )\n  \n              # If the user is new, register them\n              if (not user) and user_attributes and self.auth_user_registration:\n                  user = self.add_user(\n                      username=username,\n                      first_name=self.ldap_extract(\n                          user_attributes, self.auth_ldap_firstname_field, \"\"\n                      ),\n                      last_name=self.ldap_extract(\n                          user_attributes, self.auth_ldap_lastname_field, \"\"\n                      ),\n                      email=self.ldap_extract(\n                          user_attributes,\n                          self.auth_ldap_email_field,\n                          f\"{username}@email.notfound\",\n                      ),\n                      role=self._ldap_calculate_user_roles(user_attributes),\n                  )\n                  log.debug(\"New user registered: {0}\".format(user))\n  \n                  # If user registration failed, go away\n                  if not user:\n                      log.info(LOGMSG_ERR_SEC_ADD_REGISTER_USER.format(username))\n                      return None\n  \n              # LOGIN SUCCESS (only if user is now registered)\n              if user:\n                  self.update_user_auth_stat(user)\n                  return user\n              else:\n                  return None\n  \n          except ldap.LDAPError as e:\n              msg = None\n              if isinstance(e, dict):\n                  msg = getattr(e, \"message\", None)\n              if (msg is not None) and (\"desc\" in msg):\n                  log.error(LOGMSG_ERR_SEC_AUTH_LDAP.format(e.message[\"desc\"]))\n                  return None\n              else:\n                  log.error(e)\n                  return None\n  ```\n\n- 2、通过阅读下面的源码，这样配置`AUTH_LDAP_SEARCH_FILTER = \"(objectClass=user)\"`,才满足我们的需求，想用那个字段登陆，就用那个字段登录。\n\n  ```python\n      def _search_ldap(self, ldap, con, username):\n          \"\"\"\n          Searches LDAP for user.\n  \n          :param ldap: The ldap module reference\n          :param con: The ldap connection\n          :param username: username to match with AUTH_LDAP_UID_FIELD\n          :return: ldap object array\n          \"\"\"\n          # always check AUTH_LDAP_SEARCH is set before calling this method\n          assert self.auth_ldap_search, \"AUTH_LDAP_SEARCH must be set\"\n  \n          # build the filter string for the LDAP search\n          # LDAP search的核心逻辑\n          if self.auth_ldap_search_filter:\n              filter_str = \"(&{0}({1}={2}))\".format(\n                  self.auth_ldap_search_filter, self.auth_ldap_uid_field, username\n              )\n          else:\n              filter_str = \"({0}={1})\".format(self.auth_ldap_uid_field, username)\n  \n          # build what fields to request in the LDAP search\n          request_fields = [\n              self.auth_ldap_firstname_field,\n              self.auth_ldap_lastname_field,\n              self.auth_ldap_email_field,\n          ]\n          if len(self.auth_roles_mapping) > 0:\n              request_fields.append(self.auth_ldap_group_field)\n  \n          # preform the LDAP search\n          log.debug(\n              \"LDAP search for '{0}' with fields {1} in scope '{2}'\".format(\n                  filter_str, request_fields, self.auth_ldap_search\n              )\n          )\n          raw_search_result = con.search_s(\n              self.auth_ldap_search, ldap.SCOPE_SUBTREE, filter_str, request_fields\n          )\n          log.debug(\"LDAP search returned: {0}\".format(raw_search_result))\n  \n          # Remove any search referrals from results\n          search_result = [\n              (dn, attrs)\n              for dn, attrs in raw_search_result\n              if dn is not None and isinstance(attrs, dict)\n          ]\n  \n          # only continue if 0 or 1 results were returned\n          if len(search_result) > 1:\n              log.error(\n                  \"LDAP search for '{0}' in scope '{1}' returned multiple results\".format(\n                      filter_str, self.auth_ldap_search\n                  )\n              )\n              return None, None\n  \n          try:\n              # extract the DN\n              user_dn = search_result[0][0]\n              # extract the other attributes\n              user_info = search_result[0][1]\n              # return\n              return user_dn, user_info\n          except (IndexError, NameError):\n              return None, None\n  ```\n\n  \n","source":"_posts/bigdata/superset/superset配置LDAP踩坑指南.md","raw":"---\ntitle: 通过阅读Flask-AppBuilder源码为Superset正确配置LDAP\ntags:\n  - Apache Superset\ncategories:\n  - - Superset\nabbrlink: 32468\ndate: 2023-05-11 11:45:16\nupdated: 2023-05-11 11:45:16\ntop_img:\ncover:\ndescription:\nkeywords:\n---\n\n## 前言\n\n由于公司LDAP奇怪的信息管理，导致superset按照官方文档配置LDAP的时候，遇到一系列问题，不是认证失败，就是认证信息不合理。因此开始漫长的阅读[Flask-AppBuilder](https://github.com/dpgaspar/Flask-AppBuilder/)源码，来正确的配置LDAP。\n\n\n\n## 配置过程\n\n- 1、在superset的配置文件中，开启Flask-AppBuilder的日志输出。\n\n  ```python\n  # Whether to bump the logging level to ERROR on the flask_appbuilder package\n  # Set to False if/when debugging FAB related issues like\n  # permission management\n  SILENCE_FAB = False\n  ```\n\n- 2、正确的配置入下\n\n  ```python\n  AUTH_LDAP_SERVER = \"ldap://sdsdsds:389\"\n  AUTH_LDAP_SEARCH = \"DC=xx,DC=cc,DC=com\"\n  \n  # see flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap\n  # see flask_appbuilder.security.manager.BaseSecurityManager.auth_ldap_append_domain\n  AUTH_LDAP_APPEND_DOMAIN = \"xx.xx.com\"\n  AUTH_LDAP_SEARCH_FILTER = \"(objectClass=user)\"\n  AUTH_LDAP_UID_FIELD = \"sAMAccountName\"\n  # 由于大量用户邮箱相同，superset向数据库插入用户信息报邮箱相同的错误，导致认证失败，换一个不重复的字段\n  AUTH_LDAP_EMAIL_FIELD = \"userPrincipalName\"\n  ```\n\n- 3、进入docker，运行`superset init `,以解决Admin权限的用户也无法查看所有用户的bug。\n\n\n\n## 源码分析\n\nLDAP认证源码集中在`flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap`方法内：https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py\n\n- 1、查看源码发现，发现配置`AUTH_LDAP_APPEND_DOMAIN`这个字段可以直接通过网域账号通过认证，这样才满足我们的需求。而官方文档配置`AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD`这两个字段的方式，根本不满足我们的需求。\n\n  ```python\n      def auth_user_ldap(self, username, password):\n          \"\"\"\n          Method for authenticating user with LDAP.\n  \n          NOTE: this depends on python-ldap module\n  \n          :param username: the username\n          :param password: the password\n          \"\"\"\n          # If no username is provided, go away\n          if (username is None) or username == \"\":\n              return None\n  \n          # Search the DB for this user\n          user = self.find_user(username=username)\n  \n          # If user is not active, go away\n          if user and (not user.is_active):\n              return None\n  \n          # If user is not registered, and not self-registration, go away\n          if (not user) and (not self.auth_user_registration):\n              return None\n  \n          # Ensure python-ldap is installed\n          try:\n              import ldap\n          except ImportError:\n              log.error(\"python-ldap library is not installed\")\n              return None\n  \n          try:\n              # LDAP certificate settings\n              if self.auth_ldap_tls_cacertdir:\n                  ldap.set_option(ldap.OPT_X_TLS_CACERTDIR, self.auth_ldap_tls_cacertdir)\n              if self.auth_ldap_tls_cacertfile:\n                  ldap.set_option(\n                      ldap.OPT_X_TLS_CACERTFILE, self.auth_ldap_tls_cacertfile\n                  )\n              if self.auth_ldap_tls_certfile:\n                  ldap.set_option(ldap.OPT_X_TLS_CERTFILE, self.auth_ldap_tls_certfile)\n              if self.auth_ldap_tls_keyfile:\n                  ldap.set_option(ldap.OPT_X_TLS_KEYFILE, self.auth_ldap_tls_keyfile)\n              if self.auth_ldap_allow_self_signed:\n                  ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n                  ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n              elif self.auth_ldap_tls_demand:\n                  ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_DEMAND)\n                  ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n  \n              # Initialise LDAP connection\n              con = ldap.initialize(self.auth_ldap_server)\n              con.set_option(ldap.OPT_REFERRALS, 0)\n              if self.auth_ldap_use_tls:\n                  try:\n                      con.start_tls_s()\n                  except Exception:\n                      log.error(\n                          LOGMSG_ERR_SEC_AUTH_LDAP_TLS.format(self.auth_ldap_server)\n                      )\n                      return None\n  \n              # Define variables, so we can check if they are set in later steps\n              user_dn = None\n              user_attributes = {}\n  \n              # Flow 1 - (Indirect Search Bind):\n              #  - in this flow, special bind credentials are used to preform the\n              #    LDAP search\n              #  - in this flow, AUTH_LDAP_SEARCH must be set\n              if self.auth_ldap_bind_user:\n                  # Bind with AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD\n                  # (authorizes for LDAP search)\n                  self._ldap_bind_indirect(ldap, con)\n  \n                  # Search for `username`\n                  #  - returns the `user_dn` needed for binding to validate credentials\n                  #  - returns the `user_attributes` needed for\n                  #    AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN\n                  if self.auth_ldap_search:\n                      user_dn, user_attributes = self._search_ldap(ldap, con, username)\n                  else:\n                      log.error(\n                          \"AUTH_LDAP_SEARCH must be set when using AUTH_LDAP_BIND_USER\"\n                      )\n                      return None\n  \n                  # If search failed, go away\n                  if user_dn is None:\n                      log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))\n                      return None\n  \n                  # Bind with user_dn/password (validates credentials)\n                  if not self._ldap_bind(ldap, con, user_dn, password):\n                      if user:\n                          self.update_user_auth_stat(user, False)\n  \n                      # Invalid credentials, go away\n                      log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\n                      return None\n  \n              # Flow 2 - (Direct Search Bind):\n              #  - in this flow, the credentials provided by the end-user are used\n              #    to preform the LDAP search\n              #  - in this flow, we only search LDAP if AUTH_LDAP_SEARCH is set\n              #     - features like AUTH_USER_REGISTRATION & AUTH_ROLES_SYNC_AT_LOGIN\n              #       will only work if AUTH_LDAP_SEARCH is set\n              else:\n                  # Copy the provided username (so we can apply formatters)\n                  bind_username = username\n  \n                  # update `bind_username` by applying AUTH_LDAP_APPEND_DOMAIN\n                  #  - for Microsoft AD, which allows binding with userPrincipalName\n                  if self.auth_ldap_append_domain:\n                      bind_username = bind_username + \"@\" + self.auth_ldap_append_domain\n  \n                  # Update `bind_username` by applying AUTH_LDAP_USERNAME_FORMAT\n                  #  - for transforming the username into a DN,\n                  #    for example: \"uid=%s,ou=example,o=test\"\n                  if self.auth_ldap_username_format:\n                      bind_username = self.auth_ldap_username_format % bind_username\n  \n                  # Bind with bind_username/password\n                  # (validates credentials & authorizes for LDAP search)\n                  if not self._ldap_bind(ldap, con, bind_username, password):\n                      if user:\n                          self.update_user_auth_stat(user, False)\n  \n                      # Invalid credentials, go away\n                      log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(bind_username))\n                      return None\n  \n                  # Search for `username` (if AUTH_LDAP_SEARCH is set)\n                  #  - returns the `user_attributes`\n                  #    needed for AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN\n                  #  - we search on `username` not `bind_username`,\n                  #    because AUTH_LDAP_APPEND_DOMAIN and AUTH_LDAP_USERNAME_FORMAT\n                  #    would result in an invalid search filter\n                  if self.auth_ldap_search:\n                      user_dn, user_attributes = self._search_ldap(ldap, con, username)\n  \n                      # If search failed, go away\n                      if user_dn is None:\n                          log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))\n                          return None\n  \n              # Sync the user's roles\n              if user and user_attributes and self.auth_roles_sync_at_login:\n                  user.roles = self._ldap_calculate_user_roles(user_attributes)\n                  log.debug(\n                      \"Calculated new roles for user='{0}' as: {1}\".format(\n                          user_dn, user.roles\n                      )\n                  )\n  \n              # If the user is new, register them\n              if (not user) and user_attributes and self.auth_user_registration:\n                  user = self.add_user(\n                      username=username,\n                      first_name=self.ldap_extract(\n                          user_attributes, self.auth_ldap_firstname_field, \"\"\n                      ),\n                      last_name=self.ldap_extract(\n                          user_attributes, self.auth_ldap_lastname_field, \"\"\n                      ),\n                      email=self.ldap_extract(\n                          user_attributes,\n                          self.auth_ldap_email_field,\n                          f\"{username}@email.notfound\",\n                      ),\n                      role=self._ldap_calculate_user_roles(user_attributes),\n                  )\n                  log.debug(\"New user registered: {0}\".format(user))\n  \n                  # If user registration failed, go away\n                  if not user:\n                      log.info(LOGMSG_ERR_SEC_ADD_REGISTER_USER.format(username))\n                      return None\n  \n              # LOGIN SUCCESS (only if user is now registered)\n              if user:\n                  self.update_user_auth_stat(user)\n                  return user\n              else:\n                  return None\n  \n          except ldap.LDAPError as e:\n              msg = None\n              if isinstance(e, dict):\n                  msg = getattr(e, \"message\", None)\n              if (msg is not None) and (\"desc\" in msg):\n                  log.error(LOGMSG_ERR_SEC_AUTH_LDAP.format(e.message[\"desc\"]))\n                  return None\n              else:\n                  log.error(e)\n                  return None\n  ```\n\n- 2、通过阅读下面的源码，这样配置`AUTH_LDAP_SEARCH_FILTER = \"(objectClass=user)\"`,才满足我们的需求，想用那个字段登陆，就用那个字段登录。\n\n  ```python\n      def _search_ldap(self, ldap, con, username):\n          \"\"\"\n          Searches LDAP for user.\n  \n          :param ldap: The ldap module reference\n          :param con: The ldap connection\n          :param username: username to match with AUTH_LDAP_UID_FIELD\n          :return: ldap object array\n          \"\"\"\n          # always check AUTH_LDAP_SEARCH is set before calling this method\n          assert self.auth_ldap_search, \"AUTH_LDAP_SEARCH must be set\"\n  \n          # build the filter string for the LDAP search\n          # LDAP search的核心逻辑\n          if self.auth_ldap_search_filter:\n              filter_str = \"(&{0}({1}={2}))\".format(\n                  self.auth_ldap_search_filter, self.auth_ldap_uid_field, username\n              )\n          else:\n              filter_str = \"({0}={1})\".format(self.auth_ldap_uid_field, username)\n  \n          # build what fields to request in the LDAP search\n          request_fields = [\n              self.auth_ldap_firstname_field,\n              self.auth_ldap_lastname_field,\n              self.auth_ldap_email_field,\n          ]\n          if len(self.auth_roles_mapping) > 0:\n              request_fields.append(self.auth_ldap_group_field)\n  \n          # preform the LDAP search\n          log.debug(\n              \"LDAP search for '{0}' with fields {1} in scope '{2}'\".format(\n                  filter_str, request_fields, self.auth_ldap_search\n              )\n          )\n          raw_search_result = con.search_s(\n              self.auth_ldap_search, ldap.SCOPE_SUBTREE, filter_str, request_fields\n          )\n          log.debug(\"LDAP search returned: {0}\".format(raw_search_result))\n  \n          # Remove any search referrals from results\n          search_result = [\n              (dn, attrs)\n              for dn, attrs in raw_search_result\n              if dn is not None and isinstance(attrs, dict)\n          ]\n  \n          # only continue if 0 or 1 results were returned\n          if len(search_result) > 1:\n              log.error(\n                  \"LDAP search for '{0}' in scope '{1}' returned multiple results\".format(\n                      filter_str, self.auth_ldap_search\n                  )\n              )\n              return None, None\n  \n          try:\n              # extract the DN\n              user_dn = search_result[0][0]\n              # extract the other attributes\n              user_info = search_result[0][1]\n              # return\n              return user_dn, user_info\n          except (IndexError, NameError):\n              return None, None\n  ```\n\n  \n","slug":"bigdata/superset/superset配置LDAP踩坑指南","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clx17vktb009v8j5mbwpaflso","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>由于公司LDAP奇怪的信息管理，导致superset按照官方文档配置LDAP的时候，遇到一系列问题，不是认证失败，就是认证信息不合理。因此开始漫长的阅读<a href=\"https://github.com/dpgaspar/Flask-AppBuilder/\">Flask-AppBuilder</a>源码，来正确的配置LDAP。</p>\n<h2 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h2><ul>\n<li><p>1、在superset的配置文件中，开启Flask-AppBuilder的日志输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Whether to bump the logging level to ERROR on the flask_appbuilder package</span></span><br><span class=\"line\"><span class=\"comment\"># Set to False if/when debugging FAB related issues like</span></span><br><span class=\"line\"><span class=\"comment\"># permission management</span></span><br><span class=\"line\">SILENCE_FAB = <span class=\"literal\">False</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、正确的配置入下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AUTH_LDAP_SERVER = <span class=\"string\">&quot;ldap://sdsdsds:389&quot;</span></span><br><span class=\"line\">AUTH_LDAP_SEARCH = <span class=\"string\">&quot;DC=xx,DC=cc,DC=com&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># see flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap</span></span><br><span class=\"line\"><span class=\"comment\"># see flask_appbuilder.security.manager.BaseSecurityManager.auth_ldap_append_domain</span></span><br><span class=\"line\">AUTH_LDAP_APPEND_DOMAIN = <span class=\"string\">&quot;xx.xx.com&quot;</span></span><br><span class=\"line\">AUTH_LDAP_SEARCH_FILTER = <span class=\"string\">&quot;(objectClass=user)&quot;</span></span><br><span class=\"line\">AUTH_LDAP_UID_FIELD = <span class=\"string\">&quot;sAMAccountName&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 由于大量用户邮箱相同，superset向数据库插入用户信息报邮箱相同的错误，导致认证失败，换一个不重复的字段</span></span><br><span class=\"line\">AUTH_LDAP_EMAIL_FIELD = <span class=\"string\">&quot;userPrincipalName&quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、进入docker，运行<code>superset init </code>,以解决Admin权限的用户也无法查看所有用户的bug。</p>\n</li>\n</ul>\n<h2 id=\"源码分析\"><a href=\"#源码分析\" class=\"headerlink\" title=\"源码分析\"></a>源码分析</h2><p>LDAP认证源码集中在<code>flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap</code>方法内：<a href=\"https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py\">https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py</a></p>\n<ul>\n<li><p>1、查看源码发现，发现配置<code>AUTH_LDAP_APPEND_DOMAIN</code>这个字段可以直接通过网域账号通过认证，这样才满足我们的需求。而官方文档配置<code>AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD</code>这两个字段的方式，根本不满足我们的需求。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">auth_user_ldap</span>(<span class=\"params\">self, username, password</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Method for authenticating user with LDAP.</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    NOTE: this depends on python-ldap module</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    :param username: the username</span></span><br><span class=\"line\"><span class=\"string\">    :param password: the password</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># If no username is provided, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (username <span class=\"keyword\">is</span> <span class=\"literal\">None</span>) <span class=\"keyword\">or</span> username == <span class=\"string\">&quot;&quot;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Search the DB for this user</span></span><br><span class=\"line\">    user = self.find_user(username=username)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># If user is not active, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> user <span class=\"keyword\">and</span> (<span class=\"keyword\">not</span> user.is_active):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># If user is not registered, and not self-registration, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">not</span> user) <span class=\"keyword\">and</span> (<span class=\"keyword\">not</span> self.auth_user_registration):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Ensure python-ldap is installed</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"keyword\">import</span> ldap</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ImportError:</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;python-ldap library is not installed&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># LDAP certificate settings</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_cacertdir:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_CACERTDIR, self.auth_ldap_tls_cacertdir)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_cacertfile:</span><br><span class=\"line\">            ldap.set_option(</span><br><span class=\"line\">                ldap.OPT_X_TLS_CACERTFILE, self.auth_ldap_tls_cacertfile</span><br><span class=\"line\">            )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_certfile:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_CERTFILE, self.auth_ldap_tls_certfile)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_keyfile:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_KEYFILE, self.auth_ldap_tls_keyfile)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_allow_self_signed:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_NEWCTX, <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.auth_ldap_tls_demand:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_DEMAND)</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_NEWCTX, <span class=\"number\">0</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Initialise LDAP connection</span></span><br><span class=\"line\">        con = ldap.initialize(self.auth_ldap_server)</span><br><span class=\"line\">        con.set_option(ldap.OPT_REFERRALS, <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_use_tls:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                con.start_tls_s()</span><br><span class=\"line\">            <span class=\"keyword\">except</span> Exception:</span><br><span class=\"line\">                log.error(</span><br><span class=\"line\">                    LOGMSG_ERR_SEC_AUTH_LDAP_TLS.<span class=\"built_in\">format</span>(self.auth_ldap_server)</span><br><span class=\"line\">                )</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Define variables, so we can check if they are set in later steps</span></span><br><span class=\"line\">        user_dn = <span class=\"literal\">None</span></span><br><span class=\"line\">        user_attributes = &#123;&#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Flow 1 - (Indirect Search Bind):</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, special bind credentials are used to preform the</span></span><br><span class=\"line\">        <span class=\"comment\">#    LDAP search</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, AUTH_LDAP_SEARCH must be set</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_bind_user:</span><br><span class=\"line\">            <span class=\"comment\"># Bind with AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD</span></span><br><span class=\"line\">            <span class=\"comment\"># (authorizes for LDAP search)</span></span><br><span class=\"line\">            self._ldap_bind_indirect(ldap, con)</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Search for `username`</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_dn` needed for binding to validate credentials</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_attributes` needed for</span></span><br><span class=\"line\">            <span class=\"comment\">#    AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_search:</span><br><span class=\"line\">                user_dn, user_attributes = self._search_ldap(ldap, con, username)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                log.error(</span><br><span class=\"line\">                    <span class=\"string\">&quot;AUTH_LDAP_SEARCH must be set when using AUTH_LDAP_BIND_USER&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># If search failed, go away</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> user_dn <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Bind with user_dn/password (validates credentials)</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self._ldap_bind(ldap, con, user_dn, password):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">                    self.update_user_auth_stat(user, <span class=\"literal\">False</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># Invalid credentials, go away</span></span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Flow 2 - (Direct Search Bind):</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, the credentials provided by the end-user are used</span></span><br><span class=\"line\">        <span class=\"comment\">#    to preform the LDAP search</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, we only search LDAP if AUTH_LDAP_SEARCH is set</span></span><br><span class=\"line\">        <span class=\"comment\">#     - features like AUTH_USER_REGISTRATION &amp; AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">        <span class=\"comment\">#       will only work if AUTH_LDAP_SEARCH is set</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Copy the provided username (so we can apply formatters)</span></span><br><span class=\"line\">            bind_username = username</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># update `bind_username` by applying AUTH_LDAP_APPEND_DOMAIN</span></span><br><span class=\"line\">            <span class=\"comment\">#  - for Microsoft AD, which allows binding with userPrincipalName</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_append_domain:</span><br><span class=\"line\">                bind_username = bind_username + <span class=\"string\">&quot;@&quot;</span> + self.auth_ldap_append_domain</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Update `bind_username` by applying AUTH_LDAP_USERNAME_FORMAT</span></span><br><span class=\"line\">            <span class=\"comment\">#  - for transforming the username into a DN,</span></span><br><span class=\"line\">            <span class=\"comment\">#    for example: &quot;uid=%s,ou=example,o=test&quot;</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_username_format:</span><br><span class=\"line\">                bind_username = self.auth_ldap_username_format % bind_username</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Bind with bind_username/password</span></span><br><span class=\"line\">            <span class=\"comment\"># (validates credentials &amp; authorizes for LDAP search)</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self._ldap_bind(ldap, con, bind_username, password):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">                    self.update_user_auth_stat(user, <span class=\"literal\">False</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># Invalid credentials, go away</span></span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.<span class=\"built_in\">format</span>(bind_username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Search for `username` (if AUTH_LDAP_SEARCH is set)</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_attributes`</span></span><br><span class=\"line\">            <span class=\"comment\">#    needed for AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">            <span class=\"comment\">#  - we search on `username` not `bind_username`,</span></span><br><span class=\"line\">            <span class=\"comment\">#    because AUTH_LDAP_APPEND_DOMAIN and AUTH_LDAP_USERNAME_FORMAT</span></span><br><span class=\"line\">            <span class=\"comment\">#    would result in an invalid search filter</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_search:</span><br><span class=\"line\">                user_dn, user_attributes = self._search_ldap(ldap, con, username)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># If search failed, go away</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> user_dn <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                    log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Sync the user&#x27;s roles</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> user <span class=\"keyword\">and</span> user_attributes <span class=\"keyword\">and</span> self.auth_roles_sync_at_login:</span><br><span class=\"line\">            user.roles = self._ldap_calculate_user_roles(user_attributes)</span><br><span class=\"line\">            log.debug(</span><br><span class=\"line\">                <span class=\"string\">&quot;Calculated new roles for user=&#x27;&#123;0&#125;&#x27; as: &#123;1&#125;&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                    user_dn, user.roles</span><br><span class=\"line\">                )</span><br><span class=\"line\">            )</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># If the user is new, register them</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"keyword\">not</span> user) <span class=\"keyword\">and</span> user_attributes <span class=\"keyword\">and</span> self.auth_user_registration:</span><br><span class=\"line\">            user = self.add_user(</span><br><span class=\"line\">                username=username,</span><br><span class=\"line\">                first_name=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes, self.auth_ldap_firstname_field, <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">                ),</span><br><span class=\"line\">                last_name=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes, self.auth_ldap_lastname_field, <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">                ),</span><br><span class=\"line\">                email=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes,</span><br><span class=\"line\">                    self.auth_ldap_email_field,</span><br><span class=\"line\">                    <span class=\"string\">f&quot;<span class=\"subst\">&#123;username&#125;</span>@email.notfound&quot;</span>,</span><br><span class=\"line\">                ),</span><br><span class=\"line\">                role=self._ldap_calculate_user_roles(user_attributes),</span><br><span class=\"line\">            )</span><br><span class=\"line\">            log.debug(<span class=\"string\">&quot;New user registered: &#123;0&#125;&quot;</span>.<span class=\"built_in\">format</span>(user))</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># If user registration failed, go away</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> user:</span><br><span class=\"line\">                log.info(LOGMSG_ERR_SEC_ADD_REGISTER_USER.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># LOGIN SUCCESS (only if user is now registered)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">            self.update_user_auth_stat(user)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> user</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">except</span> ldap.LDAPError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        msg = <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(e, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            msg = <span class=\"built_in\">getattr</span>(e, <span class=\"string\">&quot;message&quot;</span>, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (msg <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>) <span class=\"keyword\">and</span> (<span class=\"string\">&quot;desc&quot;</span> <span class=\"keyword\">in</span> msg):</span><br><span class=\"line\">            log.error(LOGMSG_ERR_SEC_AUTH_LDAP.<span class=\"built_in\">format</span>(e.message[<span class=\"string\">&quot;desc&quot;</span>]))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            log.error(e)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、通过阅读下面的源码，这样配置<code>AUTH_LDAP_SEARCH_FILTER = &quot;(objectClass=user)&quot;</code>,才满足我们的需求，想用那个字段登陆，就用那个字段登录。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_search_ldap</span>(<span class=\"params\">self, ldap, con, username</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Searches LDAP for user.</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    :param ldap: The ldap module reference</span></span><br><span class=\"line\"><span class=\"string\">    :param con: The ldap connection</span></span><br><span class=\"line\"><span class=\"string\">    :param username: username to match with AUTH_LDAP_UID_FIELD</span></span><br><span class=\"line\"><span class=\"string\">    :return: ldap object array</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># always check AUTH_LDAP_SEARCH is set before calling this method</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> self.auth_ldap_search, <span class=\"string\">&quot;AUTH_LDAP_SEARCH must be set&quot;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># build the filter string for the LDAP search</span></span><br><span class=\"line\">    <span class=\"comment\"># LDAP search的核心逻辑</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> self.auth_ldap_search_filter:</span><br><span class=\"line\">        filter_str = <span class=\"string\">&quot;(&amp;&#123;0&#125;(&#123;1&#125;=&#123;2&#125;))&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">            self.auth_ldap_search_filter, self.auth_ldap_uid_field, username</span><br><span class=\"line\">        )</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        filter_str = <span class=\"string\">&quot;(&#123;0&#125;=&#123;1&#125;)&quot;</span>.<span class=\"built_in\">format</span>(self.auth_ldap_uid_field, username)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># build what fields to request in the LDAP search</span></span><br><span class=\"line\">    request_fields = [</span><br><span class=\"line\">        self.auth_ldap_firstname_field,</span><br><span class=\"line\">        self.auth_ldap_lastname_field,</span><br><span class=\"line\">        self.auth_ldap_email_field,</span><br><span class=\"line\">    ]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(self.auth_roles_mapping) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        request_fields.append(self.auth_ldap_group_field)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># preform the LDAP search</span></span><br><span class=\"line\">    log.debug(</span><br><span class=\"line\">        <span class=\"string\">&quot;LDAP search for &#x27;&#123;0&#125;&#x27; with fields &#123;1&#125; in scope &#x27;&#123;2&#125;&#x27;&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">            filter_str, request_fields, self.auth_ldap_search</span><br><span class=\"line\">        )</span><br><span class=\"line\">    )</span><br><span class=\"line\">    raw_search_result = con.search_s(</span><br><span class=\"line\">        self.auth_ldap_search, ldap.SCOPE_SUBTREE, filter_str, request_fields</span><br><span class=\"line\">    )</span><br><span class=\"line\">    log.debug(<span class=\"string\">&quot;LDAP search returned: &#123;0&#125;&quot;</span>.<span class=\"built_in\">format</span>(raw_search_result))</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Remove any search referrals from results</span></span><br><span class=\"line\">    search_result = [</span><br><span class=\"line\">        (dn, attrs)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dn, attrs <span class=\"keyword\">in</span> raw_search_result</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dn <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> <span class=\"built_in\">isinstance</span>(attrs, <span class=\"built_in\">dict</span>)</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># only continue if 0 or 1 results were returned</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(search_result) &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        log.error(</span><br><span class=\"line\">            <span class=\"string\">&quot;LDAP search for &#x27;&#123;0&#125;&#x27; in scope &#x27;&#123;1&#125;&#x27; returned multiple results&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                filter_str, self.auth_ldap_search</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># extract the DN</span></span><br><span class=\"line\">        user_dn = search_result[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"comment\"># extract the other attributes</span></span><br><span class=\"line\">        user_info = search_result[<span class=\"number\">0</span>][<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># return</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> user_dn, user_info</span><br><span class=\"line\">    <span class=\"keyword\">except</span> (IndexError, NameError):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"张洪Heo","link":"https://blog.zhheo.com/","avatar":"https://npm.elemecdn.com/guli-heo/img/avatar2.png","descr":"分享设计与科技生活"},{"name":"瞬間の筆記","link":"https://hikki.site","descr":"喜欢的东西就努力去追求，万一成功了呢!","avatar":"https://cdn.jsdelivr.net/gh/0000rookie/imgs/202206120315.png"}]}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>由于公司LDAP奇怪的信息管理，导致superset按照官方文档配置LDAP的时候，遇到一系列问题，不是认证失败，就是认证信息不合理。因此开始漫长的阅读<a href=\"https://github.com/dpgaspar/Flask-AppBuilder/\">Flask-AppBuilder</a>源码，来正确的配置LDAP。</p>\n<h2 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h2><ul>\n<li><p>1、在superset的配置文件中，开启Flask-AppBuilder的日志输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Whether to bump the logging level to ERROR on the flask_appbuilder package</span></span><br><span class=\"line\"><span class=\"comment\"># Set to False if/when debugging FAB related issues like</span></span><br><span class=\"line\"><span class=\"comment\"># permission management</span></span><br><span class=\"line\">SILENCE_FAB = <span class=\"literal\">False</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、正确的配置入下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AUTH_LDAP_SERVER = <span class=\"string\">&quot;ldap://sdsdsds:389&quot;</span></span><br><span class=\"line\">AUTH_LDAP_SEARCH = <span class=\"string\">&quot;DC=xx,DC=cc,DC=com&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># see flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap</span></span><br><span class=\"line\"><span class=\"comment\"># see flask_appbuilder.security.manager.BaseSecurityManager.auth_ldap_append_domain</span></span><br><span class=\"line\">AUTH_LDAP_APPEND_DOMAIN = <span class=\"string\">&quot;xx.xx.com&quot;</span></span><br><span class=\"line\">AUTH_LDAP_SEARCH_FILTER = <span class=\"string\">&quot;(objectClass=user)&quot;</span></span><br><span class=\"line\">AUTH_LDAP_UID_FIELD = <span class=\"string\">&quot;sAMAccountName&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 由于大量用户邮箱相同，superset向数据库插入用户信息报邮箱相同的错误，导致认证失败，换一个不重复的字段</span></span><br><span class=\"line\">AUTH_LDAP_EMAIL_FIELD = <span class=\"string\">&quot;userPrincipalName&quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>3、进入docker，运行<code>superset init </code>,以解决Admin权限的用户也无法查看所有用户的bug。</p>\n</li>\n</ul>\n<h2 id=\"源码分析\"><a href=\"#源码分析\" class=\"headerlink\" title=\"源码分析\"></a>源码分析</h2><p>LDAP认证源码集中在<code>flask_appbuilder.security.manager.BaseSecurityManager.auth_user_ldap</code>方法内：<a href=\"https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py\">https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py</a></p>\n<ul>\n<li><p>1、查看源码发现，发现配置<code>AUTH_LDAP_APPEND_DOMAIN</code>这个字段可以直接通过网域账号通过认证，这样才满足我们的需求。而官方文档配置<code>AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD</code>这两个字段的方式，根本不满足我们的需求。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">auth_user_ldap</span>(<span class=\"params\">self, username, password</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Method for authenticating user with LDAP.</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    NOTE: this depends on python-ldap module</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    :param username: the username</span></span><br><span class=\"line\"><span class=\"string\">    :param password: the password</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># If no username is provided, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (username <span class=\"keyword\">is</span> <span class=\"literal\">None</span>) <span class=\"keyword\">or</span> username == <span class=\"string\">&quot;&quot;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Search the DB for this user</span></span><br><span class=\"line\">    user = self.find_user(username=username)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># If user is not active, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> user <span class=\"keyword\">and</span> (<span class=\"keyword\">not</span> user.is_active):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># If user is not registered, and not self-registration, go away</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">not</span> user) <span class=\"keyword\">and</span> (<span class=\"keyword\">not</span> self.auth_user_registration):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Ensure python-ldap is installed</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"keyword\">import</span> ldap</span><br><span class=\"line\">    <span class=\"keyword\">except</span> ImportError:</span><br><span class=\"line\">        log.error(<span class=\"string\">&quot;python-ldap library is not installed&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># LDAP certificate settings</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_cacertdir:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_CACERTDIR, self.auth_ldap_tls_cacertdir)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_cacertfile:</span><br><span class=\"line\">            ldap.set_option(</span><br><span class=\"line\">                ldap.OPT_X_TLS_CACERTFILE, self.auth_ldap_tls_cacertfile</span><br><span class=\"line\">            )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_certfile:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_CERTFILE, self.auth_ldap_tls_certfile)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_tls_keyfile:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_KEYFILE, self.auth_ldap_tls_keyfile)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_allow_self_signed:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_NEWCTX, <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.auth_ldap_tls_demand:</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_DEMAND)</span><br><span class=\"line\">            ldap.set_option(ldap.OPT_X_TLS_NEWCTX, <span class=\"number\">0</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Initialise LDAP connection</span></span><br><span class=\"line\">        con = ldap.initialize(self.auth_ldap_server)</span><br><span class=\"line\">        con.set_option(ldap.OPT_REFERRALS, <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_use_tls:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                con.start_tls_s()</span><br><span class=\"line\">            <span class=\"keyword\">except</span> Exception:</span><br><span class=\"line\">                log.error(</span><br><span class=\"line\">                    LOGMSG_ERR_SEC_AUTH_LDAP_TLS.<span class=\"built_in\">format</span>(self.auth_ldap_server)</span><br><span class=\"line\">                )</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Define variables, so we can check if they are set in later steps</span></span><br><span class=\"line\">        user_dn = <span class=\"literal\">None</span></span><br><span class=\"line\">        user_attributes = &#123;&#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Flow 1 - (Indirect Search Bind):</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, special bind credentials are used to preform the</span></span><br><span class=\"line\">        <span class=\"comment\">#    LDAP search</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, AUTH_LDAP_SEARCH must be set</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.auth_ldap_bind_user:</span><br><span class=\"line\">            <span class=\"comment\"># Bind with AUTH_LDAP_BIND_USER/AUTH_LDAP_BIND_PASSWORD</span></span><br><span class=\"line\">            <span class=\"comment\"># (authorizes for LDAP search)</span></span><br><span class=\"line\">            self._ldap_bind_indirect(ldap, con)</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Search for `username`</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_dn` needed for binding to validate credentials</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_attributes` needed for</span></span><br><span class=\"line\">            <span class=\"comment\">#    AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_search:</span><br><span class=\"line\">                user_dn, user_attributes = self._search_ldap(ldap, con, username)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                log.error(</span><br><span class=\"line\">                    <span class=\"string\">&quot;AUTH_LDAP_SEARCH must be set when using AUTH_LDAP_BIND_USER&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># If search failed, go away</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> user_dn <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Bind with user_dn/password (validates credentials)</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self._ldap_bind(ldap, con, user_dn, password):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">                    self.update_user_auth_stat(user, <span class=\"literal\">False</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># Invalid credentials, go away</span></span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Flow 2 - (Direct Search Bind):</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, the credentials provided by the end-user are used</span></span><br><span class=\"line\">        <span class=\"comment\">#    to preform the LDAP search</span></span><br><span class=\"line\">        <span class=\"comment\">#  - in this flow, we only search LDAP if AUTH_LDAP_SEARCH is set</span></span><br><span class=\"line\">        <span class=\"comment\">#     - features like AUTH_USER_REGISTRATION &amp; AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">        <span class=\"comment\">#       will only work if AUTH_LDAP_SEARCH is set</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Copy the provided username (so we can apply formatters)</span></span><br><span class=\"line\">            bind_username = username</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># update `bind_username` by applying AUTH_LDAP_APPEND_DOMAIN</span></span><br><span class=\"line\">            <span class=\"comment\">#  - for Microsoft AD, which allows binding with userPrincipalName</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_append_domain:</span><br><span class=\"line\">                bind_username = bind_username + <span class=\"string\">&quot;@&quot;</span> + self.auth_ldap_append_domain</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Update `bind_username` by applying AUTH_LDAP_USERNAME_FORMAT</span></span><br><span class=\"line\">            <span class=\"comment\">#  - for transforming the username into a DN,</span></span><br><span class=\"line\">            <span class=\"comment\">#    for example: &quot;uid=%s,ou=example,o=test&quot;</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_username_format:</span><br><span class=\"line\">                bind_username = self.auth_ldap_username_format % bind_username</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Bind with bind_username/password</span></span><br><span class=\"line\">            <span class=\"comment\"># (validates credentials &amp; authorizes for LDAP search)</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self._ldap_bind(ldap, con, bind_username, password):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">                    self.update_user_auth_stat(user, <span class=\"literal\">False</span>)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># Invalid credentials, go away</span></span><br><span class=\"line\">                log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.<span class=\"built_in\">format</span>(bind_username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># Search for `username` (if AUTH_LDAP_SEARCH is set)</span></span><br><span class=\"line\">            <span class=\"comment\">#  - returns the `user_attributes`</span></span><br><span class=\"line\">            <span class=\"comment\">#    needed for AUTH_USER_REGISTRATION/AUTH_ROLES_SYNC_AT_LOGIN</span></span><br><span class=\"line\">            <span class=\"comment\">#  - we search on `username` not `bind_username`,</span></span><br><span class=\"line\">            <span class=\"comment\">#    because AUTH_LDAP_APPEND_DOMAIN and AUTH_LDAP_USERNAME_FORMAT</span></span><br><span class=\"line\">            <span class=\"comment\">#    would result in an invalid search filter</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.auth_ldap_search:</span><br><span class=\"line\">                user_dn, user_attributes = self._search_ldap(ldap, con, username)</span><br><span class=\"line\">  </span><br><span class=\"line\">                <span class=\"comment\"># If search failed, go away</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> user_dn <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                    log.info(LOGMSG_WAR_SEC_NOLDAP_OBJ.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                    <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># Sync the user&#x27;s roles</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> user <span class=\"keyword\">and</span> user_attributes <span class=\"keyword\">and</span> self.auth_roles_sync_at_login:</span><br><span class=\"line\">            user.roles = self._ldap_calculate_user_roles(user_attributes)</span><br><span class=\"line\">            log.debug(</span><br><span class=\"line\">                <span class=\"string\">&quot;Calculated new roles for user=&#x27;&#123;0&#125;&#x27; as: &#123;1&#125;&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                    user_dn, user.roles</span><br><span class=\"line\">                )</span><br><span class=\"line\">            )</span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># If the user is new, register them</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"keyword\">not</span> user) <span class=\"keyword\">and</span> user_attributes <span class=\"keyword\">and</span> self.auth_user_registration:</span><br><span class=\"line\">            user = self.add_user(</span><br><span class=\"line\">                username=username,</span><br><span class=\"line\">                first_name=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes, self.auth_ldap_firstname_field, <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">                ),</span><br><span class=\"line\">                last_name=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes, self.auth_ldap_lastname_field, <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">                ),</span><br><span class=\"line\">                email=self.ldap_extract(</span><br><span class=\"line\">                    user_attributes,</span><br><span class=\"line\">                    self.auth_ldap_email_field,</span><br><span class=\"line\">                    <span class=\"string\">f&quot;<span class=\"subst\">&#123;username&#125;</span>@email.notfound&quot;</span>,</span><br><span class=\"line\">                ),</span><br><span class=\"line\">                role=self._ldap_calculate_user_roles(user_attributes),</span><br><span class=\"line\">            )</span><br><span class=\"line\">            log.debug(<span class=\"string\">&quot;New user registered: &#123;0&#125;&quot;</span>.<span class=\"built_in\">format</span>(user))</span><br><span class=\"line\">  </span><br><span class=\"line\">            <span class=\"comment\"># If user registration failed, go away</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> user:</span><br><span class=\"line\">                log.info(LOGMSG_ERR_SEC_ADD_REGISTER_USER.<span class=\"built_in\">format</span>(username))</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\"># LOGIN SUCCESS (only if user is now registered)</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> user:</span><br><span class=\"line\">            self.update_user_auth_stat(user)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> user</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">except</span> ldap.LDAPError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        msg = <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(e, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            msg = <span class=\"built_in\">getattr</span>(e, <span class=\"string\">&quot;message&quot;</span>, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (msg <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>) <span class=\"keyword\">and</span> (<span class=\"string\">&quot;desc&quot;</span> <span class=\"keyword\">in</span> msg):</span><br><span class=\"line\">            log.error(LOGMSG_ERR_SEC_AUTH_LDAP.<span class=\"built_in\">format</span>(e.message[<span class=\"string\">&quot;desc&quot;</span>]))</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            log.error(e)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>2、通过阅读下面的源码，这样配置<code>AUTH_LDAP_SEARCH_FILTER = &quot;(objectClass=user)&quot;</code>,才满足我们的需求，想用那个字段登陆，就用那个字段登录。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_search_ldap</span>(<span class=\"params\">self, ldap, con, username</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Searches LDAP for user.</span></span><br><span class=\"line\"><span class=\"string\">  </span></span><br><span class=\"line\"><span class=\"string\">    :param ldap: The ldap module reference</span></span><br><span class=\"line\"><span class=\"string\">    :param con: The ldap connection</span></span><br><span class=\"line\"><span class=\"string\">    :param username: username to match with AUTH_LDAP_UID_FIELD</span></span><br><span class=\"line\"><span class=\"string\">    :return: ldap object array</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># always check AUTH_LDAP_SEARCH is set before calling this method</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> self.auth_ldap_search, <span class=\"string\">&quot;AUTH_LDAP_SEARCH must be set&quot;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># build the filter string for the LDAP search</span></span><br><span class=\"line\">    <span class=\"comment\"># LDAP search的核心逻辑</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> self.auth_ldap_search_filter:</span><br><span class=\"line\">        filter_str = <span class=\"string\">&quot;(&amp;&#123;0&#125;(&#123;1&#125;=&#123;2&#125;))&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">            self.auth_ldap_search_filter, self.auth_ldap_uid_field, username</span><br><span class=\"line\">        )</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        filter_str = <span class=\"string\">&quot;(&#123;0&#125;=&#123;1&#125;)&quot;</span>.<span class=\"built_in\">format</span>(self.auth_ldap_uid_field, username)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># build what fields to request in the LDAP search</span></span><br><span class=\"line\">    request_fields = [</span><br><span class=\"line\">        self.auth_ldap_firstname_field,</span><br><span class=\"line\">        self.auth_ldap_lastname_field,</span><br><span class=\"line\">        self.auth_ldap_email_field,</span><br><span class=\"line\">    ]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(self.auth_roles_mapping) &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        request_fields.append(self.auth_ldap_group_field)</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># preform the LDAP search</span></span><br><span class=\"line\">    log.debug(</span><br><span class=\"line\">        <span class=\"string\">&quot;LDAP search for &#x27;&#123;0&#125;&#x27; with fields &#123;1&#125; in scope &#x27;&#123;2&#125;&#x27;&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">            filter_str, request_fields, self.auth_ldap_search</span><br><span class=\"line\">        )</span><br><span class=\"line\">    )</span><br><span class=\"line\">    raw_search_result = con.search_s(</span><br><span class=\"line\">        self.auth_ldap_search, ldap.SCOPE_SUBTREE, filter_str, request_fields</span><br><span class=\"line\">    )</span><br><span class=\"line\">    log.debug(<span class=\"string\">&quot;LDAP search returned: &#123;0&#125;&quot;</span>.<span class=\"built_in\">format</span>(raw_search_result))</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Remove any search referrals from results</span></span><br><span class=\"line\">    search_result = [</span><br><span class=\"line\">        (dn, attrs)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dn, attrs <span class=\"keyword\">in</span> raw_search_result</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dn <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> <span class=\"built_in\">isinstance</span>(attrs, <span class=\"built_in\">dict</span>)</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># only continue if 0 or 1 results were returned</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(search_result) &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        log.error(</span><br><span class=\"line\">            <span class=\"string\">&quot;LDAP search for &#x27;&#123;0&#125;&#x27; in scope &#x27;&#123;1&#125;&#x27; returned multiple results&quot;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                filter_str, self.auth_ldap_search</span><br><span class=\"line\">            )</span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"comment\"># extract the DN</span></span><br><span class=\"line\">        user_dn = search_result[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"comment\"># extract the other attributes</span></span><br><span class=\"line\">        user_info = search_result[<span class=\"number\">0</span>][<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># return</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> user_dn, user_info</span><br><span class=\"line\">    <span class=\"keyword\">except</span> (IndexError, NameError):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"clx17vksu000h8j5mcag0ee87","category_id":"clx17vksu000g8j5m471xd00b","_id":"clx17vksv000p8j5m6q4p272a"},{"post_id":"clx17vkst00098j5md6o74ysg","category_id":"clx17vksu000g8j5m471xd00b","_id":"clx17vksw000s8j5m3q2k89cm"},{"post_id":"clx17vksv000m8j5mflxf23ys","category_id":"clx17vksu000g8j5m471xd00b","_id":"clx17vksw000x8j5mdi8z7ynl"},{"post_id":"clx17vksq00018j5m86cvdt7q","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vksw000z8j5mb7qugwa8"},{"post_id":"clx17vksq00018j5m86cvdt7q","category_id":"clx17vksv000l8j5mdw9x62i3","_id":"clx17vksw00138j5m0wopgvbk"},{"post_id":"clx17vkst000b8j5mago0hgbs","category_id":"clx17vksw000q8j5mai9zei8n","_id":"clx17vksx00178j5m679x356j"},{"post_id":"clx17vksw000w8j5mfxa87l48","category_id":"clx17vksu000g8j5m471xd00b","_id":"clx17vksx001b8j5mc88x14ju"},{"post_id":"clx17vksu000f8j5mgr6xb4x5","category_id":"clx17vksw000v8j5m2zcvble4","_id":"clx17vksx001f8j5maja60ivu"},{"post_id":"clx17vksw000y8j5m2ys488ia","category_id":"clx17vksu000g8j5m471xd00b","_id":"clx17vksx001j8j5m2t0o0sc6"},{"post_id":"clx17vksv000o8j5m43et1f2f","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vksz001r8j5m591ocpnt"},{"post_id":"clx17vksv000o8j5m43et1f2f","category_id":"clx17vksx001c8j5m4tvofhj0","_id":"clx17vksz001u8j5m4hsz7y16"},{"post_id":"clx17vksw000r8j5mhd8od7jd","category_id":"clx17vksx001k8j5m52obbea3","_id":"clx17vksz001y8j5mgv9x3juf"},{"post_id":"clx17vksw000u8j5m9pda04rk","category_id":"clx17vksx001k8j5m52obbea3","_id":"clx17vksz00258j5mbz7a38oo"},{"post_id":"clx17vksw00128j5mfqp5atgp","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt0002a8j5m5zy474t4"},{"post_id":"clx17vksw00128j5mfqp5atgp","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt0002d8j5mdist1bex"},{"post_id":"clx17vksz00218j5me1gy329g","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt0002h8j5m8zh76n65"},{"post_id":"clx17vksz00218j5me1gy329g","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt0002l8j5mcl84e35l"},{"post_id":"clx17vksx00168j5m2r6n6sjw","category_id":"clx17vksz00248j5m6vgq9sd2","_id":"clx17vkt1002p8j5m5u02d7y5"},{"post_id":"clx17vksx001a8j5m4q7mbyoa","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt1002t8j5m9cqidhhj"},{"post_id":"clx17vksx001a8j5m4q7mbyoa","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt1002x8j5m8zs4f1s3"},{"post_id":"clx17vksv000j8j5m1iol1841","category_id":"clx17vksw00148j5mdoq701hy","_id":"clx17vkt1002z8j5md7utee7n"},{"post_id":"clx17vksv000j8j5m1iol1841","category_id":"clx17vkt0002g8j5m87ep6ffw","_id":"clx17vkt200338j5meq54g4yt"},{"post_id":"clx17vksx001e8j5m5q87b2ku","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt200358j5mc5k879af"},{"post_id":"clx17vksx001e8j5m5q87b2ku","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt200398j5m1likdoqz"},{"post_id":"clx17vksx001i8j5m1a9c4y0i","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt2003c8j5mb23a71pc"},{"post_id":"clx17vksx001i8j5m1a9c4y0i","category_id":"clx17vkt1002w8j5m1ax8g51t","_id":"clx17vkt2003g8j5m123z0c62"},{"post_id":"clx17vksx001m8j5m989a4kum","category_id":"clx17vksz00248j5m6vgq9sd2","_id":"clx17vkt2003k8j5mdmr6bomm"},{"post_id":"clx17vksy001o8j5m0cpn4c0k","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt3003q8j5m5u832azm"},{"post_id":"clx17vksy001o8j5m0cpn4c0k","category_id":"clx17vkt200388j5mffr7gi2d","_id":"clx17vkt3003s8j5m8p1c7ogc"},{"post_id":"clx17vksy001q8j5mcpeb51st","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt3003x8j5m9gwf12lz"},{"post_id":"clx17vksy001q8j5mcpeb51st","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt3003z8j5me1bjbf1f"},{"post_id":"clx17vksz001t8j5m3g309yc7","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt400458j5m59k2fuzd"},{"post_id":"clx17vksz001t8j5m3g309yc7","category_id":"clx17vkt3003o8j5mhnephtp2","_id":"clx17vkt400488j5mbx7z4yj7"},{"post_id":"clx17vksz001x8j5m299xeejj","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt4004b8j5mbfis9z4d"},{"post_id":"clx17vksz001x8j5m299xeejj","category_id":"clx17vkt3003v8j5m8g8i3vzs","_id":"clx17vkt4004e8j5m0bjvers3"},{"post_id":"clx17vksz00238j5mb7926llr","category_id":"clx17vkt400428j5m6rst8v6l","_id":"clx17vkt5004i8j5m9qtyapqm"},{"post_id":"clx17vkt000288j5m3yzz2rua","category_id":"clx17vkt400498j5mhlvffsbv","_id":"clx17vkt5004p8j5mb8vl1ik7"},{"post_id":"clx17vkt0002b8j5m7kqo4oj4","category_id":"clx17vkt400428j5m6rst8v6l","_id":"clx17vkt5004u8j5mfkwa4ysq"},{"post_id":"clx17vkt0002e8j5mhv92hnn3","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt600528j5m8cexgkev"},{"post_id":"clx17vkt0002e8j5mhv92hnn3","category_id":"clx17vkt5004o8j5m7u3u7sun","_id":"clx17vkt600558j5mc1068tkl"},{"post_id":"clx17vkt5004s8j5mbfzkci1d","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt600588j5mhao393ib"},{"post_id":"clx17vkt5004s8j5mbfzkci1d","category_id":"clx17vksz001w8j5m7m9650yh","_id":"clx17vkt6005a8j5m8xsh7va0"},{"post_id":"clx17vkt0002i8j5m614z84ny","category_id":"clx17vkt400498j5mhlvffsbv","_id":"clx17vkt6005d8j5m08qxhbx4"},{"post_id":"clx17vkt0002m8j5m2r65euoi","category_id":"clx17vkt6004z8j5mhuevd9jq","_id":"clx17vkt6005f8j5m96nt5s1s"},{"post_id":"clx17vkt1002q8j5m18xlb7le","category_id":"clx17vkt600568j5mbotiahqp","_id":"clx17vkt6005i8j5m4ov432un"},{"post_id":"clx17vkt1002u8j5mclucawt5","category_id":"clx17vkt600568j5mbotiahqp","_id":"clx17vkt6005l8j5mfhll07wd"},{"post_id":"clx17vkt1002y8j5m07w56q10","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt6005p8j5m30k24bjz"},{"post_id":"clx17vkt1002y8j5m07w56q10","category_id":"clx17vkt6005h8j5mcskq9h2a","_id":"clx17vkt6005s8j5m4az63cu9"},{"post_id":"clx17vkt100318j5m50y08otz","category_id":"clx17vkt6005k8j5m055qaraz","_id":"clx17vkt7005v8j5mc7n17ixz"},{"post_id":"clx17vkt200348j5m11qb1z2e","category_id":"clx17vkt6004z8j5mhuevd9jq","_id":"clx17vkt7005z8j5m04f90y0a"},{"post_id":"clx17vkt200368j5maxbeh87t","category_id":"clx17vkt600568j5mbotiahqp","_id":"clx17vkt700638j5ma0nlh7ci"},{"post_id":"clx17vkt2003h8j5maqsk8ncg","category_id":"clx17vkt600568j5mbotiahqp","_id":"clx17vkt7006c8j5m18ms9err"},{"post_id":"clx17vkt2003l8j5mdfxrevys","category_id":"clx17vkt700688j5mgk7fa44p","_id":"clx17vkt7006g8j5m8elu4sqy"},{"post_id":"clx17vkt3003n8j5m7qhbhgdj","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8006k8j5ma9n4a64m"},{"post_id":"clx17vkt3003n8j5m7qhbhgdj","category_id":"clx17vkt7006b8j5mah5221yo","_id":"clx17vkt8006m8j5mgje86mc4"},{"post_id":"clx17vkt3003r8j5mfp1v6n5a","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8006p8j5mblpf8fhm"},{"post_id":"clx17vkt3003r8j5mfp1v6n5a","category_id":"clx17vkt7006b8j5mah5221yo","_id":"clx17vkt8006q8j5m8muqf4ki"},{"post_id":"clx17vkt3003u8j5mgtsxcm0k","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8006t8j5m0w1h9x88"},{"post_id":"clx17vkt3003u8j5mgtsxcm0k","category_id":"clx17vkt7006b8j5mah5221yo","_id":"clx17vkt8006v8j5mhltp74er"},{"post_id":"clx17vkt3003y8j5mevlhe0il","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8006z8j5m3q2n6856"},{"post_id":"clx17vkt3003y8j5mevlhe0il","category_id":"clx17vkt7006b8j5mah5221yo","_id":"clx17vkt800708j5m71rf36to"},{"post_id":"clx17vkt300418j5m3o0k6ocs","category_id":"clx17vkt8006r8j5m5uvzem2o","_id":"clx17vkt800738j5mer9t4owo"},{"post_id":"clx17vkt400448j5mdrjzb912","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt800788j5mf82fdwh0"},{"post_id":"clx17vkt400448j5mdrjzb912","category_id":"clx17vkt8006w8j5m0d7gddsi","_id":"clx17vkt800798j5m4lpzh0eh"},{"post_id":"clx17vkt400478j5m8ul20f57","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8007c8j5mctt76p2u"},{"post_id":"clx17vkt400478j5m8ul20f57","category_id":"clx17vkt8006w8j5m0d7gddsi","_id":"clx17vkt8007d8j5m0x0rg0lm"},{"post_id":"clx17vkt4004a8j5mgay9ht0n","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt8007f8j5mfspgb7u7"},{"post_id":"clx17vkt4004a8j5mgay9ht0n","category_id":"clx17vkt8006w8j5m0d7gddsi","_id":"clx17vkt8007h8j5m4lln4ynl"},{"post_id":"clx17vkt4004c8j5m1wvidu30","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt9007j8j5mbn25filv"},{"post_id":"clx17vkt4004c8j5m1wvidu30","category_id":"clx17vkt8006w8j5m0d7gddsi","_id":"clx17vkt9007l8j5mflvucnxk"},{"post_id":"clx17vkt4004g8j5m2r4megu8","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt9007p8j5m0rbj7ban"},{"post_id":"clx17vkt4004g8j5m2r4megu8","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt9007s8j5m1q5afvlp"},{"post_id":"clx17vkt5004k8j5ma8n7axdy","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt9007v8j5m6552gwp4"},{"post_id":"clx17vkt5004k8j5ma8n7axdy","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt9007y8j5m46au58r8"},{"post_id":"clx17vkt5004n8j5mcnu7buzu","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt900818j5m552aagvx"},{"post_id":"clx17vkt5004n8j5mcnu7buzu","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt900838j5mey5kaea8"},{"post_id":"clx17vkt5004q8j5m4z9zfdy6","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt900858j5m4s6k8pm6"},{"post_id":"clx17vkt5004q8j5m4z9zfdy6","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt900878j5m025feorb"},{"post_id":"clx17vkt5004v8j5m252tgt9i","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt900898j5m4qbadha7"},{"post_id":"clx17vkt5004v8j5m252tgt9i","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt9008b8j5mhz2w5b5c"},{"post_id":"clx17vkt5004y8j5m7065cmv3","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkt9008e8j5md16qfudy"},{"post_id":"clx17vkt5004y8j5m7065cmv3","category_id":"clx17vkt8007e8j5mcvkp2c3m","_id":"clx17vkt9008h8j5m1x119wm3"},{"post_id":"clx17vkt600518j5mgxax6s91","category_id":"clx17vkst00078j5mah6bhjwp","_id":"clx17vkta008k8j5m3rbjatmm"},{"post_id":"clx17vkt600518j5mgxax6s91","category_id":"clx17vkt900888j5m64kvbr74","_id":"clx17vkta008m8j5meoi17n2j"},{"post_id":"clx17vkt2003b8j5m8o5khunh","category_id":"clx17vkt600568j5mbotiahqp","_id":"clx17vkta008o8j5m5ildhw4m"},{"post_id":"clx17vkt2003b8j5m8o5khunh","category_id":"clx17vkt9008c8j5mb1tqfmgd","_id":"clx17vkta008r8j5mhrrc5w1v"},{"post_id":"clx17vkt2003e8j5m4srz4x6k","category_id":"clx17vkt400498j5mhlvffsbv","_id":"clx17vkta008t8j5m7rr396l4"},{"post_id":"clx17vkt2003e8j5m4srz4x6k","category_id":"clx17vkt9008j8j5m7k8c7gwy","_id":"clx17vkta008v8j5mapkl5itg"},{"post_id":"clx17vktb009u8j5m2bj6c98t","category_id":"clx17vktb009x8j5m0rt7f06i","_id":"clx17vktc00a28j5m73px01vk"},{"post_id":"clx17vktb009v8j5mbwpaflso","category_id":"clx17vktb009x8j5m0rt7f06i","_id":"clx17vktc00a38j5m0b9g8mf5"}],"PostTag":[{"post_id":"clx17vksq00018j5m86cvdt7q","tag_id":"clx17vkss00048j5m6ojf9ar6","_id":"clx17vksu000d8j5mfiw2ab74"},{"post_id":"clx17vkss00038j5m2intd84w","tag_id":"clx17vksu000c8j5me2mj22pq","_id":"clx17vksv000k8j5mbmdf2c0p"},{"post_id":"clx17vksw000w8j5mfxa87l48","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vksw00118j5mgp87cvbr"},{"post_id":"clx17vkst00098j5md6o74ysg","tag_id":"clx17vksv000i8j5m7ecgf7g5","_id":"clx17vksx00158j5m68xablly"},{"post_id":"clx17vkst00098j5md6o74ysg","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vksx00198j5m1u6teaar"},{"post_id":"clx17vkst00098j5md6o74ysg","tag_id":"clx17vksw000t8j5mad7k4jfe","_id":"clx17vksx001d8j5m862tdw2r"},{"post_id":"clx17vksw000y8j5m2ys488ia","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vksx001h8j5m8e2t55ys"},{"post_id":"clx17vkst000b8j5mago0hgbs","tag_id":"clx17vksw00108j5m1r0e66y4","_id":"clx17vksx001l8j5meu9o9ov7"},{"post_id":"clx17vksu000f8j5mgr6xb4x5","tag_id":"clx17vksx00188j5m0q82ebul","_id":"clx17vksz001v8j5mdmfnc46e"},{"post_id":"clx17vksu000f8j5mgr6xb4x5","tag_id":"clx17vksx001g8j5mfkv3bg5j","_id":"clx17vksz001z8j5m1cuw6p9f"},{"post_id":"clx17vksu000f8j5mgr6xb4x5","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vksz00228j5m3451ckll"},{"post_id":"clx17vksu000h8j5mcag0ee87","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vkt0002f8j5mf9in09uh"},{"post_id":"clx17vksu000h8j5mcag0ee87","tag_id":"clx17vksz00208j5m7qqdhhyf","_id":"clx17vkt0002j8j5m8d4pehui"},{"post_id":"clx17vksu000h8j5mcag0ee87","tag_id":"clx17vkt000278j5m5qgk9xww","_id":"clx17vkt1002n8j5mfb7t18nq"},{"post_id":"clx17vksv000j8j5m1iol1841","tag_id":"clx17vkt0002c8j5mcxdm3fxc","_id":"clx17vkt1002r8j5m8gs5bb6p"},{"post_id":"clx17vksv000m8j5mflxf23ys","tag_id":"clx17vksv000n8j5maqas5po2","_id":"clx17vkt1002v8j5m24r13ypz"},{"post_id":"clx17vksv000o8j5m43et1f2f","tag_id":"clx17vkt1002s8j5mbxo74s8h","_id":"clx17vkt2003a8j5m96wn74cj"},{"post_id":"clx17vksv000o8j5m43et1f2f","tag_id":"clx17vkt100308j5md2yu9ddq","_id":"clx17vkt2003d8j5m0pz9cv2e"},{"post_id":"clx17vksw000r8j5mhd8od7jd","tag_id":"clx17vkt200378j5mdush9jn9","_id":"clx17vkt2003j8j5mff3u1sfb"},{"post_id":"clx17vksw000u8j5m9pda04rk","tag_id":"clx17vkt2003f8j5m71dgep24","_id":"clx17vkt3003p8j5mhcdhbhpd"},{"post_id":"clx17vksw00128j5mfqp5atgp","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt3003w8j5m2ky4b8nf"},{"post_id":"clx17vksx00168j5m2r6n6sjw","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt400438j5meefma3i4"},{"post_id":"clx17vksx001a8j5m4q7mbyoa","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt4004f8j5mh2mialc0"},{"post_id":"clx17vksx001a8j5m4q7mbyoa","tag_id":"clx17vkt400468j5m8oe0ezf6","_id":"clx17vkt5004j8j5m5fkefug0"},{"post_id":"clx17vksx001e8j5m5q87b2ku","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt5004m8j5mdzupe84s"},{"post_id":"clx17vkt5004s8j5mbfzkci1d","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt5004x8j5m3kc8gill"},{"post_id":"clx17vksx001i8j5m1a9c4y0i","tag_id":"clx17vkt5004l8j5ma7ut8n7r","_id":"clx17vkt600508j5m9aixfwkk"},{"post_id":"clx17vksx001i8j5m1a9c4y0i","tag_id":"clx17vkt5004r8j5m6mtza2b6","_id":"clx17vkt600548j5m8m9qh6o0"},{"post_id":"clx17vksx001m8j5m989a4kum","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt600578j5maqhk20u4"},{"post_id":"clx17vksy001o8j5m0cpn4c0k","tag_id":"clx17vkt600538j5m9y5z8wfs","_id":"clx17vkt6005b8j5mhghqamkd"},{"post_id":"clx17vksy001q8j5mcpeb51st","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt6005g8j5m45fwekf1"},{"post_id":"clx17vksz001t8j5m3g309yc7","tag_id":"clx17vkt6005e8j5mdqjghdby","_id":"clx17vkt6005n8j5m5drrezqg"},{"post_id":"clx17vksz001t8j5m3g309yc7","tag_id":"clx17vkt6005j8j5m9mlzegd4","_id":"clx17vkt6005q8j5mhco3aa7f"},{"post_id":"clx17vksz001x8j5m299xeejj","tag_id":"clx17vkt6005m8j5m0qed17nm","_id":"clx17vkt7005u8j5m94oucnee"},{"post_id":"clx17vksz00218j5me1gy329g","tag_id":"clx17vkt3003m8j5mbofogqgw","_id":"clx17vkt7005y8j5md0bxh3i4"},{"post_id":"clx17vksz00238j5mb7926llr","tag_id":"clx17vkt7005w8j5mhzkh1mec","_id":"clx17vkt700628j5maa20hxqg"},{"post_id":"clx17vkt000288j5m3yzz2rua","tag_id":"clx17vkt700608j5m5wmvhr9l","_id":"clx17vkt700668j5memnz4guh"},{"post_id":"clx17vkt0002b8j5m7kqo4oj4","tag_id":"clx17vkt7005w8j5mhzkh1mec","_id":"clx17vkt700698j5m2dh9es44"},{"post_id":"clx17vkt0002e8j5mhv92hnn3","tag_id":"clx17vkt700678j5m828iat1f","_id":"clx17vkt7006d8j5m7ki0b27w"},{"post_id":"clx17vkt0002i8j5m614z84ny","tag_id":"clx17vkt7006a8j5mdnyk4te4","_id":"clx17vkt7006h8j5mgvgpfpqu"},{"post_id":"clx17vkt0002m8j5m2r65euoi","tag_id":"clx17vkt7006e8j5m9ncl209b","_id":"clx17vkt8006l8j5m0qfmexrg"},{"post_id":"clx17vkt1002q8j5m18xlb7le","tag_id":"clx17vkt7006j8j5mfpyueud5","_id":"clx17vkt8006u8j5mco12a5a4"},{"post_id":"clx17vkt1002q8j5m18xlb7le","tag_id":"clx17vkt8006o8j5mej9i52h7","_id":"clx17vkt8006x8j5m7bfz40nh"},{"post_id":"clx17vkt1002u8j5mclucawt5","tag_id":"clx17vkt7006j8j5mfpyueud5","_id":"clx17vkt800748j5m89r62o3b"},{"post_id":"clx17vkt1002u8j5mclucawt5","tag_id":"clx17vkt700608j5m5wmvhr9l","_id":"clx17vkt800758j5m95lmbtfm"},{"post_id":"clx17vkt1002y8j5m07w56q10","tag_id":"clx17vkt800728j5m042thmoa","_id":"clx17vkt9007m8j5m3ivy9mft"},{"post_id":"clx17vkt1002y8j5m07w56q10","tag_id":"clx17vkt800778j5m4dp5a3ic","_id":"clx17vkt9007o8j5mdwp2el9y"},{"post_id":"clx17vkt1002y8j5m07w56q10","tag_id":"clx17vkt8007b8j5mhor81ulr","_id":"clx17vkt9007r8j5m41jqd8mc"},{"post_id":"clx17vkt1002y8j5m07w56q10","tag_id":"clx17vkt8007g8j5mb3ybe3ql","_id":"clx17vkt9007u8j5mc04r23nf"},{"post_id":"clx17vkt100318j5m50y08otz","tag_id":"clx17vkt9007k8j5m92lkh2gn","_id":"clx17vkt9007x8j5m1h1k1bqr"},{"post_id":"clx17vkt200348j5m11qb1z2e","tag_id":"clx17vkt7006e8j5m9ncl209b","_id":"clx17vkt900808j5m9ahu9pb3"},{"post_id":"clx17vkt200368j5maxbeh87t","tag_id":"clx17vkt7006j8j5mfpyueud5","_id":"clx17vkt9008d8j5m2n0fht8e"},{"post_id":"clx17vkt200368j5maxbeh87t","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vkt9008f8j5m4je72rh4"},{"post_id":"clx17vkt200368j5maxbeh87t","tag_id":"clx17vkt900868j5mamhz4hnl","_id":"clx17vkt9008i8j5mgwwsemkf"},{"post_id":"clx17vkt2003b8j5m8o5khunh","tag_id":"clx17vkt7006j8j5mfpyueud5","_id":"clx17vkta008n8j5m4iljcdwn"},{"post_id":"clx17vkt2003b8j5m8o5khunh","tag_id":"clx17vkt800728j5m042thmoa","_id":"clx17vkta008p8j5m1sde5yfb"},{"post_id":"clx17vkt2003e8j5m4srz4x6k","tag_id":"clx17vkta008l8j5mcmx2g2d6","_id":"clx17vkta008s8j5mermnhy6t"},{"post_id":"clx17vkt2003h8j5maqsk8ncg","tag_id":"clx17vkt7006j8j5mfpyueud5","_id":"clx17vkta008w8j5mc1le1o3z"},{"post_id":"clx17vkt2003l8j5mdfxrevys","tag_id":"clx17vkta008u8j5m0n3l354t","_id":"clx17vkta008y8j5mh2a16t6p"},{"post_id":"clx17vkt3003n8j5m7qhbhgdj","tag_id":"clx17vkt8006o8j5mej9i52h7","_id":"clx17vkta00908j5m59frd0ml"},{"post_id":"clx17vkt3003r8j5mfp1v6n5a","tag_id":"clx17vkt8006o8j5mej9i52h7","_id":"clx17vkta00928j5mhfgj3eh9"},{"post_id":"clx17vkt3003u8j5mgtsxcm0k","tag_id":"clx17vkt8006o8j5mej9i52h7","_id":"clx17vkta00948j5m07dbej64"},{"post_id":"clx17vkt3003y8j5mevlhe0il","tag_id":"clx17vkt8006o8j5mej9i52h7","_id":"clx17vkta00968j5mf5xtdph1"},{"post_id":"clx17vkt300418j5m3o0k6ocs","tag_id":"clx17vkta00958j5mak899u8w","_id":"clx17vkta00988j5mg1806xdj"},{"post_id":"clx17vkt400448j5mdrjzb912","tag_id":"clx17vkta00978j5m4zt257pa","_id":"clx17vkta009a8j5mawpdhpza"},{"post_id":"clx17vkt400478j5m8ul20f57","tag_id":"clx17vkta00978j5m4zt257pa","_id":"clx17vkta009c8j5m5pos4urg"},{"post_id":"clx17vkt4004a8j5mgay9ht0n","tag_id":"clx17vkta00978j5m4zt257pa","_id":"clx17vkta009e8j5m0wjv0l5c"},{"post_id":"clx17vkt4004c8j5m1wvidu30","tag_id":"clx17vkta00978j5m4zt257pa","_id":"clx17vkta009g8j5m7wnpf6lv"},{"post_id":"clx17vkt4004g8j5m2r4megu8","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vkta009i8j5m10cpa2ks"},{"post_id":"clx17vkt5004k8j5ma8n7axdy","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vkta009k8j5m5eyv6p0r"},{"post_id":"clx17vkt5004n8j5mcnu7buzu","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vktb009m8j5mh4nrcbl4"},{"post_id":"clx17vkt5004q8j5m4z9zfdy6","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vktb009o8j5m4xhx17wk"},{"post_id":"clx17vkt5004v8j5m252tgt9i","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vktb009q8j5mg58m0hxq"},{"post_id":"clx17vkt5004y8j5m7065cmv3","tag_id":"clx17vkt900828j5m7ui1dhot","_id":"clx17vktb009s8j5m9612ghdu"},{"post_id":"clx17vkt600518j5mgxax6s91","tag_id":"clx17vktb009r8j5m97u6dv5w","_id":"clx17vktb009t8j5mgtyc0ci0"},{"post_id":"clx17vktb009u8j5m2bj6c98t","tag_id":"clx17vktb009w8j5mgxzibml3","_id":"clx17vktc00a08j5m8u6g4cic"},{"post_id":"clx17vktb009v8j5mbwpaflso","tag_id":"clx17vktb009w8j5mgxzibml3","_id":"clx17vktc00a18j5m7xct1l7d"}],"Tag":[{"name":"hudi","_id":"clx17vkss00048j5m6ojf9ar6"},{"name":"EMO语录","_id":"clx17vksu000c8j5me2mj22pq"},{"name":"CDN","_id":"clx17vksv000i8j5m7ecgf7g5"},{"name":"hexo","_id":"clx17vksv000n8j5maqas5po2"},{"name":"cloudflare","_id":"clx17vksw000t8j5mad7k4jfe"},{"name":"SEO","_id":"clx17vksw00108j5m1r0e66y4"},{"name":"WSL","_id":"clx17vksx00188j5m0q82ebul"},{"name":"Linxu","_id":"clx17vksx001g8j5mfkv3bg5j"},{"name":"github pages","_id":"clx17vksz00208j5m7qqdhhyf"},{"name":"CFW","_id":"clx17vkt000278j5m5qgk9xww"},{"name":"dns","_id":"clx17vkt0002c8j5mcxdm3fxc"},{"name":"BugFix","_id":"clx17vkt1002s8j5mbxo74s8h"},{"name":"PR","_id":"clx17vkt100308j5md2yu9ddq"},{"name":"DAG","_id":"clx17vkt200378j5mdush9jn9"},{"name":"LSM-Tree","_id":"clx17vkt2003f8j5m71dgep24"},{"name":"Doris","_id":"clx17vkt3003m8j5mbofogqgw"},{"name":"数据中台","_id":"clx17vkt400468j5m8oe0ezf6"},{"name":"HBase","_id":"clx17vkt5004l8j5ma7ut8n7r"},{"name":"MVCC","_id":"clx17vkt5004r8j5m6mtza2b6"},{"name":"Kyuubi","_id":"clx17vkt600538j5m9y5z8wfs"},{"name":"yarn","_id":"clx17vkt6005e8j5mdqjghdby"},{"name":"hadoop","_id":"clx17vkt6005j8j5m9mlzegd4"},{"name":"用户画像","_id":"clx17vkt6005m8j5m0qed17nm"},{"name":"os","_id":"clx17vkt7005w8j5mhzkh1mec"},{"name":"dolphinscheduler","_id":"clx17vkt700608j5m5wmvhr9l"},{"name":"kudu","_id":"clx17vkt700678j5m828iat1f"},{"name":"RPC","_id":"clx17vkt7006a8j5mdnyk4te4"},{"name":"Netty","_id":"clx17vkt7006e8j5m9ncl209b"},{"name":"Troubleshooting","_id":"clx17vkt7006j8j5mfpyueud5"},{"name":"paimon","_id":"clx17vkt8006o8j5mej9i52h7"},{"name":"Kylin","_id":"clx17vkt800728j5m042thmoa"},{"name":"JVM","_id":"clx17vkt800778j5m4dp5a3ic"},{"name":"MAT","_id":"clx17vkt8007b8j5mhor81ulr"},{"name":"Arthas","_id":"clx17vkt8007g8j5mb3ybe3ql"},{"name":"Threads","_id":"clx17vkt9007k8j5m92lkh2gn"},{"name":"Flink","_id":"clx17vkt900828j5m7ui1dhot"},{"name":"Hudi","_id":"clx17vkt900868j5mamhz4hnl"},{"name":"thrift","_id":"clx17vkta008l8j5mcmx2g2d6"},{"name":"LSM","_id":"clx17vkta008u8j5m0n3l354t"},{"name":"Druid","_id":"clx17vkta00958j5mak899u8w"},{"name":"spark","_id":"clx17vkta00978j5m4zt257pa"},{"name":"scala","_id":"clx17vktb009r8j5m97u6dv5w"},{"name":"Apache Superset","_id":"clx17vktb009w8j5mgxzibml3"}]}}